{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/nipun/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "import keras\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataloader import APPLIANCE_ORDER, get_train_test\n",
    "\n",
    "\n",
    "num_folds = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "appliance = \"hvac\"\n",
    "\n",
    "\n",
    "appliance_num = APPLIANCE_ORDER.index(appliance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nipun/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1574: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_3\" was not an Input tensor, it was generated by layer batch_normalization_4.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: batch_normalization_4/cond/Merge:0\n",
      "  str(x.name))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input layers to a `Model` must be `InputLayer` objects. Received inputs: Tensor(\"batch_normalization_4/cond/Merge:0\", shape=(?, 25), dtype=float32). Input 0 (0-based) originates from layer type `BatchNormalization`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d08c540b2da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1632\u001b[0m                     'from layer type `{}`.'.format(inputs,\n\u001b[1;32m   1633\u001b[0m                                                    \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m                                                    layer.__class__.__name__))\n\u001b[0m\u001b[1;32m   1635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input layers to a `Model` must be `InputLayer` objects. Received inputs: Tensor(\"batch_normalization_4/cond/Merge:0\", shape=(?, 25), dtype=float32). Input 0 (0-based) originates from layer type `BatchNormalization`."
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "from keras.layers.merge import Subtract, Minimum\n",
    "from keras import regularizers\n",
    "agg_input = keras.layers.Input(shape=[25],name='Aggregate')\n",
    "agg_input = keras.layers.BatchNormalization()(agg_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "appliance_dense_1 = keras.layers.Dense(units=60,name='Appliance-layer-1',activation='relu')(agg_input)\n",
    "appliance_bn = keras.layers.BatchNormalization()(appliance_dense_1)\n",
    "dropout = keras.layers.Dropout(rate=0.1,name='Droput-Appliance')(appliance_bn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = keras.layers.Dense(units=1,name='Appliance-output',activation='relu')(dropout)\n",
    "#out = Minimum(name='Clip-to-agg')([out, agg_input])\n",
    "\n",
    "\n",
    "model = keras.Model(agg_input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"377pt\" viewBox=\"0.00 0.00 406.29 377.00\" width=\"406pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 373)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-373 402.294,-373 402.294,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 112177300760 -->\n",
       "<g class=\"node\" id=\"node1\"><title>112177300760</title>\n",
       "<polygon fill=\"none\" points=\"61.0449,-324.5 61.0449,-368.5 337.249,-368.5 337.249,-324.5 61.0449,-324.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.988\" y=\"-342.3\">Aggregate: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"204.932,-324.5 204.932,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.766\" y=\"-353.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"204.932,-346.5 260.601,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.766\" y=\"-331.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"260.601,-324.5 260.601,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298.925\" y=\"-353.3\">(None, 25)</text>\n",
       "<polyline fill=\"none\" points=\"260.601,-346.5 337.249,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298.925\" y=\"-331.3\">(None, 25)</text>\n",
       "</g>\n",
       "<!-- 112177300536 -->\n",
       "<g class=\"node\" id=\"node2\"><title>112177300536</title>\n",
       "<polygon fill=\"none\" points=\"53.0195,-243.5 53.0195,-287.5 345.274,-287.5 345.274,-243.5 53.0195,-243.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.988\" y=\"-261.3\">Appliance-layer-1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"212.957,-243.5 212.957,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240.792\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"212.957,-265.5 268.626,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240.792\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"268.626,-243.5 268.626,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"306.95\" y=\"-272.3\">(None, 25)</text>\n",
       "<polyline fill=\"none\" points=\"268.626,-265.5 345.274,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"306.95\" y=\"-250.3\">(None, 60)</text>\n",
       "</g>\n",
       "<!-- 112177300760&#45;&gt;112177300536 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>112177300760-&gt;112177300536</title>\n",
       "<path d=\"M199.147,-324.329C199.147,-316.183 199.147,-306.699 199.147,-297.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"202.647,-297.729 199.147,-287.729 195.647,-297.729 202.647,-297.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 112180405024 -->\n",
       "<g class=\"node\" id=\"node3\"><title>112180405024</title>\n",
       "<polygon fill=\"none\" points=\"0,-162.5 0,-206.5 398.294,-206.5 398.294,-162.5 0,-162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.988\" y=\"-180.3\">batch_normalization_3: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"265.977,-162.5 265.977,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293.811\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"265.977,-184.5 321.646,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293.811\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"321.646,-162.5 321.646,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"359.97\" y=\"-191.3\">(None, 60)</text>\n",
       "<polyline fill=\"none\" points=\"321.646,-184.5 398.294,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"359.97\" y=\"-169.3\">(None, 60)</text>\n",
       "</g>\n",
       "<!-- 112177300536&#45;&gt;112180405024 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>112177300536-&gt;112180405024</title>\n",
       "<path d=\"M199.147,-243.329C199.147,-235.183 199.147,-225.699 199.147,-216.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"202.647,-216.729 199.147,-206.729 195.647,-216.729 202.647,-216.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 112180344648 -->\n",
       "<g class=\"node\" id=\"node4\"><title>112180344648</title>\n",
       "<polygon fill=\"none\" points=\"47.0347,-81.5 47.0347,-125.5 351.259,-125.5 351.259,-81.5 47.0347,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.988\" y=\"-99.3\">Droput-Appliance: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"218.942,-81.5 218.942,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.776\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"218.942,-103.5 274.611,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.776\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"274.611,-81.5 274.611,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"312.935\" y=\"-110.3\">(None, 60)</text>\n",
       "<polyline fill=\"none\" points=\"274.611,-103.5 351.259,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"312.935\" y=\"-88.3\">(None, 60)</text>\n",
       "</g>\n",
       "<!-- 112180405024&#45;&gt;112180344648 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>112180405024-&gt;112180344648</title>\n",
       "<path d=\"M199.147,-162.329C199.147,-154.183 199.147,-144.699 199.147,-135.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"202.647,-135.729 199.147,-125.729 195.647,-135.729 202.647,-135.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 112176533968 -->\n",
       "<g class=\"node\" id=\"node5\"><title>112176533968</title>\n",
       "<polygon fill=\"none\" points=\"54.814,-0.5 54.814,-44.5 343.48,-44.5 343.48,-0.5 54.814,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.988\" y=\"-18.3\">Appliance-output: Dense</text>\n",
       "<polyline fill=\"none\" points=\"211.163,-0.5 211.163,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.997\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"211.163,-22.5 266.832,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.997\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"266.832,-0.5 266.832,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.156\" y=\"-29.3\">(None, 60)</text>\n",
       "<polyline fill=\"none\" points=\"266.832,-22.5 343.48,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.156\" y=\"-7.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 112180344648&#45;&gt;112176533968 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>112180344648-&gt;112176533968</title>\n",
       "<path d=\"M199.147,-81.3294C199.147,-73.1826 199.147,-63.6991 199.147,-54.7971\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"202.647,-54.729 199.147,-44.729 195.647,-54.729 202.647,-54.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam','mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh = OneHotEncoder()\n",
    "d=pd.DataFrame(oh.fit_transform(np.array(range(1, 25)).reshape(-1, 1)).toarray())\n",
    "\n",
    "fold_num = 0\n",
    "appliance_num = APPLIANCE_ORDER.index(appliance)\n",
    "train, test = get_train_test(1, num_folds=num_folds, fold_num=fold_num)\n",
    "train_aggregate = train[:, 0, :, :].reshape(-1, 24, 1)\n",
    "test_aggregate = test[:, 0, :, :].reshape(-1, 24, 1)\n",
    "\n",
    "train_appliance = train[:, appliance_num, :, :].reshape(-1,1)\n",
    "test_appliance = test[:, appliance_num, :, :].reshape(-1, 1)\n",
    "\n",
    "train_aggregate_time = np.zeros((train_aggregate.shape[0], 24, 25))\n",
    "test_aggregate_time = np.zeros((test_aggregate.shape[0], 24, 25))\n",
    "\n",
    "\n",
    "for home in range(train_aggregate.shape[0]):\n",
    "    temp = d.copy()\n",
    "    temp['power'] = train_aggregate[home, :, :]\n",
    "    train_aggregate_time[home, :, :] = temp.values\n",
    "\n",
    "for home in range(test_aggregate.shape[0]):\n",
    "    temp = d.copy()\n",
    "    temp['power'] = test_aggregate[home, :, :]\n",
    "    test_aggregate_time[home, :, :] = temp.values\n",
    "    \n",
    "tr_agg = train_aggregate_time.reshape(-1, 25)\n",
    "te_agg = test_aggregate_time.reshape(-1, 25)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 130636 samples, validate on 14516 samples\n",
      "Epoch 1/20\n",
      "130636/130636 [==============================] - 5s 42us/step - loss: 213.8691 - val_loss: 283.7583\n",
      "Epoch 2/20\n",
      "130636/130636 [==============================] - 5s 41us/step - loss: 218.7812 - val_loss: 206.0123\n",
      "Epoch 3/20\n",
      "130636/130636 [==============================] - 5s 42us/step - loss: 217.5342 - val_loss: 160.1241\n",
      "Epoch 4/20\n",
      "130636/130636 [==============================] - 6s 46us/step - loss: 216.0054 - val_loss: 128.8608\n",
      "Epoch 5/20\n",
      "130636/130636 [==============================] - 5s 42us/step - loss: 214.0833 - val_loss: 152.8342\n",
      "Epoch 6/20\n",
      "130636/130636 [==============================] - 6s 48us/step - loss: 218.9402 - val_loss: 167.0258\n",
      "Epoch 7/20\n",
      "130636/130636 [==============================] - 7s 50us/step - loss: 221.8388 - val_loss: 132.2530\n",
      "Epoch 8/20\n",
      "130636/130636 [==============================] - 5s 40us/step - loss: 222.2079 - val_loss: 139.3665\n",
      "Epoch 9/20\n",
      "130636/130636 [==============================] - 5s 40us/step - loss: 221.0037 - val_loss: 165.2608\n",
      "Epoch 10/20\n",
      "130636/130636 [==============================] - 5s 39us/step - loss: 222.1720 - val_loss: 265.6752\n",
      "Epoch 11/20\n",
      "130636/130636 [==============================] - 5s 40us/step - loss: 221.3780 - val_loss: 123.6044\n",
      "Epoch 12/20\n",
      "130636/130636 [==============================] - 5s 39us/step - loss: 219.5617 - val_loss: 150.0730\n",
      "Epoch 13/20\n",
      "130636/130636 [==============================] - 5s 40us/step - loss: 219.7022 - val_loss: 108.8356\n",
      "Epoch 14/20\n",
      "130636/130636 [==============================] - 6s 45us/step - loss: 221.7781 - val_loss: 128.8367\n",
      "Epoch 15/20\n",
      "130636/130636 [==============================] - 6s 43us/step - loss: 218.5918 - val_loss: 114.0322\n",
      "Epoch 16/20\n",
      "130636/130636 [==============================] - 6s 45us/step - loss: 224.1938 - val_loss: 132.8711\n",
      "Epoch 17/20\n",
      "130636/130636 [==============================] - 8s 64us/step - loss: 221.1851 - val_loss: 154.0409\n",
      "Epoch 18/20\n",
      "130636/130636 [==============================] - 8s 57us/step - loss: 226.9725 - val_loss: 174.8765\n",
      "Epoch 19/20\n",
      "130636/130636 [==============================] - 6s 49us/step - loss: 222.8398 - val_loss: 176.4492\n",
      "Epoch 20/20\n",
      "130636/130636 [==============================] - 6s 48us/step - loss: 222.7407 - val_loss: 171.6682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1145af8d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tr_agg ,train_appliance, epochs=20, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(te_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 314.36666413,  308.79999951,  284.51667328, ...,  224.04999733,\n",
       "        595.64997101,  474.61667633])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_agg[:, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.36250374329443"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(pred, test_appliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2c66c588>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQCmlfNj/Bwi5E9DXnw0HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109d56c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(pred[:24]).plot()\n",
    "pd.DataFrame(test_appliance[:24]).plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_absolute_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-5806ef50fb1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_absolute_error' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_absolute_error(np.concatenate(preds), np.concatenate(gts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---FOLD 0 ----\n",
      "Train on 5443 samples, validate on 605 samples\n",
      "Epoch 1/30\n",
      "5443/5443 [==============================] - 1s 194us/step - loss: 73.1283 - val_loss: 52.3770\n",
      "Epoch 2/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 73.0440 - val_loss: 52.3354\n",
      "Epoch 3/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 73.0198 - val_loss: 52.3115\n",
      "Epoch 4/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 73.0042 - val_loss: 52.3047\n",
      "Epoch 5/30\n",
      "5443/5443 [==============================] - 0s 62us/step - loss: 72.9862 - val_loss: 52.2962\n",
      "Epoch 6/30\n",
      "5443/5443 [==============================] - 0s 51us/step - loss: 72.9709 - val_loss: 52.2949\n",
      "Epoch 7/30\n",
      "5443/5443 [==============================] - 0s 49us/step - loss: 72.9521 - val_loss: 52.2987\n",
      "Epoch 8/30\n",
      "5443/5443 [==============================] - 0s 57us/step - loss: 72.9363 - val_loss: 52.2762\n",
      "Epoch 9/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 72.9188 - val_loss: 52.2716\n",
      "Epoch 10/30\n",
      "5443/5443 [==============================] - 0s 50us/step - loss: 72.9002 - val_loss: 52.2471\n",
      "Epoch 11/30\n",
      "5443/5443 [==============================] - 0s 50us/step - loss: 72.8821 - val_loss: 52.2551\n",
      "Epoch 12/30\n",
      "5443/5443 [==============================] - 0s 53us/step - loss: 72.8482 - val_loss: 52.2202\n",
      "Epoch 13/30\n",
      "5443/5443 [==============================] - 0s 61us/step - loss: 72.8262 - val_loss: 52.2148\n",
      "Epoch 14/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 72.7885 - val_loss: 52.1976\n",
      "Epoch 15/30\n",
      "5443/5443 [==============================] - 0s 54us/step - loss: 72.7717 - val_loss: 52.1847\n",
      "Epoch 16/30\n",
      "5443/5443 [==============================] - 0s 55us/step - loss: 72.7424 - val_loss: 52.1513\n",
      "Epoch 17/30\n",
      "5443/5443 [==============================] - 0s 56us/step - loss: 72.7003 - val_loss: 52.1251\n",
      "Epoch 18/30\n",
      "5443/5443 [==============================] - 0s 66us/step - loss: 72.6801 - val_loss: 52.1156\n",
      "Epoch 19/30\n",
      "5443/5443 [==============================] - 0s 59us/step - loss: 72.6496 - val_loss: 52.0707\n",
      "Epoch 20/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 72.5883 - val_loss: 52.0444\n",
      "Epoch 21/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 72.5692 - val_loss: 52.0022\n",
      "Epoch 22/30\n",
      "5443/5443 [==============================] - 0s 51us/step - loss: 72.4895 - val_loss: 51.9854\n",
      "Epoch 23/30\n",
      "5443/5443 [==============================] - 0s 45us/step - loss: 72.4670 - val_loss: 51.9561\n",
      "Epoch 24/30\n",
      "5443/5443 [==============================] - 0s 44us/step - loss: 72.3993 - val_loss: 51.9472\n",
      "Epoch 25/30\n",
      "5443/5443 [==============================] - 0s 49us/step - loss: 72.3027 - val_loss: 51.9282\n",
      "Epoch 26/30\n",
      "5443/5443 [==============================] - 0s 65us/step - loss: 72.2767 - val_loss: 51.9115\n",
      "Epoch 27/30\n",
      "5443/5443 [==============================] - 0s 60us/step - loss: 72.2504 - val_loss: 51.8505\n",
      "Epoch 28/30\n",
      "5443/5443 [==============================] - 0s 46us/step - loss: 72.1295 - val_loss: 51.8217\n",
      "Epoch 29/30\n",
      "5443/5443 [==============================] - 0s 49us/step - loss: 72.0711 - val_loss: 51.7891\n",
      "Epoch 30/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 71.9764 - val_loss: 51.7717\n",
      "---FOLD 1 ----\n",
      "Train on 5443 samples, validate on 605 samples\n",
      "Epoch 1/30\n",
      "5443/5443 [==============================] - 1s 201us/step - loss: 69.1680 - val_loss: 52.4113\n",
      "Epoch 2/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 69.0864 - val_loss: 52.3691\n",
      "Epoch 3/30\n",
      "5443/5443 [==============================] - 0s 46us/step - loss: 69.0534 - val_loss: 52.3400\n",
      "Epoch 4/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 69.0265 - val_loss: 52.3269\n",
      "Epoch 5/30\n",
      "5443/5443 [==============================] - 0s 49us/step - loss: 69.0012 - val_loss: 52.3209\n",
      "Epoch 6/30\n",
      "5443/5443 [==============================] - 0s 52us/step - loss: 68.9811 - val_loss: 52.3276\n",
      "Epoch 7/30\n",
      "5443/5443 [==============================] - 0s 55us/step - loss: 68.9597 - val_loss: 52.3508\n",
      "Epoch 8/30\n",
      "5443/5443 [==============================] - 0s 52us/step - loss: 68.9375 - val_loss: 52.3240\n",
      "Epoch 9/30\n",
      "5443/5443 [==============================] - 0s 49us/step - loss: 68.9171 - val_loss: 52.3370\n",
      "Epoch 10/30\n",
      "5443/5443 [==============================] - 0s 50us/step - loss: 68.8993 - val_loss: 52.2736\n",
      "Epoch 11/30\n",
      "5443/5443 [==============================] - 0s 50us/step - loss: 68.8704 - val_loss: 52.3008\n",
      "Epoch 12/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 68.8432 - val_loss: 52.2676\n",
      "Epoch 13/30\n",
      "5443/5443 [==============================] - 0s 49us/step - loss: 68.8142 - val_loss: 52.2971\n",
      "Epoch 14/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 68.7901 - val_loss: 52.2582\n",
      "Epoch 15/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 68.7543 - val_loss: 52.2453\n",
      "Epoch 16/30\n",
      "5443/5443 [==============================] - 0s 49us/step - loss: 68.7291 - val_loss: 52.1885\n",
      "Epoch 17/30\n",
      "5443/5443 [==============================] - 0s 58us/step - loss: 68.6874 - val_loss: 52.1468\n",
      "Epoch 18/30\n",
      "5443/5443 [==============================] - 0s 56us/step - loss: 68.6533 - val_loss: 52.1524\n",
      "Epoch 19/30\n",
      "5443/5443 [==============================] - 0s 46us/step - loss: 68.6126 - val_loss: 52.0919\n",
      "Epoch 20/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 68.5594 - val_loss: 52.1069\n",
      "Epoch 21/30\n",
      "5443/5443 [==============================] - 0s 45us/step - loss: 68.5454 - val_loss: 52.0452\n",
      "Epoch 22/30\n",
      "5443/5443 [==============================] - 0s 54us/step - loss: 68.4676 - val_loss: 52.0345\n",
      "Epoch 23/30\n",
      "5443/5443 [==============================] - 0s 60us/step - loss: 68.4371 - val_loss: 52.0167\n",
      "Epoch 24/30\n",
      "5443/5443 [==============================] - 0s 61us/step - loss: 68.3523 - val_loss: 51.9490\n",
      "Epoch 25/30\n",
      "5443/5443 [==============================] - 0s 60us/step - loss: 68.3029 - val_loss: 51.9480\n",
      "Epoch 26/30\n",
      "5443/5443 [==============================] - 0s 59us/step - loss: 68.2451 - val_loss: 51.8873\n",
      "Epoch 27/30\n",
      "5443/5443 [==============================] - 0s 45us/step - loss: 68.1731 - val_loss: 51.8561\n",
      "Epoch 28/30\n",
      "5443/5443 [==============================] - 0s 53us/step - loss: 68.1245 - val_loss: 51.8477\n",
      "Epoch 29/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 68.0390 - val_loss: 51.7521\n",
      "Epoch 30/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 67.9402 - val_loss: 51.7220\n",
      "---FOLD 2 ----\n",
      "Train on 5443 samples, validate on 605 samples\n",
      "Epoch 1/30\n",
      "5443/5443 [==============================] - 1s 211us/step - loss: 67.4340 - val_loss: 52.3954\n",
      "Epoch 2/30\n",
      "5443/5443 [==============================] - 0s 55us/step - loss: 67.3513 - val_loss: 52.3502\n",
      "Epoch 3/30\n",
      "5443/5443 [==============================] - 0s 54us/step - loss: 67.3234 - val_loss: 52.3500\n",
      "Epoch 4/30\n",
      "5443/5443 [==============================] - 0s 61us/step - loss: 67.3020 - val_loss: 52.3353\n",
      "Epoch 5/30\n",
      "5443/5443 [==============================] - 0s 52us/step - loss: 67.2823 - val_loss: 52.3234\n",
      "Epoch 6/30\n",
      "5443/5443 [==============================] - 0s 59us/step - loss: 67.2680 - val_loss: 52.3346\n",
      "Epoch 7/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 67.2493 - val_loss: 52.3240\n",
      "Epoch 8/30\n",
      "5443/5443 [==============================] - 0s 49us/step - loss: 67.2288 - val_loss: 52.3178\n",
      "Epoch 9/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 67.2123 - val_loss: 52.3161\n",
      "Epoch 10/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 67.1939 - val_loss: 52.2727\n",
      "Epoch 11/30\n",
      "5443/5443 [==============================] - 0s 48us/step - loss: 67.1717 - val_loss: 52.2846\n",
      "Epoch 12/30\n",
      "5443/5443 [==============================] - 0s 49us/step - loss: 67.1445 - val_loss: 52.2410\n",
      "Epoch 13/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 67.1121 - val_loss: 52.2480\n",
      "Epoch 14/30\n",
      "5443/5443 [==============================] - 0s 46us/step - loss: 67.0797 - val_loss: 52.2119\n",
      "Epoch 15/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 67.0464 - val_loss: 52.2052\n",
      "Epoch 16/30\n",
      "5443/5443 [==============================] - 0s 51us/step - loss: 67.0091 - val_loss: 52.1723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 66.9639 - val_loss: 52.1329\n",
      "Epoch 18/30\n",
      "5443/5443 [==============================] - 0s 51us/step - loss: 66.9346 - val_loss: 52.1203\n",
      "Epoch 19/30\n",
      "5443/5443 [==============================] - 0s 64us/step - loss: 66.8919 - val_loss: 52.0977\n",
      "Epoch 20/30\n",
      "5443/5443 [==============================] - 0s 53us/step - loss: 66.8421 - val_loss: 52.0546\n",
      "Epoch 21/30\n",
      "5443/5443 [==============================] - 0s 46us/step - loss: 66.8011 - val_loss: 52.0091\n",
      "Epoch 22/30\n",
      "5443/5443 [==============================] - 0s 44us/step - loss: 66.7173 - val_loss: 51.9876\n",
      "Epoch 23/30\n",
      "5443/5443 [==============================] - 0s 47us/step - loss: 66.6629 - val_loss: 51.9450\n",
      "Epoch 24/30\n",
      "5443/5443 [==============================] - 0s 58us/step - loss: 66.6072 - val_loss: 51.9158\n",
      "Epoch 25/30\n",
      "5443/5443 [==============================] - 0s 54us/step - loss: 66.5553 - val_loss: 51.8828\n",
      "Epoch 26/30\n",
      "5443/5443 [==============================] - 0s 57us/step - loss: 66.4760 - val_loss: 51.8226\n",
      "Epoch 27/30\n",
      "5443/5443 [==============================] - 0s 46us/step - loss: 66.3807 - val_loss: 51.7374\n",
      "Epoch 28/30\n",
      "5443/5443 [==============================] - 0s 62us/step - loss: 66.2971 - val_loss: 51.7077\n",
      "Epoch 29/30\n",
      "5443/5443 [==============================] - 0s 67us/step - loss: 66.2065 - val_loss: 51.6826\n",
      "Epoch 30/30\n",
      "5443/5443 [==============================] - 0s 73us/step - loss: 66.1320 - val_loss: 51.6025\n",
      "---FOLD 3 ----\n",
      "Train on 5544 samples, validate on 616 samples\n",
      "Epoch 1/30\n",
      "5544/5544 [==============================] - 1s 234us/step - loss: 64.4368 - val_loss: 51.9353\n",
      "Epoch 2/30\n",
      "5544/5544 [==============================] - 0s 53us/step - loss: 64.3448 - val_loss: 51.8988\n",
      "Epoch 3/30\n",
      "5544/5544 [==============================] - 0s 48us/step - loss: 64.3234 - val_loss: 51.8865\n",
      "Epoch 4/30\n",
      "5544/5544 [==============================] - 0s 52us/step - loss: 64.3046 - val_loss: 51.8754\n",
      "Epoch 5/30\n",
      "5544/5544 [==============================] - 0s 52us/step - loss: 64.2864 - val_loss: 51.8752\n",
      "Epoch 6/30\n",
      "5544/5544 [==============================] - 0s 61us/step - loss: 64.2684 - val_loss: 51.8654\n",
      "Epoch 7/30\n",
      "5544/5544 [==============================] - 0s 57us/step - loss: 64.2523 - val_loss: 51.8554\n",
      "Epoch 8/30\n",
      "5544/5544 [==============================] - 0s 57us/step - loss: 64.2314 - val_loss: 51.8545\n",
      "Epoch 9/30\n",
      "5544/5544 [==============================] - 0s 53us/step - loss: 64.2179 - val_loss: 51.8417\n",
      "Epoch 10/30\n",
      "5544/5544 [==============================] - 0s 48us/step - loss: 64.2023 - val_loss: 51.8397\n",
      "Epoch 11/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 64.1830 - val_loss: 51.8167\n",
      "Epoch 12/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 64.1655 - val_loss: 51.8279\n",
      "Epoch 13/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 64.1436 - val_loss: 51.8089\n",
      "Epoch 14/30\n",
      "5544/5544 [==============================] - 0s 48us/step - loss: 64.1235 - val_loss: 51.7796\n",
      "Epoch 15/30\n",
      "5544/5544 [==============================] - 0s 52us/step - loss: 64.0941 - val_loss: 51.7549\n",
      "Epoch 16/30\n",
      "5544/5544 [==============================] - 0s 50us/step - loss: 64.0717 - val_loss: 51.7433\n",
      "Epoch 17/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 64.0391 - val_loss: 51.7185\n",
      "Epoch 18/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 64.0029 - val_loss: 51.6941\n",
      "Epoch 19/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.9701 - val_loss: 51.6645\n",
      "Epoch 20/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.9168 - val_loss: 51.6397\n",
      "Epoch 21/30\n",
      "5544/5544 [==============================] - 0s 49us/step - loss: 63.8987 - val_loss: 51.6381\n",
      "Epoch 22/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.8656 - val_loss: 51.5858\n",
      "Epoch 23/30\n",
      "5544/5544 [==============================] - 0s 48us/step - loss: 63.8093 - val_loss: 51.5732\n",
      "Epoch 24/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.7640 - val_loss: 51.5123\n",
      "Epoch 25/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.7192 - val_loss: 51.4599\n",
      "Epoch 26/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.6571 - val_loss: 51.4640\n",
      "Epoch 27/30\n",
      "5544/5544 [==============================] - 0s 50us/step - loss: 63.6279 - val_loss: 51.4108\n",
      "Epoch 28/30\n",
      "5544/5544 [==============================] - 0s 50us/step - loss: 63.5891 - val_loss: 51.3577\n",
      "Epoch 29/30\n",
      "5544/5544 [==============================] - 0s 48us/step - loss: 63.4989 - val_loss: 51.3090\n",
      "Epoch 30/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.4484 - val_loss: 51.2891\n",
      "---FOLD 4 ----\n",
      "Train on 5544 samples, validate on 616 samples\n",
      "Epoch 1/30\n",
      "5544/5544 [==============================] - 1s 244us/step - loss: 63.9047 - val_loss: 98.3582\n",
      "Epoch 2/30\n",
      "5544/5544 [==============================] - 0s 52us/step - loss: 63.8247 - val_loss: 98.3720\n",
      "Epoch 3/30\n",
      "5544/5544 [==============================] - 0s 53us/step - loss: 63.8025 - val_loss: 98.3719\n",
      "Epoch 4/30\n",
      "5544/5544 [==============================] - 0s 49us/step - loss: 63.7820 - val_loss: 98.3737\n",
      "Epoch 5/30\n",
      "5544/5544 [==============================] - 0s 50us/step - loss: 63.7626 - val_loss: 98.3267\n",
      "Epoch 6/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.7449 - val_loss: 98.3494\n",
      "Epoch 7/30\n",
      "5544/5544 [==============================] - 0s 48us/step - loss: 63.7281 - val_loss: 98.3881\n",
      "Epoch 8/30\n",
      "5544/5544 [==============================] - 0s 50us/step - loss: 63.7124 - val_loss: 98.4221\n",
      "Epoch 9/30\n",
      "5544/5544 [==============================] - 0s 53us/step - loss: 63.7015 - val_loss: 98.3731\n",
      "Epoch 10/30\n",
      "5544/5544 [==============================] - 0s 48us/step - loss: 63.6840 - val_loss: 98.3912\n",
      "Epoch 11/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.6685 - val_loss: 98.3402\n",
      "Epoch 12/30\n",
      "5544/5544 [==============================] - 0s 48us/step - loss: 63.6526 - val_loss: 98.3796\n",
      "Epoch 13/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.6396 - val_loss: 98.3545\n",
      "Epoch 14/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.6270 - val_loss: 98.3513\n",
      "Epoch 15/30\n",
      "5544/5544 [==============================] - 0s 48us/step - loss: 63.6057 - val_loss: 98.2783\n",
      "Epoch 16/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.5900 - val_loss: 98.2762\n",
      "Epoch 17/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.5637 - val_loss: 98.2636\n",
      "Epoch 18/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.5520 - val_loss: 98.2935\n",
      "Epoch 19/30\n",
      "5544/5544 [==============================] - 0s 48us/step - loss: 63.5170 - val_loss: 98.1727\n",
      "Epoch 20/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.4957 - val_loss: 98.1478\n",
      "Epoch 21/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.4744 - val_loss: 98.1803\n",
      "Epoch 22/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.4515 - val_loss: 98.1031\n",
      "Epoch 23/30\n",
      "5544/5544 [==============================] - 0s 49us/step - loss: 63.4238 - val_loss: 98.1192\n",
      "Epoch 24/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.3982 - val_loss: 97.9429\n",
      "Epoch 25/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.3629 - val_loss: 97.9134\n",
      "Epoch 26/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.2981 - val_loss: 97.7715\n",
      "Epoch 27/30\n",
      "5544/5544 [==============================] - 0s 52us/step - loss: 63.2771 - val_loss: 98.0224\n",
      "Epoch 28/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.2516 - val_loss: 97.8463\n",
      "Epoch 29/30\n",
      "5544/5544 [==============================] - 0s 47us/step - loss: 63.1791 - val_loss: 97.7433\n",
      "Epoch 30/30\n",
      "5544/5544 [==============================] - 0s 48us/step - loss: 63.1596 - val_loss: 97.8062\n"
     ]
    }
   ],
   "source": [
    "gts = []\n",
    "preds = []\n",
    "for fold_num in range(5):\n",
    "    print(\"---FOLD {} ----\".format(fold_num))\n",
    "    train, test = get_train_test(num_folds=num_folds, fold_num=fold_num)\n",
    "    \n",
    "    train_aggregate = train[:, 0, :, :].reshape(-1, 24)\n",
    "\n",
    "    test_aggregate = test[:, 0, :, :].reshape(-1, 24)\n",
    "\n",
    "    train_appliance = train[:, appliance_num, :, :].reshape(-1, 24)\n",
    "    test_appliance = test[:, appliance_num, :, :].reshape( -1, 24)\n",
    "    pred_appliance = learn(train_aggregate, train_appliance, test_aggregate)\n",
    "    gts.append(test_appliance)\n",
    "    preds.append(pred_appliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2       ,  0.26666668,  0.2       , ...,  1.41666663,\n",
       "         1.26666665,  2.6833334 ],\n",
       "       [ 1.64999998,  1.36666667,  0.64999998, ...,  0.33333334,\n",
       "         1.64999998,  1.68333328],\n",
       "       [ 0.63333333,  0.55000001,  0.98333335, ...,  1.63333333,\n",
       "         0.64999998,  0.66666669],\n",
       "       ..., \n",
       "       [ 0.2       ,  0.34999999,  0.        , ...,  0.05      ,\n",
       "         0.        ,  0.05      ],\n",
       "       [ 0.        ,  0.        ,  0.05      , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.55000001,\n",
       "         0.55000001,  0.34999999]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'dr', 'dw', 'mw']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCE_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Aggregate (InputLayer)          (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Appliance-layer-1 (Dense)       (None, 60)           1500        Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 60)           240         Appliance-layer-1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Droput-Appliance (Dropout)      (None, 60)           0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Appliance-output (Dense)        (None, 24)           1464        Droput-Appliance[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Clip-to-agg (Minimum)           (None, 24)           0           Appliance-output[0][0]           \n",
      "                                                                 Aggregate[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,204\n",
      "Trainable params: 3,084\n",
      "Non-trainable params: 120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a37dec278>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a37de1080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(pred).T.plot(legend=False, color='k',alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a3c8c7eb8>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzDEiAoIgCHOMiIAgCMIc8/8BGVWfLqSe5+YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3cade208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(test_appliance[:20]).T.plot(legend=False, color='k',alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.313343553566867"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(test_appliance, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvac\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/1200\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 861.6758 - val_loss: 873.3601\n",
      "Epoch 2/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 861.6250 - val_loss: 873.5482\n",
      "Epoch 3/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 861.5760 - val_loss: 873.6012\n",
      "Epoch 4/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 861.5365 - val_loss: 873.5959\n",
      "Epoch 5/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 861.4762 - val_loss: 873.5596\n",
      "Epoch 6/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 861.4200 - val_loss: 873.4823\n",
      "Epoch 7/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 861.3570 - val_loss: 873.4054\n",
      "Epoch 8/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 861.2753 - val_loss: 873.3150\n",
      "Epoch 9/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 861.1904 - val_loss: 873.1873\n",
      "Epoch 10/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 861.1010 - val_loss: 873.0433\n",
      "Epoch 11/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 860.9788 - val_loss: 872.8816\n",
      "Epoch 12/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 860.8643 - val_loss: 872.7054\n",
      "Epoch 13/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 860.7261 - val_loss: 872.5202\n",
      "Epoch 14/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 860.5772 - val_loss: 872.3228\n",
      "Epoch 15/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 860.4184 - val_loss: 872.0978\n",
      "Epoch 16/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 860.1903 - val_loss: 871.8464\n",
      "Epoch 17/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 859.9790 - val_loss: 871.6041\n",
      "Epoch 18/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 859.6792 - val_loss: 871.3432\n",
      "Epoch 19/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 859.4261 - val_loss: 871.0826\n",
      "Epoch 20/1200\n",
      "378/378 [==============================] - 0s 78us/step - loss: 859.1948 - val_loss: 870.7943\n",
      "Epoch 21/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 858.8856 - val_loss: 870.5470\n",
      "Epoch 22/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 858.5888 - val_loss: 870.3472\n",
      "Epoch 23/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 858.2187 - val_loss: 870.0924\n",
      "Epoch 24/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 857.8917 - val_loss: 869.7753\n",
      "Epoch 25/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 857.4957 - val_loss: 869.3962\n",
      "Epoch 26/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 857.1074 - val_loss: 868.9473\n",
      "Epoch 27/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 856.7855 - val_loss: 868.5977\n",
      "Epoch 28/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 856.3874 - val_loss: 868.2659\n",
      "Epoch 29/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 856.0527 - val_loss: 868.0314\n",
      "Epoch 30/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 855.6085 - val_loss: 867.4533\n",
      "Epoch 31/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 855.1231 - val_loss: 867.0668\n",
      "Epoch 32/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 854.6620 - val_loss: 866.5694\n",
      "Epoch 33/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 854.3162 - val_loss: 866.2644\n",
      "Epoch 34/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 853.6720 - val_loss: 865.7380\n",
      "Epoch 35/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 853.2963 - val_loss: 865.3718\n",
      "Epoch 36/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 852.8005 - val_loss: 864.9247\n",
      "Epoch 37/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 852.2566 - val_loss: 864.5698\n",
      "Epoch 38/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 851.7107 - val_loss: 863.9986\n",
      "Epoch 39/1200\n",
      "378/378 [==============================] - ETA: 0s - loss: 901.220 - 0s 65us/step - loss: 851.3296 - val_loss: 863.2130\n",
      "Epoch 40/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 850.5385 - val_loss: 862.6241\n",
      "Epoch 41/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 850.0891 - val_loss: 862.1584\n",
      "Epoch 42/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 849.4519 - val_loss: 861.6096\n",
      "Epoch 43/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 848.9225 - val_loss: 861.0890\n",
      "Epoch 44/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 848.4171 - val_loss: 860.8582\n",
      "Epoch 45/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 847.7260 - val_loss: 860.4472\n",
      "Epoch 46/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 847.0388 - val_loss: 859.9848\n",
      "Epoch 47/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 846.4338 - val_loss: 859.2390\n",
      "Epoch 48/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 845.8610 - val_loss: 858.6147\n",
      "Epoch 49/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 845.0457 - val_loss: 858.0341\n",
      "Epoch 50/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 844.5916 - val_loss: 857.2897\n",
      "Epoch 51/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 844.1220 - val_loss: 856.7491\n",
      "Epoch 52/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 843.2018 - val_loss: 856.5461\n",
      "Epoch 53/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 842.3504 - val_loss: 856.0625\n",
      "Epoch 54/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 841.5156 - val_loss: 855.7181\n",
      "Epoch 55/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 840.9050 - val_loss: 855.0372\n",
      "Epoch 56/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 840.1271 - val_loss: 854.1555\n",
      "Epoch 57/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 839.4320 - val_loss: 853.4973\n",
      "Epoch 58/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 838.6937 - val_loss: 853.2796\n",
      "Epoch 59/1200\n",
      "378/378 [==============================] - 0s 52us/step - loss: 838.0760 - val_loss: 852.4861\n",
      "Epoch 60/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 837.3108 - val_loss: 851.5249\n",
      "Epoch 61/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 836.0193 - val_loss: 850.6883\n",
      "Epoch 62/1200\n",
      "378/378 [==============================] - 0s 78us/step - loss: 835.5421 - val_loss: 849.6951\n",
      "Epoch 63/1200\n",
      "378/378 [==============================] - ETA: 0s - loss: 785.508 - 0s 69us/step - loss: 834.2957 - val_loss: 849.3578\n",
      "Epoch 64/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 833.8985 - val_loss: 848.7181\n",
      "Epoch 65/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 833.0059 - val_loss: 847.6966\n",
      "Epoch 66/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 831.7955 - val_loss: 846.5121\n",
      "Epoch 67/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 831.2527 - val_loss: 845.3404\n",
      "Epoch 68/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 829.9467 - val_loss: 845.1183\n",
      "Epoch 69/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 829.0843 - val_loss: 844.4662\n",
      "Epoch 70/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 828.3196 - val_loss: 844.4104\n",
      "Epoch 71/1200\n",
      "378/378 [==============================] - 0s 85us/step - loss: 827.4761 - val_loss: 843.4888\n",
      "Epoch 72/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 826.2903 - val_loss: 842.2989\n",
      "Epoch 73/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 825.2629 - val_loss: 841.1722\n",
      "Epoch 74/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 824.2229 - val_loss: 840.3722\n",
      "Epoch 75/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 823.2886 - val_loss: 839.8376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 821.8676 - val_loss: 839.4928\n",
      "Epoch 77/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 820.9967 - val_loss: 838.3092\n",
      "Epoch 78/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 820.4025 - val_loss: 836.8970\n",
      "Epoch 79/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 819.4352 - val_loss: 836.1084\n",
      "Epoch 80/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 817.9449 - val_loss: 835.1929\n",
      "Epoch 81/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 817.1013 - val_loss: 834.1696\n",
      "Epoch 82/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 816.0722 - val_loss: 833.7141\n",
      "Epoch 83/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 815.0462 - val_loss: 833.4419\n",
      "Epoch 84/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 814.2793 - val_loss: 832.0678\n",
      "Epoch 85/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 813.3380 - val_loss: 831.6582\n",
      "Epoch 86/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 811.9161 - val_loss: 830.5837\n",
      "Epoch 87/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 810.2392 - val_loss: 829.2568\n",
      "Epoch 88/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 809.1964 - val_loss: 828.0934\n",
      "Epoch 89/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 808.2817 - val_loss: 827.0780\n",
      "Epoch 90/1200\n",
      "378/378 [==============================] - 0s 50us/step - loss: 807.5661 - val_loss: 826.2071\n",
      "Epoch 91/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 806.9174 - val_loss: 824.5810\n",
      "Epoch 92/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 804.4729 - val_loss: 824.6091\n",
      "Epoch 93/1200\n",
      "378/378 [==============================] - 0s 52us/step - loss: 803.7224 - val_loss: 823.3245\n",
      "Epoch 94/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 802.3275 - val_loss: 822.0080\n",
      "Epoch 95/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 800.4685 - val_loss: 821.1284\n",
      "Epoch 96/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 800.1089 - val_loss: 821.2063\n",
      "Epoch 97/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 798.2124 - val_loss: 819.8222\n",
      "Epoch 98/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 797.5663 - val_loss: 818.3208\n",
      "Epoch 99/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 795.8737 - val_loss: 817.2110\n",
      "Epoch 100/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 794.5761 - val_loss: 817.2280\n",
      "Epoch 101/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 794.0532 - val_loss: 816.5001\n",
      "Epoch 102/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 793.0007 - val_loss: 815.2573\n",
      "Epoch 103/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 790.6118 - val_loss: 813.6113\n",
      "Epoch 104/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 789.4829 - val_loss: 813.2874\n",
      "Epoch 105/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 787.8730 - val_loss: 811.5936\n",
      "Epoch 106/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 787.0115 - val_loss: 809.8455\n",
      "Epoch 107/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 786.8846 - val_loss: 808.1366\n",
      "Epoch 108/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 784.8839 - val_loss: 806.8978\n",
      "Epoch 109/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 782.4273 - val_loss: 805.8839\n",
      "Epoch 110/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 781.6844 - val_loss: 805.1762\n",
      "Epoch 111/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 780.9991 - val_loss: 805.7179\n",
      "Epoch 112/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 780.8314 - val_loss: 805.4167\n",
      "Epoch 113/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 778.8919 - val_loss: 803.8149\n",
      "Epoch 114/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 775.1434 - val_loss: 800.9390\n",
      "Epoch 115/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 775.1869 - val_loss: 799.8796\n",
      "Epoch 116/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 773.4036 - val_loss: 798.3397\n",
      "Epoch 117/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 772.4763 - val_loss: 797.4890\n",
      "Epoch 118/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 769.3953 - val_loss: 795.8498\n",
      "Epoch 119/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 768.0411 - val_loss: 795.1043\n",
      "Epoch 120/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 766.0573 - val_loss: 793.3776\n",
      "Epoch 121/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 765.7334 - val_loss: 793.9896\n",
      "Epoch 122/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 764.5108 - val_loss: 794.2586\n",
      "Epoch 123/1200\n",
      "378/378 [==============================] - 0s 79us/step - loss: 763.7147 - val_loss: 792.7381\n",
      "Epoch 124/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 761.7887 - val_loss: 790.7040\n",
      "Epoch 125/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 760.4717 - val_loss: 789.0272\n",
      "Epoch 126/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 758.1942 - val_loss: 787.4254\n",
      "Epoch 127/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 756.1984 - val_loss: 785.0219\n",
      "Epoch 128/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 756.1296 - val_loss: 783.5906\n",
      "Epoch 129/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 755.5870 - val_loss: 782.9989\n",
      "Epoch 130/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 753.3208 - val_loss: 782.4457\n",
      "Epoch 131/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 750.8071 - val_loss: 781.9804\n",
      "Epoch 132/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 749.3612 - val_loss: 779.6764\n",
      "Epoch 133/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 748.5298 - val_loss: 779.7609\n",
      "Epoch 134/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 746.9289 - val_loss: 780.3961\n",
      "Epoch 135/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 746.2544 - val_loss: 779.4441\n",
      "Epoch 136/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 743.2100 - val_loss: 777.1192\n",
      "Epoch 137/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 743.6764 - val_loss: 778.2771\n",
      "Epoch 138/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 740.1253 - val_loss: 776.4686\n",
      "Epoch 139/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 736.7012 - val_loss: 774.9558\n",
      "Epoch 140/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 737.3728 - val_loss: 770.7289\n",
      "Epoch 141/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 735.3785 - val_loss: 771.6391\n",
      "Epoch 142/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 733.0305 - val_loss: 769.9692\n",
      "Epoch 143/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 732.6514 - val_loss: 767.9550\n",
      "Epoch 144/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 730.6765 - val_loss: 765.2406\n",
      "Epoch 145/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 728.5347 - val_loss: 763.3150\n",
      "Epoch 146/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 727.3300 - val_loss: 762.9287\n",
      "Epoch 147/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 725.0227 - val_loss: 761.6065\n",
      "Epoch 148/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 724.7959 - val_loss: 761.6295\n",
      "Epoch 149/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 723.3444 - val_loss: 761.1568\n",
      "Epoch 150/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 722.6322 - val_loss: 762.0483\n",
      "Epoch 151/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 720.7097 - val_loss: 761.4540\n",
      "Epoch 152/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 719.0351 - val_loss: 762.7504\n",
      "Epoch 153/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 716.4204 - val_loss: 758.8611\n",
      "Epoch 154/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 715.0654 - val_loss: 755.1601\n",
      "Epoch 155/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 713.3943 - val_loss: 753.7768\n",
      "Epoch 156/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 711.4239 - val_loss: 750.6877\n",
      "Epoch 157/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 706.7469 - val_loss: 750.3478\n",
      "Epoch 158/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 708.5856 - val_loss: 750.8574\n",
      "Epoch 159/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 706.5494 - val_loss: 750.7085\n",
      "Epoch 160/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 702.5539 - val_loss: 748.8145\n",
      "Epoch 161/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 702.3931 - val_loss: 747.4444\n",
      "Epoch 162/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 701.8018 - val_loss: 746.7929\n",
      "Epoch 163/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 699.5636 - val_loss: 746.4243\n",
      "Epoch 164/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 697.8670 - val_loss: 746.7427\n",
      "Epoch 165/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 696.9578 - val_loss: 747.2441\n",
      "Epoch 166/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 692.6361 - val_loss: 742.4880\n",
      "Epoch 167/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 692.4993 - val_loss: 739.6233\n",
      "Epoch 168/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 692.2193 - val_loss: 737.9539\n",
      "Epoch 169/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 687.6405 - val_loss: 739.4895\n",
      "Epoch 170/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 686.2844 - val_loss: 738.8050\n",
      "Epoch 171/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 686.3165 - val_loss: 737.8091\n",
      "Epoch 172/1200\n",
      "378/378 [==============================] - ETA: 0s - loss: 623.229 - 0s 60us/step - loss: 682.1345 - val_loss: 735.3086\n",
      "Epoch 173/1200\n",
      "378/378 [==============================] - 0s 78us/step - loss: 682.2746 - val_loss: 729.1058\n",
      "Epoch 174/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 679.3525 - val_loss: 729.5256\n",
      "Epoch 175/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 680.5910 - val_loss: 729.9612\n",
      "Epoch 176/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 677.4450 - val_loss: 727.9751\n",
      "Epoch 177/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 675.8041 - val_loss: 729.8268\n",
      "Epoch 178/1200\n",
      "378/378 [==============================] - 0s 80us/step - loss: 673.8723 - val_loss: 729.2391\n",
      "Epoch 179/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 674.0074 - val_loss: 727.9573\n",
      "Epoch 180/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 670.5083 - val_loss: 728.3031\n",
      "Epoch 181/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 668.3693 - val_loss: 723.5186\n",
      "Epoch 182/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 667.1623 - val_loss: 721.3244\n",
      "Epoch 183/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 664.2033 - val_loss: 723.2034\n",
      "Epoch 184/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 662.2828 - val_loss: 722.3490\n",
      "Epoch 185/1200\n",
      "378/378 [==============================] - 0s 79us/step - loss: 661.3611 - val_loss: 720.7400\n",
      "Epoch 186/1200\n",
      "378/378 [==============================] - 0s 78us/step - loss: 660.8190 - val_loss: 720.3728\n",
      "Epoch 187/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 658.1441 - val_loss: 717.3408\n",
      "Epoch 188/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 652.0268 - val_loss: 715.5075\n",
      "Epoch 189/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 655.1107 - val_loss: 709.7927\n",
      "Epoch 190/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 652.4552 - val_loss: 709.3608\n",
      "Epoch 191/1200\n",
      "378/378 [==============================] - ETA: 0s - loss: 734.675 - 0s 63us/step - loss: 651.3264 - val_loss: 712.7939\n",
      "Epoch 192/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 648.7757 - val_loss: 713.0662\n",
      "Epoch 193/1200\n",
      "378/378 [==============================] - 0s 52us/step - loss: 648.3346 - val_loss: 711.2241\n",
      "Epoch 194/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 643.4493 - val_loss: 710.7616\n",
      "Epoch 195/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 644.8085 - val_loss: 710.7244\n",
      "Epoch 196/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 642.4588 - val_loss: 708.5221\n",
      "Epoch 197/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 638.8254 - val_loss: 705.4664\n",
      "Epoch 198/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 637.6141 - val_loss: 702.7421\n",
      "Epoch 199/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 639.6406 - val_loss: 701.0755\n",
      "Epoch 200/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 635.5905 - val_loss: 699.2300\n",
      "Epoch 201/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 636.4824 - val_loss: 698.6729\n",
      "Epoch 202/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 630.8032 - val_loss: 697.7015\n",
      "Epoch 203/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 629.7851 - val_loss: 697.9490\n",
      "Epoch 204/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 630.3775 - val_loss: 698.7539\n",
      "Epoch 205/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 624.4608 - val_loss: 698.0528\n",
      "Epoch 206/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 623.7205 - val_loss: 688.4609\n",
      "Epoch 207/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 624.9881 - val_loss: 687.2817\n",
      "Epoch 208/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 619.3314 - val_loss: 687.8373\n",
      "Epoch 209/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 622.1777 - val_loss: 688.6912\n",
      "Epoch 210/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 620.7377 - val_loss: 688.3311\n",
      "Epoch 211/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 616.0714 - val_loss: 690.8004\n",
      "Epoch 212/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 613.8049 - val_loss: 688.1987\n",
      "Epoch 213/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 613.4538 - val_loss: 684.3053\n",
      "Epoch 214/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 613.0972 - val_loss: 683.1325\n",
      "Epoch 215/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 607.8720 - val_loss: 682.5860\n",
      "Epoch 216/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 606.4589 - val_loss: 680.2977\n",
      "Epoch 217/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 604.0804 - val_loss: 677.5810\n",
      "Epoch 218/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 603.1846 - val_loss: 675.5505\n",
      "Epoch 219/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 600.8896 - val_loss: 676.2226\n",
      "Epoch 220/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 600.5958 - val_loss: 673.0505\n",
      "Epoch 221/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 598.9118 - val_loss: 671.3073\n",
      "Epoch 222/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 596.6445 - val_loss: 667.4477\n",
      "Epoch 223/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 595.9094 - val_loss: 669.5511\n",
      "Epoch 224/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 595.4174 - val_loss: 664.8431\n",
      "Epoch 225/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 592.5990 - val_loss: 664.2566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 591.0601 - val_loss: 667.7407\n",
      "Epoch 227/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 590.2017 - val_loss: 672.0573\n",
      "Epoch 228/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 587.4802 - val_loss: 673.0465\n",
      "Epoch 229/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 584.5988 - val_loss: 670.6803\n",
      "Epoch 230/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 582.6109 - val_loss: 668.3926\n",
      "Epoch 231/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 580.0059 - val_loss: 666.4742\n",
      "Epoch 232/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 580.6943 - val_loss: 662.2218\n",
      "Epoch 233/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 578.3093 - val_loss: 663.7275\n",
      "Epoch 234/1200\n",
      "378/378 [==============================] - 0s 51us/step - loss: 578.7892 - val_loss: 660.7896\n",
      "Epoch 235/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 574.7354 - val_loss: 659.6871\n",
      "Epoch 236/1200\n",
      "378/378 [==============================] - 0s 78us/step - loss: 573.4538 - val_loss: 656.0897\n",
      "Epoch 237/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 574.4749 - val_loss: 652.9439\n",
      "Epoch 238/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 568.6511 - val_loss: 654.4756\n",
      "Epoch 239/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 568.4474 - val_loss: 651.5882\n",
      "Epoch 240/1200\n",
      "378/378 [==============================] - 0s 84us/step - loss: 565.5670 - val_loss: 649.2284\n",
      "Epoch 241/1200\n",
      "378/378 [==============================] - 0s 83us/step - loss: 563.0908 - val_loss: 650.8106\n",
      "Epoch 242/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 563.0746 - val_loss: 650.7889\n",
      "Epoch 243/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 561.4121 - val_loss: 645.3970\n",
      "Epoch 244/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 559.6830 - val_loss: 646.2314\n",
      "Epoch 245/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 554.9894 - val_loss: 646.6909\n",
      "Epoch 246/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 556.3032 - val_loss: 646.1358\n",
      "Epoch 247/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 554.7204 - val_loss: 646.3460\n",
      "Epoch 248/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 552.9174 - val_loss: 641.4487\n",
      "Epoch 249/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 553.8520 - val_loss: 638.0221\n",
      "Epoch 250/1200\n",
      "378/378 [==============================] - ETA: 0s - loss: 598.213 - 0s 85us/step - loss: 550.5372 - val_loss: 637.2869\n",
      "Epoch 251/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 546.6332 - val_loss: 633.8563\n",
      "Epoch 252/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 543.6810 - val_loss: 629.8529\n",
      "Epoch 253/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 544.4584 - val_loss: 630.4255\n",
      "Epoch 254/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 545.1043 - val_loss: 627.5374\n",
      "Epoch 255/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 538.4646 - val_loss: 628.5368\n",
      "Epoch 256/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 540.4055 - val_loss: 632.5018\n",
      "Epoch 257/1200\n",
      "378/378 [==============================] - 0s 79us/step - loss: 539.5033 - val_loss: 631.2741\n",
      "Epoch 258/1200\n",
      "378/378 [==============================] - 0s 79us/step - loss: 542.6665 - val_loss: 630.6113\n",
      "Epoch 259/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 533.9037 - val_loss: 626.9341\n",
      "Epoch 260/1200\n",
      "378/378 [==============================] - 0s 80us/step - loss: 532.1291 - val_loss: 622.5525\n",
      "Epoch 261/1200\n",
      "378/378 [==============================] - 0s 80us/step - loss: 532.6170 - val_loss: 622.7609\n",
      "Epoch 262/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 529.3091 - val_loss: 620.2355\n",
      "Epoch 263/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 527.7406 - val_loss: 619.7889\n",
      "Epoch 264/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 526.3759 - val_loss: 622.8920\n",
      "Epoch 265/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 522.6980 - val_loss: 625.4176\n",
      "Epoch 266/1200\n",
      "378/378 [==============================] - 0s 49us/step - loss: 521.9290 - val_loss: 620.0987\n",
      "Epoch 267/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 518.8712 - val_loss: 620.5596\n",
      "Epoch 268/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 518.3490 - val_loss: 618.7090\n",
      "Epoch 269/1200\n",
      "378/378 [==============================] - 0s 52us/step - loss: 518.8519 - val_loss: 615.9942\n",
      "Epoch 270/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 519.3331 - val_loss: 610.0910\n",
      "Epoch 271/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 515.7127 - val_loss: 609.4400\n",
      "Epoch 272/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 511.8918 - val_loss: 613.5469\n",
      "Epoch 273/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 510.0902 - val_loss: 616.4359\n",
      "Epoch 274/1200\n",
      "378/378 [==============================] - 0s 80us/step - loss: 509.3166 - val_loss: 615.3487\n",
      "Epoch 275/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 509.3031 - val_loss: 611.2874\n",
      "Epoch 276/1200\n",
      "378/378 [==============================] - 0s 83us/step - loss: 506.3982 - val_loss: 607.5184\n",
      "Epoch 277/1200\n",
      "378/378 [==============================] - 0s 81us/step - loss: 507.5292 - val_loss: 607.4360\n",
      "Epoch 278/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 508.5922 - val_loss: 603.8846\n",
      "Epoch 279/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 502.0153 - val_loss: 604.3254\n",
      "Epoch 280/1200\n",
      "378/378 [==============================] - 0s 52us/step - loss: 498.3774 - val_loss: 600.4630\n",
      "Epoch 281/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 498.2698 - val_loss: 598.7120\n",
      "Epoch 282/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 496.2628 - val_loss: 598.3591\n",
      "Epoch 283/1200\n",
      "378/378 [==============================] - 0s 51us/step - loss: 495.2556 - val_loss: 598.5445\n",
      "Epoch 284/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 492.6348 - val_loss: 595.6563\n",
      "Epoch 285/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 492.6302 - val_loss: 596.0065\n",
      "Epoch 286/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 490.7454 - val_loss: 593.6728\n",
      "Epoch 287/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 487.6838 - val_loss: 592.9609\n",
      "Epoch 288/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 487.3073 - val_loss: 592.9598\n",
      "Epoch 289/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 485.6768 - val_loss: 594.1364\n",
      "Epoch 290/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 487.2099 - val_loss: 592.4205\n",
      "Epoch 291/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 487.3313 - val_loss: 585.9013\n",
      "Epoch 292/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 482.2455 - val_loss: 584.4949\n",
      "Epoch 293/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 480.7533 - val_loss: 575.0522\n",
      "Epoch 294/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 480.5009 - val_loss: 582.2086\n",
      "Epoch 295/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 475.8516 - val_loss: 579.9039\n",
      "Epoch 296/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 476.1131 - val_loss: 579.1644\n",
      "Epoch 297/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 472.5192 - val_loss: 579.8072\n",
      "Epoch 298/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 476.0537 - val_loss: 572.3508\n",
      "Epoch 299/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 471.7959 - val_loss: 568.5027\n",
      "Epoch 300/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 468.0311 - val_loss: 568.7580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 466.8226 - val_loss: 564.1934\n",
      "Epoch 302/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 465.5623 - val_loss: 567.4359\n",
      "Epoch 303/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 461.6911 - val_loss: 568.5410\n",
      "Epoch 304/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 461.6834 - val_loss: 567.2562\n",
      "Epoch 305/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 461.4503 - val_loss: 563.1809\n",
      "Epoch 306/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 456.8970 - val_loss: 560.1777\n",
      "Epoch 307/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 458.7173 - val_loss: 559.7529\n",
      "Epoch 308/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 461.2059 - val_loss: 566.4567\n",
      "Epoch 309/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 456.1499 - val_loss: 571.1294\n",
      "Epoch 310/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 455.1633 - val_loss: 561.5898\n",
      "Epoch 311/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 454.7908 - val_loss: 553.6853\n",
      "Epoch 312/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 445.6800 - val_loss: 555.0749\n",
      "Epoch 313/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 448.2891 - val_loss: 557.1968\n",
      "Epoch 314/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 447.3903 - val_loss: 552.2487\n",
      "Epoch 315/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 442.7879 - val_loss: 557.4897\n",
      "Epoch 316/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 444.5309 - val_loss: 558.4380\n",
      "Epoch 317/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 443.6717 - val_loss: 552.1576\n",
      "Epoch 318/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 446.0353 - val_loss: 552.4751\n",
      "Epoch 319/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 437.6722 - val_loss: 550.8611\n",
      "Epoch 320/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 443.2430 - val_loss: 546.4237\n",
      "Epoch 321/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 441.3042 - val_loss: 538.9200\n",
      "Epoch 322/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 436.9505 - val_loss: 533.0215\n",
      "Epoch 323/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 436.5067 - val_loss: 533.8202\n",
      "Epoch 324/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 432.8020 - val_loss: 541.3714\n",
      "Epoch 325/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 433.8649 - val_loss: 539.3386\n",
      "Epoch 326/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 429.5325 - val_loss: 532.4656\n",
      "Epoch 327/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 430.0360 - val_loss: 529.3360\n",
      "Epoch 328/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 434.3941 - val_loss: 531.5947\n",
      "Epoch 329/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 429.0410 - val_loss: 521.3144\n",
      "Epoch 330/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 425.6439 - val_loss: 515.0950\n",
      "Epoch 331/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 428.4886 - val_loss: 517.5937\n",
      "Epoch 332/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 421.4857 - val_loss: 518.2531\n",
      "Epoch 333/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 419.8157 - val_loss: 517.3786\n",
      "Epoch 334/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 419.8858 - val_loss: 514.2231\n",
      "Epoch 335/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 420.6413 - val_loss: 516.7731\n",
      "Epoch 336/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 413.0863 - val_loss: 516.7921\n",
      "Epoch 337/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 415.0288 - val_loss: 520.3393\n",
      "Epoch 338/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 416.9665 - val_loss: 520.4495\n",
      "Epoch 339/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 412.2586 - val_loss: 517.4352\n",
      "Epoch 340/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 414.3576 - val_loss: 518.5213\n",
      "Epoch 341/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 408.5314 - val_loss: 515.1488\n",
      "Epoch 342/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 405.6323 - val_loss: 509.7951\n",
      "Epoch 343/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 408.4750 - val_loss: 508.6143\n",
      "Epoch 344/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 407.3804 - val_loss: 507.5924\n",
      "Epoch 345/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 406.2249 - val_loss: 508.8705\n",
      "Epoch 346/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 402.9901 - val_loss: 504.9023\n",
      "Epoch 347/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 403.5957 - val_loss: 499.6693\n",
      "Epoch 348/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 404.0349 - val_loss: 499.4787\n",
      "Epoch 349/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 400.1583 - val_loss: 497.8265\n",
      "Epoch 350/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 396.9708 - val_loss: 496.5209\n",
      "Epoch 351/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 396.4090 - val_loss: 498.5539\n",
      "Epoch 352/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 395.4706 - val_loss: 497.7398\n",
      "Epoch 353/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 394.8504 - val_loss: 500.4842\n",
      "Epoch 354/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 395.6020 - val_loss: 500.2669\n",
      "Epoch 355/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 395.6391 - val_loss: 494.9661\n",
      "Epoch 356/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 392.5072 - val_loss: 485.8094\n",
      "Epoch 357/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 391.2049 - val_loss: 481.0948\n",
      "Epoch 358/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 390.6149 - val_loss: 478.6800\n",
      "Epoch 359/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 387.9990 - val_loss: 480.8519\n",
      "Epoch 360/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 386.8403 - val_loss: 484.7016\n",
      "Epoch 361/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 393.4338 - val_loss: 483.1625\n",
      "Epoch 362/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 381.1055 - val_loss: 475.2269\n",
      "Epoch 363/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 382.2443 - val_loss: 477.9981\n",
      "Epoch 364/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 386.3887 - val_loss: 480.4228\n",
      "Epoch 365/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 381.3000 - val_loss: 474.0056\n",
      "Epoch 366/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 373.3185 - val_loss: 473.7462\n",
      "Epoch 367/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 378.4744 - val_loss: 472.9366\n",
      "Epoch 368/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 376.3234 - val_loss: 466.0213\n",
      "Epoch 369/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 373.5867 - val_loss: 467.6647\n",
      "Epoch 370/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 376.5069 - val_loss: 468.4485\n",
      "Epoch 371/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 375.8484 - val_loss: 467.4024\n",
      "Epoch 372/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 375.0912 - val_loss: 469.4707\n",
      "Epoch 373/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 372.0222 - val_loss: 465.0332\n",
      "Epoch 374/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 372.2279 - val_loss: 457.3610\n",
      "Epoch 375/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 367.5156 - val_loss: 452.6885\n",
      "Epoch 376/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 367.4373 - val_loss: 451.0325\n",
      "Epoch 377/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 368.8694 - val_loss: 453.3037\n",
      "Epoch 378/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 365.9784 - val_loss: 450.8207\n",
      "Epoch 379/1200\n",
      "378/378 [==============================] - 0s 52us/step - loss: 364.4875 - val_loss: 450.9689\n",
      "Epoch 380/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 361.4080 - val_loss: 447.9566\n",
      "Epoch 381/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 360.6460 - val_loss: 443.6472\n",
      "Epoch 382/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 359.7739 - val_loss: 442.4860\n",
      "Epoch 383/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 360.2209 - val_loss: 436.6541\n",
      "Epoch 384/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 363.7504 - val_loss: 434.1910\n",
      "Epoch 385/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 353.4710 - val_loss: 439.4018\n",
      "Epoch 386/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 356.6520 - val_loss: 438.9189\n",
      "Epoch 387/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 352.4533 - val_loss: 430.9902\n",
      "Epoch 388/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 349.1422 - val_loss: 427.6420\n",
      "Epoch 389/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 361.9474 - val_loss: 425.5625\n",
      "Epoch 390/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 353.2154 - val_loss: 421.1461\n",
      "Epoch 391/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 345.3644 - val_loss: 423.0228\n",
      "Epoch 392/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 349.4860 - val_loss: 425.1579\n",
      "Epoch 393/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 350.4134 - val_loss: 425.7432\n",
      "Epoch 394/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 349.4427 - val_loss: 426.5830\n",
      "Epoch 395/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 341.9711 - val_loss: 425.7708\n",
      "Epoch 396/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 342.1021 - val_loss: 427.3131\n",
      "Epoch 397/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 343.3549 - val_loss: 428.6984\n",
      "Epoch 398/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 345.8369 - val_loss: 428.4607\n",
      "Epoch 399/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 341.1888 - val_loss: 428.7229\n",
      "Epoch 400/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 336.1681 - val_loss: 428.1881\n",
      "Epoch 401/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 339.5794 - val_loss: 424.1055\n",
      "Epoch 402/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 340.3656 - val_loss: 421.1044\n",
      "Epoch 403/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 336.2070 - val_loss: 419.7169\n",
      "Epoch 404/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 335.5311 - val_loss: 417.0744\n",
      "Epoch 405/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 331.1305 - val_loss: 413.6109\n",
      "Epoch 406/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 335.9450 - val_loss: 413.3633\n",
      "Epoch 407/1200\n",
      "378/378 [==============================] - 0s 81us/step - loss: 332.9721 - val_loss: 411.0009\n",
      "Epoch 408/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 328.7248 - val_loss: 408.1644\n",
      "Epoch 409/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 332.5541 - val_loss: 409.3129\n",
      "Epoch 410/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 328.5866 - val_loss: 407.2266\n",
      "Epoch 411/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 331.1359 - val_loss: 401.1888\n",
      "Epoch 412/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 333.7011 - val_loss: 393.9319\n",
      "Epoch 413/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 324.4095 - val_loss: 390.9994\n",
      "Epoch 414/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 322.1326 - val_loss: 388.4264\n",
      "Epoch 415/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 324.6245 - val_loss: 388.4559\n",
      "Epoch 416/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 323.1269 - val_loss: 386.6739\n",
      "Epoch 417/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 323.8242 - val_loss: 382.9098\n",
      "Epoch 418/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 319.3815 - val_loss: 379.6705\n",
      "Epoch 419/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 320.9817 - val_loss: 378.9218\n",
      "Epoch 420/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 321.9764 - val_loss: 378.9751\n",
      "Epoch 421/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 320.0919 - val_loss: 376.9676\n",
      "Epoch 422/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 318.6856 - val_loss: 378.1141\n",
      "Epoch 423/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 315.4290 - val_loss: 376.5969\n",
      "Epoch 424/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 313.6474 - val_loss: 376.0362\n",
      "Epoch 425/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 316.4303 - val_loss: 378.8129\n",
      "Epoch 426/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 310.8794 - val_loss: 376.7651\n",
      "Epoch 427/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 314.6177 - val_loss: 376.5041\n",
      "Epoch 428/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 314.2172 - val_loss: 375.6291\n",
      "Epoch 429/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 308.6489 - val_loss: 372.4767\n",
      "Epoch 430/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 310.1298 - val_loss: 369.0070\n",
      "Epoch 431/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 308.3877 - val_loss: 366.3979\n",
      "Epoch 432/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 305.5095 - val_loss: 365.2942\n",
      "Epoch 433/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 304.4215 - val_loss: 363.0444\n",
      "Epoch 434/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 303.4472 - val_loss: 363.6241\n",
      "Epoch 435/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 303.4734 - val_loss: 364.2969\n",
      "Epoch 436/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 298.8823 - val_loss: 364.5836\n",
      "Epoch 437/1200\n",
      "378/378 [==============================] - 0s 84us/step - loss: 305.4909 - val_loss: 365.7219\n",
      "Epoch 438/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 299.4521 - val_loss: 366.2013\n",
      "Epoch 439/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 298.8626 - val_loss: 366.0349\n",
      "Epoch 440/1200\n",
      "378/378 [==============================] - 0s 96us/step - loss: 298.0896 - val_loss: 365.8728\n",
      "Epoch 441/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 302.9068 - val_loss: 360.1109\n",
      "Epoch 442/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 297.7314 - val_loss: 359.8595\n",
      "Epoch 443/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 298.1899 - val_loss: 368.0512\n",
      "Epoch 444/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 303.5445 - val_loss: 373.6629\n",
      "Epoch 445/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 300.9299 - val_loss: 369.6307\n",
      "Epoch 446/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 301.2349 - val_loss: 368.2471\n",
      "Epoch 447/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 290.3547 - val_loss: 364.5648\n",
      "Epoch 448/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 296.0625 - val_loss: 362.9662\n",
      "Epoch 449/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 295.1738 - val_loss: 357.4589\n",
      "Epoch 450/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 297.5133 - val_loss: 353.7390\n",
      "Epoch 451/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 72us/step - loss: 298.5943 - val_loss: 350.4925\n",
      "Epoch 452/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 296.5991 - val_loss: 348.6069\n",
      "Epoch 453/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 291.8966 - val_loss: 347.9338\n",
      "Epoch 454/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 285.3122 - val_loss: 347.9650\n",
      "Epoch 455/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 287.5591 - val_loss: 350.1213\n",
      "Epoch 456/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 289.0762 - val_loss: 349.7348\n",
      "Epoch 457/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 293.2203 - val_loss: 345.8786\n",
      "Epoch 458/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 288.0536 - val_loss: 346.4514\n",
      "Epoch 459/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 280.2823 - val_loss: 346.7843\n",
      "Epoch 460/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 283.2736 - val_loss: 344.5657\n",
      "Epoch 461/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 292.3486 - val_loss: 343.0078\n",
      "Epoch 462/1200\n",
      "378/378 [==============================] - 0s 52us/step - loss: 280.4390 - val_loss: 345.1843\n",
      "Epoch 463/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 282.4065 - val_loss: 344.5301\n",
      "Epoch 464/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 272.8982 - val_loss: 342.8046\n",
      "Epoch 465/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 286.3259 - val_loss: 340.8859\n",
      "Epoch 466/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 282.9634 - val_loss: 339.4659\n",
      "Epoch 467/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 278.2079 - val_loss: 335.6612\n",
      "Epoch 468/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 277.1589 - val_loss: 332.9203\n",
      "Epoch 469/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 274.0377 - val_loss: 333.4334\n",
      "Epoch 470/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 278.2938 - val_loss: 332.6150\n",
      "Epoch 471/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 272.5163 - val_loss: 334.0542\n",
      "Epoch 472/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 275.8096 - val_loss: 332.2783\n",
      "Epoch 473/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 275.2505 - val_loss: 330.3134\n",
      "Epoch 474/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 271.5080 - val_loss: 328.4451\n",
      "Epoch 475/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 273.7446 - val_loss: 327.9969\n",
      "Epoch 476/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 272.0499 - val_loss: 330.2568\n",
      "Epoch 477/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 274.1864 - val_loss: 330.4259\n",
      "Epoch 478/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 266.8865 - val_loss: 327.9113\n",
      "Epoch 479/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 275.3379 - val_loss: 325.6012\n",
      "Epoch 480/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 269.8669 - val_loss: 325.5883\n",
      "Epoch 481/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 269.2367 - val_loss: 325.0650\n",
      "Epoch 482/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 264.7013 - val_loss: 325.4140\n",
      "Epoch 483/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 263.4491 - val_loss: 324.5905\n",
      "Epoch 484/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 266.8842 - val_loss: 324.1946\n",
      "Epoch 485/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 260.5097 - val_loss: 322.4162\n",
      "Epoch 486/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 267.5787 - val_loss: 321.2602\n",
      "Epoch 487/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 263.8433 - val_loss: 318.6250\n",
      "Epoch 488/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 260.1282 - val_loss: 316.4758\n",
      "Epoch 489/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 264.5254 - val_loss: 315.4143\n",
      "Epoch 490/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 262.7329 - val_loss: 316.5861\n",
      "Epoch 491/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 267.2895 - val_loss: 315.5789\n",
      "Epoch 492/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 258.2883 - val_loss: 315.9461\n",
      "Epoch 493/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 262.3745 - val_loss: 314.0243\n",
      "Epoch 494/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 263.8611 - val_loss: 310.5622\n",
      "Epoch 495/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 254.9751 - val_loss: 308.8983\n",
      "Epoch 496/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 261.2493 - val_loss: 308.9390\n",
      "Epoch 497/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 257.3930 - val_loss: 308.9884\n",
      "Epoch 498/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 255.2359 - val_loss: 307.9017\n",
      "Epoch 499/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 252.8232 - val_loss: 306.3950\n",
      "Epoch 500/1200\n",
      "378/378 [==============================] - ETA: 0s - loss: 225.231 - 0s 62us/step - loss: 250.4245 - val_loss: 304.7165\n",
      "Epoch 501/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 255.0335 - val_loss: 304.8134\n",
      "Epoch 502/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 249.4723 - val_loss: 305.3369\n",
      "Epoch 503/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 253.7742 - val_loss: 304.5545\n",
      "Epoch 504/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 248.7750 - val_loss: 304.0240\n",
      "Epoch 505/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 258.4997 - val_loss: 302.7599\n",
      "Epoch 506/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 249.8330 - val_loss: 300.2548\n",
      "Epoch 507/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 247.2056 - val_loss: 298.7635\n",
      "Epoch 508/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 246.3166 - val_loss: 299.7013\n",
      "Epoch 509/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 251.5352 - val_loss: 299.7654\n",
      "Epoch 510/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 250.5525 - val_loss: 295.5105\n",
      "Epoch 511/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 250.8232 - val_loss: 296.7495\n",
      "Epoch 512/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 251.5652 - val_loss: 294.5824\n",
      "Epoch 513/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 247.2676 - val_loss: 295.1436\n",
      "Epoch 514/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 249.1994 - val_loss: 292.5194\n",
      "Epoch 515/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 242.0368 - val_loss: 291.7338\n",
      "Epoch 516/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 246.7648 - val_loss: 289.5958\n",
      "Epoch 517/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 242.9008 - val_loss: 288.7723\n",
      "Epoch 518/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 242.8853 - val_loss: 290.3622\n",
      "Epoch 519/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 240.5423 - val_loss: 291.1343\n",
      "Epoch 520/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 246.3193 - val_loss: 293.6337\n",
      "Epoch 521/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 244.3009 - val_loss: 291.9268\n",
      "Epoch 522/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 242.8353 - val_loss: 291.4716\n",
      "Epoch 523/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 243.3470 - val_loss: 287.4048\n",
      "Epoch 524/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 237.8582 - val_loss: 286.6118\n",
      "Epoch 525/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 241.1573 - val_loss: 288.2948\n",
      "Epoch 526/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 245.4952 - val_loss: 290.9533\n",
      "Epoch 527/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 240.6389 - val_loss: 291.3424\n",
      "Epoch 528/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 237.4399 - val_loss: 290.4399\n",
      "Epoch 529/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 238.4230 - val_loss: 288.5519\n",
      "Epoch 530/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 237.6022 - val_loss: 285.2167\n",
      "Epoch 531/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 238.2878 - val_loss: 281.6371\n",
      "Epoch 532/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 234.1709 - val_loss: 280.5003\n",
      "Epoch 533/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 240.1627 - val_loss: 280.7922\n",
      "Epoch 534/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 236.5915 - val_loss: 281.8878\n",
      "Epoch 535/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 237.7702 - val_loss: 279.6999\n",
      "Epoch 536/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 233.0767 - val_loss: 277.0597\n",
      "Epoch 537/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 233.2116 - val_loss: 275.3020\n",
      "Epoch 538/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 233.1398 - val_loss: 274.2243\n",
      "Epoch 539/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 231.4727 - val_loss: 274.1967\n",
      "Epoch 540/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 231.8038 - val_loss: 272.2613\n",
      "Epoch 541/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 233.1757 - val_loss: 273.3762\n",
      "Epoch 542/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 235.3758 - val_loss: 270.8683\n",
      "Epoch 543/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 235.8549 - val_loss: 266.8894\n",
      "Epoch 544/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 229.5482 - val_loss: 264.9989\n",
      "Epoch 545/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 232.4657 - val_loss: 265.7557\n",
      "Epoch 546/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 230.2943 - val_loss: 267.6048\n",
      "Epoch 547/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 235.2591 - val_loss: 267.4178\n",
      "Epoch 548/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 228.6844 - val_loss: 266.1162\n",
      "Epoch 549/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 230.7435 - val_loss: 265.4864\n",
      "Epoch 550/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 233.9259 - val_loss: 260.3442\n",
      "Epoch 551/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 233.7346 - val_loss: 261.1176\n",
      "Epoch 552/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 227.9435 - val_loss: 262.4528\n",
      "Epoch 553/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 227.0452 - val_loss: 263.9343\n",
      "Epoch 554/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 230.7849 - val_loss: 261.6512\n",
      "Epoch 555/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 227.6062 - val_loss: 256.1314\n",
      "Epoch 556/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 221.9134 - val_loss: 255.4097\n",
      "Epoch 557/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 228.5449 - val_loss: 254.6882\n",
      "Epoch 558/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 233.9345 - val_loss: 246.9409\n",
      "Epoch 559/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 223.9462 - val_loss: 245.2659\n",
      "Epoch 560/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 223.5292 - val_loss: 240.0981\n",
      "Epoch 561/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 221.8075 - val_loss: 238.3171\n",
      "Epoch 562/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 223.9976 - val_loss: 237.0776\n",
      "Epoch 563/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 222.1878 - val_loss: 232.9576\n",
      "Epoch 564/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 223.2489 - val_loss: 228.6078\n",
      "Epoch 565/1200\n",
      "378/378 [==============================] - 0s 78us/step - loss: 220.1674 - val_loss: 226.1999\n",
      "Epoch 566/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 222.5705 - val_loss: 222.7076\n",
      "Epoch 567/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 221.7134 - val_loss: 219.1592\n",
      "Epoch 568/1200\n",
      "378/378 [==============================] - ETA: 0s - loss: 203.471 - 0s 65us/step - loss: 215.1151 - val_loss: 218.0713\n",
      "Epoch 569/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 215.5767 - val_loss: 217.4249\n",
      "Epoch 570/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 213.5940 - val_loss: 216.2668\n",
      "Epoch 571/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 212.5722 - val_loss: 216.0125\n",
      "Epoch 572/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 214.3379 - val_loss: 216.4479\n",
      "Epoch 573/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 213.7045 - val_loss: 217.7700\n",
      "Epoch 574/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 214.4590 - val_loss: 219.4227\n",
      "Epoch 575/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 214.3701 - val_loss: 220.0898\n",
      "Epoch 576/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 208.1906 - val_loss: 221.7502\n",
      "Epoch 577/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 211.6328 - val_loss: 223.4314\n",
      "Epoch 578/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 208.5275 - val_loss: 224.9093\n",
      "Epoch 579/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 218.8056 - val_loss: 226.1775\n",
      "Epoch 580/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 205.6023 - val_loss: 226.9672\n",
      "Epoch 581/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 206.5557 - val_loss: 228.3215\n",
      "Epoch 582/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 204.9375 - val_loss: 229.9316\n",
      "Epoch 583/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 204.2014 - val_loss: 230.6011\n",
      "Epoch 584/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 204.1957 - val_loss: 232.1829\n",
      "Epoch 585/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 206.7328 - val_loss: 233.6931\n",
      "Epoch 586/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 202.9323 - val_loss: 234.9360\n",
      "Epoch 587/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 211.8940 - val_loss: 235.7469\n",
      "Epoch 588/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 209.0337 - val_loss: 235.9674\n",
      "Epoch 589/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 202.8639 - val_loss: 236.7471\n",
      "Epoch 590/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 201.9725 - val_loss: 239.0877\n",
      "Epoch 591/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 205.1605 - val_loss: 239.2196\n",
      "Epoch 592/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 204.4862 - val_loss: 239.9176\n",
      "Epoch 593/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 203.3629 - val_loss: 240.3592\n",
      "Epoch 594/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 203.4535 - val_loss: 241.3260\n",
      "Epoch 595/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 199.6782 - val_loss: 242.1586\n",
      "Epoch 596/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 203.3194 - val_loss: 242.4774\n",
      "Epoch 597/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 201.4196 - val_loss: 242.5410\n",
      "Epoch 598/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 200.1832 - val_loss: 242.7423\n",
      "Epoch 599/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 195.7621 - val_loss: 242.5413\n",
      "Epoch 600/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 197.5039 - val_loss: 243.0612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 195.9973 - val_loss: 241.6939\n",
      "Epoch 602/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 195.1185 - val_loss: 242.0318\n",
      "Epoch 603/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 197.5129 - val_loss: 242.7992\n",
      "Epoch 604/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 198.0263 - val_loss: 243.1244\n",
      "Epoch 605/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 197.1979 - val_loss: 243.4956\n",
      "Epoch 606/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 195.3220 - val_loss: 243.4562\n",
      "Epoch 607/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 189.8853 - val_loss: 243.9957\n",
      "Epoch 608/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 192.3356 - val_loss: 245.0779\n",
      "Epoch 609/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 198.8262 - val_loss: 245.1820\n",
      "Epoch 610/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 191.9235 - val_loss: 244.4236\n",
      "Epoch 611/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 192.4508 - val_loss: 244.3595\n",
      "Epoch 612/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 193.7849 - val_loss: 243.8462\n",
      "Epoch 613/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 193.7587 - val_loss: 243.0950\n",
      "Epoch 614/1200\n",
      "378/378 [==============================] - 0s 78us/step - loss: 190.0576 - val_loss: 243.4058\n",
      "Epoch 615/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 195.2442 - val_loss: 243.6382\n",
      "Epoch 616/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 192.6892 - val_loss: 243.7492\n",
      "Epoch 617/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 189.4215 - val_loss: 242.6978\n",
      "Epoch 618/1200\n",
      "378/378 [==============================] - 0s 79us/step - loss: 185.9324 - val_loss: 242.8008\n",
      "Epoch 619/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 187.6758 - val_loss: 242.9199\n",
      "Epoch 620/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 186.2221 - val_loss: 243.4993\n",
      "Epoch 621/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 186.0156 - val_loss: 244.2964\n",
      "Epoch 622/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 189.3331 - val_loss: 245.2770\n",
      "Epoch 623/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 184.3195 - val_loss: 244.6338\n",
      "Epoch 624/1200\n",
      "378/378 [==============================] - 0s 51us/step - loss: 183.7228 - val_loss: 243.9304\n",
      "Epoch 625/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 180.3069 - val_loss: 243.7771\n",
      "Epoch 626/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 181.1237 - val_loss: 242.9780\n",
      "Epoch 627/1200\n",
      "378/378 [==============================] - 0s 79us/step - loss: 179.6031 - val_loss: 242.0718\n",
      "Epoch 628/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 181.1433 - val_loss: 242.0641\n",
      "Epoch 629/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 180.1768 - val_loss: 242.7116\n",
      "Epoch 630/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 178.1166 - val_loss: 243.0739\n",
      "Epoch 631/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 175.7965 - val_loss: 242.6964\n",
      "Epoch 632/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 179.4693 - val_loss: 242.5552\n",
      "Epoch 633/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 180.1411 - val_loss: 241.5699\n",
      "Epoch 634/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 175.2368 - val_loss: 241.1550\n",
      "Epoch 635/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 176.0284 - val_loss: 238.7627\n",
      "Epoch 636/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 178.7996 - val_loss: 238.6074\n",
      "Epoch 637/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 179.3217 - val_loss: 237.7309\n",
      "Epoch 638/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 179.6539 - val_loss: 237.2565\n",
      "Epoch 639/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 173.2213 - val_loss: 236.1539\n",
      "Epoch 640/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 176.0288 - val_loss: 234.4307\n",
      "Epoch 641/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 177.8064 - val_loss: 234.3690\n",
      "Epoch 642/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 174.9536 - val_loss: 234.9365\n",
      "Epoch 643/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 172.1024 - val_loss: 234.7824\n",
      "Epoch 644/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 173.5627 - val_loss: 235.9786\n",
      "Epoch 645/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 173.4061 - val_loss: 236.7243\n",
      "Epoch 646/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 173.9229 - val_loss: 235.6862\n",
      "Epoch 647/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 173.0245 - val_loss: 234.9280\n",
      "Epoch 648/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 168.9269 - val_loss: 235.0493\n",
      "Epoch 649/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 175.3446 - val_loss: 234.5092\n",
      "Epoch 650/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 177.0982 - val_loss: 235.9198\n",
      "Epoch 651/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 167.6704 - val_loss: 237.3663\n",
      "Epoch 652/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 168.9260 - val_loss: 236.3349\n",
      "Epoch 653/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 170.1194 - val_loss: 236.6945\n",
      "Epoch 654/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 171.5320 - val_loss: 236.7324\n",
      "Epoch 655/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 168.2519 - val_loss: 236.6098\n",
      "Epoch 656/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 163.9971 - val_loss: 237.1004\n",
      "Epoch 657/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 168.4349 - val_loss: 236.1774\n",
      "Epoch 658/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 168.6287 - val_loss: 234.4012\n",
      "Epoch 659/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 166.2158 - val_loss: 234.6311\n",
      "Epoch 660/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 169.2164 - val_loss: 234.6092\n",
      "Epoch 661/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 168.0919 - val_loss: 232.7108\n",
      "Epoch 662/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 167.1969 - val_loss: 231.2064\n",
      "Epoch 663/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 166.6850 - val_loss: 229.6101\n",
      "Epoch 664/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 164.5075 - val_loss: 228.5970\n",
      "Epoch 665/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 164.0680 - val_loss: 227.7894\n",
      "Epoch 666/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 165.3534 - val_loss: 227.7605\n",
      "Epoch 667/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 163.5956 - val_loss: 227.5131\n",
      "Epoch 668/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 161.6299 - val_loss: 226.8069\n",
      "Epoch 669/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 164.6308 - val_loss: 227.8913\n",
      "Epoch 670/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 158.7477 - val_loss: 227.7778\n",
      "Epoch 671/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 159.9530 - val_loss: 226.9908\n",
      "Epoch 672/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 162.7071 - val_loss: 226.2670\n",
      "Epoch 673/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 160.1986 - val_loss: 225.6940\n",
      "Epoch 674/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 163.2978 - val_loss: 223.0995\n",
      "Epoch 675/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 161.2669 - val_loss: 221.4327\n",
      "Epoch 676/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 158.5259 - val_loss: 220.1924\n",
      "Epoch 677/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 158.8467 - val_loss: 219.2359\n",
      "Epoch 678/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 159.5054 - val_loss: 217.6012\n",
      "Epoch 679/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 157.9430 - val_loss: 214.9707\n",
      "Epoch 680/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 160.3855 - val_loss: 213.9414\n",
      "Epoch 681/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 161.1648 - val_loss: 213.0541\n",
      "Epoch 682/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 157.7600 - val_loss: 212.9528\n",
      "Epoch 683/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 158.6735 - val_loss: 212.2302\n",
      "Epoch 684/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 154.6262 - val_loss: 210.2572\n",
      "Epoch 685/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 159.1810 - val_loss: 207.8989\n",
      "Epoch 686/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 159.8533 - val_loss: 207.1895\n",
      "Epoch 687/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 152.6644 - val_loss: 206.6079\n",
      "Epoch 688/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 157.0951 - val_loss: 205.3884\n",
      "Epoch 689/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 153.6622 - val_loss: 204.3981\n",
      "Epoch 690/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 155.7728 - val_loss: 203.0806\n",
      "Epoch 691/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 154.7852 - val_loss: 202.3855\n",
      "Epoch 692/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 151.5315 - val_loss: 201.8306\n",
      "Epoch 693/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 157.0889 - val_loss: 202.1298\n",
      "Epoch 694/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 153.8358 - val_loss: 202.6204\n",
      "Epoch 695/1200\n",
      "378/378 [==============================] - 0s 52us/step - loss: 153.4633 - val_loss: 201.7104\n",
      "Epoch 696/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 153.2063 - val_loss: 200.6529\n",
      "Epoch 697/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 150.7382 - val_loss: 199.4013\n",
      "Epoch 698/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 157.2745 - val_loss: 197.9119\n",
      "Epoch 699/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 157.3307 - val_loss: 197.5428\n",
      "Epoch 700/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 148.2170 - val_loss: 197.5419\n",
      "Epoch 701/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 151.0050 - val_loss: 197.2745\n",
      "Epoch 702/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 152.3732 - val_loss: 196.9582\n",
      "Epoch 703/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 150.6923 - val_loss: 195.9592\n",
      "Epoch 704/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 151.2024 - val_loss: 194.7573\n",
      "Epoch 705/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 152.6755 - val_loss: 193.0141\n",
      "Epoch 706/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 151.7177 - val_loss: 192.3673\n",
      "Epoch 707/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 151.3651 - val_loss: 192.4992\n",
      "Epoch 708/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 152.3180 - val_loss: 192.8674\n",
      "Epoch 709/1200\n",
      "378/378 [==============================] - 0s 79us/step - loss: 149.4251 - val_loss: 193.3199\n",
      "Epoch 710/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 148.1630 - val_loss: 194.2223\n",
      "Epoch 711/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 153.6444 - val_loss: 193.6177\n",
      "Epoch 712/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 150.5719 - val_loss: 192.9452\n",
      "Epoch 713/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 153.1378 - val_loss: 192.5272\n",
      "Epoch 714/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 148.9066 - val_loss: 192.0422\n",
      "Epoch 715/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 145.7555 - val_loss: 191.4953\n",
      "Epoch 716/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 152.7491 - val_loss: 190.5401\n",
      "Epoch 717/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 150.1365 - val_loss: 189.4533\n",
      "Epoch 718/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 148.8939 - val_loss: 188.2559\n",
      "Epoch 719/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 148.8660 - val_loss: 187.2530\n",
      "Epoch 720/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 148.3429 - val_loss: 186.1860\n",
      "Epoch 721/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 150.4488 - val_loss: 186.1077\n",
      "Epoch 722/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 146.9487 - val_loss: 186.6531\n",
      "Epoch 723/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 146.7532 - val_loss: 187.5184\n",
      "Epoch 724/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 151.0958 - val_loss: 186.9352\n",
      "Epoch 725/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 147.6215 - val_loss: 186.4688\n",
      "Epoch 726/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 149.6610 - val_loss: 186.9190\n",
      "Epoch 727/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 146.1087 - val_loss: 185.9863\n",
      "Epoch 728/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 143.9398 - val_loss: 184.7672\n",
      "Epoch 729/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 146.4537 - val_loss: 184.4714\n",
      "Epoch 730/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 143.2209 - val_loss: 183.8040\n",
      "Epoch 731/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 145.7290 - val_loss: 183.6733\n",
      "Epoch 732/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 146.3727 - val_loss: 183.5773\n",
      "Epoch 733/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 144.8219 - val_loss: 183.9585\n",
      "Epoch 734/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 144.1730 - val_loss: 183.6287\n",
      "Epoch 735/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 147.2942 - val_loss: 183.8494\n",
      "Epoch 736/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 143.5654 - val_loss: 184.6987\n",
      "Epoch 737/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 143.1213 - val_loss: 184.3667\n",
      "Epoch 738/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 143.8666 - val_loss: 183.3003\n",
      "Epoch 739/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 144.5787 - val_loss: 182.4760\n",
      "Epoch 740/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 144.7622 - val_loss: 182.4444\n",
      "Epoch 741/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 145.0642 - val_loss: 182.3486\n",
      "Epoch 742/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 144.3585 - val_loss: 181.7574\n",
      "Epoch 743/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 145.2552 - val_loss: 181.4497\n",
      "Epoch 744/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 145.3561 - val_loss: 181.3811\n",
      "Epoch 745/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 145.2511 - val_loss: 181.7713\n",
      "Epoch 746/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 144.8288 - val_loss: 181.0977\n",
      "Epoch 747/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 141.8114 - val_loss: 180.3123\n",
      "Epoch 748/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 140.9040 - val_loss: 179.3858\n",
      "Epoch 749/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 140.1733 - val_loss: 179.6457\n",
      "Epoch 750/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 143.5257 - val_loss: 179.4301\n",
      "Epoch 751/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 69us/step - loss: 143.8401 - val_loss: 179.4809\n",
      "Epoch 752/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 141.1121 - val_loss: 179.0129\n",
      "Epoch 753/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 141.8423 - val_loss: 177.7560\n",
      "Epoch 754/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 142.8218 - val_loss: 176.9243\n",
      "Epoch 755/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 140.0450 - val_loss: 177.1064\n",
      "Epoch 756/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 140.6023 - val_loss: 177.6994\n",
      "Epoch 757/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 144.2093 - val_loss: 178.8064\n",
      "Epoch 758/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 141.0330 - val_loss: 178.9154\n",
      "Epoch 759/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 144.4634 - val_loss: 176.8519\n",
      "Epoch 760/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 146.4589 - val_loss: 175.5351\n",
      "Epoch 761/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 141.7761 - val_loss: 175.8496\n",
      "Epoch 762/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 139.6084 - val_loss: 176.4510\n",
      "Epoch 763/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 139.8358 - val_loss: 175.8519\n",
      "Epoch 764/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 140.0344 - val_loss: 175.6215\n",
      "Epoch 765/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 137.9287 - val_loss: 175.5654\n",
      "Epoch 766/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 139.2681 - val_loss: 174.9790\n",
      "Epoch 767/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 142.0230 - val_loss: 174.4569\n",
      "Epoch 768/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 140.2599 - val_loss: 174.5758\n",
      "Epoch 769/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 139.1051 - val_loss: 174.3299\n",
      "Epoch 770/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 137.7895 - val_loss: 174.6840\n",
      "Epoch 771/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 139.7565 - val_loss: 175.0588\n",
      "Epoch 772/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 141.5597 - val_loss: 175.0508\n",
      "Epoch 773/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 139.1210 - val_loss: 174.5455\n",
      "Epoch 774/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 140.4842 - val_loss: 173.5203\n",
      "Epoch 775/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 136.9281 - val_loss: 172.4058\n",
      "Epoch 776/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 138.8643 - val_loss: 172.9651\n",
      "Epoch 777/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 138.5246 - val_loss: 172.2131\n",
      "Epoch 778/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 138.8800 - val_loss: 172.2216\n",
      "Epoch 779/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 138.4455 - val_loss: 171.6069\n",
      "Epoch 780/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 137.4735 - val_loss: 171.1392\n",
      "Epoch 781/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 136.2270 - val_loss: 170.7721\n",
      "Epoch 782/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 138.1455 - val_loss: 170.6064\n",
      "Epoch 783/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 137.1827 - val_loss: 171.1831\n",
      "Epoch 784/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 135.7467 - val_loss: 171.2076\n",
      "Epoch 785/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 136.6911 - val_loss: 171.1630\n",
      "Epoch 786/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 137.8327 - val_loss: 171.7567\n",
      "Epoch 787/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 139.2762 - val_loss: 172.3546\n",
      "Epoch 788/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 138.8020 - val_loss: 171.9857\n",
      "Epoch 789/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 138.6402 - val_loss: 170.9615\n",
      "Epoch 790/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 135.6503 - val_loss: 170.1065\n",
      "Epoch 791/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 137.0015 - val_loss: 170.2722\n",
      "Epoch 792/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 136.2344 - val_loss: 170.1425\n",
      "Epoch 793/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 137.3588 - val_loss: 170.4070\n",
      "Epoch 794/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 137.0272 - val_loss: 170.3405\n",
      "Epoch 795/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 136.4537 - val_loss: 170.1880\n",
      "Epoch 796/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 137.7187 - val_loss: 169.7342\n",
      "Epoch 797/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 134.4527 - val_loss: 169.1494\n",
      "Epoch 798/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 135.8206 - val_loss: 168.3596\n",
      "Epoch 799/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 137.3473 - val_loss: 167.9329\n",
      "Epoch 800/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 136.3956 - val_loss: 167.2803\n",
      "Epoch 801/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 136.3363 - val_loss: 166.7982\n",
      "Epoch 802/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 136.0319 - val_loss: 166.4483\n",
      "Epoch 803/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 135.5772 - val_loss: 166.0474\n",
      "Epoch 804/1200\n",
      "378/378 [==============================] - 0s 78us/step - loss: 135.8448 - val_loss: 165.6126\n",
      "Epoch 805/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 134.7774 - val_loss: 165.5255\n",
      "Epoch 806/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 134.1467 - val_loss: 165.8263\n",
      "Epoch 807/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 135.8361 - val_loss: 165.5724\n",
      "Epoch 808/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 136.0467 - val_loss: 165.9037\n",
      "Epoch 809/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 135.1740 - val_loss: 165.8490\n",
      "Epoch 810/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 138.1573 - val_loss: 165.0800\n",
      "Epoch 811/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 136.4180 - val_loss: 164.9756\n",
      "Epoch 812/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 134.9587 - val_loss: 165.1034\n",
      "Epoch 813/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 135.2200 - val_loss: 165.7284\n",
      "Epoch 814/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 134.9682 - val_loss: 166.3200\n",
      "Epoch 815/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 135.8329 - val_loss: 165.4559\n",
      "Epoch 816/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 134.8951 - val_loss: 164.6642\n",
      "Epoch 817/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 136.6579 - val_loss: 164.4440\n",
      "Epoch 818/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 134.6208 - val_loss: 163.3258\n",
      "Epoch 819/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 135.1129 - val_loss: 162.5293\n",
      "Epoch 820/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 136.7295 - val_loss: 162.4623\n",
      "Epoch 821/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 135.0577 - val_loss: 163.2668\n",
      "Epoch 822/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 133.6490 - val_loss: 164.0032\n",
      "Epoch 823/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 136.0053 - val_loss: 163.2380\n",
      "Epoch 824/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 134.1019 - val_loss: 162.8740\n",
      "Epoch 825/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 136.5848 - val_loss: 163.2163\n",
      "Epoch 826/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 133.4340 - val_loss: 163.5679\n",
      "Epoch 827/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 132.9364 - val_loss: 164.2094\n",
      "Epoch 828/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 134.4686 - val_loss: 164.3922\n",
      "Epoch 829/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 133.8895 - val_loss: 165.1646\n",
      "Epoch 830/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 135.2417 - val_loss: 164.4995\n",
      "Epoch 831/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 132.2832 - val_loss: 163.9185\n",
      "Epoch 832/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 134.4231 - val_loss: 163.2450\n",
      "Epoch 833/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 133.2750 - val_loss: 163.5583\n",
      "Epoch 834/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 133.6249 - val_loss: 163.1486\n",
      "Epoch 835/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 132.5286 - val_loss: 163.3526\n",
      "Epoch 836/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 133.2679 - val_loss: 162.2023\n",
      "Epoch 837/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 133.0597 - val_loss: 162.5375\n",
      "Epoch 838/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 133.9539 - val_loss: 162.2555\n",
      "Epoch 839/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 130.7939 - val_loss: 162.3588\n",
      "Epoch 840/1200\n",
      "378/378 [==============================] - 0s 87us/step - loss: 131.4249 - val_loss: 162.3647\n",
      "Epoch 841/1200\n",
      "378/378 [==============================] - 0s 80us/step - loss: 134.4587 - val_loss: 162.4073\n",
      "Epoch 842/1200\n",
      "378/378 [==============================] - 0s 78us/step - loss: 135.0172 - val_loss: 162.2071\n",
      "Epoch 843/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 130.9369 - val_loss: 162.6028\n",
      "Epoch 844/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 134.2619 - val_loss: 162.2492\n",
      "Epoch 845/1200\n",
      "378/378 [==============================] - 0s 82us/step - loss: 131.7573 - val_loss: 160.9472\n",
      "Epoch 846/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 131.6004 - val_loss: 160.3967\n",
      "Epoch 847/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 131.6150 - val_loss: 160.5842\n",
      "Epoch 848/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 132.5035 - val_loss: 160.0566\n",
      "Epoch 849/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 131.5038 - val_loss: 158.8229\n",
      "Epoch 850/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 131.1709 - val_loss: 158.5264\n",
      "Epoch 851/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 131.6211 - val_loss: 158.9377\n",
      "Epoch 852/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 130.2321 - val_loss: 159.2492\n",
      "Epoch 853/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 132.1052 - val_loss: 159.4514\n",
      "Epoch 854/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 130.9416 - val_loss: 159.7313\n",
      "Epoch 855/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 130.7010 - val_loss: 159.5690\n",
      "Epoch 856/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 131.6069 - val_loss: 159.9344\n",
      "Epoch 857/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 133.2962 - val_loss: 159.8696\n",
      "Epoch 858/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 132.0289 - val_loss: 159.1838\n",
      "Epoch 859/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 132.5117 - val_loss: 159.1567\n",
      "Epoch 860/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 131.1330 - val_loss: 159.2469\n",
      "Epoch 861/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 130.5864 - val_loss: 159.2647\n",
      "Epoch 862/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 131.1464 - val_loss: 158.8706\n",
      "Epoch 863/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 131.0279 - val_loss: 159.0346\n",
      "Epoch 864/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 129.5429 - val_loss: 160.0973\n",
      "Epoch 865/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 130.4594 - val_loss: 160.3001\n",
      "Epoch 866/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 133.4923 - val_loss: 160.2780\n",
      "Epoch 867/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 131.7346 - val_loss: 160.7310\n",
      "Epoch 868/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 131.6398 - val_loss: 161.0130\n",
      "Epoch 869/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 132.3916 - val_loss: 161.0715\n",
      "Epoch 870/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 129.4819 - val_loss: 160.7440\n",
      "Epoch 871/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 129.2039 - val_loss: 159.7281\n",
      "Epoch 872/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 130.3301 - val_loss: 159.3034\n",
      "Epoch 873/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 131.0825 - val_loss: 159.6076\n",
      "Epoch 874/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 130.8190 - val_loss: 159.0013\n",
      "Epoch 875/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 130.7394 - val_loss: 157.5516\n",
      "Epoch 876/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 130.4008 - val_loss: 156.9249\n",
      "Epoch 877/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 130.1884 - val_loss: 156.7328\n",
      "Epoch 878/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 130.3544 - val_loss: 156.5733\n",
      "Epoch 879/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 130.1997 - val_loss: 156.7648\n",
      "Epoch 880/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 130.4403 - val_loss: 156.3386\n",
      "Epoch 881/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 131.3151 - val_loss: 156.8675\n",
      "Epoch 882/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 129.1871 - val_loss: 157.4668\n",
      "Epoch 883/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 129.5574 - val_loss: 157.2483\n",
      "Epoch 884/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 131.9258 - val_loss: 156.6321\n",
      "Epoch 885/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 129.5895 - val_loss: 156.2793\n",
      "Epoch 886/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 130.7291 - val_loss: 156.2003\n",
      "Epoch 887/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 130.3155 - val_loss: 156.1383\n",
      "Epoch 888/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 130.3686 - val_loss: 155.9041\n",
      "Epoch 889/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 128.8791 - val_loss: 156.2629\n",
      "Epoch 890/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 130.8342 - val_loss: 156.0195\n",
      "Epoch 891/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 129.9477 - val_loss: 156.0692\n",
      "Epoch 892/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 131.0748 - val_loss: 156.4693\n",
      "Epoch 893/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 130.6206 - val_loss: 156.3341\n",
      "Epoch 894/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 130.6677 - val_loss: 156.9917\n",
      "Epoch 895/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 131.8553 - val_loss: 157.2199\n",
      "Epoch 896/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 130.5403 - val_loss: 157.4063\n",
      "Epoch 897/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 130.5535 - val_loss: 156.8760\n",
      "Epoch 898/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 127.7939 - val_loss: 156.8758\n",
      "Epoch 899/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 128.4945 - val_loss: 156.9681\n",
      "Epoch 900/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 127.5150 - val_loss: 157.3256\n",
      "Epoch 901/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 58us/step - loss: 131.2487 - val_loss: 157.5953\n",
      "Epoch 902/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 128.8343 - val_loss: 157.2367\n",
      "Epoch 903/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 128.8887 - val_loss: 157.4135\n",
      "Epoch 904/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 129.7206 - val_loss: 157.2423\n",
      "Epoch 905/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 128.5323 - val_loss: 156.6119\n",
      "Epoch 906/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 128.8117 - val_loss: 156.1647\n",
      "Epoch 907/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 128.3200 - val_loss: 157.0516\n",
      "Epoch 908/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 130.3164 - val_loss: 156.7784\n",
      "Epoch 909/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 128.7272 - val_loss: 156.5041\n",
      "Epoch 910/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 128.8271 - val_loss: 156.3484\n",
      "Epoch 911/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 127.6824 - val_loss: 156.9482\n",
      "Epoch 912/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 129.9799 - val_loss: 157.0935\n",
      "Epoch 913/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 128.2463 - val_loss: 156.1992\n",
      "Epoch 914/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 131.2380 - val_loss: 155.3351\n",
      "Epoch 915/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 129.1707 - val_loss: 155.2748\n",
      "Epoch 916/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 127.7685 - val_loss: 154.7493\n",
      "Epoch 917/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 127.6567 - val_loss: 154.1366\n",
      "Epoch 918/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 128.6598 - val_loss: 154.1129\n",
      "Epoch 919/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 129.4885 - val_loss: 154.1898\n",
      "Epoch 920/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 127.5593 - val_loss: 154.3059\n",
      "Epoch 921/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 128.1186 - val_loss: 153.8693\n",
      "Epoch 922/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 128.9019 - val_loss: 153.5358\n",
      "Epoch 923/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 127.5620 - val_loss: 153.7766\n",
      "Epoch 924/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 129.1548 - val_loss: 153.3082\n",
      "Epoch 925/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 127.9547 - val_loss: 152.3608\n",
      "Epoch 926/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 127.5437 - val_loss: 152.5468\n",
      "Epoch 927/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 128.5390 - val_loss: 152.8978\n",
      "Epoch 928/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 128.2063 - val_loss: 153.5379\n",
      "Epoch 929/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 127.8936 - val_loss: 153.6628\n",
      "Epoch 930/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 127.9632 - val_loss: 154.2535\n",
      "Epoch 931/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 128.1765 - val_loss: 154.4550\n",
      "Epoch 932/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 130.0914 - val_loss: 153.8461\n",
      "Epoch 933/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 128.3882 - val_loss: 152.6416\n",
      "Epoch 934/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 127.2880 - val_loss: 152.4544\n",
      "Epoch 935/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 128.9314 - val_loss: 152.0166\n",
      "Epoch 936/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 126.4801 - val_loss: 152.0659\n",
      "Epoch 937/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 128.5911 - val_loss: 152.8876\n",
      "Epoch 938/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 126.6981 - val_loss: 153.0374\n",
      "Epoch 939/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 127.3647 - val_loss: 152.8522\n",
      "Epoch 940/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 127.0446 - val_loss: 152.5354\n",
      "Epoch 941/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 127.6030 - val_loss: 152.1489\n",
      "Epoch 942/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 126.1031 - val_loss: 152.6468\n",
      "Epoch 943/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 128.1259 - val_loss: 154.0173\n",
      "Epoch 944/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 127.2130 - val_loss: 154.0038\n",
      "Epoch 945/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 126.1714 - val_loss: 152.9046\n",
      "Epoch 946/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 126.6311 - val_loss: 152.8725\n",
      "Epoch 947/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 126.8969 - val_loss: 150.9767\n",
      "Epoch 948/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 126.5032 - val_loss: 150.2013\n",
      "Epoch 949/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 126.6715 - val_loss: 150.0471\n",
      "Epoch 950/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 127.0039 - val_loss: 149.4912\n",
      "Epoch 951/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 127.9842 - val_loss: 149.5528\n",
      "Epoch 952/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 125.7486 - val_loss: 149.2142\n",
      "Epoch 953/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 127.2022 - val_loss: 149.0744\n",
      "Epoch 954/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 127.4781 - val_loss: 148.9446\n",
      "Epoch 955/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 126.7652 - val_loss: 149.2284\n",
      "Epoch 956/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 127.2799 - val_loss: 149.2225\n",
      "Epoch 957/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 125.1230 - val_loss: 149.2863\n",
      "Epoch 958/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 127.9160 - val_loss: 149.4180\n",
      "Epoch 959/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 127.0718 - val_loss: 148.1465\n",
      "Epoch 960/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 128.1339 - val_loss: 147.2237\n",
      "Epoch 961/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 126.1879 - val_loss: 146.8790\n",
      "Epoch 962/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 127.1380 - val_loss: 147.2160\n",
      "Epoch 963/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 127.2084 - val_loss: 147.1170\n",
      "Epoch 964/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 127.8861 - val_loss: 146.6337\n",
      "Epoch 965/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 127.8015 - val_loss: 146.2334\n",
      "Epoch 966/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 125.6712 - val_loss: 146.9646\n",
      "Epoch 967/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 127.5766 - val_loss: 146.8703\n",
      "Epoch 968/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 125.0147 - val_loss: 146.2903\n",
      "Epoch 969/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 126.9131 - val_loss: 145.3401\n",
      "Epoch 970/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 127.2043 - val_loss: 144.6399\n",
      "Epoch 971/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 126.6159 - val_loss: 144.4159\n",
      "Epoch 972/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 127.1975 - val_loss: 145.3588\n",
      "Epoch 973/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 126.0637 - val_loss: 146.2059\n",
      "Epoch 974/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 126.3816 - val_loss: 146.7078\n",
      "Epoch 975/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 125.8872 - val_loss: 147.1731\n",
      "Epoch 976/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 126.1502 - val_loss: 147.3619\n",
      "Epoch 977/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 125.5858 - val_loss: 146.6092\n",
      "Epoch 978/1200\n",
      "378/378 [==============================] - ETA: 0s - loss: 122.103 - 0s 57us/step - loss: 126.1857 - val_loss: 145.7113\n",
      "Epoch 979/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 126.3411 - val_loss: 145.3653\n",
      "Epoch 980/1200\n",
      "378/378 [==============================] - ETA: 0s - loss: 134.913 - 0s 69us/step - loss: 123.7892 - val_loss: 145.1129\n",
      "Epoch 981/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 125.5321 - val_loss: 145.1229\n",
      "Epoch 982/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 126.5754 - val_loss: 144.7718\n",
      "Epoch 983/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 127.0344 - val_loss: 144.0724\n",
      "Epoch 984/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 126.2336 - val_loss: 144.6437\n",
      "Epoch 985/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 126.7124 - val_loss: 145.0506\n",
      "Epoch 986/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 126.2528 - val_loss: 145.2518\n",
      "Epoch 987/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 126.6039 - val_loss: 145.3619\n",
      "Epoch 988/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 125.8369 - val_loss: 144.8830\n",
      "Epoch 989/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 127.1443 - val_loss: 144.8916\n",
      "Epoch 990/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 126.5094 - val_loss: 144.4866\n",
      "Epoch 991/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 126.9209 - val_loss: 143.9473\n",
      "Epoch 992/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 127.1186 - val_loss: 143.6302\n",
      "Epoch 993/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 125.5730 - val_loss: 143.9544\n",
      "Epoch 994/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 125.2005 - val_loss: 144.5036\n",
      "Epoch 995/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 126.6467 - val_loss: 143.9855\n",
      "Epoch 996/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 125.3442 - val_loss: 143.5151\n",
      "Epoch 997/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 125.6385 - val_loss: 143.8391\n",
      "Epoch 998/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 125.4538 - val_loss: 144.2088\n",
      "Epoch 999/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 125.7979 - val_loss: 144.2828\n",
      "Epoch 1000/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 125.4258 - val_loss: 144.5204\n",
      "Epoch 1001/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 125.3692 - val_loss: 144.3916\n",
      "Epoch 1002/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 125.5780 - val_loss: 144.4352\n",
      "Epoch 1003/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 126.3222 - val_loss: 144.3075\n",
      "Epoch 1004/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 125.5364 - val_loss: 144.0059\n",
      "Epoch 1005/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 124.8725 - val_loss: 144.1241\n",
      "Epoch 1006/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 126.6605 - val_loss: 144.0738\n",
      "Epoch 1007/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 126.2814 - val_loss: 143.8156\n",
      "Epoch 1008/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 126.2741 - val_loss: 143.7790\n",
      "Epoch 1009/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 125.2282 - val_loss: 143.4773\n",
      "Epoch 1010/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 126.5146 - val_loss: 143.5982\n",
      "Epoch 1011/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 126.1244 - val_loss: 143.8826\n",
      "Epoch 1012/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 126.4758 - val_loss: 143.0919\n",
      "Epoch 1013/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 124.4582 - val_loss: 142.8338\n",
      "Epoch 1014/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 124.8152 - val_loss: 143.4398\n",
      "Epoch 1015/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 124.8134 - val_loss: 143.5744\n",
      "Epoch 1016/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 125.6625 - val_loss: 143.5286\n",
      "Epoch 1017/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 126.2342 - val_loss: 143.2448\n",
      "Epoch 1018/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 125.4006 - val_loss: 143.3668\n",
      "Epoch 1019/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 127.1338 - val_loss: 143.2800\n",
      "Epoch 1020/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 125.6276 - val_loss: 143.5518\n",
      "Epoch 1021/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 126.6615 - val_loss: 143.8728\n",
      "Epoch 1022/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 125.1962 - val_loss: 144.1974\n",
      "Epoch 1023/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 125.6387 - val_loss: 144.1856\n",
      "Epoch 1024/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 125.0263 - val_loss: 143.7612\n",
      "Epoch 1025/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 125.8463 - val_loss: 143.3102\n",
      "Epoch 1026/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 124.8110 - val_loss: 142.9511\n",
      "Epoch 1027/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 125.7099 - val_loss: 143.6474\n",
      "Epoch 1028/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 124.9759 - val_loss: 143.6543\n",
      "Epoch 1029/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 125.4368 - val_loss: 143.3408\n",
      "Epoch 1030/1200\n",
      "378/378 [==============================] - 0s 80us/step - loss: 124.8783 - val_loss: 143.0277\n",
      "Epoch 1031/1200\n",
      "378/378 [==============================] - 0s 85us/step - loss: 125.3796 - val_loss: 143.0148\n",
      "Epoch 1032/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 125.8672 - val_loss: 143.0869\n",
      "Epoch 1033/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 124.1521 - val_loss: 143.1710\n",
      "Epoch 1034/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 125.8218 - val_loss: 142.9893\n",
      "Epoch 1035/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 125.8189 - val_loss: 142.9500\n",
      "Epoch 1036/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 123.2360 - val_loss: 143.3508\n",
      "Epoch 1037/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 124.8874 - val_loss: 143.4866\n",
      "Epoch 1038/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 126.1318 - val_loss: 143.0448\n",
      "Epoch 1039/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 125.5162 - val_loss: 142.7715\n",
      "Epoch 1040/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 125.2554 - val_loss: 142.4538\n",
      "Epoch 1041/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 124.8394 - val_loss: 142.2251\n",
      "Epoch 1042/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 125.5604 - val_loss: 142.2302\n",
      "Epoch 1043/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 125.5776 - val_loss: 142.2683\n",
      "Epoch 1044/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 125.6382 - val_loss: 142.5031\n",
      "Epoch 1045/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 124.6138 - val_loss: 142.9105\n",
      "Epoch 1046/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 124.8469 - val_loss: 142.7384\n",
      "Epoch 1047/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 124.1700 - val_loss: 142.6798\n",
      "Epoch 1048/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 125.3593 - val_loss: 142.2424\n",
      "Epoch 1049/1200\n",
      "378/378 [==============================] - 0s 76us/step - loss: 125.8228 - val_loss: 142.2871\n",
      "Epoch 1050/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 60us/step - loss: 125.2956 - val_loss: 142.1063\n",
      "Epoch 1051/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 124.9575 - val_loss: 142.0573\n",
      "Epoch 1052/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 125.7557 - val_loss: 142.2309\n",
      "Epoch 1053/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 124.8284 - val_loss: 142.2785\n",
      "Epoch 1054/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 125.0709 - val_loss: 141.6630\n",
      "Epoch 1055/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 124.2905 - val_loss: 141.4887\n",
      "Epoch 1056/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 124.9071 - val_loss: 141.7251\n",
      "Epoch 1057/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 124.4817 - val_loss: 141.7464\n",
      "Epoch 1058/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 124.8999 - val_loss: 142.0220\n",
      "Epoch 1059/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 125.0797 - val_loss: 141.8336\n",
      "Epoch 1060/1200\n",
      "378/378 [==============================] - 0s 78us/step - loss: 125.3316 - val_loss: 141.7489\n",
      "Epoch 1061/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 125.7429 - val_loss: 141.8990\n",
      "Epoch 1062/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 125.6719 - val_loss: 142.2001\n",
      "Epoch 1063/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 124.3309 - val_loss: 142.3553\n",
      "Epoch 1064/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 125.7523 - val_loss: 142.2432\n",
      "Epoch 1065/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 124.6251 - val_loss: 142.1963\n",
      "Epoch 1066/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 123.9430 - val_loss: 142.3044\n",
      "Epoch 1067/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 124.4199 - val_loss: 142.1399\n",
      "Epoch 1068/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 125.3761 - val_loss: 142.1628\n",
      "Epoch 1069/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 125.0975 - val_loss: 142.3098\n",
      "Epoch 1070/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 124.5546 - val_loss: 142.3755\n",
      "Epoch 1071/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 124.9002 - val_loss: 141.8514\n",
      "Epoch 1072/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 125.3088 - val_loss: 142.0386\n",
      "Epoch 1073/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 124.5487 - val_loss: 142.3071\n",
      "Epoch 1074/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 125.1464 - val_loss: 142.0742\n",
      "Epoch 1075/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 123.7956 - val_loss: 142.1123\n",
      "Epoch 1076/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 124.7436 - val_loss: 141.9383\n",
      "Epoch 1077/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 124.5519 - val_loss: 141.7711\n",
      "Epoch 1078/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 124.1448 - val_loss: 141.8183\n",
      "Epoch 1079/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 123.8185 - val_loss: 142.1649\n",
      "Epoch 1080/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 126.6364 - val_loss: 142.1765\n",
      "Epoch 1081/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 124.0935 - val_loss: 141.9420\n",
      "Epoch 1082/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 124.6334 - val_loss: 142.1702\n",
      "Epoch 1083/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 125.0834 - val_loss: 142.0033\n",
      "Epoch 1084/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 123.7256 - val_loss: 141.9453\n",
      "Epoch 1085/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 125.2604 - val_loss: 141.9525\n",
      "Epoch 1086/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 124.8643 - val_loss: 142.4245\n",
      "Epoch 1087/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 125.2002 - val_loss: 142.4776\n",
      "Epoch 1088/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 125.5072 - val_loss: 142.3628\n",
      "Epoch 1089/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 124.4265 - val_loss: 142.0207\n",
      "Epoch 1090/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 124.9546 - val_loss: 141.6657\n",
      "Epoch 1091/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 123.9998 - val_loss: 141.8546\n",
      "Epoch 1092/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 124.8724 - val_loss: 142.0768\n",
      "Epoch 1093/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 124.9349 - val_loss: 142.3022\n",
      "Epoch 1094/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 124.3811 - val_loss: 142.3056\n",
      "Epoch 1095/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 125.0690 - val_loss: 142.1365\n",
      "Epoch 1096/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 124.3582 - val_loss: 141.9137\n",
      "Epoch 1097/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 123.9709 - val_loss: 141.7634\n",
      "Epoch 1098/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 124.6340 - val_loss: 141.8521\n",
      "Epoch 1099/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 124.6444 - val_loss: 142.1182\n",
      "Epoch 1100/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 123.6320 - val_loss: 142.5577\n",
      "Epoch 1101/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 125.8366 - val_loss: 143.1310\n",
      "Epoch 1102/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 125.8013 - val_loss: 142.6554\n",
      "Epoch 1103/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 123.9258 - val_loss: 142.3574\n",
      "Epoch 1104/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 125.2899 - val_loss: 142.3501\n",
      "Epoch 1105/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 123.8346 - val_loss: 142.4392\n",
      "Epoch 1106/1200\n",
      "378/378 [==============================] - 0s 75us/step - loss: 123.6227 - val_loss: 142.3475\n",
      "Epoch 1107/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 125.0517 - val_loss: 142.1999\n",
      "Epoch 1108/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 123.3900 - val_loss: 142.1228\n",
      "Epoch 1109/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 125.4712 - val_loss: 142.4093\n",
      "Epoch 1110/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 123.6124 - val_loss: 142.4463\n",
      "Epoch 1111/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 124.5116 - val_loss: 142.3051\n",
      "Epoch 1112/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 125.1899 - val_loss: 142.0689\n",
      "Epoch 1113/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 124.4681 - val_loss: 141.8438\n",
      "Epoch 1114/1200\n",
      "378/378 [==============================] - 0s 56us/step - loss: 124.2230 - val_loss: 141.8012\n",
      "Epoch 1115/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 123.6500 - val_loss: 141.8051\n",
      "Epoch 1116/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 123.6306 - val_loss: 141.7209\n",
      "Epoch 1117/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 123.4751 - val_loss: 141.8776\n",
      "Epoch 1118/1200\n",
      "378/378 [==============================] - 0s 77us/step - loss: 124.0914 - val_loss: 141.9587\n",
      "Epoch 1119/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 123.7546 - val_loss: 141.7848\n",
      "Epoch 1120/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 125.5402 - val_loss: 141.5519\n",
      "Epoch 1121/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 123.8907 - val_loss: 141.3006\n",
      "Epoch 1122/1200\n",
      "378/378 [==============================] - 0s 53us/step - loss: 123.4812 - val_loss: 141.3282\n",
      "Epoch 1123/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 123.5322 - val_loss: 141.5582\n",
      "Epoch 1124/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 123.4372 - val_loss: 141.9631\n",
      "Epoch 1125/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 124.8126 - val_loss: 142.0176\n",
      "Epoch 1126/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 124.9579 - val_loss: 142.3643\n",
      "Epoch 1127/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 124.5124 - val_loss: 142.6140\n",
      "Epoch 1128/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 124.2519 - val_loss: 142.6917\n",
      "Epoch 1129/1200\n",
      "378/378 [==============================] - 0s 55us/step - loss: 123.5582 - val_loss: 142.2753\n",
      "Epoch 1130/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 124.5353 - val_loss: 142.1007\n",
      "Epoch 1131/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 123.5244 - val_loss: 142.0694\n",
      "Epoch 1132/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 123.2153 - val_loss: 142.2844\n",
      "Epoch 1133/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 122.0192 - val_loss: 142.2773\n",
      "Epoch 1134/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 124.2110 - val_loss: 141.9711\n",
      "Epoch 1135/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 124.2966 - val_loss: 141.9179\n",
      "Epoch 1136/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 123.7421 - val_loss: 142.0258\n",
      "Epoch 1137/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 123.3694 - val_loss: 142.0786\n",
      "Epoch 1138/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 124.0555 - val_loss: 141.9726\n",
      "Epoch 1139/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 123.5390 - val_loss: 142.0078\n",
      "Epoch 1140/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 123.5206 - val_loss: 142.2427\n",
      "Epoch 1141/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 124.2404 - val_loss: 142.2174\n",
      "Epoch 1142/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 122.9523 - val_loss: 141.9530\n",
      "Epoch 1143/1200\n",
      "378/378 [==============================] - 0s 72us/step - loss: 123.7911 - val_loss: 141.8737\n",
      "Epoch 1144/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 122.8854 - val_loss: 141.9570\n",
      "Epoch 1145/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 123.2736 - val_loss: 142.2759\n",
      "Epoch 1146/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 124.1634 - val_loss: 142.3992\n",
      "Epoch 1147/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 124.3880 - val_loss: 142.4955\n",
      "Epoch 1148/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 124.6136 - val_loss: 142.2744\n",
      "Epoch 1149/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 125.1204 - val_loss: 142.1270\n",
      "Epoch 1150/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 123.9992 - val_loss: 141.7508\n",
      "Epoch 1151/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 123.5072 - val_loss: 141.8853\n",
      "Epoch 1152/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 125.3986 - val_loss: 141.7988\n",
      "Epoch 1153/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 123.4952 - val_loss: 141.5593\n",
      "Epoch 1154/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 123.4986 - val_loss: 141.6102\n",
      "Epoch 1155/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 125.2648 - val_loss: 141.6142\n",
      "Epoch 1156/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 123.2657 - val_loss: 141.5289\n",
      "Epoch 1157/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 123.4156 - val_loss: 141.7165\n",
      "Epoch 1158/1200\n",
      "378/378 [==============================] - 0s 71us/step - loss: 125.0243 - val_loss: 141.8695\n",
      "Epoch 1159/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 124.3159 - val_loss: 141.9041\n",
      "Epoch 1160/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 124.6251 - val_loss: 141.9751\n",
      "Epoch 1161/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 124.3438 - val_loss: 141.7704\n",
      "Epoch 1162/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 123.4059 - val_loss: 141.7583\n",
      "Epoch 1163/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 124.5572 - val_loss: 141.7318\n",
      "Epoch 1164/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 123.4743 - val_loss: 141.5799\n",
      "Epoch 1165/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 123.5350 - val_loss: 141.6565\n",
      "Epoch 1166/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 124.2055 - val_loss: 141.5345\n",
      "Epoch 1167/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 122.3096 - val_loss: 141.4560\n",
      "Epoch 1168/1200\n",
      "378/378 [==============================] - 0s 65us/step - loss: 123.0334 - val_loss: 141.5562\n",
      "Epoch 1169/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 123.8410 - val_loss: 141.5947\n",
      "Epoch 1170/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 123.8666 - val_loss: 141.6328\n",
      "Epoch 1171/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 123.5209 - val_loss: 141.3680\n",
      "Epoch 1172/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 123.5970 - val_loss: 141.5636\n",
      "Epoch 1173/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 122.5290 - val_loss: 142.0339\n",
      "Epoch 1174/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 124.3837 - val_loss: 141.7428\n",
      "Epoch 1175/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 122.5497 - val_loss: 141.3935\n",
      "Epoch 1176/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 124.2001 - val_loss: 141.2115\n",
      "Epoch 1177/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 123.4468 - val_loss: 141.7166\n",
      "Epoch 1178/1200\n",
      "378/378 [==============================] - 0s 63us/step - loss: 124.4115 - val_loss: 141.6340\n",
      "Epoch 1179/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 123.0489 - val_loss: 141.4606\n",
      "Epoch 1180/1200\n",
      "378/378 [==============================] - 0s 67us/step - loss: 123.4979 - val_loss: 141.5475\n",
      "Epoch 1181/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 123.6353 - val_loss: 141.7357\n",
      "Epoch 1182/1200\n",
      "378/378 [==============================] - 0s 62us/step - loss: 123.8467 - val_loss: 141.8324\n",
      "Epoch 1183/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 123.0567 - val_loss: 142.1894\n",
      "Epoch 1184/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 123.8287 - val_loss: 142.3512\n",
      "Epoch 1185/1200\n",
      "378/378 [==============================] - 0s 74us/step - loss: 124.9956 - val_loss: 141.8927\n",
      "Epoch 1186/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 124.3703 - val_loss: 141.6691\n",
      "Epoch 1187/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 123.9704 - val_loss: 141.8165\n",
      "Epoch 1188/1200\n",
      "378/378 [==============================] - 0s 64us/step - loss: 124.0630 - val_loss: 141.7483\n",
      "Epoch 1189/1200\n",
      "378/378 [==============================] - 0s 73us/step - loss: 123.8144 - val_loss: 141.0427\n",
      "Epoch 1190/1200\n",
      "378/378 [==============================] - 0s 57us/step - loss: 123.2449 - val_loss: 141.0138\n",
      "Epoch 1191/1200\n",
      "378/378 [==============================] - 0s 66us/step - loss: 124.6767 - val_loss: 141.3843\n",
      "Epoch 1192/1200\n",
      "378/378 [==============================] - 0s 70us/step - loss: 123.3529 - val_loss: 141.7851\n",
      "Epoch 1193/1200\n",
      "378/378 [==============================] - 0s 59us/step - loss: 123.7481 - val_loss: 141.8214\n",
      "Epoch 1194/1200\n",
      "378/378 [==============================] - 0s 61us/step - loss: 123.1579 - val_loss: 141.8627\n",
      "Epoch 1195/1200\n",
      "378/378 [==============================] - 0s 54us/step - loss: 123.7127 - val_loss: 142.1985\n",
      "Epoch 1196/1200\n",
      "378/378 [==============================] - 0s 58us/step - loss: 123.4831 - val_loss: 142.1557\n",
      "Epoch 1197/1200\n",
      "378/378 [==============================] - 0s 68us/step - loss: 123.0605 - val_loss: 141.7553\n",
      "Epoch 1198/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 72us/step - loss: 123.0695 - val_loss: 142.0170\n",
      "Epoch 1199/1200\n",
      "378/378 [==============================] - 0s 69us/step - loss: 123.9528 - val_loss: 141.8256\n",
      "Epoch 1200/1200\n",
      "378/378 [==============================] - 0s 60us/step - loss: 123.4059 - val_loss: 141.9478\n",
      "fridge\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/500\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 88.1316 - val_loss: 95.8144\n",
      "Epoch 2/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 88.0745 - val_loss: 96.2379\n",
      "Epoch 3/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 88.0268 - val_loss: 96.3581\n",
      "Epoch 4/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 87.9702 - val_loss: 96.4108\n",
      "Epoch 5/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 87.9054 - val_loss: 96.4465\n",
      "Epoch 6/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 87.8276 - val_loss: 96.3910\n",
      "Epoch 7/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 87.7534 - val_loss: 96.3494\n",
      "Epoch 8/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 87.6533 - val_loss: 96.2920\n",
      "Epoch 9/500\n",
      "378/378 [==============================] - 0s 74us/step - loss: 87.5450 - val_loss: 96.1829\n",
      "Epoch 10/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 87.4259 - val_loss: 96.0688\n",
      "Epoch 11/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 87.2871 - val_loss: 95.9718\n",
      "Epoch 12/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 87.1367 - val_loss: 95.8511\n",
      "Epoch 13/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 86.9897 - val_loss: 95.7058\n",
      "Epoch 14/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 86.7771 - val_loss: 95.5987\n",
      "Epoch 15/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 86.6218 - val_loss: 95.4846\n",
      "Epoch 16/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 86.4165 - val_loss: 95.3797\n",
      "Epoch 17/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 86.2143 - val_loss: 95.2605\n",
      "Epoch 18/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 85.9352 - val_loss: 95.0916\n",
      "Epoch 19/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 85.7180 - val_loss: 94.9375\n",
      "Epoch 20/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 85.4469 - val_loss: 94.7909\n",
      "Epoch 21/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 85.2078 - val_loss: 94.6392\n",
      "Epoch 22/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 84.9618 - val_loss: 94.4792\n",
      "Epoch 23/500\n",
      "378/378 [==============================] - ETA: 0s - loss: 84.34 - 0s 70us/step - loss: 84.6814 - val_loss: 94.2945\n",
      "Epoch 24/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 84.3771 - val_loss: 94.0940\n",
      "Epoch 25/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 84.1005 - val_loss: 93.8935\n",
      "Epoch 26/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 83.7410 - val_loss: 93.6721\n",
      "Epoch 27/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 83.4833 - val_loss: 93.4728\n",
      "Epoch 28/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 83.1991 - val_loss: 93.2463\n",
      "Epoch 29/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 82.8843 - val_loss: 93.0339\n",
      "Epoch 30/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 82.6009 - val_loss: 92.7688\n",
      "Epoch 31/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 82.2057 - val_loss: 92.4891\n",
      "Epoch 32/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 81.7873 - val_loss: 92.1808\n",
      "Epoch 33/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 81.5225 - val_loss: 91.8736\n",
      "Epoch 34/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 81.0550 - val_loss: 91.5369\n",
      "Epoch 35/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 80.7933 - val_loss: 91.0908\n",
      "Epoch 36/500\n",
      "378/378 [==============================] - 0s 53us/step - loss: 80.3668 - val_loss: 90.6386\n",
      "Epoch 37/500\n",
      "378/378 [==============================] - 0s 52us/step - loss: 79.9051 - val_loss: 90.2710\n",
      "Epoch 38/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 79.4905 - val_loss: 89.8484\n",
      "Epoch 39/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 79.1980 - val_loss: 89.4437\n",
      "Epoch 40/500\n",
      "378/378 [==============================] - 0s 50us/step - loss: 78.7799 - val_loss: 89.1184\n",
      "Epoch 41/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 78.3313 - val_loss: 88.7298\n",
      "Epoch 42/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 77.9835 - val_loss: 88.2761\n",
      "Epoch 43/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 77.6173 - val_loss: 87.8008\n",
      "Epoch 44/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 77.0130 - val_loss: 87.2565\n",
      "Epoch 45/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 76.6161 - val_loss: 86.5709\n",
      "Epoch 46/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 76.1791 - val_loss: 85.9740\n",
      "Epoch 47/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 75.7589 - val_loss: 85.6228\n",
      "Epoch 48/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 75.2661 - val_loss: 85.1708\n",
      "Epoch 49/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 74.8919 - val_loss: 84.7547\n",
      "Epoch 50/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 74.5458 - val_loss: 84.3174\n",
      "Epoch 51/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 74.1949 - val_loss: 83.7577\n",
      "Epoch 52/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 73.6632 - val_loss: 83.6360\n",
      "Epoch 53/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 73.1632 - val_loss: 83.3994\n",
      "Epoch 54/500\n",
      "378/378 [==============================] - 0s 53us/step - loss: 72.8908 - val_loss: 83.1333\n",
      "Epoch 55/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 72.6058 - val_loss: 82.7005\n",
      "Epoch 56/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 72.1779 - val_loss: 82.4096\n",
      "Epoch 57/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 71.7752 - val_loss: 82.0266\n",
      "Epoch 58/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 71.3065 - val_loss: 81.3765\n",
      "Epoch 59/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 71.1642 - val_loss: 80.7937\n",
      "Epoch 60/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 70.5420 - val_loss: 80.4319\n",
      "Epoch 61/500\n",
      "378/378 [==============================] - 0s 53us/step - loss: 70.3093 - val_loss: 80.0710\n",
      "Epoch 62/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 69.8274 - val_loss: 79.7825\n",
      "Epoch 63/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 69.2715 - val_loss: 79.3588\n",
      "Epoch 64/500\n",
      "378/378 [==============================] - 0s 53us/step - loss: 69.4363 - val_loss: 78.9055\n",
      "Epoch 65/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 69.1052 - val_loss: 78.7575\n",
      "Epoch 66/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 68.6465 - val_loss: 78.6912\n",
      "Epoch 67/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 68.2485 - val_loss: 78.3998\n",
      "Epoch 68/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 67.9178 - val_loss: 77.8783\n",
      "Epoch 69/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 67.5372 - val_loss: 77.7012\n",
      "Epoch 70/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 67.3346 - val_loss: 77.4905\n",
      "Epoch 71/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 67.1120 - val_loss: 77.4289\n",
      "Epoch 72/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 66.5189 - val_loss: 77.0370\n",
      "Epoch 73/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 66.6943 - val_loss: 76.7088\n",
      "Epoch 74/500\n",
      "378/378 [==============================] - 0s 76us/step - loss: 66.3055 - val_loss: 76.0986\n",
      "Epoch 75/500\n",
      "378/378 [==============================] - 0s 77us/step - loss: 66.0167 - val_loss: 74.8097\n",
      "Epoch 76/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 65.5434 - val_loss: 74.1792\n",
      "Epoch 77/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 65.4108 - val_loss: 73.8223\n",
      "Epoch 78/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 65.6757 - val_loss: 73.2410\n",
      "Epoch 79/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 64.7448 - val_loss: 73.0481\n",
      "Epoch 80/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 64.9707 - val_loss: 72.6340\n",
      "Epoch 81/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 64.4841 - val_loss: 72.2431\n",
      "Epoch 82/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 64.3061 - val_loss: 72.0716\n",
      "Epoch 83/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 64.0628 - val_loss: 71.2890\n",
      "Epoch 84/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 63.9162 - val_loss: 70.3109\n",
      "Epoch 85/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 63.7713 - val_loss: 70.0143\n",
      "Epoch 86/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 63.4607 - val_loss: 69.8780\n",
      "Epoch 87/500\n",
      "378/378 [==============================] - 0s 53us/step - loss: 63.6051 - val_loss: 69.4670\n",
      "Epoch 88/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 63.1338 - val_loss: 68.5437\n",
      "Epoch 89/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 62.8058 - val_loss: 68.1088\n",
      "Epoch 90/500\n",
      "378/378 [==============================] - 0s 75us/step - loss: 62.7738 - val_loss: 67.6605\n",
      "Epoch 91/500\n",
      "378/378 [==============================] - 0s 79us/step - loss: 62.6078 - val_loss: 67.4713\n",
      "Epoch 92/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 62.5808 - val_loss: 66.8859\n",
      "Epoch 93/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 62.1785 - val_loss: 66.6015\n",
      "Epoch 94/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 62.0880 - val_loss: 66.1216\n",
      "Epoch 95/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 61.7670 - val_loss: 66.0125\n",
      "Epoch 96/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 62.0529 - val_loss: 65.6827\n",
      "Epoch 97/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 61.4837 - val_loss: 65.1672\n",
      "Epoch 98/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 61.0321 - val_loss: 64.3826\n",
      "Epoch 99/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 61.3161 - val_loss: 63.4313\n",
      "Epoch 100/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 61.0029 - val_loss: 62.6111\n",
      "Epoch 101/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 60.5744 - val_loss: 61.8116\n",
      "Epoch 102/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 60.8547 - val_loss: 60.6079\n",
      "Epoch 103/500\n",
      "378/378 [==============================] - 0s 75us/step - loss: 59.7871 - val_loss: 59.2888\n",
      "Epoch 104/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 59.1316 - val_loss: 58.2870\n",
      "Epoch 105/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 58.9153 - val_loss: 57.7050\n",
      "Epoch 106/500\n",
      "378/378 [==============================] - 0s 74us/step - loss: 58.4257 - val_loss: 57.0258\n",
      "Epoch 107/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 57.9116 - val_loss: 56.3955\n",
      "Epoch 108/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 57.0917 - val_loss: 55.8788\n",
      "Epoch 109/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 56.1926 - val_loss: 56.0115\n",
      "Epoch 110/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 55.6049 - val_loss: 56.1195\n",
      "Epoch 111/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 55.6023 - val_loss: 56.1508\n",
      "Epoch 112/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 55.6010 - val_loss: 56.3446\n",
      "Epoch 113/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 55.0523 - val_loss: 56.8281\n",
      "Epoch 114/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 54.5554 - val_loss: 57.3489\n",
      "Epoch 115/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 54.7123 - val_loss: 57.8110\n",
      "Epoch 116/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 54.5486 - val_loss: 58.2071\n",
      "Epoch 117/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 54.2351 - val_loss: 58.7068\n",
      "Epoch 118/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 54.2227 - val_loss: 58.8815\n",
      "Epoch 119/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 53.8758 - val_loss: 58.7624\n",
      "Epoch 120/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 53.8210 - val_loss: 58.6992\n",
      "Epoch 121/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 53.6747 - val_loss: 59.0127\n",
      "Epoch 122/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 53.3699 - val_loss: 59.1261\n",
      "Epoch 123/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 52.9384 - val_loss: 59.2297\n",
      "Epoch 124/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 53.0774 - val_loss: 59.4053\n",
      "Epoch 125/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 52.7693 - val_loss: 59.2491\n",
      "Epoch 126/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 52.6807 - val_loss: 58.9303\n",
      "Epoch 127/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 52.2594 - val_loss: 58.8529\n",
      "Epoch 128/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 52.3835 - val_loss: 58.8723\n",
      "Epoch 129/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 52.5574 - val_loss: 58.9298\n",
      "Epoch 130/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 51.8542 - val_loss: 58.9884\n",
      "Epoch 131/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 51.8999 - val_loss: 59.1291\n",
      "Epoch 132/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 51.7593 - val_loss: 59.1480\n",
      "Epoch 133/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 51.3203 - val_loss: 59.1755\n",
      "Epoch 134/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 51.6315 - val_loss: 59.2802\n",
      "Epoch 135/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 51.4844 - val_loss: 59.2596\n",
      "Epoch 136/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 50.9974 - val_loss: 59.1138\n",
      "Epoch 137/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 51.1368 - val_loss: 59.0232\n",
      "Epoch 138/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 50.5918 - val_loss: 59.0650\n",
      "Epoch 139/500\n",
      "378/378 [==============================] - 0s 51us/step - loss: 50.2205 - val_loss: 59.1118\n",
      "Epoch 140/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 50.1494 - val_loss: 59.0345\n",
      "Epoch 141/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 50.3763 - val_loss: 59.0534\n",
      "Epoch 142/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 50.3557 - val_loss: 59.1506\n",
      "Epoch 143/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 49.8193 - val_loss: 59.3169\n",
      "Epoch 144/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 49.6334 - val_loss: 59.2356\n",
      "Epoch 145/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 49.2562 - val_loss: 59.1947\n",
      "Epoch 146/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 49.0522 - val_loss: 59.0915\n",
      "Epoch 147/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 49.3029 - val_loss: 58.9265\n",
      "Epoch 148/500\n",
      "378/378 [==============================] - 0s 74us/step - loss: 49.1609 - val_loss: 58.8574\n",
      "Epoch 149/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 49.1060 - val_loss: 58.7290\n",
      "Epoch 150/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 48.9558 - val_loss: 58.5719\n",
      "Epoch 151/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 48.2903 - val_loss: 58.6365\n",
      "Epoch 152/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 60us/step - loss: 48.7455 - val_loss: 58.7200\n",
      "Epoch 153/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 48.3835 - val_loss: 58.7535\n",
      "Epoch 154/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 47.9729 - val_loss: 58.5803\n",
      "Epoch 155/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 47.7768 - val_loss: 58.4102\n",
      "Epoch 156/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 47.9975 - val_loss: 58.4240\n",
      "Epoch 157/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 47.5548 - val_loss: 58.2681\n",
      "Epoch 158/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 47.4685 - val_loss: 58.0900\n",
      "Epoch 159/500\n",
      "378/378 [==============================] - ETA: 0s - loss: 42.86 - 0s 63us/step - loss: 47.5154 - val_loss: 57.7013\n",
      "Epoch 160/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 47.1863 - val_loss: 57.4743\n",
      "Epoch 161/500\n",
      "378/378 [==============================] - 0s 76us/step - loss: 46.9537 - val_loss: 57.2933\n",
      "Epoch 162/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 46.9806 - val_loss: 57.0006\n",
      "Epoch 163/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 46.8928 - val_loss: 56.7898\n",
      "Epoch 164/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 46.6331 - val_loss: 56.6194\n",
      "Epoch 165/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 46.7064 - val_loss: 56.5654\n",
      "Epoch 166/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 46.2947 - val_loss: 56.5058\n",
      "Epoch 167/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 46.2298 - val_loss: 56.1838\n",
      "Epoch 168/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 46.0209 - val_loss: 56.0077\n",
      "Epoch 169/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 45.9825 - val_loss: 55.7651\n",
      "Epoch 170/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 45.6790 - val_loss: 55.6840\n",
      "Epoch 171/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 45.7976 - val_loss: 55.5329\n",
      "Epoch 172/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 45.3542 - val_loss: 55.4471\n",
      "Epoch 173/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 45.5493 - val_loss: 55.2646\n",
      "Epoch 174/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 45.0140 - val_loss: 54.9887\n",
      "Epoch 175/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 45.3024 - val_loss: 54.8001\n",
      "Epoch 176/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 45.0673 - val_loss: 54.6221\n",
      "Epoch 177/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 44.8200 - val_loss: 54.3706\n",
      "Epoch 178/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 44.3657 - val_loss: 54.1392\n",
      "Epoch 179/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 44.1507 - val_loss: 53.9226\n",
      "Epoch 180/500\n",
      "378/378 [==============================] - 0s 52us/step - loss: 44.3621 - val_loss: 53.9395\n",
      "Epoch 181/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 44.0843 - val_loss: 53.8741\n",
      "Epoch 182/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 44.1947 - val_loss: 53.7594\n",
      "Epoch 183/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 43.8533 - val_loss: 53.4751\n",
      "Epoch 184/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 43.6224 - val_loss: 53.2509\n",
      "Epoch 185/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 43.6435 - val_loss: 52.9322\n",
      "Epoch 186/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 43.2750 - val_loss: 52.7059\n",
      "Epoch 187/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 43.0539 - val_loss: 52.5540\n",
      "Epoch 188/500\n",
      "378/378 [==============================] - 0s 53us/step - loss: 43.1471 - val_loss: 52.3039\n",
      "Epoch 189/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 43.2033 - val_loss: 52.1153\n",
      "Epoch 190/500\n",
      "378/378 [==============================] - 0s 53us/step - loss: 42.6683 - val_loss: 51.9400\n",
      "Epoch 191/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 42.5331 - val_loss: 51.8920\n",
      "Epoch 192/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 42.8298 - val_loss: 51.7993\n",
      "Epoch 193/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 42.2998 - val_loss: 51.7918\n",
      "Epoch 194/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 41.8000 - val_loss: 51.6252\n",
      "Epoch 195/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 42.0413 - val_loss: 51.6065\n",
      "Epoch 196/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 42.0020 - val_loss: 51.3927\n",
      "Epoch 197/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 41.8876 - val_loss: 51.0430\n",
      "Epoch 198/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 41.8668 - val_loss: 50.9796\n",
      "Epoch 199/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 41.8284 - val_loss: 50.9143\n",
      "Epoch 200/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 41.7214 - val_loss: 50.9138\n",
      "Epoch 201/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 41.3369 - val_loss: 50.8131\n",
      "Epoch 202/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 40.9086 - val_loss: 50.5855\n",
      "Epoch 203/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 40.9607 - val_loss: 50.3284\n",
      "Epoch 204/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 40.9969 - val_loss: 50.3303\n",
      "Epoch 205/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 40.4711 - val_loss: 50.2338\n",
      "Epoch 206/500\n",
      "378/378 [==============================] - 0s 53us/step - loss: 40.4295 - val_loss: 50.1752\n",
      "Epoch 207/500\n",
      "378/378 [==============================] - 0s 52us/step - loss: 40.6127 - val_loss: 50.0675\n",
      "Epoch 208/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 40.1774 - val_loss: 49.7717\n",
      "Epoch 209/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 40.3314 - val_loss: 49.4425\n",
      "Epoch 210/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 39.5199 - val_loss: 49.2999\n",
      "Epoch 211/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 39.6914 - val_loss: 49.2061\n",
      "Epoch 212/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 39.6681 - val_loss: 48.9254\n",
      "Epoch 213/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 39.4805 - val_loss: 48.9175\n",
      "Epoch 214/500\n",
      "378/378 [==============================] - 0s 52us/step - loss: 39.3537 - val_loss: 48.6549\n",
      "Epoch 215/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 39.2520 - val_loss: 48.5858\n",
      "Epoch 216/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 38.9264 - val_loss: 48.5333\n",
      "Epoch 217/500\n",
      "378/378 [==============================] - ETA: 0s - loss: 42.22 - 0s 53us/step - loss: 39.3600 - val_loss: 48.4741\n",
      "Epoch 218/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 38.5822 - val_loss: 48.5064\n",
      "Epoch 219/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 38.8144 - val_loss: 48.2840\n",
      "Epoch 220/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 38.2996 - val_loss: 48.0999\n",
      "Epoch 221/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 38.0675 - val_loss: 47.7553\n",
      "Epoch 222/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 38.2736 - val_loss: 47.7284\n",
      "Epoch 223/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 37.6097 - val_loss: 47.9932\n",
      "Epoch 224/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 37.5389 - val_loss: 47.8010\n",
      "Epoch 225/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 37.2905 - val_loss: 47.7011\n",
      "Epoch 226/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 37.0084 - val_loss: 47.4650\n",
      "Epoch 227/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 37.0427 - val_loss: 47.2359\n",
      "Epoch 228/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 36.9174 - val_loss: 46.8247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 36.5264 - val_loss: 46.8311\n",
      "Epoch 230/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 36.4357 - val_loss: 46.5030\n",
      "Epoch 231/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 35.9429 - val_loss: 46.4659\n",
      "Epoch 232/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 36.0450 - val_loss: 46.5706\n",
      "Epoch 233/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 35.4820 - val_loss: 46.6719\n",
      "Epoch 234/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 35.6763 - val_loss: 46.7527\n",
      "Epoch 235/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 35.2906 - val_loss: 46.6468\n",
      "Epoch 236/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 35.1209 - val_loss: 45.9970\n",
      "Epoch 237/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 34.6649 - val_loss: 45.5891\n",
      "Epoch 238/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 34.9724 - val_loss: 45.2375\n",
      "Epoch 239/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 34.5367 - val_loss: 45.0117\n",
      "Epoch 240/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 33.9597 - val_loss: 44.9334\n",
      "Epoch 241/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 34.0425 - val_loss: 45.2130\n",
      "Epoch 242/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 33.6893 - val_loss: 45.4747\n",
      "Epoch 243/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 33.2113 - val_loss: 45.6903\n",
      "Epoch 244/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 32.4511 - val_loss: 45.6688\n",
      "Epoch 245/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 31.9660 - val_loss: 45.4703\n",
      "Epoch 246/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 32.1832 - val_loss: 45.4965\n",
      "Epoch 247/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 31.4320 - val_loss: 45.6113\n",
      "Epoch 248/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 31.2259 - val_loss: 45.2389\n",
      "Epoch 249/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 30.7363 - val_loss: 44.7620\n",
      "Epoch 250/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 30.5603 - val_loss: 44.4531\n",
      "Epoch 251/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 30.3477 - val_loss: 43.8704\n",
      "Epoch 252/500\n",
      "378/378 [==============================] - 0s 74us/step - loss: 29.5671 - val_loss: 43.3556\n",
      "Epoch 253/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 29.7841 - val_loss: 42.3897\n",
      "Epoch 254/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 29.1052 - val_loss: 41.6001\n",
      "Epoch 255/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 29.0037 - val_loss: 41.2156\n",
      "Epoch 256/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 29.1958 - val_loss: 40.6382\n",
      "Epoch 257/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 28.5934 - val_loss: 39.9645\n",
      "Epoch 258/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 28.3775 - val_loss: 39.5194\n",
      "Epoch 259/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 28.2623 - val_loss: 39.0410\n",
      "Epoch 260/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 27.9839 - val_loss: 38.7062\n",
      "Epoch 261/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 27.9531 - val_loss: 38.5955\n",
      "Epoch 262/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 28.0834 - val_loss: 38.2195\n",
      "Epoch 263/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 28.5537 - val_loss: 37.7790\n",
      "Epoch 264/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 27.5409 - val_loss: 37.4499\n",
      "Epoch 265/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 28.1549 - val_loss: 36.9983\n",
      "Epoch 266/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 27.7884 - val_loss: 37.1954\n",
      "Epoch 267/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 27.7651 - val_loss: 37.2403\n",
      "Epoch 268/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 27.4893 - val_loss: 37.2748\n",
      "Epoch 269/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 27.4844 - val_loss: 36.8801\n",
      "Epoch 270/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 27.0268 - val_loss: 36.7024\n",
      "Epoch 271/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 27.3873 - val_loss: 36.4982\n",
      "Epoch 272/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 27.1481 - val_loss: 36.3429\n",
      "Epoch 273/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 27.7048 - val_loss: 36.3952\n",
      "Epoch 274/500\n",
      "378/378 [==============================] - 0s 54us/step - loss: 27.5762 - val_loss: 36.4007\n",
      "Epoch 275/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 27.4691 - val_loss: 36.4640\n",
      "Epoch 276/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 27.2289 - val_loss: 36.5494\n",
      "Epoch 277/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 27.6247 - val_loss: 36.4956\n",
      "Epoch 278/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 26.9469 - val_loss: 36.4279\n",
      "Epoch 279/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 26.9144 - val_loss: 36.2616\n",
      "Epoch 280/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 27.7250 - val_loss: 36.2756\n",
      "Epoch 281/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 27.3291 - val_loss: 36.2807\n",
      "Epoch 282/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 27.0158 - val_loss: 36.3839\n",
      "Epoch 283/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 26.9516 - val_loss: 36.3587\n",
      "Epoch 284/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 26.5418 - val_loss: 36.3111\n",
      "Epoch 285/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 26.9028 - val_loss: 36.2095\n",
      "Epoch 286/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 27.6106 - val_loss: 36.3710\n",
      "Epoch 287/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 26.6023 - val_loss: 36.3898\n",
      "Epoch 288/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 26.5400 - val_loss: 36.3330\n",
      "Epoch 289/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 26.9395 - val_loss: 36.4638\n",
      "Epoch 290/500\n",
      "378/378 [==============================] - 0s 76us/step - loss: 26.8601 - val_loss: 36.3663\n",
      "Epoch 291/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 26.7374 - val_loss: 36.5208\n",
      "Epoch 292/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 26.5331 - val_loss: 36.6710\n",
      "Epoch 293/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 26.5480 - val_loss: 36.7248\n",
      "Epoch 294/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 26.0708 - val_loss: 36.6543\n",
      "Epoch 295/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 26.4865 - val_loss: 36.7591\n",
      "Epoch 296/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 26.3726 - val_loss: 36.9718\n",
      "Epoch 297/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 26.3883 - val_loss: 37.0277\n",
      "Epoch 298/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 26.6646 - val_loss: 37.0087\n",
      "Epoch 299/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 27.1274 - val_loss: 37.1588\n",
      "Epoch 300/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 26.6114 - val_loss: 37.2049\n",
      "Epoch 301/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 26.1574 - val_loss: 37.2398\n",
      "Epoch 302/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 26.3077 - val_loss: 37.2608\n",
      "Epoch 303/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 26.9304 - val_loss: 37.2949\n",
      "Epoch 304/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 26.2683 - val_loss: 37.4642\n",
      "Epoch 305/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 26.4159 - val_loss: 37.3589\n",
      "Epoch 306/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 26.5373 - val_loss: 37.4315\n",
      "Epoch 307/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 26.2809 - val_loss: 37.4161\n",
      "Epoch 308/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 26.8511 - val_loss: 37.2855\n",
      "Epoch 309/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 27.0582 - val_loss: 37.1617\n",
      "Epoch 310/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 25.9772 - val_loss: 37.1751\n",
      "Epoch 311/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 26.3679 - val_loss: 37.2325\n",
      "Epoch 312/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 26.3632 - val_loss: 37.3866\n",
      "Epoch 313/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 26.2250 - val_loss: 37.5633\n",
      "Epoch 314/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 26.3783 - val_loss: 37.5071\n",
      "Epoch 315/500\n",
      "378/378 [==============================] - 0s 84us/step - loss: 25.9650 - val_loss: 37.3556\n",
      "Epoch 316/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 26.4635 - val_loss: 37.1856\n",
      "Epoch 317/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 26.7576 - val_loss: 37.3051\n",
      "Epoch 318/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 26.4319 - val_loss: 37.1971\n",
      "Epoch 319/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 26.3314 - val_loss: 37.2303\n",
      "Epoch 320/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 26.7549 - val_loss: 37.2077\n",
      "Epoch 321/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 25.8307 - val_loss: 37.0622\n",
      "Epoch 322/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 26.3092 - val_loss: 36.9944\n",
      "Epoch 323/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 26.3154 - val_loss: 36.9142\n",
      "Epoch 324/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 26.4775 - val_loss: 36.9604\n",
      "Epoch 325/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 26.4470 - val_loss: 37.1499\n",
      "Epoch 326/500\n",
      "378/378 [==============================] - 0s 77us/step - loss: 26.3271 - val_loss: 37.0730\n",
      "Epoch 327/500\n",
      "378/378 [==============================] - 0s 77us/step - loss: 26.3840 - val_loss: 37.0527\n",
      "Epoch 328/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 26.7538 - val_loss: 36.8040\n",
      "Epoch 329/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 25.8410 - val_loss: 36.6892\n",
      "Epoch 330/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 26.5877 - val_loss: 36.6439\n",
      "Epoch 331/500\n",
      "378/378 [==============================] - 0s 76us/step - loss: 25.9486 - val_loss: 36.9714\n",
      "Epoch 332/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 25.7663 - val_loss: 37.0795\n",
      "Epoch 333/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 26.4433 - val_loss: 37.0258\n",
      "Epoch 334/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 26.1029 - val_loss: 36.9290\n",
      "Epoch 335/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 26.7314 - val_loss: 36.9296\n",
      "Epoch 336/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 26.0687 - val_loss: 37.0579\n",
      "Epoch 337/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 26.0111 - val_loss: 37.0293\n",
      "Epoch 338/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 25.9303 - val_loss: 37.0006\n",
      "Epoch 339/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 26.2475 - val_loss: 37.1697\n",
      "Epoch 340/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 26.3137 - val_loss: 37.2748\n",
      "Epoch 341/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 26.4775 - val_loss: 37.1511\n",
      "Epoch 342/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 26.2986 - val_loss: 37.0899\n",
      "Epoch 343/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 26.0741 - val_loss: 37.2701\n",
      "Epoch 344/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 26.2734 - val_loss: 37.1513\n",
      "Epoch 345/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 26.0178 - val_loss: 37.0181\n",
      "Epoch 346/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 26.0710 - val_loss: 37.2332\n",
      "Epoch 347/500\n",
      "378/378 [==============================] - ETA: 0s - loss: 27.76 - 0s 61us/step - loss: 26.2455 - val_loss: 37.1775\n",
      "Epoch 348/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 26.2359 - val_loss: 37.1989\n",
      "Epoch 349/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 26.0947 - val_loss: 37.0347\n",
      "Epoch 350/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 26.0381 - val_loss: 37.0166\n",
      "Epoch 351/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 25.9990 - val_loss: 37.1568\n",
      "Epoch 352/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 26.0310 - val_loss: 37.4054\n",
      "Epoch 353/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 26.1746 - val_loss: 37.3624\n",
      "Epoch 354/500\n",
      "378/378 [==============================] - 0s 79us/step - loss: 25.6928 - val_loss: 37.3304\n",
      "Epoch 355/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 26.3223 - val_loss: 37.3736\n",
      "Epoch 356/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 25.9132 - val_loss: 37.3865\n",
      "Epoch 357/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 25.8924 - val_loss: 37.4153\n",
      "Epoch 358/500\n",
      "378/378 [==============================] - 0s 79us/step - loss: 26.3737 - val_loss: 37.2957\n",
      "Epoch 359/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 25.9300 - val_loss: 37.1664\n",
      "Epoch 360/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 25.8671 - val_loss: 37.2557\n",
      "Epoch 361/500\n",
      "378/378 [==============================] - 0s 76us/step - loss: 25.9642 - val_loss: 37.1354\n",
      "Epoch 362/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 26.0746 - val_loss: 37.4048\n",
      "Epoch 363/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 26.0701 - val_loss: 37.4050\n",
      "Epoch 364/500\n",
      "378/378 [==============================] - 0s 77us/step - loss: 26.5249 - val_loss: 37.4745\n",
      "Epoch 365/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 26.3520 - val_loss: 37.3575\n",
      "Epoch 366/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 25.7336 - val_loss: 37.5530\n",
      "Epoch 367/500\n",
      "378/378 [==============================] - 0s 77us/step - loss: 26.1949 - val_loss: 37.6183\n",
      "Epoch 368/500\n",
      "378/378 [==============================] - ETA: 0s - loss: 24.90 - 0s 72us/step - loss: 25.9739 - val_loss: 37.4887\n",
      "Epoch 369/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 26.0770 - val_loss: 37.4998\n",
      "Epoch 370/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 26.4340 - val_loss: 37.2024\n",
      "Epoch 371/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 26.3632 - val_loss: 37.1317\n",
      "Epoch 372/500\n",
      "378/378 [==============================] - ETA: 0s - loss: 25.54 - 0s 64us/step - loss: 26.0512 - val_loss: 37.2138\n",
      "Epoch 373/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 26.0333 - val_loss: 37.1916\n",
      "Epoch 374/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 26.4939 - val_loss: 37.0949\n",
      "Epoch 375/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 26.0553 - val_loss: 37.3288\n",
      "Epoch 376/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 26.0574 - val_loss: 37.4806\n",
      "Epoch 377/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 26.1959 - val_loss: 37.6703\n",
      "Epoch 378/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 25.9529 - val_loss: 37.5044\n",
      "Epoch 379/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 25.8556 - val_loss: 37.1539\n",
      "Epoch 380/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 25.9744 - val_loss: 37.2770\n",
      "Epoch 381/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 25.6953 - val_loss: 37.3538\n",
      "Epoch 382/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 71us/step - loss: 26.4543 - val_loss: 37.5263\n",
      "Epoch 383/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 25.6554 - val_loss: 37.3908\n",
      "Epoch 384/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 25.6309 - val_loss: 37.2594\n",
      "Epoch 385/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 25.7620 - val_loss: 37.3892\n",
      "Epoch 386/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 26.2300 - val_loss: 37.3910\n",
      "Epoch 387/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 25.8049 - val_loss: 37.2997\n",
      "Epoch 388/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 26.2075 - val_loss: 37.3389\n",
      "Epoch 389/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 25.8600 - val_loss: 37.4664\n",
      "Epoch 390/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 25.7242 - val_loss: 37.4238\n",
      "Epoch 391/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 25.8511 - val_loss: 37.3241\n",
      "Epoch 392/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 26.0235 - val_loss: 37.3475\n",
      "Epoch 393/500\n",
      "378/378 [==============================] - 0s 76us/step - loss: 25.7932 - val_loss: 37.5451\n",
      "Epoch 394/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 26.0461 - val_loss: 37.5383\n",
      "Epoch 395/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 25.5685 - val_loss: 37.7274\n",
      "Epoch 396/500\n",
      "378/378 [==============================] - 0s 74us/step - loss: 25.7709 - val_loss: 37.9335\n",
      "Epoch 397/500\n",
      "378/378 [==============================] - 0s 79us/step - loss: 26.0725 - val_loss: 37.8796\n",
      "Epoch 398/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 26.0564 - val_loss: 37.7778\n",
      "Epoch 399/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 26.1189 - val_loss: 37.3946\n",
      "Epoch 400/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 25.9043 - val_loss: 37.3736\n",
      "Epoch 401/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 25.9128 - val_loss: 37.5537\n",
      "Epoch 402/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 25.8570 - val_loss: 37.5603\n",
      "Epoch 403/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 26.3392 - val_loss: 37.6424\n",
      "Epoch 404/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 25.7723 - val_loss: 37.5660\n",
      "Epoch 405/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 25.4539 - val_loss: 37.4943\n",
      "Epoch 406/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 25.7146 - val_loss: 37.4599\n",
      "Epoch 407/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 25.8827 - val_loss: 37.3287\n",
      "Epoch 408/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 25.8472 - val_loss: 37.3049\n",
      "Epoch 409/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 25.6728 - val_loss: 37.3405\n",
      "Epoch 410/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 25.2889 - val_loss: 37.4726\n",
      "Epoch 411/500\n",
      "378/378 [==============================] - 0s 89us/step - loss: 25.7957 - val_loss: 37.4983\n",
      "Epoch 412/500\n",
      "378/378 [==============================] - 0s 82us/step - loss: 25.9084 - val_loss: 37.6001\n",
      "Epoch 413/500\n",
      "378/378 [==============================] - 0s 78us/step - loss: 26.1971 - val_loss: 37.5300\n",
      "Epoch 414/500\n",
      "378/378 [==============================] - 0s 74us/step - loss: 25.4322 - val_loss: 37.6006\n",
      "Epoch 415/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 25.9858 - val_loss: 37.5376\n",
      "Epoch 416/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 25.4677 - val_loss: 37.5302\n",
      "Epoch 417/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 25.5259 - val_loss: 37.4437\n",
      "Epoch 418/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 26.1706 - val_loss: 37.4696\n",
      "Epoch 419/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 25.9313 - val_loss: 37.6053\n",
      "Epoch 420/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 25.7781 - val_loss: 37.6903\n",
      "Epoch 421/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 25.6650 - val_loss: 37.5188\n",
      "Epoch 422/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 25.4181 - val_loss: 37.4302\n",
      "Epoch 423/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 26.0359 - val_loss: 37.6101\n",
      "Epoch 424/500\n",
      "378/378 [==============================] - 0s 52us/step - loss: 25.7094 - val_loss: 37.8395\n",
      "Epoch 425/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 25.5501 - val_loss: 37.6704\n",
      "Epoch 426/500\n",
      "378/378 [==============================] - 0s 53us/step - loss: 26.3104 - val_loss: 37.5840\n",
      "Epoch 427/500\n",
      "378/378 [==============================] - 0s 53us/step - loss: 25.9106 - val_loss: 37.6005\n",
      "Epoch 428/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 25.7648 - val_loss: 37.4899\n",
      "Epoch 429/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 26.2539 - val_loss: 37.3895\n",
      "Epoch 430/500\n",
      "378/378 [==============================] - 0s 58us/step - loss: 25.9692 - val_loss: 37.3922\n",
      "Epoch 431/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 25.9859 - val_loss: 37.6013\n",
      "Epoch 432/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 25.8607 - val_loss: 37.7536\n",
      "Epoch 433/500\n",
      "378/378 [==============================] - 0s 78us/step - loss: 25.8351 - val_loss: 37.6587\n",
      "Epoch 434/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 25.2554 - val_loss: 37.5388\n",
      "Epoch 435/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 25.4849 - val_loss: 37.6290\n",
      "Epoch 436/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 25.5175 - val_loss: 37.7637\n",
      "Epoch 437/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 25.2946 - val_loss: 37.9248\n",
      "Epoch 438/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 25.3380 - val_loss: 38.0804\n",
      "Epoch 439/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 26.0811 - val_loss: 37.9223\n",
      "Epoch 440/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 25.7769 - val_loss: 37.6416\n",
      "Epoch 441/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 25.7506 - val_loss: 37.8429\n",
      "Epoch 442/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 25.6307 - val_loss: 37.7252\n",
      "Epoch 443/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 25.7946 - val_loss: 37.6067\n",
      "Epoch 444/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 25.5334 - val_loss: 37.5713\n",
      "Epoch 445/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 25.6699 - val_loss: 37.5168\n",
      "Epoch 446/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 26.0194 - val_loss: 37.5993\n",
      "Epoch 447/500\n",
      "378/378 [==============================] - 0s 83us/step - loss: 25.2562 - val_loss: 37.6028\n",
      "Epoch 448/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 25.7517 - val_loss: 37.7402\n",
      "Epoch 449/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 25.8100 - val_loss: 38.1415\n",
      "Epoch 450/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 25.8018 - val_loss: 37.8456\n",
      "Epoch 451/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 25.5412 - val_loss: 37.6172\n",
      "Epoch 452/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 25.5606 - val_loss: 38.1084\n",
      "Epoch 453/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 25.5534 - val_loss: 38.2407\n",
      "Epoch 454/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 25.4825 - val_loss: 38.1270\n",
      "Epoch 455/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 25.7454 - val_loss: 37.8495\n",
      "Epoch 456/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 25.5437 - val_loss: 37.7755\n",
      "Epoch 457/500\n",
      "378/378 [==============================] - 0s 66us/step - loss: 25.3114 - val_loss: 37.6891\n",
      "Epoch 458/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 25.7191 - val_loss: 38.9231\n",
      "Epoch 459/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 25.8202 - val_loss: 39.0972\n",
      "Epoch 460/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 25.8386 - val_loss: 38.5954\n",
      "Epoch 461/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 25.6295 - val_loss: 38.3170\n",
      "Epoch 462/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 25.5793 - val_loss: 38.0113\n",
      "Epoch 463/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 26.1763 - val_loss: 37.6889\n",
      "Epoch 464/500\n",
      "378/378 [==============================] - 0s 62us/step - loss: 25.6175 - val_loss: 37.4008\n",
      "Epoch 465/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 25.8446 - val_loss: 37.5338\n",
      "Epoch 466/500\n",
      "378/378 [==============================] - 0s 57us/step - loss: 25.5862 - val_loss: 37.3714\n",
      "Epoch 467/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 25.5431 - val_loss: 37.3404\n",
      "Epoch 468/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 25.6248 - val_loss: 37.1828\n",
      "Epoch 469/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 25.6138 - val_loss: 37.1806\n",
      "Epoch 470/500\n",
      "378/378 [==============================] - 0s 80us/step - loss: 25.8049 - val_loss: 37.3199\n",
      "Epoch 471/500\n",
      "378/378 [==============================] - 0s 74us/step - loss: 25.6880 - val_loss: 37.4284\n",
      "Epoch 472/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 25.5989 - val_loss: 37.4764\n",
      "Epoch 473/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 25.6108 - val_loss: 37.3021\n",
      "Epoch 474/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 25.3470 - val_loss: 37.2631\n",
      "Epoch 475/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 25.6029 - val_loss: 37.3899\n",
      "Epoch 476/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 25.6863 - val_loss: 37.3658\n",
      "Epoch 477/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 25.5795 - val_loss: 37.3403\n",
      "Epoch 478/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 25.5206 - val_loss: 37.4749\n",
      "Epoch 479/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 25.6501 - val_loss: 37.6648\n",
      "Epoch 480/500\n",
      "378/378 [==============================] - 0s 55us/step - loss: 25.6184 - val_loss: 37.6781\n",
      "Epoch 481/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 25.3232 - val_loss: 37.6825\n",
      "Epoch 482/500\n",
      "378/378 [==============================] - 0s 68us/step - loss: 25.3998 - val_loss: 37.6626\n",
      "Epoch 483/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 25.4875 - val_loss: 37.7364\n",
      "Epoch 484/500\n",
      "378/378 [==============================] - 0s 71us/step - loss: 25.7755 - val_loss: 37.7992\n",
      "Epoch 485/500\n",
      "378/378 [==============================] - 0s 73us/step - loss: 25.4825 - val_loss: 37.7678\n",
      "Epoch 486/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 25.6559 - val_loss: 37.6909\n",
      "Epoch 487/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 25.4232 - val_loss: 37.6220\n",
      "Epoch 488/500\n",
      "378/378 [==============================] - 0s 60us/step - loss: 25.2503 - val_loss: 37.7119\n",
      "Epoch 489/500\n",
      "378/378 [==============================] - 0s 67us/step - loss: 25.5312 - val_loss: 37.8580\n",
      "Epoch 490/500\n",
      "378/378 [==============================] - 0s 63us/step - loss: 25.5913 - val_loss: 37.8984\n",
      "Epoch 491/500\n",
      "378/378 [==============================] - 0s 61us/step - loss: 25.6962 - val_loss: 37.9373\n",
      "Epoch 492/500\n",
      "378/378 [==============================] - 0s 56us/step - loss: 25.2738 - val_loss: 37.9355\n",
      "Epoch 493/500\n",
      "378/378 [==============================] - 0s 59us/step - loss: 25.1702 - val_loss: 37.9450\n",
      "Epoch 494/500\n",
      "378/378 [==============================] - 0s 64us/step - loss: 25.4244 - val_loss: 37.9705\n",
      "Epoch 495/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 25.8825 - val_loss: 37.8193\n",
      "Epoch 496/500\n",
      "378/378 [==============================] - 0s 70us/step - loss: 25.3666 - val_loss: 37.7413\n",
      "Epoch 497/500\n",
      "378/378 [==============================] - 0s 65us/step - loss: 25.6104 - val_loss: 37.8944\n",
      "Epoch 498/500\n",
      "378/378 [==============================] - 0s 69us/step - loss: 25.4288 - val_loss: 37.7728\n",
      "Epoch 499/500\n",
      "378/378 [==============================] - 0s 74us/step - loss: 25.1783 - val_loss: 37.6563\n",
      "Epoch 500/500\n",
      "378/378 [==============================] - 0s 72us/step - loss: 25.7188 - val_loss: 37.7672\n",
      "mw\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 6.1719 - val_loss: 7.7921\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 6.1502 - val_loss: 7.5480\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 6.1206 - val_loss: 7.4985\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 6.1205 - val_loss: 7.5016\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 6.0928 - val_loss: 7.5018\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 6.0780 - val_loss: 7.4947\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 6.0511 - val_loss: 7.4923\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 6.0314 - val_loss: 7.5000\n",
      "Epoch 9/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 6.0055 - val_loss: 7.4824\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 5.9892 - val_loss: 7.4499\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 5.9613 - val_loss: 7.4421\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 5.9432 - val_loss: 7.4204\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 5.9266 - val_loss: 7.4231\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 5.9072 - val_loss: 7.4284\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 5.8930 - val_loss: 7.3997\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 5.8720 - val_loss: 7.3703\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 5.8502 - val_loss: 7.3541\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 5.8157 - val_loss: 7.3116\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 5.8147 - val_loss: 7.2810\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 5.7993 - val_loss: 7.2523\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 5.7798 - val_loss: 7.2217\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 5.7636 - val_loss: 7.2132\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 5.7393 - val_loss: 7.1459\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 5.7256 - val_loss: 7.1357\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 5.7152 - val_loss: 7.0881\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 5.6617 - val_loss: 6.9793\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 5.6391 - val_loss: 6.9306\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 5.6253 - val_loss: 6.9342\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 5.6246 - val_loss: 6.9720\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 4.149 - 0s 64us/step - loss: 5.5893 - val_loss: 6.9515\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 5.5502 - val_loss: 6.8726\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 5.5100 - val_loss: 6.8080\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 5.5013 - val_loss: 6.7284\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 5.4755 - val_loss: 6.6799\n",
      "Epoch 35/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 6.530 - 0s 62us/step - loss: 5.4514 - val_loss: 6.6484\n",
      "Epoch 36/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 60us/step - loss: 5.4370 - val_loss: 6.5938\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 5.3771 - val_loss: 6.5750\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 5.3928 - val_loss: 6.4727\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 5.3832 - val_loss: 6.4049\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 5.3568 - val_loss: 6.3898\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 5.3282 - val_loss: 6.3975\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 5.3410 - val_loss: 6.3897\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 5.3318 - val_loss: 6.3251\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 5.3022 - val_loss: 6.2650\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 5.3325 - val_loss: 6.2457\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 5.2659 - val_loss: 6.2636\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 5.2359 - val_loss: 6.2334\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 5.2394 - val_loss: 6.1982\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 5.2484 - val_loss: 6.1455\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 5.2408 - val_loss: 6.1311\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 5.2203 - val_loss: 6.1462\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 5.2003 - val_loss: 6.0964\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 5.1932 - val_loss: 6.0749\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 5.1779 - val_loss: 6.0457\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 5.1420 - val_loss: 6.0682\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 5.1940 - val_loss: 6.0438\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 5.1629 - val_loss: 6.0560\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 5.1557 - val_loss: 6.0590\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 5.1473 - val_loss: 6.0401\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 5.1090 - val_loss: 6.0255\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 5.1621 - val_loss: 6.0175\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 5.1620 - val_loss: 6.0065\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 5.1237 - val_loss: 6.0004\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 50us/step - loss: 5.1132 - val_loss: 6.0129\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 5.1378 - val_loss: 6.0115\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 5.1108 - val_loss: 5.9993\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 5.1058 - val_loss: 6.0097\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 5.0512 - val_loss: 5.9772\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 5.0709 - val_loss: 5.9531\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 5.1086 - val_loss: 5.9365\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 5.1068 - val_loss: 5.9747\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 4.502 - 0s 66us/step - loss: 5.0554 - val_loss: 6.0016\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 77us/step - loss: 5.0493 - val_loss: 5.9874\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 5.0577 - val_loss: 5.9731\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 5.0463 - val_loss: 5.9526\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 5.0375 - val_loss: 5.9897\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 5.0230 - val_loss: 6.0182\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 5.0745 - val_loss: 5.9702\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 5.0613 - val_loss: 5.9784\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 5.0239 - val_loss: 5.9831\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 5.0415 - val_loss: 5.9679\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 5.0424 - val_loss: 5.9516\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 5.0349 - val_loss: 5.9753\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.9933 - val_loss: 5.9793\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 5.0220 - val_loss: 6.0350\n",
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 5.0333 - val_loss: 6.0134\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.9967 - val_loss: 5.9781\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 5.0030 - val_loss: 5.9746\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 5.0741 - val_loss: 5.9840\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 5.0193 - val_loss: 6.0249\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 4.501 - 0s 54us/step - loss: 5.0044 - val_loss: 6.0056\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 5.0429 - val_loss: 6.0240\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 5.0152 - val_loss: 6.0399\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 5.0049 - val_loss: 6.0213\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.9999 - val_loss: 6.0336\n",
      "Epoch 96/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 5.0282 - val_loss: 6.0310\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 5.0646 - val_loss: 5.9991\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.9764 - val_loss: 5.9857\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 5.0090 - val_loss: 6.0066\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 5.0137 - val_loss: 6.0441\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 5.0065 - val_loss: 6.0254\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 5.0234 - val_loss: 6.0050\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.9880 - val_loss: 6.0383\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.9930 - val_loss: 6.0056\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 4.9793 - val_loss: 5.9734\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 4.9659 - val_loss: 6.0190\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.9913 - val_loss: 6.0447\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 77us/step - loss: 4.9927 - val_loss: 6.0645\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.9694 - val_loss: 6.0405\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.9622 - val_loss: 5.9827\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.9509 - val_loss: 6.0139\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.9771 - val_loss: 6.0395\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.9640 - val_loss: 6.0149\n",
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 5.0031 - val_loss: 6.0368\n",
      "Epoch 115/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 59us/step - loss: 4.9696 - val_loss: 6.0797\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.9577 - val_loss: 6.0551\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.9623 - val_loss: 6.1042\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.9707 - val_loss: 6.0335\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 5.0024 - val_loss: 6.0284\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.9581 - val_loss: 6.0330\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.9609 - val_loss: 6.0524\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 4.9240 - val_loss: 6.0314\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.9939 - val_loss: 6.0547\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.9872 - val_loss: 6.0526\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.9711 - val_loss: 6.0453\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.9663 - val_loss: 6.0907\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.9734 - val_loss: 6.1093\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.9484 - val_loss: 6.0292\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.9519 - val_loss: 6.0673\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.9648 - val_loss: 6.1208\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.9429 - val_loss: 6.0903\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.9341 - val_loss: 6.0979\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.9427 - val_loss: 6.0497\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.9664 - val_loss: 6.0389\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 5.0161 - val_loss: 6.0325\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.9726 - val_loss: 6.0641\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 5.0391 - val_loss: 6.0626\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.9248 - val_loss: 6.0430\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.9681 - val_loss: 6.0288\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 5.0201 - val_loss: 6.0164\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.9374 - val_loss: 6.0376\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.9622 - val_loss: 6.0220\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.9601 - val_loss: 6.0451\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.9478 - val_loss: 6.0624\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.9316 - val_loss: 6.0459\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.9931 - val_loss: 6.0816\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 5.0193 - val_loss: 6.1158\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.9594 - val_loss: 6.1145\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.9364 - val_loss: 6.1707\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.9685 - val_loss: 6.1474\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.9347 - val_loss: 6.1371\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.9389 - val_loss: 6.1531\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.9491 - val_loss: 6.1395\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.9357 - val_loss: 6.1668\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.9533 - val_loss: 6.1567\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.9714 - val_loss: 6.1779\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.9428 - val_loss: 6.2091\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 75us/step - loss: 4.9071 - val_loss: 6.1591\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 88us/step - loss: 4.9124 - val_loss: 6.1707\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 106us/step - loss: 4.9099 - val_loss: 6.1955\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.9483 - val_loss: 6.1644\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.9789 - val_loss: 6.2018\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.9585 - val_loss: 6.2575\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 4.9522 - val_loss: 6.2585\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.9376 - val_loss: 6.2348\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 4.9418 - val_loss: 6.2320\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.9263 - val_loss: 6.2238\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.9433 - val_loss: 6.2268\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.9099 - val_loss: 6.2599\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.9660 - val_loss: 6.3080\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.9220 - val_loss: 6.3191\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.9171 - val_loss: 6.3095\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.9014 - val_loss: 6.3272\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.9181 - val_loss: 6.3636\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 4.9437 - val_loss: 6.4034\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 78us/step - loss: 4.9564 - val_loss: 6.3660\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.9280 - val_loss: 6.4180\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.9179 - val_loss: 6.2952\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 78us/step - loss: 4.9435 - val_loss: 6.2962\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.9308 - val_loss: 6.3334\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.8904 - val_loss: 6.3538\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.8688 - val_loss: 6.3602\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 78us/step - loss: 4.8913 - val_loss: 6.2810\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 4.9089 - val_loss: 6.2757\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.9130 - val_loss: 6.2397\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.9361 - val_loss: 6.2091\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 4.715 - 0s 68us/step - loss: 4.9427 - val_loss: 6.2434\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.9024 - val_loss: 6.2293\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.9178 - val_loss: 6.2078\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.8890 - val_loss: 6.2659\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.8892 - val_loss: 6.2833\n",
      "Epoch 192/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 4.8753 - val_loss: 6.2686\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.9244 - val_loss: 6.2210\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.8965 - val_loss: 6.2218\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.9657 - val_loss: 6.2518\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 4.8966 - val_loss: 6.2566\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.9004 - val_loss: 6.2156\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 3.954 - 0s 60us/step - loss: 4.9074 - val_loss: 6.1582\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 4.9028 - val_loss: 6.2456\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.8914 - val_loss: 6.2968\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.9013 - val_loss: 6.3210\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 4.8718 - val_loss: 6.3210\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.9197 - val_loss: 6.3432\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.8684 - val_loss: 6.2573\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.8744 - val_loss: 6.2471\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.9000 - val_loss: 6.2228\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.8900 - val_loss: 6.2761\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.8752 - val_loss: 6.3162\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.8960 - val_loss: 6.3109\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 4.8383 - val_loss: 6.2761\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.8734 - val_loss: 6.3269\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.8995 - val_loss: 6.3656\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.8845 - val_loss: 6.4068\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.9170 - val_loss: 6.3690\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.9012 - val_loss: 6.3449\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.8449 - val_loss: 6.3036\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.8655 - val_loss: 6.2834\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.8942 - val_loss: 6.2704\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.8947 - val_loss: 6.3461\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.8704 - val_loss: 6.3497\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.8587 - val_loss: 6.2899\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 6.939 - 0s 53us/step - loss: 4.8380 - val_loss: 6.2806\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.8520 - val_loss: 6.3245\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.8899 - val_loss: 6.3064\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.8784 - val_loss: 6.2610\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 4.9361 - val_loss: 6.3126\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 4.9236 - val_loss: 6.2722\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.8506 - val_loss: 6.2824\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 4.8682 - val_loss: 6.2444\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 4.8740 - val_loss: 6.3422\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.8550 - val_loss: 6.4226\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.8605 - val_loss: 6.3263\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.8610 - val_loss: 6.3323\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.9232 - val_loss: 6.2807\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.9379 - val_loss: 6.4076\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.8520 - val_loss: 6.3931\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.8302 - val_loss: 6.3860\n",
      "Epoch 238/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.8257 - val_loss: 6.3303\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 4.8775 - val_loss: 6.3429\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 4.8401 - val_loss: 6.3537\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.8863 - val_loss: 6.3867\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.8271 - val_loss: 6.3794\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.9148 - val_loss: 6.4198\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.8592 - val_loss: 6.4128\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.8730 - val_loss: 6.4092\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.8664 - val_loss: 6.4133\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 50us/step - loss: 4.8858 - val_loss: 6.3995\n",
      "Epoch 248/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.8681 - val_loss: 6.3400\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.8762 - val_loss: 6.3448\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.8015 - val_loss: 6.3701\n",
      "dw\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 13.7100 - val_loss: 16.7083\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 13.6727 - val_loss: 16.1160\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.6445 - val_loss: 15.8820\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.6167 - val_loss: 15.7394\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 13.5966 - val_loss: 15.6338\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.5760 - val_loss: 15.5676\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 13.5645 - val_loss: 15.5178\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 13.5483 - val_loss: 15.4716\n",
      "Epoch 9/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 13.5323 - val_loss: 15.4292\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 13.5220 - val_loss: 15.3901\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.5102 - val_loss: 15.3589\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 13.5021 - val_loss: 15.3317\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 13.4946 - val_loss: 15.3056\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 13.4892 - val_loss: 15.2854\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 13.4791 - val_loss: 15.2554\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 13.4737 - val_loss: 15.2354\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.4677 - val_loss: 15.2232\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 13.4667 - val_loss: 15.2156\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.4656 - val_loss: 15.2058\n",
      "Epoch 20/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 60us/step - loss: 13.4626 - val_loss: 15.1969\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 13.4582 - val_loss: 15.1886\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.4515 - val_loss: 15.1808\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 13.4535 - val_loss: 15.1700\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 13.4502 - val_loss: 15.1639\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 13.4486 - val_loss: 15.1594\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 13.4426 - val_loss: 15.1569\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 13.4424 - val_loss: 15.1533\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.4396 - val_loss: 15.1513\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.4426 - val_loss: 15.1494\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.4397 - val_loss: 15.1482\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 13.4407 - val_loss: 15.1462\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 13.4357 - val_loss: 15.1401\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.4400 - val_loss: 15.1382\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.4340 - val_loss: 15.1376\n",
      "Epoch 35/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 13.4328 - val_loss: 15.1404\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 13.4348 - val_loss: 15.1387\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.4318 - val_loss: 15.1349\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.4329 - val_loss: 15.1337\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.4298 - val_loss: 15.1350\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.4318 - val_loss: 15.1313\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.4296 - val_loss: 15.1258\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.4265 - val_loss: 15.1225\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.4254 - val_loss: 15.1211\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.4228 - val_loss: 15.1216\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.4213 - val_loss: 15.1198\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 13.4276 - val_loss: 15.1186\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.4199 - val_loss: 15.1163\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 13.4250 - val_loss: 15.1144\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.4164 - val_loss: 15.1147\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.4166 - val_loss: 15.1145\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 13.39 - 0s 56us/step - loss: 13.4217 - val_loss: 15.1149\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 13.4105 - val_loss: 15.1176\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.4152 - val_loss: 15.1166\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.4091 - val_loss: 15.1162\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.4152 - val_loss: 15.1144\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 13.4072 - val_loss: 15.1159\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.4174 - val_loss: 15.1149\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.4066 - val_loss: 15.1111\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.4076 - val_loss: 15.1121\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.4089 - val_loss: 15.1148\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.4023 - val_loss: 15.1165\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 13.4031 - val_loss: 15.1212\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.4078 - val_loss: 15.1211\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.3993 - val_loss: 15.1230\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 13.3992 - val_loss: 15.1196\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.3920 - val_loss: 15.1156\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.4050 - val_loss: 15.1151\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 13.3928 - val_loss: 15.1176\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 75us/step - loss: 13.3871 - val_loss: 15.1169\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 13.3889 - val_loss: 15.1153\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.3974 - val_loss: 15.1165\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - 0s 77us/step - loss: 13.3900 - val_loss: 15.1141\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 13.3942 - val_loss: 15.1078\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 13.3904 - val_loss: 15.1119\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.3854 - val_loss: 15.1125\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 13.3696 - val_loss: 15.1122\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 13.3814 - val_loss: 15.1084\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.3775 - val_loss: 15.1089\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 13.3707 - val_loss: 15.1138\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.3697 - val_loss: 15.1118\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.3811 - val_loss: 15.1144\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.3688 - val_loss: 15.1214\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 13.3651 - val_loss: 15.1181\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 13.3638 - val_loss: 15.1200\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.3772 - val_loss: 15.1197\n",
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 13.3678 - val_loss: 15.1219\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 77us/step - loss: 13.3690 - val_loss: 15.1162\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 13.3604 - val_loss: 15.1098\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 13.3664 - val_loss: 15.1068\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 13.3585 - val_loss: 15.1102\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 13.3632 - val_loss: 15.1119\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.3528 - val_loss: 15.1083\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 13.3584 - val_loss: 15.1063\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 13.3618 - val_loss: 15.1092\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 13.3587 - val_loss: 15.1157\n",
      "Epoch 96/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 13.3707 - val_loss: 15.1235\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 13.3486 - val_loss: 15.1175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 13.3463 - val_loss: 15.1098\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.3554 - val_loss: 15.1098\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 13.3591 - val_loss: 15.1103\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 13.3534 - val_loss: 15.1135\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 13.3427 - val_loss: 15.1174\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.3326 - val_loss: 15.1210\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.3441 - val_loss: 15.1213\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.3416 - val_loss: 15.1260\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.3240 - val_loss: 15.1266\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 13.3272 - val_loss: 15.1244\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.3226 - val_loss: 15.1258\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 13.3299 - val_loss: 15.1285\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.3395 - val_loss: 15.1160\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.3260 - val_loss: 15.1184\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 13.3205 - val_loss: 15.1142\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 77us/step - loss: 13.3312 - val_loss: 15.1193\n",
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 13.3315 - val_loss: 15.1280\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 13.3257 - val_loss: 15.1273\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 13.3161 - val_loss: 15.1260\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 13.3074 - val_loss: 15.1240\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 13.3251 - val_loss: 15.1333\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.3186 - val_loss: 15.1327\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 13.3191 - val_loss: 15.1318\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 13.3030 - val_loss: 15.1350\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 13.3037 - val_loss: 15.1301\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.3151 - val_loss: 15.1338\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.3129 - val_loss: 15.1405\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 13.3042 - val_loss: 15.1405\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.3048 - val_loss: 15.1396\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 13.3051 - val_loss: 15.1481\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 13.2881 - val_loss: 15.1410\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 13.2966 - val_loss: 15.1409\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 13.3035 - val_loss: 15.1373\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 13.3132 - val_loss: 15.1376\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 84us/step - loss: 13.3217 - val_loss: 15.1382\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 13.2884 - val_loss: 15.1356\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 13.3201 - val_loss: 15.1371\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.2931 - val_loss: 15.1375\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 13.3158 - val_loss: 15.1377\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.2953 - val_loss: 15.1322\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 13.3025 - val_loss: 15.1286\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.3023 - val_loss: 15.1330\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 13.2859 - val_loss: 15.1332\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 13.2834 - val_loss: 15.1331\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.2908 - val_loss: 15.1306\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 13.2798 - val_loss: 15.1260\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 75us/step - loss: 13.3011 - val_loss: 15.1327\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 13.2883 - val_loss: 15.1419\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 13.2811 - val_loss: 15.1355\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.3207 - val_loss: 15.1345\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.2986 - val_loss: 15.1462\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.3011 - val_loss: 15.1405\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2823 - val_loss: 15.1293\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.2997 - val_loss: 15.1331\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 13.2850 - val_loss: 15.1347\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.2955 - val_loss: 15.1375\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.3002 - val_loss: 15.1480\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2736 - val_loss: 15.1475\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 13.2980 - val_loss: 15.1433\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.2819 - val_loss: 15.1370\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 13.2709 - val_loss: 15.1294\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 78us/step - loss: 13.2908 - val_loss: 15.1319\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.2813 - val_loss: 15.1303\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 13.2733 - val_loss: 15.1379\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.2820 - val_loss: 15.1402\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 13.2809 - val_loss: 15.1265\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.2649 - val_loss: 15.1299\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 13.2671 - val_loss: 15.1342\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.2752 - val_loss: 15.1327\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.2887 - val_loss: 15.1453\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.2754 - val_loss: 15.1469\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 13.2698 - val_loss: 15.1386\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.2653 - val_loss: 15.1384\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2651 - val_loss: 15.1370\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 13.2963 - val_loss: 15.1368\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.2742 - val_loss: 15.1308\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 13.2634 - val_loss: 15.1281\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.2971 - val_loss: 15.1246\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 13.2711 - val_loss: 15.1319\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 13.2744 - val_loss: 15.1319\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.2721 - val_loss: 15.1271\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 13.2754 - val_loss: 15.1250\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 13.2831 - val_loss: 15.1295\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 13.2705 - val_loss: 15.1223\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.2827 - val_loss: 15.1177\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2678 - val_loss: 15.1172\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.2814 - val_loss: 15.1254\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.2542 - val_loss: 15.1446\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2861 - val_loss: 15.1441\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.2825 - val_loss: 15.1367\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 13.2695 - val_loss: 15.1297\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.2532 - val_loss: 15.1293\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.2865 - val_loss: 15.1271\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 13.2712 - val_loss: 15.1328\n",
      "Epoch 192/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 13.2655 - val_loss: 15.1349\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2719 - val_loss: 15.1229\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2586 - val_loss: 15.1226\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.2650 - val_loss: 15.1250\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.2834 - val_loss: 15.1192\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.2605 - val_loss: 15.1233\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2697 - val_loss: 15.1296\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 13.2606 - val_loss: 15.1395\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.2707 - val_loss: 15.1302\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2860 - val_loss: 15.1179\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2664 - val_loss: 15.1233\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.2559 - val_loss: 15.1262\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 13.2541 - val_loss: 15.1233\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.2617 - val_loss: 15.1246\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.2533 - val_loss: 15.1330\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.2661 - val_loss: 15.1280\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 13.2935 - val_loss: 15.1256\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.2785 - val_loss: 15.1221\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.2645 - val_loss: 15.1241\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.2686 - val_loss: 15.1285\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.2601 - val_loss: 15.1342\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 13.2686 - val_loss: 15.1243\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 13.2534 - val_loss: 15.1196\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.2768 - val_loss: 15.1212\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.2693 - val_loss: 15.1249\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 13.2641 - val_loss: 15.1256\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.2522 - val_loss: 15.1245\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 13.2509 - val_loss: 15.1230\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.2661 - val_loss: 15.1211\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.2579 - val_loss: 15.1140\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.2700 - val_loss: 15.1113\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 13.2586 - val_loss: 15.1147\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.2603 - val_loss: 15.1178\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2399 - val_loss: 15.1190\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.2756 - val_loss: 15.1195\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.2813 - val_loss: 15.1216\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 13.2608 - val_loss: 15.1173\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 13.2574 - val_loss: 15.1183\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 13.2546 - val_loss: 15.1200\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 13.2521 - val_loss: 15.1174\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.2708 - val_loss: 15.1093\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 13.2712 - val_loss: 15.1111\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 13.2580 - val_loss: 15.1241\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 13.2766 - val_loss: 15.1332\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.2620 - val_loss: 15.1273\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.2793 - val_loss: 15.1155\n",
      "Epoch 238/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.2555 - val_loss: 15.1068\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.2681 - val_loss: 15.1085\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 13.2583 - val_loss: 15.1109\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 13.2497 - val_loss: 15.1146\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 13.2553 - val_loss: 15.1184\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 13.2497 - val_loss: 15.1254\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 13.2754 - val_loss: 15.1238\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 13.2797 - val_loss: 15.1162\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 13.2552 - val_loss: 15.1196\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 13.2464 - val_loss: 15.1223\n",
      "Epoch 248/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 13.2670 - val_loss: 15.1179\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 13.2690 - val_loss: 15.1186\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 13.2576 - val_loss: 15.1194\n",
      "wm\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/300\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 4.9752 - val_loss: 5.9047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.9455 - val_loss: 5.3586\n",
      "Epoch 3/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.9205 - val_loss: 5.1274\n",
      "Epoch 4/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.8909 - val_loss: 4.9769\n",
      "Epoch 5/300\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.8735 - val_loss: 4.8782\n",
      "Epoch 6/300\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.8586 - val_loss: 4.8188\n",
      "Epoch 7/300\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.8410 - val_loss: 4.7703\n",
      "Epoch 8/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.8296 - val_loss: 4.7314\n",
      "Epoch 9/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.8108 - val_loss: 4.6850\n",
      "Epoch 10/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.8011 - val_loss: 4.6565\n",
      "Epoch 11/300\n",
      "378/378 [==============================] - 0s 77us/step - loss: 4.7945 - val_loss: 4.6270\n",
      "Epoch 12/300\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.7807 - val_loss: 4.5903\n",
      "Epoch 13/300\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.7719 - val_loss: 4.5685\n",
      "Epoch 14/300\n",
      "378/378 [==============================] - 0s 70us/step - loss: 4.7636 - val_loss: 4.5519\n",
      "Epoch 15/300\n",
      "378/378 [==============================] - 0s 75us/step - loss: 4.7579 - val_loss: 4.5323\n",
      "Epoch 16/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.7477 - val_loss: 4.5216\n",
      "Epoch 17/300\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.7486 - val_loss: 4.5049\n",
      "Epoch 18/300\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.7446 - val_loss: 4.4870\n",
      "Epoch 19/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.7397 - val_loss: 4.4896\n",
      "Epoch 20/300\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.7340 - val_loss: 4.4852\n",
      "Epoch 21/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.7395 - val_loss: 4.4788\n",
      "Epoch 22/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.7336 - val_loss: 4.4742\n",
      "Epoch 23/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.7279 - val_loss: 4.4566\n",
      "Epoch 24/300\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.7291 - val_loss: 4.4354\n",
      "Epoch 25/300\n",
      "378/378 [==============================] - 0s 72us/step - loss: 4.7369 - val_loss: 4.4260\n",
      "Epoch 26/300\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.7278 - val_loss: 4.4247\n",
      "Epoch 27/300\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.7235 - val_loss: 4.4233\n",
      "Epoch 28/300\n",
      "378/378 [==============================] - 0s 74us/step - loss: 4.7229 - val_loss: 4.4244\n",
      "Epoch 29/300\n",
      "378/378 [==============================] - 0s 76us/step - loss: 4.7226 - val_loss: 4.4245\n",
      "Epoch 30/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.7157 - val_loss: 4.4278\n",
      "Epoch 31/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.7141 - val_loss: 4.4324\n",
      "Epoch 32/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.7158 - val_loss: 4.4355\n",
      "Epoch 33/300\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.7130 - val_loss: 4.4370\n",
      "Epoch 34/300\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.7108 - val_loss: 4.4366\n",
      "Epoch 35/300\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.7025 - val_loss: 4.4371\n",
      "Epoch 36/300\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.7176 - val_loss: 4.4400\n",
      "Epoch 37/300\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.7063 - val_loss: 4.4416\n",
      "Epoch 38/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.7086 - val_loss: 4.4427\n",
      "Epoch 39/300\n",
      "378/378 [==============================] - 0s 78us/step - loss: 4.7084 - val_loss: 4.4426\n",
      "Epoch 40/300\n",
      "378/378 [==============================] - 0s 75us/step - loss: 4.7047 - val_loss: 4.4424\n",
      "Epoch 41/300\n",
      "378/378 [==============================] - 0s 70us/step - loss: 4.7044 - val_loss: 4.4422\n",
      "Epoch 42/300\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.7046 - val_loss: 4.4442\n",
      "Epoch 43/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.7036 - val_loss: 4.4478\n",
      "Epoch 44/300\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.6993 - val_loss: 4.4497\n",
      "Epoch 45/300\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.7037 - val_loss: 4.4521\n",
      "Epoch 46/300\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.6965 - val_loss: 4.4518\n",
      "Epoch 47/300\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.6908 - val_loss: 4.4517\n",
      "Epoch 48/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.6966 - val_loss: 4.4525\n",
      "Epoch 49/300\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.6956 - val_loss: 4.4551\n",
      "Epoch 50/300\n",
      "378/378 [==============================] - 0s 70us/step - loss: 4.7000 - val_loss: 4.4596\n",
      "Epoch 51/300\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.6904 - val_loss: 4.4606\n",
      "Epoch 52/300\n",
      "378/378 [==============================] - 0s 83us/step - loss: 4.6895 - val_loss: 4.4630\n",
      "Epoch 53/300\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.6855 - val_loss: 4.4634\n",
      "Epoch 54/300\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.6807 - val_loss: 4.4645\n",
      "Epoch 55/300\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.6902 - val_loss: 4.4689\n",
      "Epoch 56/300\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.6872 - val_loss: 4.4740\n",
      "Epoch 57/300\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.6908 - val_loss: 4.4719\n",
      "Epoch 58/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6829 - val_loss: 4.4733\n",
      "Epoch 59/300\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.6790 - val_loss: 4.4780\n",
      "Epoch 60/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6673 - val_loss: 4.4792\n",
      "Epoch 61/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.6897 - val_loss: 4.4783\n",
      "Epoch 62/300\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.6758 - val_loss: 4.4798\n",
      "Epoch 63/300\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.6886 - val_loss: 4.4840\n",
      "Epoch 64/300\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.6710 - val_loss: 4.4920\n",
      "Epoch 65/300\n",
      "378/378 [==============================] - 0s 74us/step - loss: 4.6713 - val_loss: 4.4933\n",
      "Epoch 66/300\n",
      "378/378 [==============================] - 0s 78us/step - loss: 4.6781 - val_loss: 4.4949\n",
      "Epoch 67/300\n",
      "378/378 [==============================] - 0s 76us/step - loss: 4.6793 - val_loss: 4.4951\n",
      "Epoch 68/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6656 - val_loss: 4.4981\n",
      "Epoch 69/300\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.6708 - val_loss: 4.4981\n",
      "Epoch 70/300\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.6709 - val_loss: 4.4989\n",
      "Epoch 71/300\n",
      "378/378 [==============================] - 0s 70us/step - loss: 4.6682 - val_loss: 4.4962\n",
      "Epoch 72/300\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.6616 - val_loss: 4.5005\n",
      "Epoch 73/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6629 - val_loss: 4.5012\n",
      "Epoch 74/300\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.6772 - val_loss: 4.4985\n",
      "Epoch 75/300\n",
      "378/378 [==============================] - 0s 74us/step - loss: 4.6723 - val_loss: 4.5037\n",
      "Epoch 76/300\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.6731 - val_loss: 4.5132\n",
      "Epoch 77/300\n",
      "378/378 [==============================] - 0s 77us/step - loss: 4.6717 - val_loss: 4.5075\n",
      "Epoch 78/300\n",
      "378/378 [==============================] - 0s 77us/step - loss: 4.6684 - val_loss: 4.5101\n",
      "Epoch 79/300\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.6881 - val_loss: 4.5152\n",
      "Epoch 80/300\n",
      "378/378 [==============================] - 0s 70us/step - loss: 4.6781 - val_loss: 4.5159\n",
      "Epoch 81/300\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.6867 - val_loss: 4.5115\n",
      "Epoch 82/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6545 - val_loss: 4.5095\n",
      "Epoch 83/300\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.6580 - val_loss: 4.5053\n",
      "Epoch 84/300\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.6814 - val_loss: 4.5085\n",
      "Epoch 85/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6732 - val_loss: 4.5084\n",
      "Epoch 86/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.6544 - val_loss: 4.5123\n",
      "Epoch 87/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6573 - val_loss: 4.5129\n",
      "Epoch 88/300\n",
      "378/378 [==============================] - 0s 77us/step - loss: 4.6699 - val_loss: 4.5124\n",
      "Epoch 89/300\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.6754 - val_loss: 4.5133\n",
      "Epoch 90/300\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.6631 - val_loss: 4.5149\n",
      "Epoch 91/300\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.6598 - val_loss: 4.5186\n",
      "Epoch 92/300\n",
      "378/378 [==============================] - 0s 75us/step - loss: 4.6628 - val_loss: 4.5046\n",
      "Epoch 93/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.6659 - val_loss: 4.5027\n",
      "Epoch 94/300\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.6683 - val_loss: 4.5063\n",
      "Epoch 95/300\n",
      "378/378 [==============================] - 0s 74us/step - loss: 4.6624 - val_loss: 4.5105\n",
      "Epoch 96/300\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.6641 - val_loss: 4.5120\n",
      "Epoch 97/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 4.823 - 0s 68us/step - loss: 4.6637 - val_loss: 4.5153\n",
      "Epoch 98/300\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.6591 - val_loss: 4.5176\n",
      "Epoch 99/300\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.6571 - val_loss: 4.5124\n",
      "Epoch 100/300\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.6591 - val_loss: 4.5093\n",
      "Epoch 101/300\n",
      "378/378 [==============================] - 0s 74us/step - loss: 4.6504 - val_loss: 4.5076\n",
      "Epoch 102/300\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.6707 - val_loss: 4.4973\n",
      "Epoch 103/300\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.6561 - val_loss: 4.5002\n",
      "Epoch 104/300\n",
      "378/378 [==============================] - 0s 67us/step - loss: 4.6598 - val_loss: 4.5015\n",
      "Epoch 105/300\n",
      "378/378 [==============================] - 0s 68us/step - loss: 4.6692 - val_loss: 4.4989\n",
      "Epoch 106/300\n",
      "378/378 [==============================] - 0s 72us/step - loss: 4.6543 - val_loss: 4.5009\n",
      "Epoch 107/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6544 - val_loss: 4.5014\n",
      "Epoch 108/300\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.6598 - val_loss: 4.5006\n",
      "Epoch 109/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.6658 - val_loss: 4.5033\n",
      "Epoch 110/300\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.6805 - val_loss: 4.5011\n",
      "Epoch 111/300\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.6492 - val_loss: 4.5056\n",
      "Epoch 112/300\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.6510 - val_loss: 4.5077\n",
      "Epoch 113/300\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.6516 - val_loss: 4.5103\n",
      "Epoch 114/300\n",
      "378/378 [==============================] - 0s 69us/step - loss: 4.6641 - val_loss: 4.5057\n",
      "Epoch 115/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.6606 - val_loss: 4.4898\n",
      "Epoch 116/300\n",
      "378/378 [==============================] - 0s 78us/step - loss: 4.6595 - val_loss: 4.4010\n",
      "Epoch 117/300\n",
      "378/378 [==============================] - 0s 74us/step - loss: 4.6786 - val_loss: 4.3777\n",
      "Epoch 118/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.6788 - val_loss: 4.3731\n",
      "Epoch 119/300\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.6825 - val_loss: 4.3703\n",
      "Epoch 120/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.6844 - val_loss: 4.3726\n",
      "Epoch 121/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.6735 - val_loss: 4.3738\n",
      "Epoch 122/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6739 - val_loss: 4.3720\n",
      "Epoch 123/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6728 - val_loss: 4.3660\n",
      "Epoch 124/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6719 - val_loss: 4.3601\n",
      "Epoch 125/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6795 - val_loss: 4.3614\n",
      "Epoch 126/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 4.6715 - val_loss: 4.3642\n",
      "Epoch 127/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6707 - val_loss: 4.3652\n",
      "Epoch 128/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6615 - val_loss: 4.3698\n",
      "Epoch 129/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.6728 - val_loss: 4.3714\n",
      "Epoch 130/300\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.6707 - val_loss: 4.3795\n",
      "Epoch 131/300\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.6609 - val_loss: 4.3821\n",
      "Epoch 132/300\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.6438 - val_loss: 4.3811\n",
      "Epoch 133/300\n",
      "378/378 [==============================] - 0s 74us/step - loss: 4.6522 - val_loss: 4.3827\n",
      "Epoch 134/300\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.6593 - val_loss: 4.3794\n",
      "Epoch 135/300\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.6602 - val_loss: 4.3861\n",
      "Epoch 136/300\n",
      "378/378 [==============================] - 0s 76us/step - loss: 4.6458 - val_loss: 4.4016\n",
      "Epoch 137/300\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.6554 - val_loss: 4.4014\n",
      "Epoch 138/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.6651 - val_loss: 4.4040\n",
      "Epoch 139/300\n",
      "378/378 [==============================] - 0s 71us/step - loss: 4.6510 - val_loss: 4.4035\n",
      "Epoch 140/300\n",
      "378/378 [==============================] - 0s 75us/step - loss: 4.6459 - val_loss: 4.4028\n",
      "Epoch 141/300\n",
      "378/378 [==============================] - 0s 73us/step - loss: 4.6527 - val_loss: 4.4019\n",
      "Epoch 142/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6576 - val_loss: 4.4060\n",
      "Epoch 143/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.6705 - val_loss: 4.4051\n",
      "Epoch 144/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.6617 - val_loss: 4.3996\n",
      "Epoch 145/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6506 - val_loss: 4.4037\n",
      "Epoch 146/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.6651 - val_loss: 4.4146\n",
      "Epoch 147/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6376 - val_loss: 4.4103\n",
      "Epoch 148/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6326 - val_loss: 4.4033\n",
      "Epoch 149/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6640 - val_loss: 4.3942\n",
      "Epoch 150/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6431 - val_loss: 4.3954\n",
      "Epoch 151/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6420 - val_loss: 4.4005\n",
      "Epoch 152/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6529 - val_loss: 4.4014\n",
      "Epoch 153/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6400 - val_loss: 4.3977\n",
      "Epoch 154/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6434 - val_loss: 4.3990\n",
      "Epoch 155/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6492 - val_loss: 4.3965\n",
      "Epoch 156/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 4.6527 - val_loss: 4.3986\n",
      "Epoch 157/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6634 - val_loss: 4.4158\n",
      "Epoch 158/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6442 - val_loss: 4.4269\n",
      "Epoch 159/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6495 - val_loss: 4.4361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6609 - val_loss: 4.4387\n",
      "Epoch 161/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6489 - val_loss: 4.4361\n",
      "Epoch 162/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.6309 - val_loss: 4.4363\n",
      "Epoch 163/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6325 - val_loss: 4.4374\n",
      "Epoch 164/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 4.6371 - val_loss: 4.4353\n",
      "Epoch 165/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6313 - val_loss: 4.4399\n",
      "Epoch 166/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6454 - val_loss: 4.4449\n",
      "Epoch 167/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6466 - val_loss: 4.4436\n",
      "Epoch 168/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6381 - val_loss: 4.4428\n",
      "Epoch 169/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 4.6346 - val_loss: 4.4399\n",
      "Epoch 170/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6245 - val_loss: 4.4416\n",
      "Epoch 171/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6369 - val_loss: 4.4502\n",
      "Epoch 172/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 4.6324 - val_loss: 4.4630\n",
      "Epoch 173/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6341 - val_loss: 4.4702\n",
      "Epoch 174/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6363 - val_loss: 4.4666\n",
      "Epoch 175/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6165 - val_loss: 4.4517\n",
      "Epoch 176/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6274 - val_loss: 4.4509\n",
      "Epoch 177/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6306 - val_loss: 4.4572\n",
      "Epoch 178/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6317 - val_loss: 4.4503\n",
      "Epoch 179/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6254 - val_loss: 4.4548\n",
      "Epoch 180/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6262 - val_loss: 4.4614\n",
      "Epoch 181/300\n",
      "378/378 [==============================] - 0s 64us/step - loss: 4.6543 - val_loss: 4.4902\n",
      "Epoch 182/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6222 - val_loss: 4.4960\n",
      "Epoch 183/300\n",
      "378/378 [==============================] - 0s 66us/step - loss: 4.6425 - val_loss: 4.4825\n",
      "Epoch 184/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6354 - val_loss: 4.4638\n",
      "Epoch 185/300\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.6431 - val_loss: 4.4446\n",
      "Epoch 186/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6242 - val_loss: 4.4364\n",
      "Epoch 187/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 4.6306 - val_loss: 4.4347\n",
      "Epoch 188/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6327 - val_loss: 4.4605\n",
      "Epoch 189/300\n",
      "378/378 [==============================] - 0s 63us/step - loss: 4.6233 - val_loss: 4.4631\n",
      "Epoch 190/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6393 - val_loss: 4.4538\n",
      "Epoch 191/300\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.6211 - val_loss: 4.4603\n",
      "Epoch 192/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6211 - val_loss: 4.4502\n",
      "Epoch 193/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6402 - val_loss: 4.4505\n",
      "Epoch 194/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6257 - val_loss: 4.4521\n",
      "Epoch 195/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 4.6351 - val_loss: 4.4516\n",
      "Epoch 196/300\n",
      "378/378 [==============================] - 0s 62us/step - loss: 4.6446 - val_loss: 4.4510\n",
      "Epoch 197/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6349 - val_loss: 4.4291\n",
      "Epoch 198/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6226 - val_loss: 4.4263\n",
      "Epoch 199/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6101 - val_loss: 4.4372\n",
      "Epoch 200/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.6420 - val_loss: 4.4446\n",
      "Epoch 201/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6257 - val_loss: 4.4471\n",
      "Epoch 202/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6166 - val_loss: 4.4440\n",
      "Epoch 203/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6062 - val_loss: 4.4449\n",
      "Epoch 204/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6216 - val_loss: 4.4473\n",
      "Epoch 205/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.6120 - val_loss: 4.4500\n",
      "Epoch 206/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6334 - val_loss: 4.4324\n",
      "Epoch 207/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6353 - val_loss: 4.4310\n",
      "Epoch 208/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.6085 - val_loss: 4.4394\n",
      "Epoch 209/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.6050 - val_loss: 4.4397\n",
      "Epoch 210/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6133 - val_loss: 4.4431\n",
      "Epoch 211/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6129 - val_loss: 4.4372\n",
      "Epoch 212/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6162 - val_loss: 4.4447\n",
      "Epoch 213/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6136 - val_loss: 4.4500\n",
      "Epoch 214/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 4.6116 - val_loss: 4.4533\n",
      "Epoch 215/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6301 - val_loss: 4.4553\n",
      "Epoch 216/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6209 - val_loss: 4.4370\n",
      "Epoch 217/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.6217 - val_loss: 4.4458\n",
      "Epoch 218/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6281 - val_loss: 4.4358\n",
      "Epoch 219/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.6252 - val_loss: 4.4352\n",
      "Epoch 220/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6143 - val_loss: 4.4216\n",
      "Epoch 221/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6170 - val_loss: 4.4136\n",
      "Epoch 222/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 4.5996 - val_loss: 4.4183\n",
      "Epoch 223/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6253 - val_loss: 4.4253\n",
      "Epoch 224/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6446 - val_loss: 4.4295\n",
      "Epoch 225/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 4.6230 - val_loss: 4.4371\n",
      "Epoch 226/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6101 - val_loss: 4.4559\n",
      "Epoch 227/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6116 - val_loss: 4.4728\n",
      "Epoch 228/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6271 - val_loss: 4.5084\n",
      "Epoch 229/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6045 - val_loss: 4.4985\n",
      "Epoch 230/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6146 - val_loss: 4.4765\n",
      "Epoch 231/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6252 - val_loss: 4.4713\n",
      "Epoch 232/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6084 - val_loss: 4.4577\n",
      "Epoch 233/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 4.6272 - val_loss: 4.4431\n",
      "Epoch 234/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6071 - val_loss: 4.4388\n",
      "Epoch 235/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6093 - val_loss: 4.4474\n",
      "Epoch 236/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6202 - val_loss: 4.4569\n",
      "Epoch 237/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6208 - val_loss: 4.4502\n",
      "Epoch 238/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6042 - val_loss: 4.4485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6076 - val_loss: 4.4532\n",
      "Epoch 240/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6164 - val_loss: 4.4488\n",
      "Epoch 241/300\n",
      "378/378 [==============================] - 0s 51us/step - loss: 4.5981 - val_loss: 4.4458\n",
      "Epoch 242/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6040 - val_loss: 4.4471\n",
      "Epoch 243/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6206 - val_loss: 4.4631\n",
      "Epoch 244/300\n",
      "378/378 [==============================] - 0s 51us/step - loss: 4.6129 - val_loss: 4.4534\n",
      "Epoch 245/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6025 - val_loss: 4.4458\n",
      "Epoch 246/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 4.6084 - val_loss: 4.4474\n",
      "Epoch 247/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 7.564 - 0s 54us/step - loss: 4.5964 - val_loss: 4.4446\n",
      "Epoch 248/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6301 - val_loss: 4.4499\n",
      "Epoch 249/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6107 - val_loss: 4.4609\n",
      "Epoch 250/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.5846 - val_loss: 4.4517\n",
      "Epoch 251/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.5901 - val_loss: 4.4501\n",
      "Epoch 252/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.6011 - val_loss: 4.4443\n",
      "Epoch 253/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.6026 - val_loss: 4.4663\n",
      "Epoch 254/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6015 - val_loss: 4.4845\n",
      "Epoch 255/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 4.5949 - val_loss: 4.4608\n",
      "Epoch 256/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 4.6303 - val_loss: 4.4604\n",
      "Epoch 257/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.5928 - val_loss: 4.4687\n",
      "Epoch 258/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.6068 - val_loss: 4.4652\n",
      "Epoch 259/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.6144 - val_loss: 4.4597\n",
      "Epoch 260/300\n",
      "378/378 [==============================] - 0s 61us/step - loss: 4.6165 - val_loss: 4.4667\n",
      "Epoch 261/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.5871 - val_loss: 4.4717\n",
      "Epoch 262/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6023 - val_loss: 4.4774\n",
      "Epoch 263/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.5843 - val_loss: 4.4759\n",
      "Epoch 264/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.5865 - val_loss: 4.4817\n",
      "Epoch 265/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.5998 - val_loss: 4.4887\n",
      "Epoch 266/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 4.6196 - val_loss: 4.5011\n",
      "Epoch 267/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6000 - val_loss: 4.4964\n",
      "Epoch 268/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.5957 - val_loss: 4.4866\n",
      "Epoch 269/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.5975 - val_loss: 4.4894\n",
      "Epoch 270/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.5960 - val_loss: 4.4785\n",
      "Epoch 271/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.5958 - val_loss: 4.4582\n",
      "Epoch 272/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6012 - val_loss: 4.4641\n",
      "Epoch 273/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.5945 - val_loss: 4.4769\n",
      "Epoch 274/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.5931 - val_loss: 4.4652\n",
      "Epoch 275/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6102 - val_loss: 4.4712\n",
      "Epoch 276/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.5920 - val_loss: 4.4738\n",
      "Epoch 277/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6131 - val_loss: 4.5223\n",
      "Epoch 278/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 4.5962 - val_loss: 4.5350\n",
      "Epoch 279/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.5853 - val_loss: 4.5171\n",
      "Epoch 280/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.5972 - val_loss: 4.5069\n",
      "Epoch 281/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 4.6020 - val_loss: 4.4684\n",
      "Epoch 282/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.5940 - val_loss: 4.4409\n",
      "Epoch 283/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.5767 - val_loss: 4.4221\n",
      "Epoch 284/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 4.6094 - val_loss: 4.4247\n",
      "Epoch 285/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.5812 - val_loss: 4.4384\n",
      "Epoch 286/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 4.6072 - val_loss: 4.4581\n",
      "Epoch 287/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.5906 - val_loss: 4.4619\n",
      "Epoch 288/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.5787 - val_loss: 4.4655\n",
      "Epoch 289/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 4.5954 - val_loss: 4.4648\n",
      "Epoch 290/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.5833 - val_loss: 4.4446\n",
      "Epoch 291/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.5987 - val_loss: 4.4296\n",
      "Epoch 292/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 4.5905 - val_loss: 4.4352\n",
      "Epoch 293/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 4.5843 - val_loss: 4.4288\n",
      "Epoch 294/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.5881 - val_loss: 4.4261\n",
      "Epoch 295/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 4.6014 - val_loss: 4.4166\n",
      "Epoch 296/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.5694 - val_loss: 4.4264\n",
      "Epoch 297/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 4.5837 - val_loss: 4.4232\n",
      "Epoch 298/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 4.6068 - val_loss: 4.4252\n",
      "Epoch 299/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 4.5910 - val_loss: 4.4376\n",
      "Epoch 300/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 4.5707 - val_loss: 4.4429\n",
      "oven\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 14.2964 - val_loss: 13.6683\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 14.2661 - val_loss: 13.3806\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 14.2495 - val_loss: 13.2886\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 14.2221 - val_loss: 13.2447\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 14.1851 - val_loss: 13.2189\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 14.1523 - val_loss: 13.1946\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 14.1238 - val_loss: 13.1788\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 14.0794 - val_loss: 13.1773\n",
      "Epoch 9/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 14.0448 - val_loss: 13.1673\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 14.0033 - val_loss: 13.1781\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 13.9363 - val_loss: 13.1761\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 13.8876 - val_loss: 13.1881\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.8409 - val_loss: 13.2097\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.7647 - val_loss: 13.2300\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 13.7087 - val_loss: 13.2421\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.6361 - val_loss: 13.2615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.5821 - val_loss: 13.2883\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 13.5197 - val_loss: 13.3014\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.4837 - val_loss: 13.2723\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.4286 - val_loss: 13.2868\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 51us/step - loss: 13.3946 - val_loss: 13.2863\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 13.3367 - val_loss: 13.2780\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.3210 - val_loss: 13.2940\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.2533 - val_loss: 13.2760\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.2190 - val_loss: 13.2820\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 13.1843 - val_loss: 13.2328\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.1756 - val_loss: 13.2484\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 13.1157 - val_loss: 13.2318\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 13.0856 - val_loss: 13.2245\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 13.0831 - val_loss: 13.2101\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 13.0154 - val_loss: 13.1785\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 12.9785 - val_loss: 13.1346\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 12.9759 - val_loss: 13.0787\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 12.8982 - val_loss: 13.0584\n",
      "Epoch 35/250\n",
      "378/378 [==============================] - 0s 51us/step - loss: 12.8963 - val_loss: 13.0734\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 12.8545 - val_loss: 13.0709\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 11.27 - 0s 57us/step - loss: 12.8044 - val_loss: 13.0675\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 51us/step - loss: 12.7331 - val_loss: 13.0300\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 12.7547 - val_loss: 12.9879\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 12.6401 - val_loss: 12.9930\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 12.6352 - val_loss: 12.9954\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 12.6024 - val_loss: 13.0032\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 12.5260 - val_loss: 12.9849\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 12.4945 - val_loss: 12.9893\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 12.5012 - val_loss: 12.9909\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 12.4683 - val_loss: 12.9894\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 12.3841 - val_loss: 13.0117\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 12.4201 - val_loss: 13.0158\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 12.4061 - val_loss: 13.0142\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 12.4269 - val_loss: 13.0209\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 12.3961 - val_loss: 13.0224\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 12.3379 - val_loss: 13.0595\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 12.3054 - val_loss: 13.0524\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 12.3153 - val_loss: 13.0685\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 12.3689 - val_loss: 13.0683\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 12.3950 - val_loss: 13.0911\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 12.2734 - val_loss: 13.0806\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 12.3442 - val_loss: 13.1190\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 12.2284 - val_loss: 13.1333\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 12.2360 - val_loss: 13.1261\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 12.1664 - val_loss: 13.1419\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 12.2435 - val_loss: 13.1210\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 12.2432 - val_loss: 13.1012\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 12.2091 - val_loss: 13.0620\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 12.1779 - val_loss: 13.0853\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 12.1844 - val_loss: 13.1355\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 12.1772 - val_loss: 13.1237\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 79us/step - loss: 12.0667 - val_loss: 13.1063\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 12.1437 - val_loss: 13.1611\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 12.1683 - val_loss: 13.1650\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 12.1523 - val_loss: 13.1400\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 12.1146 - val_loss: 13.1782\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 12.0659 - val_loss: 13.1610\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 12.1252 - val_loss: 13.1025\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 12.1035 - val_loss: 13.1336\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 12.1070 - val_loss: 13.1974\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 12.0951 - val_loss: 13.1526\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 12.0595 - val_loss: 13.1404\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 12.1094 - val_loss: 13.1639\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 12.0796 - val_loss: 13.1756\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 12.0524 - val_loss: 13.1762\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 11.9691 - val_loss: 13.1233\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 12.0076 - val_loss: 13.1442\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 12.0324 - val_loss: 13.2219\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 12.0309 - val_loss: 13.1847\n",
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 12.0153 - val_loss: 13.1855\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 11.9840 - val_loss: 13.1391\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 12.0010 - val_loss: 13.0745\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 11.9980 - val_loss: 13.1183\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 12.0938 - val_loss: 13.1391\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 12.0476 - val_loss: 13.1277\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 12.0240 - val_loss: 13.1239\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 11.9476 - val_loss: 13.1173\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 75us/step - loss: 12.0523 - val_loss: 13.1262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 12.0257 - val_loss: 13.1205\n",
      "Epoch 96/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 11.9951 - val_loss: 13.1246\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 10.69 - 0s 89us/step - loss: 12.0674 - val_loss: 13.1457\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 78us/step - loss: 12.0259 - val_loss: 13.1659\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 11.9749 - val_loss: 13.1664\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 79us/step - loss: 11.9560 - val_loss: 13.0951\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 78us/step - loss: 12.0005 - val_loss: 13.0897\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 11.9437 - val_loss: 13.0936\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 11.9355 - val_loss: 13.1093\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 11.9253 - val_loss: 13.0925\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 11.9654 - val_loss: 13.0819\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 11.9530 - val_loss: 13.0536\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 11.9782 - val_loss: 13.1146\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 12.0159 - val_loss: 13.1377\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 11.9157 - val_loss: 13.1138\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 12.0152 - val_loss: 13.0973\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 11.9349 - val_loss: 13.1733\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 11.9637 - val_loss: 13.2033\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 12.0028 - val_loss: 13.2479\n",
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 11.9323 - val_loss: 13.2061\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 11.8863 - val_loss: 13.1958\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 11.9348 - val_loss: 13.1943\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 11.9647 - val_loss: 13.1976\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 11.8578 - val_loss: 13.1564\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 11.9272 - val_loss: 13.1086\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 11.8556 - val_loss: 13.1617\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 11.9199 - val_loss: 13.1921\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 11.9291 - val_loss: 13.1671\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 11.9155 - val_loss: 13.1598\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 11.9247 - val_loss: 13.1974\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 11.9790 - val_loss: 13.1968\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 11.8574 - val_loss: 13.1596\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 11.9458 - val_loss: 13.1372\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 11.9200 - val_loss: 13.1222\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 11.8457 - val_loss: 13.1302\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 75us/step - loss: 11.8995 - val_loss: 13.1739\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 11.8785 - val_loss: 13.1294\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 77us/step - loss: 11.8127 - val_loss: 13.1281\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 11.8334 - val_loss: 13.1624\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 11.8390 - val_loss: 13.2363\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 11.8210 - val_loss: 13.2366\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 11.9254 - val_loss: 13.1854\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 11.7965 - val_loss: 13.1534\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 11.8603 - val_loss: 13.1293\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 11.8163 - val_loss: 13.0951\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 11.8588 - val_loss: 13.1417\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 12.0181 - val_loss: 13.1587\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 11.8595 - val_loss: 13.1909\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 11.8311 - val_loss: 13.1591\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 11.7801 - val_loss: 13.1253\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 11.8191 - val_loss: 13.1614\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 11.8420 - val_loss: 13.1885\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 11.8893 - val_loss: 13.1254\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 11.8173 - val_loss: 13.1416\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 11.9021 - val_loss: 13.2262\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 11.7793 - val_loss: 13.2792\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 11.8621 - val_loss: 13.1747\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 11.8493 - val_loss: 13.2295\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 11.7852 - val_loss: 13.2953\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 11.8688 - val_loss: 13.2718\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 77us/step - loss: 11.7850 - val_loss: 13.1825\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 11.8222 - val_loss: 13.3033\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 11.7481 - val_loss: 13.3418\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 11.8115 - val_loss: 13.3399\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 4.062 - 0s 72us/step - loss: 11.8182 - val_loss: 13.3042\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 11.7826 - val_loss: 13.3524\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 89us/step - loss: 11.7155 - val_loss: 13.3676\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 11.7852 - val_loss: 13.3017\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 88us/step - loss: 11.8135 - val_loss: 13.2943\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 80us/step - loss: 11.8348 - val_loss: 13.2882\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 104us/step - loss: 11.7711 - val_loss: 13.3872\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 97us/step - loss: 11.7886 - val_loss: 13.4474\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 94us/step - loss: 11.8011 - val_loss: 13.4968\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 11.8472 - val_loss: 13.4406\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 94us/step - loss: 11.7519 - val_loss: 13.3168\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 96us/step - loss: 11.7832 - val_loss: 13.2863\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 221us/step - loss: 11.8410 - val_loss: 13.2438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 11.7555 - val_loss: 13.3134\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 103us/step - loss: 11.7572 - val_loss: 13.2935\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 5.705 - 0s 81us/step - loss: 11.8272 - val_loss: 13.3853\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 85us/step - loss: 11.7173 - val_loss: 13.3733\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 83us/step - loss: 11.7760 - val_loss: 13.4292\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 95us/step - loss: 11.7964 - val_loss: 13.4054\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 88us/step - loss: 11.7681 - val_loss: 13.5772\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 84us/step - loss: 11.8035 - val_loss: 13.5072\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 101us/step - loss: 11.7398 - val_loss: 13.5268\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 103us/step - loss: 11.7325 - val_loss: 13.5482\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 101us/step - loss: 11.7560 - val_loss: 13.4497\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 105us/step - loss: 11.8839 - val_loss: 13.3948\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 94us/step - loss: 11.7289 - val_loss: 13.5968\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 80us/step - loss: 11.7128 - val_loss: 13.8617\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - 0s 102us/step - loss: 11.8069 - val_loss: 13.7134\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 109us/step - loss: 11.7205 - val_loss: 13.5338\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 95us/step - loss: 11.7590 - val_loss: 13.5201\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 103us/step - loss: 11.7764 - val_loss: 13.4853\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 98us/step - loss: 11.6835 - val_loss: 13.4666\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 89us/step - loss: 11.7787 - val_loss: 13.3984\n",
      "Epoch 192/250\n",
      "378/378 [==============================] - 0s 91us/step - loss: 11.6831 - val_loss: 13.4469\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 90us/step - loss: 11.7478 - val_loss: 13.4932\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 81us/step - loss: 11.7803 - val_loss: 13.5902\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 77us/step - loss: 11.7135 - val_loss: 13.5926\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 88us/step - loss: 11.7446 - val_loss: 13.4693\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 88us/step - loss: 11.7323 - val_loss: 13.4623\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 85us/step - loss: 11.6686 - val_loss: 13.5357\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 87us/step - loss: 11.7470 - val_loss: 13.5185\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 11.7688 - val_loss: 13.4074\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 11.8227 - val_loss: 13.4123\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 11.6919 - val_loss: 13.3879\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 11.6826 - val_loss: 13.4284\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 11.6804 - val_loss: 13.5632\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 77us/step - loss: 11.7599 - val_loss: 13.4726\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 91us/step - loss: 11.6899 - val_loss: 13.4663\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 81us/step - loss: 11.7146 - val_loss: 13.4649\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 87us/step - loss: 11.6702 - val_loss: 13.4428\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 82us/step - loss: 11.7091 - val_loss: 13.4788\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 75us/step - loss: 11.7204 - val_loss: 13.6812\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 86us/step - loss: 11.6412 - val_loss: 13.6998\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 84us/step - loss: 11.6970 - val_loss: 13.7139\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 11.7071 - val_loss: 13.7053\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 80us/step - loss: 11.6771 - val_loss: 13.6390\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 11.6819 - val_loss: 13.6882\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 91us/step - loss: 11.6219 - val_loss: 13.7036\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 88us/step - loss: 11.6942 - val_loss: 13.6024\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 86us/step - loss: 11.7456 - val_loss: 13.5345\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 83us/step - loss: 11.7372 - val_loss: 13.6789\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 93us/step - loss: 11.7224 - val_loss: 13.6671\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 92us/step - loss: 11.6801 - val_loss: 13.5459\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 84us/step - loss: 11.6470 - val_loss: 13.5507\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 11.7522 - val_loss: 13.6192\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 77us/step - loss: 11.6440 - val_loss: 13.7349\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 85us/step - loss: 11.7073 - val_loss: 13.7799\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 11.6924 - val_loss: 13.7094\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 11.7269 - val_loss: 13.7067\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 11.6744 - val_loss: 13.5788\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 11.6891 - val_loss: 13.5989\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 111us/step - loss: 11.7025 - val_loss: 13.8073\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 90us/step - loss: 11.7565 - val_loss: 13.8299\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 81us/step - loss: 11.7291 - val_loss: 13.5872\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 80us/step - loss: 11.6977 - val_loss: 13.5718\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 11.6594 - val_loss: 13.6494\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 11.7500 - val_loss: 13.5497\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 11.6683 - val_loss: 13.4712\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 11.7534 - val_loss: 13.4285\n",
      "Epoch 238/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 11.6864 - val_loss: 13.5195\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 106us/step - loss: 11.7494 - val_loss: 13.5852\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 100us/step - loss: 11.6457 - val_loss: 13.5077\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 87us/step - loss: 11.6833 - val_loss: 13.4619\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 88us/step - loss: 11.6643 - val_loss: 13.5499\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 11.6975 - val_loss: 13.5980\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - 0s 90us/step - loss: 11.7598 - val_loss: 13.6383\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 80us/step - loss: 11.7336 - val_loss: 13.4256\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 100us/step - loss: 11.7566 - val_loss: 13.4487\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 92us/step - loss: 11.6881 - val_loss: 13.4490\n",
      "Epoch 248/250\n",
      "378/378 [==============================] - 0s 91us/step - loss: 11.7035 - val_loss: 13.4296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 89us/step - loss: 11.7504 - val_loss: 13.5561\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 80us/step - loss: 11.6725 - val_loss: 13.5580\n"
     ]
    }
   ],
   "source": [
    "pred_appliance = {}\n",
    "num_iterations_dictionary = {'hvac':1200,'fridge':500,'mw':250,'dw':250,'oven':250, 'wm':300}\n",
    "for appliance in APPLIANCES_ORDER[1:]:\n",
    "    print(appliance)\n",
    "    print(\"*\"*20)\n",
    "    np.random.seed(0)\n",
    "    from keras.layers.merge import Subtract, Minimum\n",
    "    from keras import regularizers\n",
    "    agg_input = keras.layers.Input(shape=[24],name='Aggregate')\n",
    "    appliance_dense_1 = keras.layers.Dense(units=20,name='Appliance-layer-1',activation='relu')(agg_input)\n",
    "    appliance_bn = keras.layers.BatchNormalization()(appliance_dense_1)\n",
    "    dropout = keras.layers.Dropout(rate=0.1,name='Droput-Appliance')(appliance_bn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    out = keras.layers.Dense(units=24,name='Appliance-output',activation='relu')(dropout)\n",
    "    out = Minimum(name='Clip-to-agg')([out, agg_input])\n",
    "\n",
    "\n",
    "    model = keras.Model(agg_input, out)\n",
    "    model.compile('adam','mean_absolute_error')\n",
    "    model.fit(train_agg, train_appliance[appliance], epochs=num_iterations_dictionary[appliance], validation_split=0.1)\n",
    "    pred_appliance[appliance] = model.predict(test_agg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('per-appliance.pdf','wb') as f:\n",
    "    f.write(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='pdf'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Aggregate (InputLayer)          (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Appliance-layer-1 (Dense)       (None, 20)           500         Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20)           80          Appliance-layer-1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Droput-Appliance (Dropout)      (None, 20)           0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Appliance-output (Dense)        (None, 24)           504         Droput-Appliance[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Clip-to-agg (Minimum)           (None, 24)           0           Appliance-output[0][0]           \n",
      "                                                                 Aggregate[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,084\n",
      "Trainable params: 1,044\n",
      "Non-trainable params: 40\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = {}\n",
    "for appliance in APPLIANCES_ORDER[1:]:\n",
    "    try:\n",
    "        mae[appliance] = mean_absolute_error(test_appliance[appliance], pred_appliance[appliance])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dw         14.758426\n",
       "fridge     32.523749\n",
       "hvac      140.924952\n",
       "mw          6.328504\n",
       "oven       18.878965\n",
       "wm          5.767657\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dw         14.515641\n",
       "fridge     42.986721\n",
       "hvac      135.127259\n",
       "mw          6.303220\n",
       "oven       19.784436\n",
       "wm          5.618949\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 24)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0c3c11c20fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_hvac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_agg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_hvac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_fridge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "pred_hvac = model.predict(test_agg)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(pred_hvac, test_fridge))\n",
    "print(mean_absolute_error(pred_hvac, test_agg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.907349e-06\n",
       "1    -3.877686e+00\n",
       "2    -9.150000e+00\n",
       "3     0.000000e+00\n",
       "4     0.000000e+00\n",
       "5    -9.633333e+00\n",
       "6     0.000000e+00\n",
       "7     2.861023e-06\n",
       "8    -8.683333e+00\n",
       "9     9.536743e-07\n",
       "10    0.000000e+00\n",
       "11    9.536743e-07\n",
       "12   -9.583333e+00\n",
       "13   -9.516666e+00\n",
       "14   -4.711666e+01\n",
       "15    3.099442e-06\n",
       "16   -4.685000e+01\n",
       "17    9.536743e-07\n",
       "18   -7.310000e+01\n",
       "19   -7.350000e+01\n",
       "20   -4.180000e+01\n",
       "21    0.000000e+00\n",
       "22   -9.616667e+00\n",
       "23   -9.533334e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.DataFrame(pred_hvac) - pd.DataFrame(test_agg)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f56c5e38e529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#pd.Series(test_agg[1, :]).plot(label='GT')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_agg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.Series(test_mw[1, :]).plot(label='GT')\n",
    "#pd.Series(test_agg[1, :]).plot(label='GT')\n",
    "\n",
    "\n",
    "pd.Series(model.predict(test_agg[1:2])[0, :24]).plot(label='Pred')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
