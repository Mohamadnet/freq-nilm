{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 215.2054443359375\n",
      "1 204.11361694335938\n",
      "2 201.38833618164062\n",
      "3 186.33535766601562\n",
      "4 178.89883422851562\n",
      "5 168.96864318847656\n",
      "6 162.03416442871094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-31665bbb637a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from dataloader import APPLIANCE_ORDER, get_train_test\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "cuda_av = False\n",
    "if torch.cuda.is_available():\n",
    "    cuda_av = True\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "weight_appliance = {'mw':1, 'dw':1, 'dr':1,'fridge':1, 'hvac':1}\n",
    "\n",
    "# num_hidden, num_iterations, num_layers, p, num_directions = sys.argv[1:6]\n",
    "\n",
    "\n",
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, cell_type, hidden_size, num_layers, bidirectional):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        if cell_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(input_size=1, hidden_size=hidden_size,\n",
    "                              num_layers=num_layers, batch_first=True,\n",
    "                              bidirectional=bidirectional, dropout=0.1)\n",
    "        elif cell_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(input_size=1, hidden_size=hidden_size,\n",
    "                              num_layers=num_layers, batch_first=True,\n",
    "                              bidirectional=bidirectional, dropout=0.1)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(input_size=1, hidden_size=hidden_size,\n",
    "                               num_layers=num_layers, batch_first=True,\n",
    "                               bidirectional=bidirectional, dropout=0.1)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size * self.num_directions, 1)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred, hidden = self.rnn(x, None)\n",
    "        pred = self.linear(pred).view(pred.data.shape[0], -1, 1)\n",
    "        #pred = self.act(pred)\n",
    "        #pred = torch.clamp(pred, min=0.)\n",
    "        #pred = self.act(pred)\n",
    "        pred = torch.min(pred, x)\n",
    "        return pred\n",
    "\n",
    "\n",
    "class AppliancesRNN(nn.Module):\n",
    "    def __init__(self, cell_type, hidden_size, num_layers, bidirectional, num_appliance):\n",
    "        super(AppliancesRNN, self).__init__()\n",
    "        self.num_appliance = num_appliance\n",
    "        self.preds = {}\n",
    "        self.order = ORDER\n",
    "        for appliance in range(self.num_appliance):\n",
    "            if cuda_av:\n",
    "                setattr(self, \"Appliance_\" + str(appliance), CustomRNN(cell_type,\n",
    "                                                                       hidden_size,\n",
    "                                                                       num_layers,\n",
    "                                                                       bidirectional).cuda())\n",
    "            else:\n",
    "                setattr(self, \"Appliance_\" + str(appliance), CustomRNN(cell_type,\n",
    "                                                                       hidden_size,\n",
    "                                                                       num_layers,\n",
    "                                                                       bidirectional))\n",
    "\n",
    "    def forward(self, *args):\n",
    "        agg_current = args[0]\n",
    "        flag = False\n",
    "        if np.random.random() > args[1]:\n",
    "            flag = True\n",
    "        # print(\"Subtracting prediction\")\n",
    "        else:\n",
    "            pass\n",
    "        # print(\"Subtracting true\")\n",
    "        for appliance in range(self.num_appliance):\n",
    "            # print(agg_current.mean().data[0])\n",
    "            # print appliance\n",
    "            # print self.order[appliance]\n",
    "            # print args[2+appliance]\n",
    "            #print(getattr(self, \"Appliance_\" + str(appliance)))\n",
    "            self.preds[appliance] = getattr(self, \"Appliance_\" + str(appliance))(agg_current)\n",
    "            if flag:\n",
    "                agg_current = agg_current - self.preds[appliance]\n",
    "            else:\n",
    "                agg_current = agg_current - args[2 + appliance]\n",
    "\n",
    "        return torch.cat([self.preds[a] for a in range(self.num_appliance)])\n",
    "ORDER = APPLIANCE_ORDER[1:][::-1]\n",
    "\n",
    "\n",
    "cell_type = \"GRU\"\n",
    "num_hidden = 150\n",
    "num_iterations = 200\n",
    "num_layers = 1\n",
    "num_directions = 1\n",
    "\n",
    "input_dim = 1\n",
    "hidden_size = num_hidden\n",
    "num_layers = num_layers\n",
    "if num_directions == 1:\n",
    "    bidirectional = False\n",
    "else:\n",
    "    bidirectional = True\n",
    "lr = 1\n",
    "p = 0.0\n",
    "num_folds = 5\n",
    "fold_num = 0\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train, test = get_train_test(num_folds=num_folds, fold_num=fold_num)\n",
    "train_aggregate = train[:, 0, :, :].reshape(-1, 24, 1)\n",
    "test_aggregate = test[:, 0, :, :].reshape(-1, 24, 1)\n",
    "\n",
    "out_train = [None for temp in range(len(ORDER))]\n",
    "for a_num, appliance in enumerate(ORDER):\n",
    "    out_train[a_num] = Variable(\n",
    "        torch.Tensor(train[:, APPLIANCE_ORDER.index(appliance), :, :].reshape((train_aggregate.shape[0], -1, 1))))\n",
    "    if cuda_av:\n",
    "        out_train[a_num] = out_train[a_num].cuda()\n",
    "\n",
    "out_test = [None for temp in range(len(ORDER))]\n",
    "for a_num, appliance in enumerate(ORDER):\n",
    "    out_test[a_num] = Variable(\n",
    "        torch.Tensor(test[:, APPLIANCE_ORDER.index(appliance), :, :].reshape((test_aggregate.shape[0], -1, 1))))\n",
    "    if cuda_av:\n",
    "        out_test[a_num] = out_test[a_num].cuda()\n",
    "\n",
    "loss_func = nn.L1Loss()\n",
    "a = AppliancesRNN(cell_type, hidden_size, num_layers, bidirectional, len(ORDER))\n",
    "for param in a.parameters():\n",
    "    param.data = param.data.abs()\n",
    "#print(a)\n",
    "if cuda_av:\n",
    "    a = a.cuda()\n",
    "    loss_func = loss_func.cuda()\n",
    "optimizer = torch.optim.Adam(a.parameters(), lr=lr)\n",
    "\n",
    "inp = Variable(torch.Tensor(train_aggregate.reshape((train_aggregate.shape[0], -1, 1))).type(torch.FloatTensor),\n",
    "               requires_grad=True)\n",
    "for t in range(num_iterations):\n",
    "    inp = Variable(torch.Tensor(train_aggregate), requires_grad=True)\n",
    "    out = torch.cat([out_train[appliance_num] for appliance_num, appliance in enumerate(ORDER)])\n",
    "    ot =  torch.cat([out_test[appliance_num] for appliance_num, appliance in enumerate(ORDER)])\n",
    "    if cuda_av:\n",
    "        inp = inp.cuda()\n",
    "        out = out.cuda()\n",
    "        ot = ot.cuda()\n",
    "\n",
    "    params = [inp, p]\n",
    "    for a_num, appliance in enumerate(ORDER):\n",
    "        params.append(out_train[a_num])\n",
    "    # print(params)\n",
    "    pred = a(*params)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    pred_split = torch.split(pred, pred.size(0)//len(ORDER))\n",
    "\n",
    "    losses= [loss_func(pred_split[appliance_num], out_train[appliance_num])*weight_appliance[appliance] for appliance_num, appliance in enumerate(ORDER)]\n",
    "    \n",
    "    loss = sum(losses)\n",
    "    if t % 1 == 0:\n",
    "        print(t, loss.data[0])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "test_inp = Variable(torch.Tensor(test_aggregate), requires_grad=False)\n",
    "if cuda_av:\n",
    "    test_inp = test_inp.cuda()\n",
    "\n",
    "params = [test_inp, -2]\n",
    "for i in range(len(ORDER)):\n",
    "    params.append(None)\n",
    "pr = a(*params)\n",
    "pr = torch.clamp(pr, min=0.)\n",
    "test_pred = torch.split(pr, test_aggregate.shape[0])\n",
    "prediction_fold = [None for x in range(len(ORDER))]\n",
    "\n",
    "if cuda_av:\n",
    "    for appliance_num, appliance in enumerate(ORDER):\n",
    "        prediction_fold[appliance_num] = test_pred[appliance_num].cpu().data.numpy().reshape(-1, 24)\n",
    "else:\n",
    "    for appliance_num, appliance in enumerate(ORDER):\n",
    "        prediction_fold[appliance_num] = test_pred[appliance_num].data.numpy().reshape(-1, 24)\n",
    "gt_fold = [None for x in range(len(ORDER))]\n",
    "for appliance_num, appliance in enumerate(ORDER):\n",
    "    gt_fold[appliance_num] = test[:, APPLIANCE_ORDER.index(appliance), :, :].reshape(test_aggregate.shape[0], -1,\n",
    "                                                                                         1).reshape(-1, 24)\n",
    "\n",
    "\n",
    "print([x.mean() for x in pred_split])\n",
    "error = pd.Series({appliance:mean_absolute_error(gt_fold[appliance_num], prediction_fold[appliance_num]) for appliance_num, appliance in enumerate(ORDER)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
