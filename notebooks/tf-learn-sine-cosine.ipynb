{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_time_distributed_dense'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-07b111d958e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_time_distributed_dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_time_distributed_dense'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent, _time_distributed_dense\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "class AttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states \n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \n",
    "            \"Neural machine translation by jointly learning to align and translate.\" \n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        print('inputs shape:', inputs.get_shape())\n",
    "\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_points=1000\n",
    "X = np.sin(np.linspace(0, 2*np.pi, num_points)).reshape((num_points, 1))\n",
    "Y = np.cos(np.linspace(0, 2*np.pi, num_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_o = X.reshape(1000, 1,1 )\n",
    "Y_o = Y.reshape(1000, 1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = X_o[:800]\n",
    "test_X = X_o[800:]\n",
    "\n",
    "train_Y = Y_o[:800]\n",
    "test_Y = Y_o[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>-0.069129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>-0.062853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>-0.056575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>-0.050295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>-0.044012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.037728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.031442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.025155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.018867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.012579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "990 -0.069129\n",
       "991 -0.062853\n",
       "992 -0.056575\n",
       "993 -0.050295\n",
       "994 -0.044012\n",
       "995 -0.037728\n",
       "996 -0.031442\n",
       "997 -0.025155\n",
       "998 -0.018867\n",
       "999 -0.012579"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = pd.DataFrame(X)\n",
    "q.shift(2).fillna(method='bfill').tail(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>-5.657505e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>-5.029457e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>-4.401210e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>-3.772789e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>-3.144219e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-2.515525e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-1.886730e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-1.257862e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-6.289433e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-2.449294e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "990 -5.657505e-02\n",
       "991 -5.029457e-02\n",
       "992 -4.401210e-02\n",
       "993 -3.772789e-02\n",
       "994 -3.144219e-02\n",
       "995 -2.515525e-02\n",
       "996 -1.886730e-02\n",
       "997 -1.257862e-02\n",
       "998 -6.289433e-03\n",
       "999 -2.449294e-16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC -> D\n",
      "BCD -> E\n",
      "CDE -> F\n",
      "DEF -> G\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "GHI -> J\n",
      "HIJ -> K\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "KLM -> N\n",
      "LMN -> O\n",
      "MNO -> P\n",
      "NOP -> Q\n",
      "OPQ -> R\n",
      "PQR -> S\n",
      "QRS -> T\n",
      "RST -> U\n",
      "STU -> V\n",
      "TUV -> W\n",
      "UVW -> X\n",
      "VWX -> Y\n",
      "WXY -> Z\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 3, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / float(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ],\n",
       "        [ 0.03846154],\n",
       "        [ 0.07692308]],\n",
       "\n",
       "       [[ 0.03846154],\n",
       "        [ 0.07692308],\n",
       "        [ 0.11538462]],\n",
       "\n",
       "       [[ 0.07692308],\n",
       "        [ 0.11538462],\n",
       "        [ 0.15384615]],\n",
       "\n",
       "       [[ 0.11538462],\n",
       "        [ 0.15384615],\n",
       "        [ 0.19230769]],\n",
       "\n",
       "       [[ 0.15384615],\n",
       "        [ 0.19230769],\n",
       "        [ 0.23076923]],\n",
       "\n",
       "       [[ 0.19230769],\n",
       "        [ 0.23076923],\n",
       "        [ 0.26923077]],\n",
       "\n",
       "       [[ 0.23076923],\n",
       "        [ 0.26923077],\n",
       "        [ 0.30769231]],\n",
       "\n",
       "       [[ 0.26923077],\n",
       "        [ 0.30769231],\n",
       "        [ 0.34615385]],\n",
       "\n",
       "       [[ 0.30769231],\n",
       "        [ 0.34615385],\n",
       "        [ 0.38461538]],\n",
       "\n",
       "       [[ 0.34615385],\n",
       "        [ 0.38461538],\n",
       "        [ 0.42307692]],\n",
       "\n",
       "       [[ 0.38461538],\n",
       "        [ 0.42307692],\n",
       "        [ 0.46153846]],\n",
       "\n",
       "       [[ 0.42307692],\n",
       "        [ 0.46153846],\n",
       "        [ 0.5       ]],\n",
       "\n",
       "       [[ 0.46153846],\n",
       "        [ 0.5       ],\n",
       "        [ 0.53846154]],\n",
       "\n",
       "       [[ 0.5       ],\n",
       "        [ 0.53846154],\n",
       "        [ 0.57692308]],\n",
       "\n",
       "       [[ 0.53846154],\n",
       "        [ 0.57692308],\n",
       "        [ 0.61538462]],\n",
       "\n",
       "       [[ 0.57692308],\n",
       "        [ 0.61538462],\n",
       "        [ 0.65384615]],\n",
       "\n",
       "       [[ 0.61538462],\n",
       "        [ 0.65384615],\n",
       "        [ 0.69230769]],\n",
       "\n",
       "       [[ 0.65384615],\n",
       "        [ 0.69230769],\n",
       "        [ 0.73076923]],\n",
       "\n",
       "       [[ 0.69230769],\n",
       "        [ 0.73076923],\n",
       "        [ 0.76923077]],\n",
       "\n",
       "       [[ 0.73076923],\n",
       "        [ 0.76923077],\n",
       "        [ 0.80769231]],\n",
       "\n",
       "       [[ 0.76923077],\n",
       "        [ 0.80769231],\n",
       "        [ 0.84615385]],\n",
       "\n",
       "       [[ 0.80769231],\n",
       "        [ 0.84615385],\n",
       "        [ 0.88461538]],\n",
       "\n",
       "       [[ 0.84615385],\n",
       "        [ 0.88461538],\n",
       "        [ 0.92307692]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 26)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 1s - loss: 3.2654 - acc: 0.0435\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.2516 - acc: 0.0435\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.2421 - acc: 0.0435\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.2344 - acc: 0.0435\n",
      "Epoch 5/500\n",
      " - 0s - loss: 3.2261 - acc: 0.0435\n",
      "Epoch 6/500\n",
      " - 0s - loss: 3.2173 - acc: 0.0435\n",
      "Epoch 7/500\n",
      " - 0s - loss: 3.2079 - acc: 0.0435\n",
      "Epoch 8/500\n",
      " - 0s - loss: 3.1975 - acc: 0.0435\n",
      "Epoch 9/500\n",
      " - 0s - loss: 3.1853 - acc: 0.0435\n",
      "Epoch 10/500\n",
      " - 0s - loss: 3.1724 - acc: 0.0435\n",
      "Epoch 11/500\n",
      " - 0s - loss: 3.1575 - acc: 0.0435\n",
      "Epoch 12/500\n",
      " - 0s - loss: 3.1430 - acc: 0.0435\n",
      "Epoch 13/500\n",
      " - 0s - loss: 3.1278 - acc: 0.0435\n",
      "Epoch 14/500\n",
      " - 0s - loss: 3.1088 - acc: 0.0435\n",
      "Epoch 15/500\n",
      " - 0s - loss: 3.0936 - acc: 0.0435\n",
      "Epoch 16/500\n",
      " - 0s - loss: 3.0738 - acc: 0.0435\n",
      "Epoch 17/500\n",
      " - 0s - loss: 3.0562 - acc: 0.0435\n",
      "Epoch 18/500\n",
      " - 0s - loss: 3.0398 - acc: 0.0435\n",
      "Epoch 19/500\n",
      " - 0s - loss: 3.0201 - acc: 0.0435\n",
      "Epoch 20/500\n",
      " - 0s - loss: 3.0058 - acc: 0.0435\n",
      "Epoch 21/500\n",
      " - 0s - loss: 2.9811 - acc: 0.0870\n",
      "Epoch 22/500\n",
      " - 0s - loss: 2.9590 - acc: 0.0870\n",
      "Epoch 23/500\n",
      " - 0s - loss: 2.9368 - acc: 0.1304\n",
      "Epoch 24/500\n",
      " - 0s - loss: 2.9125 - acc: 0.1304\n",
      "Epoch 25/500\n",
      " - 0s - loss: 2.8869 - acc: 0.0870\n",
      "Epoch 26/500\n",
      " - 0s - loss: 2.8612 - acc: 0.0870\n",
      "Epoch 27/500\n",
      " - 0s - loss: 2.8337 - acc: 0.0435\n",
      "Epoch 28/500\n",
      " - 0s - loss: 2.8093 - acc: 0.0870\n",
      "Epoch 29/500\n",
      " - 0s - loss: 2.7772 - acc: 0.0870\n",
      "Epoch 30/500\n",
      " - 0s - loss: 2.7499 - acc: 0.0870\n",
      "Epoch 31/500\n",
      " - 0s - loss: 2.7179 - acc: 0.0870\n",
      "Epoch 32/500\n",
      " - 0s - loss: 2.6948 - acc: 0.0870\n",
      "Epoch 33/500\n",
      " - 0s - loss: 2.6657 - acc: 0.0435\n",
      "Epoch 34/500\n",
      " - 0s - loss: 2.6386 - acc: 0.0870\n",
      "Epoch 35/500\n",
      " - 0s - loss: 2.6154 - acc: 0.0870\n",
      "Epoch 36/500\n",
      " - 0s - loss: 2.5897 - acc: 0.0870\n",
      "Epoch 37/500\n",
      " - 0s - loss: 2.5652 - acc: 0.0870\n",
      "Epoch 38/500\n",
      " - 0s - loss: 2.5427 - acc: 0.0870\n",
      "Epoch 39/500\n",
      " - 0s - loss: 2.5159 - acc: 0.0870\n",
      "Epoch 40/500\n",
      " - 0s - loss: 2.4966 - acc: 0.0870\n",
      "Epoch 41/500\n",
      " - 0s - loss: 2.4724 - acc: 0.0870\n",
      "Epoch 42/500\n",
      " - 0s - loss: 2.4493 - acc: 0.0870\n",
      "Epoch 43/500\n",
      " - 0s - loss: 2.4273 - acc: 0.0870\n",
      "Epoch 44/500\n",
      " - 0s - loss: 2.4061 - acc: 0.0870\n",
      "Epoch 45/500\n",
      " - 0s - loss: 2.3807 - acc: 0.0870\n",
      "Epoch 46/500\n",
      " - 0s - loss: 2.3584 - acc: 0.1304\n",
      "Epoch 47/500\n",
      " - 0s - loss: 2.3302 - acc: 0.1304\n",
      "Epoch 48/500\n",
      " - 0s - loss: 2.3078 - acc: 0.1304\n",
      "Epoch 49/500\n",
      " - 0s - loss: 2.2863 - acc: 0.1739\n",
      "Epoch 50/500\n",
      " - 0s - loss: 2.2621 - acc: 0.2174\n",
      "Epoch 51/500\n",
      " - 0s - loss: 2.2433 - acc: 0.1739\n",
      "Epoch 52/500\n",
      " - 0s - loss: 2.2185 - acc: 0.1739\n",
      "Epoch 53/500\n",
      " - 0s - loss: 2.1919 - acc: 0.2174\n",
      "Epoch 54/500\n",
      " - 0s - loss: 2.1638 - acc: 0.2174\n",
      "Epoch 55/500\n",
      " - 0s - loss: 2.1498 - acc: 0.1739\n",
      "Epoch 56/500\n",
      " - 0s - loss: 2.1229 - acc: 0.2609\n",
      "Epoch 57/500\n",
      " - 0s - loss: 2.1063 - acc: 0.2609\n",
      "Epoch 58/500\n",
      " - 0s - loss: 2.0731 - acc: 0.3043\n",
      "Epoch 59/500\n",
      " - 0s - loss: 2.0590 - acc: 0.3043\n",
      "Epoch 60/500\n",
      " - 0s - loss: 2.0308 - acc: 0.2609\n",
      "Epoch 61/500\n",
      " - 0s - loss: 2.0176 - acc: 0.2609\n",
      "Epoch 62/500\n",
      " - 0s - loss: 1.9953 - acc: 0.3043\n",
      "Epoch 63/500\n",
      " - 0s - loss: 1.9824 - acc: 0.3478\n",
      "Epoch 64/500\n",
      " - 0s - loss: 1.9628 - acc: 0.4783\n",
      "Epoch 65/500\n",
      " - 0s - loss: 1.9472 - acc: 0.3913\n",
      "Epoch 66/500\n",
      " - 0s - loss: 1.9190 - acc: 0.3478\n",
      "Epoch 67/500\n",
      " - 0s - loss: 1.9127 - acc: 0.3913\n",
      "Epoch 68/500\n",
      " - 0s - loss: 1.8920 - acc: 0.4348\n",
      "Epoch 69/500\n",
      " - 0s - loss: 1.8831 - acc: 0.3478\n",
      "Epoch 70/500\n",
      " - 0s - loss: 1.8619 - acc: 0.3478\n",
      "Epoch 71/500\n",
      " - 0s - loss: 1.8582 - acc: 0.3913\n",
      "Epoch 72/500\n",
      " - 0s - loss: 1.8397 - acc: 0.4348\n",
      "Epoch 73/500\n",
      " - 0s - loss: 1.8306 - acc: 0.4783\n",
      "Epoch 74/500\n",
      " - 0s - loss: 1.8080 - acc: 0.4783\n",
      "Epoch 75/500\n",
      " - 0s - loss: 1.7962 - acc: 0.4783\n",
      "Epoch 76/500\n",
      " - 0s - loss: 1.7845 - acc: 0.4348\n",
      "Epoch 77/500\n",
      " - 0s - loss: 1.7721 - acc: 0.4348\n",
      "Epoch 78/500\n",
      " - 0s - loss: 1.7700 - acc: 0.4783\n",
      "Epoch 79/500\n",
      " - 0s - loss: 1.7517 - acc: 0.3913\n",
      "Epoch 80/500\n",
      " - 0s - loss: 1.7504 - acc: 0.3913\n",
      "Epoch 81/500\n",
      " - 0s - loss: 1.7315 - acc: 0.3913\n",
      "Epoch 82/500\n",
      " - 0s - loss: 1.7211 - acc: 0.3043\n",
      "Epoch 83/500\n",
      " - 0s - loss: 1.7147 - acc: 0.5217\n",
      "Epoch 84/500\n",
      " - 0s - loss: 1.6961 - acc: 0.5652\n",
      "Epoch 85/500\n",
      " - 0s - loss: 1.6884 - acc: 0.6087\n",
      "Epoch 86/500\n",
      " - 0s - loss: 1.6846 - acc: 0.5652\n",
      "Epoch 87/500\n",
      " - 0s - loss: 1.6716 - acc: 0.4348\n",
      "Epoch 88/500\n",
      " - 0s - loss: 1.6627 - acc: 0.5652\n",
      "Epoch 89/500\n",
      " - 0s - loss: 1.6535 - acc: 0.5652\n",
      "Epoch 90/500\n",
      " - 0s - loss: 1.6393 - acc: 0.5217\n",
      "Epoch 91/500\n",
      " - 0s - loss: 1.6280 - acc: 0.6087\n",
      "Epoch 92/500\n",
      " - 0s - loss: 1.6166 - acc: 0.6522\n",
      "Epoch 93/500\n",
      " - 0s - loss: 1.6147 - acc: 0.5217\n",
      "Epoch 94/500\n",
      " - 0s - loss: 1.5948 - acc: 0.6087\n",
      "Epoch 95/500\n",
      " - 0s - loss: 1.5837 - acc: 0.6087\n",
      "Epoch 96/500\n",
      " - 0s - loss: 1.5862 - acc: 0.6087\n",
      "Epoch 97/500\n",
      " - 0s - loss: 1.5732 - acc: 0.5652\n",
      "Epoch 98/500\n",
      " - 0s - loss: 1.5589 - acc: 0.5652\n",
      "Epoch 99/500\n",
      " - 0s - loss: 1.5501 - acc: 0.6087\n",
      "Epoch 100/500\n",
      " - 0s - loss: 1.5425 - acc: 0.6957\n",
      "Epoch 101/500\n",
      " - 0s - loss: 1.5343 - acc: 0.7391\n",
      "Epoch 102/500\n",
      " - 0s - loss: 1.5248 - acc: 0.6957\n",
      "Epoch 103/500\n",
      " - 0s - loss: 1.5186 - acc: 0.6957\n",
      "Epoch 104/500\n",
      " - 0s - loss: 1.5074 - acc: 0.6957\n",
      "Epoch 105/500\n",
      " - 0s - loss: 1.5012 - acc: 0.7391\n",
      "Epoch 106/500\n",
      " - 0s - loss: 1.4906 - acc: 0.6957\n",
      "Epoch 107/500\n",
      " - 0s - loss: 1.4834 - acc: 0.7391\n",
      "Epoch 108/500\n",
      " - 0s - loss: 1.4730 - acc: 0.7826\n",
      "Epoch 109/500\n",
      " - 0s - loss: 1.4671 - acc: 0.6957\n",
      "Epoch 110/500\n",
      " - 0s - loss: 1.4543 - acc: 0.6957\n",
      "Epoch 111/500\n",
      " - 0s - loss: 1.4386 - acc: 0.7826\n",
      "Epoch 112/500\n",
      " - 0s - loss: 1.4374 - acc: 0.7391\n",
      "Epoch 113/500\n",
      " - 0s - loss: 1.4268 - acc: 0.7826\n",
      "Epoch 114/500\n",
      " - 0s - loss: 1.4201 - acc: 0.7826\n",
      "Epoch 115/500\n",
      " - 0s - loss: 1.4123 - acc: 0.6957\n",
      "Epoch 116/500\n",
      " - 0s - loss: 1.4046 - acc: 0.6957\n",
      "Epoch 117/500\n",
      " - 0s - loss: 1.3965 - acc: 0.6522\n",
      "Epoch 118/500\n",
      " - 0s - loss: 1.3882 - acc: 0.6957\n",
      "Epoch 119/500\n",
      " - 0s - loss: 1.3798 - acc: 0.7826\n",
      "Epoch 120/500\n",
      " - 0s - loss: 1.3713 - acc: 0.7391\n",
      "Epoch 121/500\n",
      " - 0s - loss: 1.3636 - acc: 0.7826\n",
      "Epoch 122/500\n",
      " - 0s - loss: 1.3584 - acc: 0.6957\n",
      "Epoch 123/500\n",
      " - 0s - loss: 1.3491 - acc: 0.7391\n",
      "Epoch 124/500\n",
      " - 0s - loss: 1.3400 - acc: 0.7826\n",
      "Epoch 125/500\n",
      " - 0s - loss: 1.3286 - acc: 0.8261\n",
      "Epoch 126/500\n",
      " - 0s - loss: 1.3240 - acc: 0.7826\n",
      "Epoch 127/500\n",
      " - 0s - loss: 1.3125 - acc: 0.7826\n",
      "Epoch 128/500\n",
      " - 0s - loss: 1.3148 - acc: 0.7826\n",
      "Epoch 129/500\n",
      " - 0s - loss: 1.3040 - acc: 0.8261\n",
      "Epoch 130/500\n",
      " - 0s - loss: 1.2954 - acc: 0.8261\n",
      "Epoch 131/500\n",
      " - 0s - loss: 1.2892 - acc: 0.7826\n",
      "Epoch 132/500\n",
      " - 0s - loss: 1.2836 - acc: 0.7826\n",
      "Epoch 133/500\n",
      " - 0s - loss: 1.2825 - acc: 0.7826\n",
      "Epoch 134/500\n",
      " - 0s - loss: 1.2669 - acc: 0.8261\n",
      "Epoch 135/500\n",
      " - 0s - loss: 1.2656 - acc: 0.8261\n",
      "Epoch 136/500\n",
      " - 0s - loss: 1.2624 - acc: 0.8261\n",
      "Epoch 137/500\n",
      " - 0s - loss: 1.2488 - acc: 0.8261\n",
      "Epoch 138/500\n",
      " - 0s - loss: 1.2411 - acc: 0.8261\n",
      "Epoch 139/500\n",
      " - 0s - loss: 1.2368 - acc: 0.8261\n",
      "Epoch 140/500\n",
      " - 0s - loss: 1.2345 - acc: 0.8261\n",
      "Epoch 141/500\n",
      " - 0s - loss: 1.2211 - acc: 0.7826\n",
      "Epoch 142/500\n",
      " - 0s - loss: 1.2176 - acc: 0.8261\n",
      "Epoch 143/500\n",
      " - 0s - loss: 1.2123 - acc: 0.8261\n",
      "Epoch 144/500\n",
      " - 0s - loss: 1.2080 - acc: 0.8261\n",
      "Epoch 145/500\n",
      " - 0s - loss: 1.2020 - acc: 0.8261\n",
      "Epoch 146/500\n",
      " - 0s - loss: 1.1957 - acc: 0.8261\n",
      "Epoch 147/500\n",
      " - 0s - loss: 1.1928 - acc: 0.7826\n",
      "Epoch 148/500\n",
      " - 0s - loss: 1.1817 - acc: 0.8696\n",
      "Epoch 149/500\n",
      " - 0s - loss: 1.1719 - acc: 0.7826\n",
      "Epoch 150/500\n",
      " - 0s - loss: 1.1620 - acc: 0.8261\n",
      "Epoch 151/500\n",
      " - 0s - loss: 1.1607 - acc: 0.8261\n",
      "Epoch 152/500\n",
      " - 0s - loss: 1.1625 - acc: 0.7826\n",
      "Epoch 153/500\n",
      " - 0s - loss: 1.1556 - acc: 0.8261\n",
      "Epoch 154/500\n",
      " - 0s - loss: 1.1445 - acc: 0.8261\n",
      "Epoch 155/500\n",
      " - 0s - loss: 1.1383 - acc: 0.8261\n",
      "Epoch 156/500\n",
      " - 0s - loss: 1.1363 - acc: 0.8696\n",
      "Epoch 157/500\n",
      " - 0s - loss: 1.1245 - acc: 0.9130\n",
      "Epoch 158/500\n",
      " - 0s - loss: 1.1216 - acc: 0.8261\n",
      "Epoch 159/500\n",
      " - 0s - loss: 1.1173 - acc: 0.7826\n",
      "Epoch 160/500\n",
      " - 0s - loss: 1.1086 - acc: 0.8696\n",
      "Epoch 161/500\n",
      " - 0s - loss: 1.0996 - acc: 0.8261\n",
      "Epoch 162/500\n",
      " - 0s - loss: 1.0914 - acc: 0.8696\n",
      "Epoch 163/500\n",
      " - 0s - loss: 1.0933 - acc: 0.8696\n",
      "Epoch 164/500\n",
      " - 0s - loss: 1.0854 - acc: 0.8261\n",
      "Epoch 165/500\n",
      " - 0s - loss: 1.0766 - acc: 0.8261\n",
      "Epoch 166/500\n",
      " - 0s - loss: 1.0731 - acc: 0.8261\n",
      "Epoch 167/500\n",
      " - 0s - loss: 1.0690 - acc: 0.8261\n",
      "Epoch 168/500\n",
      " - 0s - loss: 1.0646 - acc: 0.8696\n",
      "Epoch 169/500\n",
      " - 0s - loss: 1.0641 - acc: 0.8261\n",
      "Epoch 170/500\n",
      " - 0s - loss: 1.0587 - acc: 0.8696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/500\n",
      " - 0s - loss: 1.0536 - acc: 0.9130\n",
      "Epoch 172/500\n",
      " - 0s - loss: 1.0447 - acc: 0.8696\n",
      "Epoch 173/500\n",
      " - 0s - loss: 1.0419 - acc: 0.8696\n",
      "Epoch 174/500\n",
      " - 0s - loss: 1.0303 - acc: 0.9130\n",
      "Epoch 175/500\n",
      " - 0s - loss: 1.0289 - acc: 0.9130\n",
      "Epoch 176/500\n",
      " - 0s - loss: 1.0208 - acc: 0.9130\n",
      "Epoch 177/500\n",
      " - 0s - loss: 1.0165 - acc: 0.9130\n",
      "Epoch 178/500\n",
      " - 0s - loss: 1.0148 - acc: 0.9565\n",
      "Epoch 179/500\n",
      " - 0s - loss: 1.0096 - acc: 0.8696\n",
      "Epoch 180/500\n",
      " - 0s - loss: 0.9985 - acc: 0.9565\n",
      "Epoch 181/500\n",
      " - 0s - loss: 0.9985 - acc: 0.9130\n",
      "Epoch 182/500\n",
      " - 0s - loss: 0.9867 - acc: 0.9130\n",
      "Epoch 183/500\n",
      " - 0s - loss: 0.9933 - acc: 0.9130\n",
      "Epoch 184/500\n",
      " - 0s - loss: 0.9826 - acc: 0.8696\n",
      "Epoch 185/500\n",
      " - 0s - loss: 0.9799 - acc: 0.8696\n",
      "Epoch 186/500\n",
      " - 0s - loss: 0.9702 - acc: 0.9130\n",
      "Epoch 187/500\n",
      " - 0s - loss: 0.9671 - acc: 0.9130\n",
      "Epoch 188/500\n",
      " - 0s - loss: 0.9601 - acc: 0.9130\n",
      "Epoch 189/500\n",
      " - 0s - loss: 0.9519 - acc: 0.9565\n",
      "Epoch 190/500\n",
      " - 0s - loss: 0.9506 - acc: 0.9565\n",
      "Epoch 191/500\n",
      " - 0s - loss: 0.9458 - acc: 0.8261\n",
      "Epoch 192/500\n",
      " - 0s - loss: 0.9423 - acc: 0.8696\n",
      "Epoch 193/500\n",
      " - 0s - loss: 0.9464 - acc: 0.9565\n",
      "Epoch 194/500\n",
      " - 0s - loss: 0.9316 - acc: 0.9130\n",
      "Epoch 195/500\n",
      " - 0s - loss: 0.9223 - acc: 0.9130\n",
      "Epoch 196/500\n",
      " - 0s - loss: 0.9210 - acc: 0.9130\n",
      "Epoch 197/500\n",
      " - 0s - loss: 0.9209 - acc: 0.8696\n",
      "Epoch 198/500\n",
      " - 0s - loss: 0.9097 - acc: 0.9130\n",
      "Epoch 199/500\n",
      " - 0s - loss: 0.9049 - acc: 0.9130\n",
      "Epoch 200/500\n",
      " - 0s - loss: 0.9109 - acc: 0.9565\n",
      "Epoch 201/500\n",
      " - 0s - loss: 0.8937 - acc: 0.8696\n",
      "Epoch 202/500\n",
      " - 0s - loss: 0.8884 - acc: 0.9130\n",
      "Epoch 203/500\n",
      " - 0s - loss: 0.8829 - acc: 0.9130\n",
      "Epoch 204/500\n",
      " - 0s - loss: 0.8778 - acc: 0.9565\n",
      "Epoch 205/500\n",
      " - 0s - loss: 0.8772 - acc: 0.9565\n",
      "Epoch 206/500\n",
      " - 0s - loss: 0.8745 - acc: 0.9130\n",
      "Epoch 207/500\n",
      " - 0s - loss: 0.8707 - acc: 0.9130\n",
      "Epoch 208/500\n",
      " - 0s - loss: 0.8673 - acc: 0.9130\n",
      "Epoch 209/500\n",
      " - 0s - loss: 0.8605 - acc: 0.9565\n",
      "Epoch 210/500\n",
      " - 0s - loss: 0.8513 - acc: 0.9130\n",
      "Epoch 211/500\n",
      " - 0s - loss: 0.8551 - acc: 0.9565\n",
      "Epoch 212/500\n",
      " - 0s - loss: 0.8575 - acc: 1.0000\n",
      "Epoch 213/500\n",
      " - 0s - loss: 0.8424 - acc: 0.9130\n",
      "Epoch 214/500\n",
      " - 0s - loss: 0.8348 - acc: 0.9130\n",
      "Epoch 215/500\n",
      " - 0s - loss: 0.8334 - acc: 0.9565\n",
      "Epoch 216/500\n",
      " - 0s - loss: 0.8328 - acc: 0.9130\n",
      "Epoch 217/500\n",
      " - 0s - loss: 0.8233 - acc: 0.8696\n",
      "Epoch 218/500\n",
      " - 0s - loss: 0.8137 - acc: 0.9565\n",
      "Epoch 219/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-aed7da8746fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C'] -> D\n",
      "['B', 'C', 'D'] -> E\n",
      "['C', 'D', 'E'] -> F\n",
      "['D', 'E', 'F'] -> G\n",
      "['E', 'F', 'G'] -> H\n",
      "['F', 'G', 'H'] -> I\n",
      "['G', 'H', 'I'] -> J\n",
      "['H', 'I', 'J'] -> K\n",
      "['I', 'J', 'K'] -> L\n",
      "['J', 'K', 'L'] -> M\n",
      "['K', 'L', 'M'] -> N\n",
      "['L', 'M', 'N'] -> O\n",
      "['M', 'N', 'O'] -> P\n",
      "['N', 'O', 'P'] -> Q\n",
      "['O', 'P', 'Q'] -> R\n",
      "['P', 'Q', 'R'] -> S\n",
      "['Q', 'R', 'S'] -> T\n",
      "['R', 'S', 'T'] -> U\n",
      "['S', 'T', 'U'] -> U\n",
      "['T', 'U', 'V'] -> W\n",
      "['U', 'V', 'W'] -> X\n",
      "['V', 'W', 'X'] -> Y\n",
      "['W', 'X', 'Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_points = np.linspace(0, 8*np.pi, 8000)\n",
    "\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(time_points) - seq_length, 1):\n",
    "    seq_in = np.sin(time_points)[i:i + seq_length]\n",
    "    seq_out = 2*np.sin(time_points)[i] + 5*np.cos(time_points[i]) + 3*np.cos(time_points[i]/4) + 2*np.cos(time_points[i]/2)\n",
    "    dataX.append([seq_in])\n",
    "    dataY.append(seq_out)\n",
    "    #print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "y = np.reshape(dataY, (len(dataY), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7900, 100, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/200\n",
      " - 4s - loss: 19.7875 - val_loss: 17.5747\n",
      "Epoch 2/200\n",
      " - 2s - loss: 41600132109.6835 - val_loss: 17.1974\n",
      "Epoch 3/200\n",
      " - 2s - loss: 17.8105 - val_loss: 17.2247\n",
      "Epoch 4/200\n",
      " - 2s - loss: 17.9100 - val_loss: 17.2290\n",
      "Epoch 5/200\n",
      " - 2s - loss: 17.9229 - val_loss: 17.2296\n",
      "Epoch 6/200\n",
      " - 2s - loss: 17.9244 - val_loss: 17.2296\n",
      "Epoch 7/200\n",
      " - 2s - loss: 17.9245 - val_loss: 17.2296\n",
      "Epoch 8/200\n",
      " - 2s - loss: 17.9245 - val_loss: 17.2296\n",
      "Epoch 9/200\n",
      " - 2s - loss: 17.9245 - val_loss: 17.2296\n",
      "Epoch 10/200\n",
      " - 2s - loss: 17.9245 - val_loss: 17.2296\n",
      "Epoch 11/200\n",
      " - 2s - loss: 17.9245 - val_loss: 17.2296\n",
      "Epoch 12/200\n",
      " - 2s - loss: 17.9245 - val_loss: 17.2296\n",
      "Epoch 13/200\n",
      " - 2s - loss: 17.9245 - val_loss: 17.2295\n",
      "Epoch 14/200\n",
      " - 2s - loss: 17.9245 - val_loss: 17.2295\n",
      "Epoch 15/200\n",
      " - 2s - loss: 17.9244 - val_loss: 17.2295\n",
      "Epoch 16/200\n",
      " - 2s - loss: 17.9244 - val_loss: 17.2295\n",
      "Epoch 17/200\n",
      " - 2s - loss: 17.9244 - val_loss: 17.2295\n",
      "Epoch 18/200\n",
      " - 2s - loss: 17.9244 - val_loss: 17.2295\n",
      "Epoch 19/200\n",
      " - 2s - loss: 17.9244 - val_loss: 17.2294\n",
      "Epoch 20/200\n",
      " - 2s - loss: 17.9244 - val_loss: 17.2294\n",
      "Epoch 21/200\n",
      " - 2s - loss: 17.9243 - val_loss: 17.2294\n",
      "Epoch 22/200\n",
      " - 2s - loss: 17.9243 - val_loss: 17.2294\n",
      "Epoch 23/200\n",
      " - 2s - loss: 17.9243 - val_loss: 17.2294\n",
      "Epoch 24/200\n",
      " - 2s - loss: 17.9243 - val_loss: 17.2293\n",
      "Epoch 25/200\n",
      " - 2s - loss: 17.9243 - val_loss: 17.2293\n",
      "Epoch 26/200\n",
      " - 2s - loss: 17.9242 - val_loss: 17.2293\n",
      "Epoch 27/200\n",
      " - 2s - loss: 17.9242 - val_loss: 17.2293\n",
      "Epoch 28/200\n",
      " - 2s - loss: 17.9242 - val_loss: 17.2293\n",
      "Epoch 29/200\n",
      " - 2s - loss: 17.9242 - val_loss: 17.2292\n",
      "Epoch 30/200\n",
      " - 2s - loss: 17.9241 - val_loss: 17.2292\n",
      "Epoch 31/200\n",
      " - 2s - loss: 17.9241 - val_loss: 17.2292\n",
      "Epoch 32/200\n",
      " - 2s - loss: 17.9241 - val_loss: 17.2292\n",
      "Epoch 33/200\n",
      " - 2s - loss: 17.9241 - val_loss: 17.2291\n",
      "Epoch 34/200\n",
      " - 2s - loss: 17.9240 - val_loss: 17.2291\n",
      "Epoch 35/200\n",
      " - 2s - loss: 17.9240 - val_loss: 17.2291\n",
      "Epoch 36/200\n",
      " - 2s - loss: 17.9240 - val_loss: 17.2290\n",
      "Epoch 37/200\n",
      " - 2s - loss: 17.9240 - val_loss: 17.2290\n",
      "Epoch 38/200\n",
      " - 2s - loss: 17.9239 - val_loss: 17.2290\n",
      "Epoch 39/200\n",
      " - 2s - loss: 17.9239 - val_loss: 17.2290\n",
      "Epoch 40/200\n",
      " - 2s - loss: 17.9239 - val_loss: 17.2289\n",
      "Epoch 41/200\n",
      " - 2s - loss: 17.9238 - val_loss: 17.2289\n",
      "Epoch 42/200\n",
      " - 2s - loss: 17.9238 - val_loss: 17.2289\n",
      "Epoch 43/200\n",
      " - 2s - loss: 17.9238 - val_loss: 17.2288\n",
      "Epoch 44/200\n",
      " - 2s - loss: 17.9237 - val_loss: 17.2288\n",
      "Epoch 45/200\n",
      " - 2s - loss: 17.9237 - val_loss: 17.2288\n",
      "Epoch 46/200\n",
      " - 2s - loss: 17.9237 - val_loss: 17.2287\n",
      "Epoch 47/200\n",
      " - 2s - loss: 17.9236 - val_loss: 17.2287\n",
      "Epoch 48/200\n",
      " - 2s - loss: 17.9236 - val_loss: 17.2287\n",
      "Epoch 49/200\n",
      " - 2s - loss: 17.9236 - val_loss: 17.2286\n",
      "Epoch 50/200\n",
      " - 2s - loss: 17.9235 - val_loss: 17.2286\n",
      "Epoch 51/200\n",
      " - 2s - loss: 17.9235 - val_loss: 17.2286\n",
      "Epoch 52/200\n",
      " - 2s - loss: 17.9235 - val_loss: 17.2285\n",
      "Epoch 53/200\n",
      " - 2s - loss: 17.9234 - val_loss: 17.2285\n",
      "Epoch 54/200\n",
      " - 2s - loss: 17.9234 - val_loss: 17.2285\n",
      "Epoch 55/200\n",
      " - 2s - loss: 17.9234 - val_loss: 17.2284\n",
      "Epoch 56/200\n",
      " - 2s - loss: 17.9233 - val_loss: 17.2284\n",
      "Epoch 57/200\n",
      " - 2s - loss: 17.9233 - val_loss: 17.2283\n",
      "Epoch 58/200\n",
      " - 2s - loss: 17.9232 - val_loss: 17.2283\n",
      "Epoch 59/200\n",
      " - 2s - loss: 17.9232 - val_loss: 17.2283\n",
      "Epoch 60/200\n",
      " - 2s - loss: 17.9232 - val_loss: 17.2282\n",
      "Epoch 61/200\n",
      " - 2s - loss: 17.9231 - val_loss: 17.2282\n",
      "Epoch 62/200\n",
      " - 2s - loss: 17.9231 - val_loss: 17.2281\n",
      "Epoch 63/200\n",
      " - 2s - loss: 17.9230 - val_loss: 17.2281\n",
      "Epoch 64/200\n",
      " - 2s - loss: 17.9230 - val_loss: 17.2280\n",
      "Epoch 65/200\n",
      " - 2s - loss: 17.9229 - val_loss: 17.2280\n",
      "Epoch 66/200\n",
      " - 2s - loss: 17.9229 - val_loss: 17.2280\n",
      "Epoch 67/200\n",
      " - 2s - loss: 17.9229 - val_loss: 17.2279\n",
      "Epoch 68/200\n",
      " - 2s - loss: 17.9228 - val_loss: 17.2279\n",
      "Epoch 69/200\n",
      " - 2s - loss: 17.9228 - val_loss: 17.2278\n",
      "Epoch 70/200\n",
      " - 2s - loss: 17.9227 - val_loss: 17.2278\n",
      "Epoch 71/200\n",
      " - 2s - loss: 17.9227 - val_loss: 17.2277\n",
      "Epoch 72/200\n",
      " - 2s - loss: 17.9226 - val_loss: 17.2277\n",
      "Epoch 73/200\n",
      " - 2s - loss: 17.9226 - val_loss: 17.2276\n",
      "Epoch 74/200\n",
      " - 2s - loss: 17.9225 - val_loss: 17.2276\n",
      "Epoch 75/200\n",
      " - 2s - loss: 17.9225 - val_loss: 17.2275\n",
      "Epoch 76/200\n",
      " - 2s - loss: 17.9224 - val_loss: 17.2275\n",
      "Epoch 77/200\n",
      " - 2s - loss: 17.9224 - val_loss: 17.2274\n",
      "Epoch 78/200\n",
      " - 2s - loss: 17.9223 - val_loss: 17.2274\n",
      "Epoch 79/200\n",
      " - 2s - loss: 17.9223 - val_loss: 17.2273\n",
      "Epoch 80/200\n",
      " - 2s - loss: 17.9222 - val_loss: 17.2272\n",
      "Epoch 81/200\n",
      " - 2s - loss: 17.9221 - val_loss: 17.2272\n",
      "Epoch 82/200\n",
      " - 2s - loss: 17.9221 - val_loss: 17.2271\n",
      "Epoch 83/200\n",
      " - 2s - loss: 17.9220 - val_loss: 17.2271\n",
      "Epoch 84/200\n",
      " - 2s - loss: 17.9220 - val_loss: 17.2270\n",
      "Epoch 85/200\n",
      " - 2s - loss: 17.9219 - val_loss: 17.2270\n",
      "Epoch 86/200\n",
      " - 2s - loss: 17.9219 - val_loss: 17.2269\n",
      "Epoch 87/200\n",
      " - 2s - loss: 17.9218 - val_loss: 17.2268\n",
      "Epoch 88/200\n",
      " - 2s - loss: 17.9217 - val_loss: 17.2268\n",
      "Epoch 89/200\n",
      " - 2s - loss: 17.9217 - val_loss: 17.2267\n",
      "Epoch 90/200\n",
      " - 2s - loss: 17.9216 - val_loss: 17.2267\n",
      "Epoch 91/200\n",
      " - 2s - loss: 17.9216 - val_loss: 17.2266\n",
      "Epoch 92/200\n",
      " - 2s - loss: 17.9215 - val_loss: 17.2265\n",
      "Epoch 93/200\n",
      " - 2s - loss: 17.9214 - val_loss: 17.2265\n",
      "Epoch 94/200\n",
      " - 2s - loss: 17.9214 - val_loss: 17.2264\n",
      "Epoch 95/200\n",
      " - 2s - loss: 17.9213 - val_loss: 17.2263\n",
      "Epoch 96/200\n",
      " - 2s - loss: 17.9212 - val_loss: 17.2263\n",
      "Epoch 97/200\n",
      " - 2s - loss: 17.9212 - val_loss: 17.2262\n",
      "Epoch 98/200\n",
      " - 2s - loss: 17.9211 - val_loss: 17.2261\n",
      "Epoch 99/200\n",
      " - 2s - loss: 17.9210 - val_loss: 17.2261\n",
      "Epoch 100/200\n",
      " - 2s - loss: 17.9210 - val_loss: 17.2260\n",
      "Epoch 101/200\n",
      " - 2s - loss: 17.9209 - val_loss: 17.2259\n",
      "Epoch 102/200\n",
      " - 2s - loss: 17.9208 - val_loss: 17.2258\n",
      "Epoch 103/200\n",
      " - 2s - loss: 17.9207 - val_loss: 17.2258\n",
      "Epoch 104/200\n",
      " - 2s - loss: 17.9207 - val_loss: 17.2257\n",
      "Epoch 105/200\n",
      " - 2s - loss: 17.9206 - val_loss: 17.2256\n",
      "Epoch 106/200\n",
      " - 2s - loss: 17.9205 - val_loss: 17.2256\n",
      "Epoch 107/200\n",
      " - 2s - loss: 17.9205 - val_loss: 17.2255\n",
      "Epoch 108/200\n",
      " - 2s - loss: 17.9204 - val_loss: 17.2254\n",
      "Epoch 109/200\n",
      " - 2s - loss: 17.9203 - val_loss: 17.2253\n",
      "Epoch 110/200\n",
      " - 2s - loss: 17.9202 - val_loss: 17.2252\n",
      "Epoch 111/200\n",
      " - 2s - loss: 17.9201 - val_loss: 17.2252\n",
      "Epoch 112/200\n",
      " - 2s - loss: 17.9201 - val_loss: 17.2251\n",
      "Epoch 113/200\n",
      " - 2s - loss: 17.9200 - val_loss: 17.2250\n",
      "Epoch 114/200\n",
      " - 2s - loss: 17.9199 - val_loss: 17.2249\n",
      "Epoch 115/200\n",
      " - 2s - loss: 17.9198 - val_loss: 17.2248\n",
      "Epoch 116/200\n",
      " - 2s - loss: 17.9197 - val_loss: 17.2247\n",
      "Epoch 117/200\n",
      " - 2s - loss: 17.9196 - val_loss: 17.2247\n",
      "Epoch 118/200\n",
      " - 2s - loss: 17.9196 - val_loss: 17.2246\n",
      "Epoch 119/200\n",
      " - 2s - loss: 17.9195 - val_loss: 17.2245\n",
      "Epoch 120/200\n",
      " - 2s - loss: 17.9194 - val_loss: 17.2244\n",
      "Epoch 121/200\n",
      " - 2s - loss: 17.9193 - val_loss: 17.2243\n",
      "Epoch 122/200\n",
      " - 2s - loss: 17.9192 - val_loss: 17.2242\n",
      "Epoch 123/200\n",
      " - 2s - loss: 17.9191 - val_loss: 17.2241\n",
      "Epoch 124/200\n",
      " - 2s - loss: 17.9190 - val_loss: 17.2240\n",
      "Epoch 125/200\n",
      " - 2s - loss: 17.9189 - val_loss: 17.2239\n",
      "Epoch 126/200\n",
      " - 2s - loss: 17.9188 - val_loss: 17.2238\n",
      "Epoch 127/200\n",
      " - 2s - loss: 17.9187 - val_loss: 17.2237\n",
      "Epoch 128/200\n",
      " - 2s - loss: 17.9186 - val_loss: 17.2237\n",
      "Epoch 129/200\n",
      " - 2s - loss: 17.9185 - val_loss: 17.2236\n",
      "Epoch 130/200\n",
      " - 2s - loss: 17.9185 - val_loss: 17.2235\n",
      "Epoch 131/200\n",
      " - 2s - loss: 17.9184 - val_loss: 17.2233\n",
      "Epoch 132/200\n",
      " - 2s - loss: 17.9183 - val_loss: 17.2232\n",
      "Epoch 133/200\n",
      " - 2s - loss: 17.9182 - val_loss: 17.2231\n",
      "Epoch 134/200\n",
      " - 2s - loss: 17.9180 - val_loss: 17.2230\n",
      "Epoch 135/200\n",
      " - 2s - loss: 17.9179 - val_loss: 17.2229\n",
      "Epoch 136/200\n",
      " - 2s - loss: 17.9178 - val_loss: 17.2228\n",
      "Epoch 137/200\n",
      " - 2s - loss: 17.9177 - val_loss: 17.2227\n",
      "Epoch 138/200\n",
      " - 2s - loss: 17.9176 - val_loss: 17.2226\n",
      "Epoch 139/200\n",
      " - 2s - loss: 17.9175 - val_loss: 17.2225\n",
      "Epoch 140/200\n",
      " - 2s - loss: 17.9174 - val_loss: 17.2224\n",
      "Epoch 141/200\n",
      " - 2s - loss: 17.9173 - val_loss: 17.2223\n",
      "Epoch 142/200\n",
      " - 2s - loss: 17.9172 - val_loss: 17.2222\n",
      "Epoch 143/200\n",
      " - 2s - loss: 17.9171 - val_loss: 17.2221\n",
      "Epoch 144/200\n",
      " - 2s - loss: 17.9170 - val_loss: 17.2219\n",
      "Epoch 145/200\n",
      " - 2s - loss: 17.9168 - val_loss: 17.2218\n",
      "Epoch 146/200\n",
      " - 2s - loss: 17.9167 - val_loss: 17.2217\n",
      "Epoch 147/200\n",
      " - 2s - loss: 17.9166 - val_loss: 17.2216\n",
      "Epoch 148/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 17.9165 - val_loss: 17.2215\n",
      "Epoch 149/200\n",
      " - 2s - loss: 17.9164 - val_loss: 17.2213\n",
      "Epoch 150/200\n",
      " - 2s - loss: 17.9162 - val_loss: 17.2212\n",
      "Epoch 151/200\n",
      " - 2s - loss: 17.9161 - val_loss: 17.2211\n",
      "Epoch 152/200\n",
      " - 2s - loss: 17.9160 - val_loss: 17.2210\n",
      "Epoch 153/200\n",
      " - 2s - loss: 17.9159 - val_loss: 17.2208\n",
      "Epoch 154/200\n",
      " - 2s - loss: 17.9157 - val_loss: 17.2207\n",
      "Epoch 155/200\n",
      " - 2s - loss: 17.9156 - val_loss: 17.2206\n",
      "Epoch 156/200\n",
      " - 2s - loss: 17.9155 - val_loss: 17.2204\n",
      "Epoch 157/200\n",
      " - 2s - loss: 17.9153 - val_loss: 17.2203\n",
      "Epoch 158/200\n",
      " - 2s - loss: 17.9152 - val_loss: 17.2202\n",
      "Epoch 159/200\n",
      " - 2s - loss: 17.9151 - val_loss: 17.2200\n",
      "Epoch 160/200\n",
      " - 2s - loss: 17.9149 - val_loss: 17.2199\n",
      "Epoch 161/200\n",
      " - 2s - loss: 17.9148 - val_loss: 17.2198\n",
      "Epoch 162/200\n",
      " - 2s - loss: 17.9147 - val_loss: 17.2196\n",
      "Epoch 163/200\n",
      " - 2s - loss: 17.9145 - val_loss: 17.2195\n",
      "Epoch 164/200\n",
      " - 2s - loss: 17.9144 - val_loss: 17.2193\n",
      "Epoch 165/200\n",
      " - 2s - loss: 17.9142 - val_loss: 17.2192\n",
      "Epoch 166/200\n",
      " - 2s - loss: 17.9141 - val_loss: 17.2190\n",
      "Epoch 167/200\n",
      " - 2s - loss: 17.9139 - val_loss: 17.2189\n",
      "Epoch 168/200\n",
      " - 2s - loss: 17.9138 - val_loss: 17.2187\n",
      "Epoch 169/200\n",
      " - 2s - loss: 17.9136 - val_loss: 17.2186\n",
      "Epoch 170/200\n",
      " - 2s - loss: 17.9135 - val_loss: 17.2184\n",
      "Epoch 171/200\n",
      " - 2s - loss: 17.9133 - val_loss: 17.2183\n",
      "Epoch 172/200\n",
      " - 2s - loss: 17.9132 - val_loss: 17.2181\n",
      "Epoch 173/200\n",
      " - 2s - loss: 17.9130 - val_loss: 17.2180\n",
      "Epoch 174/200\n",
      " - 2s - loss: 17.9129 - val_loss: 17.2178\n",
      "Epoch 175/200\n",
      " - 2s - loss: 17.9127 - val_loss: 17.2176\n",
      "Epoch 176/200\n",
      " - 2s - loss: 17.9125 - val_loss: 17.2175\n",
      "Epoch 177/200\n",
      " - 2s - loss: 17.9124 - val_loss: 17.2173\n",
      "Epoch 178/200\n",
      " - 2s - loss: 17.9122 - val_loss: 17.2171\n",
      "Epoch 179/200\n",
      " - 2s - loss: 17.9120 - val_loss: 17.2170\n",
      "Epoch 180/200\n",
      " - 2s - loss: 17.9119 - val_loss: 17.2168\n",
      "Epoch 181/200\n",
      " - 2s - loss: 17.9117 - val_loss: 17.2166\n",
      "Epoch 182/200\n",
      " - 2s - loss: 17.9115 - val_loss: 17.2165\n",
      "Epoch 183/200\n",
      " - 2s - loss: 17.9113 - val_loss: 17.2163\n",
      "Epoch 184/200\n",
      " - 2s - loss: 17.9112 - val_loss: 17.2161\n",
      "Epoch 185/200\n",
      " - 2s - loss: 17.9110 - val_loss: 17.2159\n",
      "Epoch 186/200\n",
      " - 2s - loss: 17.9108 - val_loss: 17.2157\n",
      "Epoch 187/200\n",
      " - 2s - loss: 17.9106 - val_loss: 17.2156\n",
      "Epoch 188/200\n",
      " - 2s - loss: 17.9104 - val_loss: 17.2154\n",
      "Epoch 189/200\n",
      " - 2s - loss: 17.9103 - val_loss: 17.2152\n",
      "Epoch 190/200\n",
      " - 2s - loss: 17.9101 - val_loss: 17.2150\n",
      "Epoch 191/200\n",
      " - 2s - loss: 17.9099 - val_loss: 17.2148\n",
      "Epoch 192/200\n",
      " - 2s - loss: 17.9097 - val_loss: 17.2146\n",
      "Epoch 193/200\n",
      " - 2s - loss: 17.9095 - val_loss: 17.2144\n",
      "Epoch 194/200\n",
      " - 2s - loss: 17.9093 - val_loss: 17.2142\n",
      "Epoch 195/200\n",
      " - 2s - loss: 17.9091 - val_loss: 17.2140\n",
      "Epoch 196/200\n",
      " - 2s - loss: 17.9089 - val_loss: 17.2138\n",
      "Epoch 197/200\n",
      " - 2s - loss: 17.9087 - val_loss: 17.2136\n",
      "Epoch 198/200\n",
      " - 2s - loss: 17.9085 - val_loss: 17.2134\n",
      "Epoch 199/200\n",
      " - 2s - loss: 17.9083 - val_loss: 17.2132\n",
      "Epoch 200/200\n",
      " - 2s - loss: 17.9081 - val_loss: 17.2130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a47ee9e10>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2]), activation='linear', recurrent_dropout=0.2, return_sequences=False))\n",
    "#model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "#model.add(LSTM(8, input_shape=(X.shape[1], X.shape[2]), activation='linear', recurrent_dropout=0.2, return_sequences=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(y.shape[1], activation='linear'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X[:6000], y[:6000], epochs=200, batch_size=200, verbose=2, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a473e0ac8>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxQKAYRhGSLEAYBiGEVIsABiGYYSU/wElIuavu1HMvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a473e0b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(y)[:][0].plot(label='GT')\n",
    "pd.DataFrame(model.predict(X))[:][0].plot(label='Pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seq2seq\n",
    "from seq2seq.models import SimpleSeq2Seq\n",
    "\n",
    "model = SimpleSeq2Seq(input_dim=1, hidden_dim=10, output_length=8, output_dim=1)\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_4 to have 3 dimensions, but got array with shape (100, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c020586772e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1415\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_4 to have 3 dimensions, but got array with shape (100, 1)"
     ]
    }
   ],
   "source": [
    "model.fit(np.sin(np.linspace(0, 1, 100)), np.cos(np.linspace(0, 1, 100)), epochs=30, batch_size=200, verbose=2, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
