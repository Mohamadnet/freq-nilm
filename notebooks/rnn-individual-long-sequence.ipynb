{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataloader import APPLIANCE_ORDER, get_train_test\n",
    "from tensor_custom_core import stf_4dim, stf_4dim_time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, cell_type, hidden_size, num_layers, bidirectional):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        if cell_type==\"RNN\":\n",
    "            self.rnn = nn.RNN(input_size=1, hidden_size=hidden_size,\n",
    "                   num_layers=num_layers, batch_first=True,\n",
    "                   bidirectional=bidirectional)\n",
    "        elif cell_type==\"GRU\":\n",
    "            self.rnn = nn.GRU(input_size=1, hidden_size=hidden_size,\n",
    "                              num_layers=num_layers, batch_first=True,\n",
    "                              bidirectional=bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(input_size=1, hidden_size=hidden_size,\n",
    "                              num_layers=num_layers, batch_first=True,\n",
    "                              bidirectional=bidirectional)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size*self.num_directions, 1 )\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred, hidden = self.rnn(x, None)\n",
    "        pred = self.linear(pred)\n",
    "        \n",
    "        #pred = pred[:, :, 23:24]\n",
    "        #pred = self.act(pred)\n",
    "        #pred = torch.clamp(pred, min=0.)\n",
    "        #pred = self.act(pred)\n",
    "        #pred = torch.min(pred, x)\n",
    "        return pred\n",
    "\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_av = True\n",
    "else:\n",
    "    cuda_av=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 0\n",
    "num_folds = 5\n",
    "cell_type=\"GRU\"\n",
    "hidden_size = 10\n",
    "lr = 1\n",
    "bidirectional = True\n",
    "appliance = \"hvac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "appliance_num = APPLIANCE_ORDER.index(appliance)\n",
    "train, test = get_train_test(2, num_folds=num_folds, fold_num=fold_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_aggregate = train[:, 0, :, :].reshape(train.shape[0], -1, 1)\n",
    "\n",
    "\n",
    "\n",
    "test_aggregate = test[:, 0, :, :].reshape(test.shape[0], -1, 1)\n",
    "\n",
    "train_appliance = train[:, appliance_num, :, :].reshape(train.shape[0], -1, 1)\n",
    "test_appliance = test[:, appliance_num, :, :].reshape(test.shape[0], -1, 1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54, 2688, 1), (54, 2688, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aggregate.shape, train_appliance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.L1Loss()\n",
    "r = CustomRNN(cell_type, hidden_size, 1, bidirectional)\n",
    "\n",
    "if cuda_av:\n",
    "    r = r.cuda()\n",
    "    loss_func = loss_func.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(r.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0037488178350031376 -0.05482201278209686\n",
      "0 892.5844116210938\n",
      "0.21765337884426117 1.7938494682312012\n",
      "1 891.1170654296875\n",
      "0.22589603066444397 7.084446430206299\n",
      "2 888.0406494140625\n",
      "0.04937722533941269 16.551998138427734\n",
      "3 881.9739990234375\n",
      "0.08351068198680878 25.708831787109375\n",
      "4 876.16845703125\n",
      "0.15687118470668793 35.672279357910156\n",
      "5 869.8837280273438\n",
      "0.2766116261482239 46.190555572509766\n",
      "6 864.0235595703125\n",
      "0.37241050601005554 57.09162902832031\n",
      "7 857.1917724609375\n",
      "0.4735007584095001 68.2833023071289\n",
      "8 850.9346923828125\n",
      "0.601460337638855 79.70441436767578\n",
      "9 843.9619140625\n",
      "0.8192564249038696 91.31637573242188\n",
      "10 836.8756103515625\n",
      "1.1024949550628662 103.07072448730469\n",
      "11 830.4198608398438\n",
      "1.29531729221344 114.91517639160156\n",
      "12 823.2904052734375\n",
      "1.4135112762451172 126.83101654052734\n",
      "13 816.8997802734375\n",
      "1.5126649141311646 138.8036651611328\n",
      "14 809.8945922851562\n",
      "1.608328938484192 150.81256103515625\n",
      "15 803.67724609375\n",
      "1.703594446182251 162.84075927734375\n",
      "16 796.9068603515625\n",
      "1.7991117238998413 174.87127685546875\n",
      "17 790.9237060546875\n",
      "1.8948533535003662 186.8909149169922\n",
      "18 784.4093017578125\n",
      "1.9906834363937378 198.89071655273438\n",
      "19 778.6414794921875\n",
      "2.0865893363952637 210.8613739013672\n",
      "20 772.3865966796875\n",
      "2.1820483207702637 222.79161071777344\n",
      "21 766.9060668945312\n",
      "2.2764384746551514 234.6680450439453\n",
      "22 761.0099487304688\n",
      "2.368960380554199 246.4782257080078\n",
      "23 755.9030151367188\n",
      "2.4588961601257324 258.2102355957031\n",
      "24 750.2607421875\n",
      "2.5458626747131348 269.8544006347656\n",
      "25 745.5337524414062\n",
      "2.629246711730957 281.40130615234375\n",
      "26 740.4171752929688\n",
      "2.7082128524780273 292.8404846191406\n",
      "27 736.0604248046875\n",
      "2.7821710109710693 304.1641845703125\n",
      "28 731.3317260742188\n",
      "2.8505311012268066 315.3658447265625\n",
      "29 726.8119506835938\n",
      "2.912336587905884 326.4379577636719\n",
      "30 723.0089721679688\n",
      "2.9671788215637207 337.3746032714844\n",
      "31 718.7432250976562\n",
      "3.0146775245666504 348.16937255859375\n",
      "32 714.808349609375\n",
      "3.0540671348571777 358.81549072265625\n",
      "33 711.5514526367188\n",
      "3.0843665599823 369.30767822265625\n",
      "34 707.83837890625\n",
      "3.104999542236328 379.6376037597656\n",
      "35 704.5242919921875\n",
      "3.116028308868408 389.7991027832031\n",
      "36 701.6202392578125\n",
      "3.1178383827209473 399.7854309082031\n",
      "37 698.6920776367188\n",
      "3.11042857170105 409.5920715332031\n",
      "38 695.7247314453125\n",
      "3.0937564373016357 419.2139892578125\n",
      "39 693.1201782226562\n",
      "3.068382501602173 428.6460876464844\n",
      "40 690.9320068359375\n",
      "3.0343966484069824 437.8821105957031\n",
      "41 688.5177001953125\n",
      "2.992459774017334 446.9178771972656\n",
      "42 686.4432373046875\n",
      "2.943587064743042 455.7502746582031\n",
      "43 684.3482666015625\n",
      "2.8884973526000977 464.3760681152344\n",
      "44 682.39306640625\n",
      "2.8276147842407227 472.794677734375\n",
      "45 680.573486328125\n",
      "2.761840343475342 481.0040283203125\n",
      "46 678.8974609375\n",
      "2.691955804824829 489.00323486328125\n",
      "47 677.3619995117188\n",
      "2.618685483932495 496.7911376953125\n",
      "48 675.9630737304688\n",
      "2.543128490447998 504.36724853515625\n",
      "49 674.6924438476562\n",
      "2.466373920440674 511.7290954589844\n",
      "50 673.5479736328125\n",
      "2.3891758918762207 518.8773803710938\n",
      "51 672.4830322265625\n",
      "2.3124582767486572 525.8115234375\n",
      "52 671.3709716796875\n",
      "2.2372312545776367 532.5333251953125\n",
      "53 670.5214233398438\n",
      "2.164625644683838 539.0418701171875\n",
      "54 669.3792724609375\n",
      "2.0959534645080566 545.3385009765625\n",
      "55 668.5787353515625\n",
      "2.0325679779052734 551.426025390625\n",
      "56 668.0289916992188\n",
      "1.9757589101791382 557.3045043945312\n",
      "57 667.3805541992188\n",
      "1.9268380403518677 562.9755859375\n",
      "58 666.562255859375\n",
      "1.8871201276779175 568.4412231445312\n",
      "59 666.0758666992188\n",
      "1.8578953742980957 573.7034301757812\n",
      "60 665.6622924804688\n",
      "1.8402177095413208 578.7656860351562\n",
      "61 665.068603515625\n",
      "1.8350077867507935 583.630859375\n",
      "62 664.7943115234375\n",
      "1.8428364992141724 588.3016967773438\n",
      "63 664.534912109375\n",
      "1.8639626502990723 592.7813110351562\n",
      "64 663.9553833007812\n",
      "1.898281216621399 597.0738525390625\n",
      "65 663.81787109375\n",
      "1.9453402757644653 601.1823120117188\n",
      "66 663.3679809570312\n",
      "2.0045394897460938 605.1099243164062\n",
      "67 663.3068237304688\n",
      "2.07511568069458 608.8599243164062\n",
      "68 662.9183349609375\n",
      "2.1561291217803955 612.4368896484375\n",
      "69 662.9701538085938\n",
      "2.2465596199035645 615.8453369140625\n",
      "70 662.6797485351562\n",
      "2.345402240753174 619.0911254882812\n",
      "71 662.4022827148438\n",
      "2.4516782760620117 622.1787719726562\n",
      "72 662.5416870117188\n",
      "2.564483165740967 625.1123046875\n",
      "73 662.1583862304688\n",
      "2.6829936504364014 627.8950805664062\n",
      "74 661.953369140625\n",
      "2.8064658641815186 630.5321044921875\n",
      "75 662.1906127929688\n",
      "2.934253454208374 633.0269165039062\n",
      "76 661.890625\n",
      "3.065783977508545 635.3846435546875\n",
      "77 661.7763061523438\n",
      "3.200554609298706 637.6095581054688\n",
      "78 661.896484375\n",
      "3.3381292819976807 639.7059326171875\n",
      "79 661.8352661132812\n",
      "3.47813081741333 641.6781616210938\n",
      "80 661.5844116210938\n",
      "3.6202337741851807 643.5300903320312\n",
      "81 661.558837890625\n",
      "3.7641522884368896 645.2664184570312\n",
      "82 661.7630004882812\n",
      "3.909639358520508 646.8919677734375\n",
      "83 661.7418823242188\n",
      "4.0564866065979 648.4107055664062\n",
      "84 661.5826416015625\n",
      "4.204501152038574 649.827880859375\n",
      "85 661.4154663085938\n",
      "4.353526592254639 651.1478271484375\n",
      "86 661.4754638671875\n",
      "4.503252983093262 652.3751831054688\n",
      "87 661.720947265625\n",
      "4.65348482131958 653.5143432617188\n",
      "88 661.605712890625\n",
      "4.803956508636475 654.5692749023438\n",
      "89 661.6641235351562\n",
      "4.9546308517456055 655.5440063476562\n",
      "90 661.5841064453125\n",
      "5.105448246002197 656.4427490234375\n",
      "91 661.4693603515625\n",
      "5.256383419036865 657.2691650390625\n",
      "92 661.3840942382812\n",
      "5.407401084899902 658.02734375\n",
      "93 661.485107421875\n",
      "5.558382511138916 658.7199096679688\n",
      "94 661.3972778320312\n",
      "5.709323406219482 659.35107421875\n",
      "95 661.3516845703125\n",
      "5.860209941864014 659.923583984375\n",
      "96 661.67529296875\n",
      "6.0108513832092285 660.4410400390625\n",
      "97 661.6182250976562\n",
      "6.161067008972168 660.9068603515625\n",
      "98 661.5945434570312\n",
      "6.310897350311279 661.3242797851562\n",
      "99 661.5368041992188\n"
     ]
    }
   ],
   "source": [
    "num_iterations=100\n",
    "for t in range(num_iterations):\n",
    "\n",
    "    inp = Variable(torch.Tensor(train_aggregate), requires_grad=True)\n",
    "    train_y = Variable(torch.Tensor(train_appliance))\n",
    "    if cuda_av:\n",
    "        inp = inp.cuda()\n",
    "        train_y = train_y.cuda()\n",
    "    pred = r(inp)\n",
    "    print(pred.std().data[0], pred.mean().data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_func(pred, train_y)\n",
    "    if t % 1 == 0:\n",
    "        print(t, loss.data[0])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inp = Variable(torch.Tensor(test_aggregate), requires_grad=False)\n",
    "test_y = Variable(torch.Tensor(test_appliance), requires_grad=False)\n",
    "if cuda_av:\n",
    "    test_inp = test_inp.cuda()\n",
    "pred = r(test_inp)\n",
    "#pred[pred<0.] = 0.\n",
    "pred = torch.clamp(pred, min=0.)\n",
    "if cuda_av:\n",
    "    prediction_fold = pred.cpu().data.numpy()\n",
    "else:\n",
    "    prediction_fold = pred.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = []\n",
    "preds = []\n",
    "\n",
    "def disagg_fold(fold_num, appliance, cell_type, hidden_size,\n",
    "                num_layers, bidirectional, lr,\n",
    "                num_iterations):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    appliance_num = APPLIANCE_ORDER.index(appliance)\n",
    "    train, test = get_train_test(num_folds=num_folds, fold_num=fold_num)\n",
    "    train_aggregate = train[:, 0, :, :].reshape(-1, 24, 1)\n",
    "    test_aggregate = test[:, 0, :, :].reshape(-1, 24, 1)\n",
    "\n",
    "    train_appliance = train[:, appliance_num, :, :].reshape(-1, 24, 1)\n",
    "    test_appliance = test[:, appliance_num, :, :].reshape(-1, 24, 1)\n",
    "    gts.append(test_appliance.reshape(-1, 24))\n",
    "    loss_func = nn.L1Loss()\n",
    "    r = CustomRNN(cell_type, hidden_size, num_layers, bidirectional)\n",
    "\n",
    "    if cuda_av:\n",
    "        r = r.cuda()\n",
    "        loss_func = loss_func.cuda()\n",
    "\n",
    "    # Setting the params all to be non-negative\n",
    "    #for param in r.parameters():\n",
    "    #    param.data = param.data.abs()\n",
    "\n",
    "    optimizer = torch.optim.Adam(r.parameters(), lr=lr)\n",
    "\n",
    "    for t in range(num_iterations):\n",
    "\n",
    "        inp = Variable(torch.Tensor(train_aggregate), requires_grad=True)\n",
    "        train_y = Variable(torch.Tensor(train_appliance))\n",
    "        if cuda_av:\n",
    "            inp = inp.cuda()\n",
    "            train_y = train_y.cuda()\n",
    "        pred = r(inp)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_func(pred, train_y)\n",
    "        if t % 5 == 0:\n",
    "            print(t, loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test_inp = Variable(torch.Tensor(test_aggregate), requires_grad=False)\n",
    "    test_y = Variable(torch.Tensor(test_appliance), requires_grad=False)\n",
    "    if cuda_av:\n",
    "        test_inp = test_inp.cuda()\n",
    "    pred = r(test_inp)\n",
    "    #pred[pred<0.] = 0.\n",
    "    pred = torch.clamp(pred, min=0.)\n",
    "    if cuda_av:\n",
    "        prediction_fold = pred.cpu().data.numpy()\n",
    "    else:\n",
    "        prediction_fold = pred.data.numpy()\n",
    "    return prediction_fold, test_appliance\n",
    "\n",
    "def disagg(appliance, cell_type, hidden_size, num_layers, bidirectional, lr, num_iterations):\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for cur_fold in range(num_folds):\n",
    "        pred, gt = disagg_fold(cur_fold, appliance, cell_type, hidden_size, num_layers\n",
    "                               ,bidirectional, lr, num_iterations)\n",
    "\n",
    "        preds.append(pred)\n",
    "        gts.append(gt)\n",
    "    return mean_absolute_error(np.concatenate(gts).flatten(), np.concatenate(preds).flatten())\n",
    "\n",
    "appliance = \"hvac\"\n",
    "cell_type=\"GRU\" # One of GRU, LSTM, RNN\n",
    "hidden_size=100 # [20, 50, 100, 150]\n",
    "num_layers=1  # [1, 2, 3, 4]\n",
    "bidirectional=False # True or False\n",
    "lr =1 # 1e-3, 1e-2, 1e-1, 1, 2\n",
    "num_iterations = 20 #200, 400, 600, 800\n",
    "\n",
    "appliance, cell_type, hidden_size, num_layers, bidirectional, lr, num_iterations = sys.argv[1:]\n",
    "hidden_size = int(hidden_size)\n",
    "num_layers = int(num_layers)\n",
    "lr = float(lr)\n",
    "num_iterations = int(num_iterations)\n",
    "\n",
    "p = disagg(appliance, cell_type, hidden_size, num_layers,\n",
    "                bidirectional, lr, num_iterations)\n",
    "\n",
    "import pickle\n",
    "pickle.dump(p, open(\"./baseline/rnn-individual-baseline-result/rnn-individual-{}-{}-{}-{}-{}-{}-{}.pkl\".format(appliance,\n",
    "\t\t\t\t\t\tcell_type, hidden_size, num_layers, bidirectional, lr, num_iterations), \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
