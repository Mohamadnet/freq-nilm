{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from common import APPLIANCES_ORDER\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = np.load('../1H-input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subset_dataset(tensor):\n",
    "    t_subset = tensor[:, :, 180:194, :]\n",
    "    all_indices = np.array(list(range(320)))\n",
    "    for i in range(1, 7):\n",
    "        valid_homes = pd.DataFrame(t_subset[:, i, :].reshape(320, 14*24)).dropna().index\n",
    "        all_indices = np.intersect1d(all_indices, valid_homes)\n",
    "    t_subset = t_subset[all_indices, :, :, :].reshape(52, 7, 14*24)\n",
    "    \n",
    "    # Create artificial aggregate\n",
    "    t_subset[:, 0, :] = 0.0\n",
    "    for i in range(1, 7):\n",
    "        t_subset[:, 0, :] = t_subset[:, 0, :] + t_subset[:, i, :]\n",
    "    # t_subset is of shape (#home, appliance, days*hours)\n",
    "    return t_subset, all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all, valid_homes = create_subset_dataset(tensor)\n",
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_objective(y_pred, y_true):\n",
    "    with tf.name_scope(None):\n",
    "        return tf.losses.absolute_difference(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/nipun/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Conv1D, Dense, Flatten, MaxPool1D, InputLayer, Activation, Dropout, MaxPooling1D\n",
    "\n",
    "\n",
    "import keras\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "n_movies = 3\n",
    "n_users=3\n",
    "n_latent_factors=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg = t_all[:30, 0, :].reshape(30*14, 24)\n",
    "train_appliance = {}\n",
    "test_appliance = {}\n",
    "for appliance_num, appliance in enumerate(APPLIANCES_ORDER[1:]):\n",
    "    train_appliance[appliance] = t_all[:30, appliance_num+1, :].reshape(30*14, 24)\n",
    "    test_appliance[appliance] = t_all[30:, appliance_num+1, :].reshape(22*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_hvac = t_all[30:, 1, :].reshape(22*14, 24)\n",
    "test_fridge = t_all[30:, 2, :].reshape(22*14, 24)\n",
    "\n",
    "test_mw = t_all[30:, 3, :].reshape(22*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "test_agg = t_all[30:, 0, :].reshape(22*14, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_hvac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-122380d9c71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_hvac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_hvac' is not defined"
     ]
    }
   ],
   "source": [
    "train_hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_hvac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-26ef9ccd3f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_hvac_fridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_hvac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fridge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_hvac_fridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_hvac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_fridge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_hvac' is not defined"
     ]
    }
   ],
   "source": [
    "train_hvac_fridge = np.hstack([train_hvac, train_fridge])\n",
    "test_hvac_fridge = np.hstack([test_hvac, test_fridge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fridge\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/500\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 104.0335 - val_loss: 80.1024\n",
      "Epoch 2/500\n",
      "378/378 [==============================] - 0s 128us/step - loss: 73.5439 - val_loss: 82.9650\n",
      "Epoch 3/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 69.5276 - val_loss: 71.1884\n",
      "Epoch 4/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 65.7872 - val_loss: 66.9487\n",
      "Epoch 5/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 61.5952 - val_loss: 60.5597\n",
      "Epoch 6/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 57.9371 - val_loss: 58.2805\n",
      "Epoch 7/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 54.5536 - val_loss: 53.5956\n",
      "Epoch 8/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 52.0083 - val_loss: 48.1487\n",
      "Epoch 9/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 49.8050 - val_loss: 47.3854\n",
      "Epoch 10/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 48.4076 - val_loss: 50.2753\n",
      "Epoch 11/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 48.1385 - val_loss: 44.8737\n",
      "Epoch 12/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 47.0126 - val_loss: 49.4019\n",
      "Epoch 13/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 46.6314 - val_loss: 44.6897\n",
      "Epoch 14/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 45.7859 - val_loss: 47.2255\n",
      "Epoch 15/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 45.4930 - val_loss: 45.9505\n",
      "Epoch 16/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 44.4689 - val_loss: 46.0596\n",
      "Epoch 17/500\n",
      "378/378 [==============================] - 0s 148us/step - loss: 43.9302 - val_loss: 43.3321\n",
      "Epoch 18/500\n",
      "378/378 [==============================] - 0s 140us/step - loss: 44.4852 - val_loss: 46.0947\n",
      "Epoch 19/500\n",
      "378/378 [==============================] - 0s 149us/step - loss: 44.3549 - val_loss: 44.9310\n",
      "Epoch 20/500\n",
      "378/378 [==============================] - 0s 174us/step - loss: 44.6102 - val_loss: 43.7336\n",
      "Epoch 21/500\n",
      "378/378 [==============================] - 0s 160us/step - loss: 44.0030 - val_loss: 48.1959\n",
      "Epoch 22/500\n",
      "378/378 [==============================] - 0s 158us/step - loss: 43.8427 - val_loss: 46.0852\n",
      "Epoch 23/500\n",
      "378/378 [==============================] - 0s 164us/step - loss: 43.7722 - val_loss: 45.0606\n",
      "Epoch 24/500\n",
      "378/378 [==============================] - 0s 156us/step - loss: 43.6814 - val_loss: 45.7697\n",
      "Epoch 25/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 43.6771 - val_loss: 46.1566\n",
      "Epoch 26/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 43.5886 - val_loss: 47.4341\n",
      "Epoch 27/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 42.9120 - val_loss: 43.5555\n",
      "Epoch 28/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 42.4706 - val_loss: 47.4463\n",
      "Epoch 29/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 42.8407 - val_loss: 44.2160\n",
      "Epoch 30/500\n",
      "378/378 [==============================] - 0s 149us/step - loss: 42.2783 - val_loss: 42.4520\n",
      "Epoch 31/500\n",
      "378/378 [==============================] - 0s 160us/step - loss: 43.3057 - val_loss: 46.7205\n",
      "Epoch 32/500\n",
      "378/378 [==============================] - 0s 184us/step - loss: 42.4322 - val_loss: 45.1091\n",
      "Epoch 33/500\n",
      "378/378 [==============================] - 0s 142us/step - loss: 42.2389 - val_loss: 40.7218\n",
      "Epoch 34/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 42.0600 - val_loss: 49.7435\n",
      "Epoch 35/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 42.2471 - val_loss: 44.0191\n",
      "Epoch 36/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 42.5494 - val_loss: 45.9218\n",
      "Epoch 37/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 41.5170 - val_loss: 41.8289\n",
      "Epoch 38/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 42.4506 - val_loss: 47.9723\n",
      "Epoch 39/500\n",
      "378/378 [==============================] - 0s 165us/step - loss: 42.6541 - val_loss: 45.8354\n",
      "Epoch 40/500\n",
      "378/378 [==============================] - 0s 177us/step - loss: 42.0716 - val_loss: 41.1500\n",
      "Epoch 41/500\n",
      "378/378 [==============================] - 0s 136us/step - loss: 41.4746 - val_loss: 48.8116\n",
      "Epoch 42/500\n",
      "378/378 [==============================] - 0s 136us/step - loss: 41.2513 - val_loss: 42.3331\n",
      "Epoch 43/500\n",
      "378/378 [==============================] - 0s 140us/step - loss: 41.2785 - val_loss: 45.9130\n",
      "Epoch 44/500\n",
      "378/378 [==============================] - 0s 141us/step - loss: 41.6845 - val_loss: 44.5290\n",
      "Epoch 45/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 41.2962 - val_loss: 42.9995\n",
      "Epoch 46/500\n",
      "378/378 [==============================] - 0s 137us/step - loss: 40.9711 - val_loss: 45.8175\n",
      "Epoch 47/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 40.8149 - val_loss: 41.8467\n",
      "Epoch 48/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 41.4019 - val_loss: 45.1568\n",
      "Epoch 49/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 40.9754 - val_loss: 43.5708\n",
      "Epoch 50/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 40.2788 - val_loss: 47.1195\n",
      "Epoch 51/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 41.0625 - val_loss: 44.8752\n",
      "Epoch 52/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 40.3319 - val_loss: 44.8103\n",
      "Epoch 53/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 41.1924 - val_loss: 44.8948\n",
      "Epoch 54/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 41.0538 - val_loss: 42.7822\n",
      "Epoch 55/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 40.2997 - val_loss: 44.7457\n",
      "Epoch 56/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 40.9821 - val_loss: 44.7372\n",
      "Epoch 57/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 40.7127 - val_loss: 41.2518\n",
      "Epoch 58/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 39.8744 - val_loss: 45.4890\n",
      "Epoch 59/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.5597 - val_loss: 43.3057\n",
      "Epoch 60/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 40.1709 - val_loss: 43.7829\n",
      "Epoch 61/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 41.0494 - val_loss: 43.0961\n",
      "Epoch 62/500\n",
      "378/378 [==============================] - 0s 130us/step - loss: 39.2322 - val_loss: 44.9770\n",
      "Epoch 63/500\n",
      "378/378 [==============================] - 0s 130us/step - loss: 39.1229 - val_loss: 41.1290\n",
      "Epoch 64/500\n",
      "378/378 [==============================] - 0s 137us/step - loss: 40.3158 - val_loss: 44.3470\n",
      "Epoch 65/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 39.8204 - val_loss: 45.1032\n",
      "Epoch 66/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 39.9711 - val_loss: 41.7657\n",
      "Epoch 67/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 40.3331 - val_loss: 43.3147\n",
      "Epoch 68/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 39.7473 - val_loss: 42.9658\n",
      "Epoch 69/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 38.9953 - val_loss: 41.1763\n",
      "Epoch 70/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 39.6315 - val_loss: 45.8329\n",
      "Epoch 71/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 39.7593 - val_loss: 42.4869\n",
      "Epoch 72/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 39.4382 - val_loss: 41.9857\n",
      "Epoch 73/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 39.6509 - val_loss: 46.6549\n",
      "Epoch 74/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 40.6982 - val_loss: 41.7623\n",
      "Epoch 75/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.1051 - val_loss: 40.3273\n",
      "Epoch 76/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 39.8424 - val_loss: 42.8496\n",
      "Epoch 77/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 38.8710 - val_loss: 42.2651\n",
      "Epoch 78/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.0188 - val_loss: 40.2959\n",
      "Epoch 79/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.6100 - val_loss: 43.2281\n",
      "Epoch 80/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 38.8302 - val_loss: 42.8166\n",
      "Epoch 81/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 39.6735 - val_loss: 42.1837\n",
      "Epoch 82/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 39.6392 - val_loss: 42.4519\n",
      "Epoch 83/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 39.2762 - val_loss: 41.7199\n",
      "Epoch 84/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.3054 - val_loss: 42.9781\n",
      "Epoch 85/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.9147 - val_loss: 41.6997\n",
      "Epoch 86/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 38.7012 - val_loss: 42.5982\n",
      "Epoch 87/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 39.7174 - val_loss: 44.3348\n",
      "Epoch 88/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 38.1136 - val_loss: 41.5925\n",
      "Epoch 89/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.2501 - val_loss: 41.0288\n",
      "Epoch 90/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 38.9085 - val_loss: 42.4723\n",
      "Epoch 91/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.5742 - val_loss: 44.1792\n",
      "Epoch 92/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.1642 - val_loss: 42.0135\n",
      "Epoch 93/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 37.9017 - val_loss: 39.7521\n",
      "Epoch 94/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.5336 - val_loss: 43.5638\n",
      "Epoch 95/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 38.6539 - val_loss: 39.9789\n",
      "Epoch 96/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.2280 - val_loss: 43.8354\n",
      "Epoch 97/500\n",
      "378/378 [==============================] - 0s 131us/step - loss: 37.6934 - val_loss: 41.9996\n",
      "Epoch 98/500\n",
      "378/378 [==============================] - 0s 136us/step - loss: 38.3388 - val_loss: 40.3986\n",
      "Epoch 99/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 38.4684 - val_loss: 43.0004\n",
      "Epoch 100/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 37.7812 - val_loss: 40.8004\n",
      "Epoch 101/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 38.3084 - val_loss: 42.5884\n",
      "Epoch 102/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 38.0219 - val_loss: 40.1661\n",
      "Epoch 103/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.8136 - val_loss: 42.0351\n",
      "Epoch 104/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.4676 - val_loss: 41.8278\n",
      "Epoch 105/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 38.0989 - val_loss: 39.3695\n",
      "Epoch 106/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 38.5142 - val_loss: 40.6178\n",
      "Epoch 107/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 37.7047 - val_loss: 42.8883\n",
      "Epoch 108/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.3515 - val_loss: 41.1268\n",
      "Epoch 109/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.6724 - val_loss: 42.5482\n",
      "Epoch 110/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.7970 - val_loss: 44.6775\n",
      "Epoch 111/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.0464 - val_loss: 41.4212\n",
      "Epoch 112/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.3740 - val_loss: 40.7406\n",
      "Epoch 113/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.1242 - val_loss: 42.7961\n",
      "Epoch 114/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.1698 - val_loss: 42.5217\n",
      "Epoch 115/500\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.6577 - val_loss: 38.7924\n",
      "Epoch 116/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 38.2747 - val_loss: 42.1709\n",
      "Epoch 117/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.2091 - val_loss: 42.9062\n",
      "Epoch 118/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 37.9202 - val_loss: 40.6951\n",
      "Epoch 119/500\n",
      "378/378 [==============================] - 0s 138us/step - loss: 37.7985 - val_loss: 40.9422\n",
      "Epoch 120/500\n",
      "378/378 [==============================] - 0s 144us/step - loss: 37.1773 - val_loss: 41.2419\n",
      "Epoch 121/500\n",
      "378/378 [==============================] - 0s 139us/step - loss: 37.9636 - val_loss: 41.1232\n",
      "Epoch 122/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 37.3918 - val_loss: 41.6713\n",
      "Epoch 123/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.3932 - val_loss: 41.4056\n",
      "Epoch 124/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.7046 - val_loss: 42.2162\n",
      "Epoch 125/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 37.0647 - val_loss: 42.7995\n",
      "Epoch 126/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.5751 - val_loss: 39.7770\n",
      "Epoch 127/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.5386 - val_loss: 43.7344\n",
      "Epoch 128/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 37.2338 - val_loss: 40.3577\n",
      "Epoch 129/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.8634 - val_loss: 42.2646\n",
      "Epoch 130/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 37.5268 - val_loss: 41.8290\n",
      "Epoch 131/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.6206 - val_loss: 40.4287\n",
      "Epoch 132/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.8056 - val_loss: 43.1433\n",
      "Epoch 133/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.6017 - val_loss: 41.4761\n",
      "Epoch 134/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 37.0967 - val_loss: 40.5701\n",
      "Epoch 135/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.5803 - val_loss: 43.6633\n",
      "Epoch 136/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.5535 - val_loss: 40.9257\n",
      "Epoch 137/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 36.9403 - val_loss: 41.8853\n",
      "Epoch 138/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 37.1445 - val_loss: 42.3455\n",
      "Epoch 139/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.7953 - val_loss: 42.5648\n",
      "Epoch 140/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 37.2671 - val_loss: 41.5168\n",
      "Epoch 141/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.3948 - val_loss: 41.5488\n",
      "Epoch 142/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.7204 - val_loss: 43.5029\n",
      "Epoch 143/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.8382 - val_loss: 42.0986\n",
      "Epoch 144/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.7946 - val_loss: 42.4184\n",
      "Epoch 145/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.5353 - val_loss: 40.5995\n",
      "Epoch 146/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.3156 - val_loss: 41.9622\n",
      "Epoch 147/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.2562 - val_loss: 41.3804\n",
      "Epoch 148/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.0928 - val_loss: 43.0357\n",
      "Epoch 149/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.3101 - val_loss: 44.4459\n",
      "Epoch 150/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.7756 - val_loss: 40.4137\n",
      "Epoch 151/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.3767 - val_loss: 44.1017\n",
      "Epoch 152/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4884 - val_loss: 40.0036\n",
      "Epoch 153/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.3941 - val_loss: 43.1410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.6051 - val_loss: 40.9227\n",
      "Epoch 155/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.8613 - val_loss: 37.9792\n",
      "Epoch 156/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.0987 - val_loss: 39.7679\n",
      "Epoch 157/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.8294 - val_loss: 40.1168\n",
      "Epoch 158/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.9135 - val_loss: 40.3052\n",
      "Epoch 159/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.9357 - val_loss: 41.5186\n",
      "Epoch 160/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.4006 - val_loss: 42.6061\n",
      "Epoch 161/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.0867 - val_loss: 39.4884\n",
      "Epoch 162/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.2109 - val_loss: 40.9371\n",
      "Epoch 163/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 35.8086 - val_loss: 40.4950\n",
      "Epoch 164/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.3686 - val_loss: 41.0350\n",
      "Epoch 165/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.5588 - val_loss: 40.5976\n",
      "Epoch 166/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.3359 - val_loss: 41.4256\n",
      "Epoch 167/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 35.9847 - val_loss: 42.4337\n",
      "Epoch 168/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 35.3983 - val_loss: 41.2303\n",
      "Epoch 169/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 35.9816 - val_loss: 41.5328\n",
      "Epoch 170/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.5666 - val_loss: 43.9853\n",
      "Epoch 171/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 35.7636 - val_loss: 39.1470\n",
      "Epoch 172/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.1742 - val_loss: 42.7128\n",
      "Epoch 173/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.5819 - val_loss: 39.3321\n",
      "Epoch 174/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.7133 - val_loss: 39.9044\n",
      "Epoch 175/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 35.7240 - val_loss: 40.8036\n",
      "Epoch 176/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 35.6581 - val_loss: 38.8584\n",
      "Epoch 177/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.0380 - val_loss: 39.5968\n",
      "Epoch 178/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.1719 - val_loss: 39.5981\n",
      "Epoch 179/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 35.2089 - val_loss: 40.9771\n",
      "Epoch 180/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.0237 - val_loss: 39.9841\n",
      "Epoch 181/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.6023 - val_loss: 40.0825\n",
      "Epoch 182/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.5747 - val_loss: 40.2889\n",
      "Epoch 183/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.0117 - val_loss: 40.0691\n",
      "Epoch 184/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 34.9315 - val_loss: 40.7599\n",
      "Epoch 185/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.4915 - val_loss: 39.3586\n",
      "Epoch 186/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.4378 - val_loss: 37.8337\n",
      "Epoch 187/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.6471 - val_loss: 40.5622\n",
      "Epoch 188/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 34.3123 - val_loss: 41.8082\n",
      "Epoch 189/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 35.4651 - val_loss: 38.2440\n",
      "Epoch 190/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.1621 - val_loss: 38.5763\n",
      "Epoch 191/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.6551 - val_loss: 38.7860\n",
      "Epoch 192/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 35.0629 - val_loss: 41.1443\n",
      "Epoch 193/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.5129 - val_loss: 39.1493\n",
      "Epoch 194/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.2658 - val_loss: 41.4198\n",
      "Epoch 195/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 34.7779 - val_loss: 40.7980\n",
      "Epoch 196/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 34.8862 - val_loss: 40.9140\n",
      "Epoch 197/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.8743 - val_loss: 41.8909\n",
      "Epoch 198/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 34.9811 - val_loss: 40.3892\n",
      "Epoch 199/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 34.7969 - val_loss: 39.4014\n",
      "Epoch 200/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.0890 - val_loss: 43.0319\n",
      "Epoch 201/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 34.5145 - val_loss: 40.5962\n",
      "Epoch 202/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 33.5780 - val_loss: 38.8348\n",
      "Epoch 203/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 35.1359 - val_loss: 41.8587\n",
      "Epoch 204/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.4528 - val_loss: 42.7902\n",
      "Epoch 205/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 34.6293 - val_loss: 40.3214\n",
      "Epoch 206/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 34.3915 - val_loss: 41.8413\n",
      "Epoch 207/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 33.8281 - val_loss: 39.3678\n",
      "Epoch 208/500\n",
      "378/378 [==============================] - 0s 131us/step - loss: 34.6807 - val_loss: 39.3762\n",
      "Epoch 209/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 34.6764 - val_loss: 40.1510\n",
      "Epoch 210/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 34.6784 - val_loss: 39.5043\n",
      "Epoch 211/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.5211 - val_loss: 39.0586\n",
      "Epoch 212/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.9104 - val_loss: 40.3789\n",
      "Epoch 213/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 34.3465 - val_loss: 40.7001\n",
      "Epoch 214/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 34.0345 - val_loss: 37.1951\n",
      "Epoch 215/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.1767 - val_loss: 36.8480\n",
      "Epoch 216/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 33.4834 - val_loss: 39.8700\n",
      "Epoch 217/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.5928 - val_loss: 37.8554\n",
      "Epoch 218/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 33.8481 - val_loss: 39.6575\n",
      "Epoch 219/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 33.7444 - val_loss: 39.6737\n",
      "Epoch 220/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.6347 - val_loss: 39.5478\n",
      "Epoch 221/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 33.2341 - val_loss: 37.9035\n",
      "Epoch 222/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.6352 - val_loss: 40.9090\n",
      "Epoch 223/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.3126 - val_loss: 39.1993\n",
      "Epoch 224/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.8085 - val_loss: 40.9065\n",
      "Epoch 225/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 34.0013 - val_loss: 39.4138\n",
      "Epoch 226/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 33.1372 - val_loss: 38.9036\n",
      "Epoch 227/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 33.5136 - val_loss: 39.4523\n",
      "Epoch 228/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 33.1981 - val_loss: 39.3788\n",
      "Epoch 229/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 34.0994 - val_loss: 40.0920\n",
      "Epoch 230/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.7761 - val_loss: 38.8550\n",
      "Epoch 231/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.1654 - val_loss: 40.7713\n",
      "Epoch 232/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 33.2325 - val_loss: 39.1207\n",
      "Epoch 233/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.4129 - val_loss: 41.0991\n",
      "Epoch 234/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 33.4390 - val_loss: 40.5542\n",
      "Epoch 235/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.3369 - val_loss: 40.3075\n",
      "Epoch 236/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 33.1326 - val_loss: 41.1542\n",
      "Epoch 237/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.1350 - val_loss: 40.2384\n",
      "Epoch 238/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.3292 - val_loss: 38.4436\n",
      "Epoch 239/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.5944 - val_loss: 41.0797\n",
      "Epoch 240/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 32.7899 - val_loss: 39.7161\n",
      "Epoch 241/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.7902 - val_loss: 40.3507\n",
      "Epoch 242/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.6278 - val_loss: 40.6046\n",
      "Epoch 243/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 32.6562 - val_loss: 38.6116\n",
      "Epoch 244/500\n",
      "378/378 [==============================] - 0s 104us/step - loss: 32.3921 - val_loss: 38.9213\n",
      "Epoch 245/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.9833 - val_loss: 37.8780\n",
      "Epoch 246/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.6613 - val_loss: 38.0931\n",
      "Epoch 247/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 32.2458 - val_loss: 38.0734\n",
      "Epoch 248/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.2783 - val_loss: 38.4257\n",
      "Epoch 249/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.8058 - val_loss: 37.4787\n",
      "Epoch 250/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 32.3892 - val_loss: 38.5760\n",
      "Epoch 251/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 31.8504 - val_loss: 38.8264\n",
      "Epoch 252/500\n",
      "378/378 [==============================] - 0s 147us/step - loss: 32.6496 - val_loss: 39.7328\n",
      "Epoch 253/500\n",
      "378/378 [==============================] - 0s 158us/step - loss: 32.8848 - val_loss: 38.6988\n",
      "Epoch 254/500\n",
      "378/378 [==============================] - 0s 143us/step - loss: 32.1924 - val_loss: 37.5768\n",
      "Epoch 255/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.1575 - val_loss: 38.1733\n",
      "Epoch 256/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.2895 - val_loss: 38.2265\n",
      "Epoch 257/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.4456 - val_loss: 39.3512\n",
      "Epoch 258/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.7110 - val_loss: 40.1061\n",
      "Epoch 259/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 32.6290 - val_loss: 39.3175\n",
      "Epoch 260/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.6322 - val_loss: 39.9267\n",
      "Epoch 261/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.0425 - val_loss: 39.3842\n",
      "Epoch 262/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.8770 - val_loss: 37.8634\n",
      "Epoch 263/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 32.2401 - val_loss: 38.3843\n",
      "Epoch 264/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.9199 - val_loss: 40.1153\n",
      "Epoch 265/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 32.0201 - val_loss: 39.0634\n",
      "Epoch 266/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.1032 - val_loss: 38.9554\n",
      "Epoch 267/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.9224 - val_loss: 38.0711\n",
      "Epoch 268/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.9938 - val_loss: 38.6844\n",
      "Epoch 269/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 32.0690 - val_loss: 40.0462\n",
      "Epoch 270/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.6396 - val_loss: 39.5965\n",
      "Epoch 271/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 32.0912 - val_loss: 40.2744\n",
      "Epoch 272/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.9187 - val_loss: 39.5437\n",
      "Epoch 273/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.0368 - val_loss: 40.3475\n",
      "Epoch 274/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.6007 - val_loss: 40.0427\n",
      "Epoch 275/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 32.3510 - val_loss: 40.2399\n",
      "Epoch 276/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.9359 - val_loss: 39.6656\n",
      "Epoch 277/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.5143 - val_loss: 40.0709\n",
      "Epoch 278/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.5180 - val_loss: 38.3469\n",
      "Epoch 279/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.4433 - val_loss: 40.1589\n",
      "Epoch 280/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.8190 - val_loss: 39.7440\n",
      "Epoch 281/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.2347 - val_loss: 40.3970\n",
      "Epoch 282/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.8447 - val_loss: 40.1331\n",
      "Epoch 283/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.4352 - val_loss: 38.7765\n",
      "Epoch 284/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.5202 - val_loss: 38.9538\n",
      "Epoch 285/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.7205 - val_loss: 40.0717\n",
      "Epoch 286/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.8096 - val_loss: 39.4729\n",
      "Epoch 287/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.4904 - val_loss: 40.7404\n",
      "Epoch 288/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.0193 - val_loss: 40.5975\n",
      "Epoch 289/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.8245 - val_loss: 41.0510\n",
      "Epoch 290/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.0844 - val_loss: 38.8660\n",
      "Epoch 291/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.2984 - val_loss: 39.3991\n",
      "Epoch 292/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.8212 - val_loss: 40.5605\n",
      "Epoch 293/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.0310 - val_loss: 40.6840\n",
      "Epoch 294/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.5821 - val_loss: 38.9809\n",
      "Epoch 295/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.7669 - val_loss: 38.2291\n",
      "Epoch 296/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.6155 - val_loss: 38.1854\n",
      "Epoch 297/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.4318 - val_loss: 39.0626\n",
      "Epoch 298/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.7502 - val_loss: 40.4638\n",
      "Epoch 299/500\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.5481 - val_loss: 40.0488\n",
      "Epoch 300/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6654 - val_loss: 39.5720\n",
      "Epoch 301/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.8151 - val_loss: 38.1565\n",
      "Epoch 302/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.0689 - val_loss: 39.8381\n",
      "Epoch 303/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.6052 - val_loss: 39.1679\n",
      "Epoch 304/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 31.3682 - val_loss: 39.7888\n",
      "Epoch 305/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.7615 - val_loss: 38.7943\n",
      "Epoch 306/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 112us/step - loss: 31.1762 - val_loss: 39.1932\n",
      "Epoch 307/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 31.3601 - val_loss: 40.4758\n",
      "Epoch 308/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.9723 - val_loss: 39.7988\n",
      "Epoch 309/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.2240 - val_loss: 39.2697\n",
      "Epoch 310/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.3497 - val_loss: 39.5279\n",
      "Epoch 311/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 31.1582 - val_loss: 39.6884\n",
      "Epoch 312/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.5861 - val_loss: 40.0486\n",
      "Epoch 313/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.2570 - val_loss: 38.6425\n",
      "Epoch 314/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.3038 - val_loss: 40.4669\n",
      "Epoch 315/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.9954 - val_loss: 41.9334\n",
      "Epoch 316/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.3679 - val_loss: 41.2329\n",
      "Epoch 317/500\n",
      "378/378 [==============================] - 0s 137us/step - loss: 31.5218 - val_loss: 40.7655\n",
      "Epoch 318/500\n",
      "378/378 [==============================] - 0s 139us/step - loss: 31.6142 - val_loss: 39.9809\n",
      "Epoch 319/500\n",
      "378/378 [==============================] - 0s 143us/step - loss: 31.4562 - val_loss: 40.3262\n",
      "Epoch 320/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 31.4370 - val_loss: 41.6874\n",
      "Epoch 321/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.4294 - val_loss: 38.9304\n",
      "Epoch 322/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.0041 - val_loss: 39.5018\n",
      "Epoch 323/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 31.4846 - val_loss: 39.0897\n",
      "Epoch 324/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.9111 - val_loss: 39.8756\n",
      "Epoch 325/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.1426 - val_loss: 39.9551\n",
      "Epoch 326/500\n",
      "378/378 [==============================] - 0s 140us/step - loss: 31.4511 - val_loss: 42.4207\n",
      "Epoch 327/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.7267 - val_loss: 40.2815\n",
      "Epoch 328/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 31.3319 - val_loss: 40.4035\n",
      "Epoch 329/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.9088 - val_loss: 40.7895\n",
      "Epoch 330/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.8061 - val_loss: 40.8743\n",
      "Epoch 331/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.0416 - val_loss: 42.1785\n",
      "Epoch 332/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.0934 - val_loss: 41.2613\n",
      "Epoch 333/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.4310 - val_loss: 40.6687\n",
      "Epoch 334/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8968 - val_loss: 40.0269\n",
      "Epoch 335/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.2072 - val_loss: 39.6861\n",
      "Epoch 336/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.3627 - val_loss: 39.9449\n",
      "Epoch 337/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.5941 - val_loss: 40.3691\n",
      "Epoch 338/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.8323 - val_loss: 40.6163\n",
      "Epoch 339/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.0530 - val_loss: 40.5178\n",
      "Epoch 340/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.3706 - val_loss: 41.5034\n",
      "Epoch 341/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.9251 - val_loss: 41.8167\n",
      "Epoch 342/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.3378 - val_loss: 41.0410\n",
      "Epoch 343/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.0038 - val_loss: 42.0460\n",
      "Epoch 344/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.5542 - val_loss: 38.7011\n",
      "Epoch 345/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.9995 - val_loss: 40.5179\n",
      "Epoch 346/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.8664 - val_loss: 39.7197\n",
      "Epoch 347/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.7968 - val_loss: 39.8042\n",
      "Epoch 348/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.3351 - val_loss: 40.9780\n",
      "Epoch 349/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.0728 - val_loss: 39.5267\n",
      "Epoch 350/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.6935 - val_loss: 41.0507\n",
      "Epoch 351/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.2417 - val_loss: 39.2725\n",
      "Epoch 352/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.7594 - val_loss: 42.9680\n",
      "Epoch 353/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.1339 - val_loss: 40.1520\n",
      "Epoch 354/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.2462 - val_loss: 39.7720\n",
      "Epoch 355/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.0445 - val_loss: 40.7611\n",
      "Epoch 356/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6748 - val_loss: 39.9286\n",
      "Epoch 357/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.8844 - val_loss: 41.1739\n",
      "Epoch 358/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.8852 - val_loss: 39.9802\n",
      "Epoch 359/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.3380 - val_loss: 40.3607\n",
      "Epoch 360/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.6271 - val_loss: 39.8885\n",
      "Epoch 361/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.7246 - val_loss: 39.4884\n",
      "Epoch 362/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.5408 - val_loss: 39.2351\n",
      "Epoch 363/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.4051 - val_loss: 40.5264\n",
      "Epoch 364/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.6440 - val_loss: 39.8891\n",
      "Epoch 365/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.2815 - val_loss: 40.2606\n",
      "Epoch 366/500\n",
      "378/378 [==============================] - 0s 131us/step - loss: 31.0982 - val_loss: 41.5259\n",
      "Epoch 367/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 31.3395 - val_loss: 40.8794\n",
      "Epoch 368/500\n",
      "378/378 [==============================] - 0s 153us/step - loss: 30.7414 - val_loss: 38.3555\n",
      "Epoch 369/500\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.4409 - val_loss: 39.9452\n",
      "Epoch 370/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.7530 - val_loss: 40.7835\n",
      "Epoch 371/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.7052 - val_loss: 41.7158\n",
      "Epoch 372/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.4735 - val_loss: 40.1631\n",
      "Epoch 373/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.3573 - val_loss: 39.7269\n",
      "Epoch 374/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 30.9443 - val_loss: 39.9814\n",
      "Epoch 375/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4814 - val_loss: 40.2537\n",
      "Epoch 376/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.3785 - val_loss: 40.2174\n",
      "Epoch 377/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.7557 - val_loss: 39.7024\n",
      "Epoch 378/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.3892 - val_loss: 39.6333\n",
      "Epoch 379/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.9077 - val_loss: 40.5202\n",
      "Epoch 380/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.9289 - val_loss: 40.3811\n",
      "Epoch 381/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1997 - val_loss: 42.2661\n",
      "Epoch 382/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.6856 - val_loss: 40.5898\n",
      "Epoch 383/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.2197 - val_loss: 40.3579\n",
      "Epoch 384/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.1481 - val_loss: 38.9558\n",
      "Epoch 385/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.6162 - val_loss: 39.1890\n",
      "Epoch 386/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.5506 - val_loss: 38.9824\n",
      "Epoch 387/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.0495 - val_loss: 38.4847\n",
      "Epoch 388/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6081 - val_loss: 39.1477\n",
      "Epoch 389/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.6526 - val_loss: 39.9162\n",
      "Epoch 390/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.9777 - val_loss: 39.2943\n",
      "Epoch 391/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.2468 - val_loss: 41.0840\n",
      "Epoch 392/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.0274 - val_loss: 39.0025\n",
      "Epoch 393/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.2719 - val_loss: 38.3878\n",
      "Epoch 394/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.9318 - val_loss: 40.7006\n",
      "Epoch 395/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.2238 - val_loss: 39.7894\n",
      "Epoch 396/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2653 - val_loss: 39.3546\n",
      "Epoch 397/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.3448 - val_loss: 40.1611\n",
      "Epoch 398/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.5298 - val_loss: 40.2074\n",
      "Epoch 399/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.8110 - val_loss: 39.4222\n",
      "Epoch 400/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2634 - val_loss: 38.2084\n",
      "Epoch 401/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8555 - val_loss: 40.3814\n",
      "Epoch 402/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.7609 - val_loss: 39.2130\n",
      "Epoch 403/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.1720 - val_loss: 39.8305\n",
      "Epoch 404/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1160 - val_loss: 39.7367\n",
      "Epoch 405/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.9318 - val_loss: 40.1310\n",
      "Epoch 406/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6782 - val_loss: 41.4852\n",
      "Epoch 407/500\n",
      "378/378 [==============================] - 0s 106us/step - loss: 30.6017 - val_loss: 41.2509\n",
      "Epoch 408/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.4026 - val_loss: 42.1216\n",
      "Epoch 409/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.4075 - val_loss: 40.1767\n",
      "Epoch 410/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.2170 - val_loss: 41.5693\n",
      "Epoch 411/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1166 - val_loss: 39.1333\n",
      "Epoch 412/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.0525 - val_loss: 39.4924\n",
      "Epoch 413/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.6204 - val_loss: 42.4384\n",
      "Epoch 414/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6252 - val_loss: 39.4593\n",
      "Epoch 415/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2652 - val_loss: 38.2620\n",
      "Epoch 416/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.5431 - val_loss: 39.5074\n",
      "Epoch 417/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.4167 - val_loss: 41.1863\n",
      "Epoch 418/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.3338 - val_loss: 40.9955\n",
      "Epoch 419/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.5837 - val_loss: 40.8305\n",
      "Epoch 420/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.2243 - val_loss: 39.9133\n",
      "Epoch 421/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.3353 - val_loss: 39.7060\n",
      "Epoch 422/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8174 - val_loss: 38.4152\n",
      "Epoch 423/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3533 - val_loss: 40.2639\n",
      "Epoch 424/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.2485 - val_loss: 39.0004\n",
      "Epoch 425/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.5796 - val_loss: 39.4051\n",
      "Epoch 426/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.5380 - val_loss: 40.9850\n",
      "Epoch 427/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.1968 - val_loss: 39.8298\n",
      "Epoch 428/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.4565 - val_loss: 39.1908\n",
      "Epoch 429/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.7562 - val_loss: 39.7028\n",
      "Epoch 430/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.8779 - val_loss: 40.3042\n",
      "Epoch 431/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.6204 - val_loss: 39.3020\n",
      "Epoch 432/500\n",
      "378/378 [==============================] - 0s 141us/step - loss: 30.6257 - val_loss: 37.8392\n",
      "Epoch 433/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.4159 - val_loss: 37.8130\n",
      "Epoch 434/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.1642 - val_loss: 38.9769\n",
      "Epoch 435/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.6226 - val_loss: 38.8080\n",
      "Epoch 436/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.0750 - val_loss: 41.5153\n",
      "Epoch 437/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.3290 - val_loss: 39.2093\n",
      "Epoch 438/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.0403 - val_loss: 39.2562\n",
      "Epoch 439/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.6607 - val_loss: 40.1348\n",
      "Epoch 440/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.8219 - val_loss: 39.7103\n",
      "Epoch 441/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4798 - val_loss: 40.2748\n",
      "Epoch 442/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.3315 - val_loss: 40.6481\n",
      "Epoch 443/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.9586 - val_loss: 41.7228\n",
      "Epoch 444/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.5015 - val_loss: 39.1771\n",
      "Epoch 445/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.3006 - val_loss: 42.6020\n",
      "Epoch 446/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.5700 - val_loss: 40.0667\n",
      "Epoch 447/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.3689 - val_loss: 41.5413\n",
      "Epoch 448/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.5158 - val_loss: 39.0737\n",
      "Epoch 449/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.5732 - val_loss: 40.6396\n",
      "Epoch 450/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.5456 - val_loss: 40.4941\n",
      "Epoch 451/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.6499 - val_loss: 40.1168\n",
      "Epoch 452/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.6815 - val_loss: 38.7778\n",
      "Epoch 453/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.9414 - val_loss: 39.3367\n",
      "Epoch 454/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.2941 - val_loss: 38.7558\n",
      "Epoch 455/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.4684 - val_loss: 39.9339\n",
      "Epoch 456/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8758 - val_loss: 41.2318\n",
      "Epoch 457/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.0184 - val_loss: 40.6614\n",
      "Epoch 458/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 117us/step - loss: 30.3796 - val_loss: 40.3136\n",
      "Epoch 459/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.4188 - val_loss: 39.4738\n",
      "Epoch 460/500\n",
      "378/378 [==============================] - 0s 134us/step - loss: 30.3670 - val_loss: 41.0653\n",
      "Epoch 461/500\n",
      "378/378 [==============================] - 0s 139us/step - loss: 31.0115 - val_loss: 40.8251\n",
      "Epoch 462/500\n",
      "378/378 [==============================] - 0s 148us/step - loss: 29.9184 - val_loss: 39.8697\n",
      "Epoch 463/500\n",
      "378/378 [==============================] - 0s 142us/step - loss: 30.1677 - val_loss: 40.4435\n",
      "Epoch 464/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.2846 - val_loss: 39.8982\n",
      "Epoch 465/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.1463 - val_loss: 40.1937\n",
      "Epoch 466/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.7215 - val_loss: 39.5725\n",
      "Epoch 467/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.6679 - val_loss: 40.0594\n",
      "Epoch 468/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2605 - val_loss: 40.2787\n",
      "Epoch 469/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.7998 - val_loss: 40.6502\n",
      "Epoch 470/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.2017 - val_loss: 40.5727\n",
      "Epoch 471/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.5701 - val_loss: 40.0939\n",
      "Epoch 472/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2308 - val_loss: 41.4846\n",
      "Epoch 473/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.5639 - val_loss: 40.9715\n",
      "Epoch 474/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.9164 - val_loss: 40.4180\n",
      "Epoch 475/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.1598 - val_loss: 39.9764\n",
      "Epoch 476/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.3017 - val_loss: 40.2078\n",
      "Epoch 477/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.2758 - val_loss: 40.3070\n",
      "Epoch 478/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.0853 - val_loss: 40.0043\n",
      "Epoch 479/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.8395 - val_loss: 39.9165\n",
      "Epoch 480/500\n",
      "378/378 [==============================] - 0s 107us/step - loss: 30.2189 - val_loss: 41.9646\n",
      "Epoch 481/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.1160 - val_loss: 42.3742\n",
      "Epoch 482/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.0229 - val_loss: 41.2193\n",
      "Epoch 483/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.6877 - val_loss: 39.6180\n",
      "Epoch 484/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.5883 - val_loss: 40.9395\n",
      "Epoch 485/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.8305 - val_loss: 41.0310\n",
      "Epoch 486/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.7814 - val_loss: 40.0478\n",
      "Epoch 487/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.7570 - val_loss: 40.6354\n",
      "Epoch 488/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.1819 - val_loss: 42.2014\n",
      "Epoch 489/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.2726 - val_loss: 41.4132\n",
      "Epoch 490/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.4976 - val_loss: 40.4997\n",
      "Epoch 491/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.5545 - val_loss: 40.1144\n",
      "Epoch 492/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.3070 - val_loss: 41.5619\n",
      "Epoch 493/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.2813 - val_loss: 40.8578\n",
      "Epoch 494/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.7680 - val_loss: 40.8972\n",
      "Epoch 495/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.4461 - val_loss: 41.0887\n",
      "Epoch 496/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.7840 - val_loss: 41.8095\n",
      "Epoch 497/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.0379 - val_loss: 41.4529\n",
      "Epoch 498/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.6094 - val_loss: 41.3542\n",
      "Epoch 499/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.9991 - val_loss: 41.5048\n",
      "Epoch 500/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2172 - val_loss: 40.0301\n",
      "mw\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 83.8219 - val_loss: 17.9068\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 158us/step - loss: 15.9060 - val_loss: 9.1364\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 8.6676 - val_loss: 8.4402\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 7.5378 - val_loss: 8.3536\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.9806 - val_loss: 8.3394\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.8319 - val_loss: 8.3364\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.6353 - val_loss: 8.3336\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.5548 - val_loss: 8.3308\n",
      "Epoch 9/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.5012 - val_loss: 8.3275\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3987 - val_loss: 8.3269\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.4453 - val_loss: 8.3271\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.4317 - val_loss: 8.3288\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.4610 - val_loss: 8.3305\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.4278 - val_loss: 8.3305\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.4111 - val_loss: 8.3299\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3986 - val_loss: 8.3305\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3594 - val_loss: 8.3305\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3660 - val_loss: 8.3305\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3618 - val_loss: 8.3305\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.4251 - val_loss: 8.3305\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3732 - val_loss: 8.3305\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3477 - val_loss: 8.3305\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3357 - val_loss: 8.3305\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3571 - val_loss: 8.3305\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3493 - val_loss: 8.3305\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3137 - val_loss: 8.3305\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3493 - val_loss: 8.3305\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3213 - val_loss: 8.3305\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3604 - val_loss: 8.3305\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3377 - val_loss: 8.3305\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3254 - val_loss: 8.3305\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3174 - val_loss: 8.3305\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3316 - val_loss: 8.3305\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3302 - val_loss: 8.3305\n",
      "Epoch 35/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 121us/step - loss: 6.3622 - val_loss: 8.3305\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3444 - val_loss: 8.3305\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3139 - val_loss: 8.3305\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3362 - val_loss: 8.3305\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3247 - val_loss: 8.3305\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3242 - val_loss: 8.3305\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3202 - val_loss: 8.3305\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3161 - val_loss: 8.3305\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3093 - val_loss: 8.3305\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3236 - val_loss: 8.3305\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - 0s 180us/step - loss: 6.3189 - val_loss: 8.3305\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - 0s 149us/step - loss: 6.3081 - val_loss: 8.3305\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 6.3318 - val_loss: 8.3305\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3134 - val_loss: 8.3305\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3074 - val_loss: 8.3305\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3299 - val_loss: 8.3305\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3110 - val_loss: 8.3305\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3149 - val_loss: 8.3305\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3224 - val_loss: 8.3305\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3119 - val_loss: 8.3305\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 6.3101 - val_loss: 8.3305\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 6.3043 - val_loss: 8.3305\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3084 - val_loss: 8.3305\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3243 - val_loss: 8.3305\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 6.3036 - val_loss: 8.3305\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3057 - val_loss: 8.3305\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3265 - val_loss: 8.3305\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3062 - val_loss: 8.3305\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3051 - val_loss: 8.3305\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3094 - val_loss: 8.3305\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3042 - val_loss: 8.3305\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3096 - val_loss: 8.3305\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3021 - val_loss: 8.3305\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3027 - val_loss: 8.3305\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3144 - val_loss: 8.3305\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3022 - val_loss: 8.3305\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3075 - val_loss: 8.3305\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3030 - val_loss: 8.3305\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3057 - val_loss: 8.3305\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3083 - val_loss: 8.3305\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3121 - val_loss: 8.3305\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3060 - val_loss: 8.3305\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.2999 - val_loss: 8.3305\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3070 - val_loss: 8.3305\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.2971 - val_loss: 8.3305\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3019 - val_loss: 8.3305\n",
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3153 - val_loss: 8.3305\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3059 - val_loss: 8.3305\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3069 - val_loss: 8.3305\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3007 - val_loss: 8.3305\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3034 - val_loss: 8.3305\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3056 - val_loss: 8.3305\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3047 - val_loss: 8.3305\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3082 - val_loss: 8.3305\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.2995 - val_loss: 8.3305\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3089 - val_loss: 8.3305\n",
      "Epoch 96/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3063 - val_loss: 8.3305\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3115 - val_loss: 8.3305\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3056 - val_loss: 8.3305\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3052 - val_loss: 8.3305\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3070 - val_loss: 8.3305\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.2973 - val_loss: 8.3305\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3007 - val_loss: 8.3305\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3049 - val_loss: 8.3305\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3045 - val_loss: 8.3305\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3040 - val_loss: 8.3305\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3008 - val_loss: 8.3305\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3032 - val_loss: 8.3305\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3068 - val_loss: 8.3305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3000 - val_loss: 8.3305\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 111us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3038 - val_loss: 8.3305\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3047 - val_loss: 8.3305\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3041 - val_loss: 8.3305\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3045 - val_loss: 8.3305\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3047 - val_loss: 8.3305\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3055 - val_loss: 8.3305\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3039 - val_loss: 8.3305\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3027 - val_loss: 8.3305\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3031 - val_loss: 8.3305\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3086 - val_loss: 8.3305\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3023 - val_loss: 8.3305\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3041 - val_loss: 8.3305\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3063 - val_loss: 8.3305\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3061 - val_loss: 8.3305\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3008 - val_loss: 8.3305\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3019 - val_loss: 8.3305\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 6.3036 - val_loss: 8.3305\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3004 - val_loss: 8.3305\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3049 - val_loss: 8.3305\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3036 - val_loss: 8.3305\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.2990 - val_loss: 8.3305\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 159us/step - loss: 6.3033 - val_loss: 8.3305\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 159us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3090 - val_loss: 8.3305\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.2981 - val_loss: 8.3305\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3005 - val_loss: 8.3305\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3094 - val_loss: 8.3305\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3125 - val_loss: 8.3305\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3011 - val_loss: 8.3305\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3069 - val_loss: 8.3305\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3057 - val_loss: 8.3305\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3042 - val_loss: 8.3305\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.2989 - val_loss: 8.3305\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3043 - val_loss: 8.3305\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3019 - val_loss: 8.3305\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3020 - val_loss: 8.3305\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3006 - val_loss: 8.3305\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3030 - val_loss: 8.3305\n",
      "Epoch 192/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 129us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3023 - val_loss: 8.3305\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.2995 - val_loss: 8.3305\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3016 - val_loss: 8.3305\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3010 - val_loss: 8.3305\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3031 - val_loss: 8.3305\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3073 - val_loss: 8.3305\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 106us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3020 - val_loss: 8.3305\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3010 - val_loss: 8.3305\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3011 - val_loss: 8.3305\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3023 - val_loss: 8.3305\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3010 - val_loss: 8.3305\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3021 - val_loss: 8.3305\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3021 - val_loss: 8.3305\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3008 - val_loss: 8.3305\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 6.3050 - val_loss: 8.3305\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3016 - val_loss: 8.3305\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 180us/step - loss: 6.3084 - val_loss: 8.3305\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 238/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3007 - val_loss: 8.3305\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - 0s 145us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3029 - val_loss: 8.3305\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 248/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 178us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "dw\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 88.8070 - val_loss: 27.0537\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 23.8474 - val_loss: 16.5754\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 16.2885 - val_loss: 15.3362\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 14.8806 - val_loss: 15.1313\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.3470 - val_loss: 15.0873\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.0234 - val_loss: 15.0768\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.8086 - val_loss: 15.0720\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.7636 - val_loss: 15.0704\n",
      "Epoch 9/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.6758 - val_loss: 15.0676\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.6207 - val_loss: 15.0651\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.6369 - val_loss: 15.0648\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.5958 - val_loss: 15.0648\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.6300 - val_loss: 15.0648\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.5852 - val_loss: 15.0648\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.5750 - val_loss: 15.0648\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.5561 - val_loss: 15.0648\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4897 - val_loss: 15.0648\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.5354 - val_loss: 15.0648\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4976 - val_loss: 15.0648\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.5699 - val_loss: 15.0648\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.5366 - val_loss: 15.0648\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.5273 - val_loss: 15.0648\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4944 - val_loss: 15.0648\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.5043 - val_loss: 15.0648\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.5022 - val_loss: 15.0648\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4721 - val_loss: 15.0648\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4894 - val_loss: 15.0648\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4748 - val_loss: 15.0648\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4939 - val_loss: 15.0648\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4882 - val_loss: 15.0648\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 147us/step - loss: 13.4665 - val_loss: 15.0648\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 136us/step - loss: 13.4645 - val_loss: 15.0648\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4709 - val_loss: 15.0648\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4623 - val_loss: 15.0648\n",
      "Epoch 35/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4907 - val_loss: 15.0648\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4793 - val_loss: 15.0648\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4697 - val_loss: 15.0648\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.4852 - val_loss: 15.0648\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.5081 - val_loss: 15.0648\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4590 - val_loss: 15.0648\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 13.4690 - val_loss: 15.0648\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4574 - val_loss: 15.0648\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4556 - val_loss: 15.0648\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 111us/step - loss: 13.4849 - val_loss: 15.0648\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.4650 - val_loss: 15.0648\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 13.4597 - val_loss: 15.0648\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4779 - val_loss: 15.0648\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4569 - val_loss: 15.0648\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4761 - val_loss: 15.0648\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4626 - val_loss: 15.0648\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4567 - val_loss: 15.0648\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.4476 - val_loss: 15.0648\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4640 - val_loss: 15.0648\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 155us/step - loss: 13.4589 - val_loss: 15.0648\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 147us/step - loss: 13.4535 - val_loss: 15.0648\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 13.4530 - val_loss: 15.0648\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4541 - val_loss: 15.0648\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4690 - val_loss: 15.0648\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.4498 - val_loss: 15.0648\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4676 - val_loss: 15.0648\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4530 - val_loss: 15.0648\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 109us/step - loss: 13.4593 - val_loss: 15.0648\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4552 - val_loss: 15.0648\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4494 - val_loss: 15.0648\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4624 - val_loss: 15.0648\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4461 - val_loss: 15.0648\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4506 - val_loss: 15.0648\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4569 - val_loss: 15.0648\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4472 - val_loss: 15.0648\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4620 - val_loss: 15.0648\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4461 - val_loss: 15.0648\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4514 - val_loss: 15.0648\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4516 - val_loss: 15.0648\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4563 - val_loss: 15.0648\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4503 - val_loss: 15.0648\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4501 - val_loss: 15.0648\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4505 - val_loss: 15.0648\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4466 - val_loss: 15.0648\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4519 - val_loss: 15.0648\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4495 - val_loss: 15.0648\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4659 - val_loss: 15.0648\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4515 - val_loss: 15.0648\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4526 - val_loss: 15.0648\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4509 - val_loss: 15.0648\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4507 - val_loss: 15.0648\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.4443 - val_loss: 15.0648\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4499 - val_loss: 15.0648\n",
      "Epoch 96/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 126us/step - loss: 13.4612 - val_loss: 15.0648\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4559 - val_loss: 15.0648\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4526 - val_loss: 15.0648\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4521 - val_loss: 15.0648\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4508 - val_loss: 15.0648\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4481 - val_loss: 15.0648\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4490 - val_loss: 15.0648\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4481 - val_loss: 15.0648\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4506 - val_loss: 15.0648\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4491 - val_loss: 15.0648\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4482 - val_loss: 15.0648\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4517 - val_loss: 15.0648\n",
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4460 - val_loss: 15.0648\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4479 - val_loss: 15.0648\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4490 - val_loss: 15.0648\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4505 - val_loss: 15.0648\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4533 - val_loss: 15.0648\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4464 - val_loss: 15.0648\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4518 - val_loss: 15.0648\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4489 - val_loss: 15.0648\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4477 - val_loss: 15.0648\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4468 - val_loss: 15.0648\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4483 - val_loss: 15.0648\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4512 - val_loss: 15.0648\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4502 - val_loss: 15.0648\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4456 - val_loss: 15.0648\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4516 - val_loss: 15.0648\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4519 - val_loss: 15.0648\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4465 - val_loss: 15.0648\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4463 - val_loss: 15.0648\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4470 - val_loss: 15.0648\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4492 - val_loss: 15.0648\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4468 - val_loss: 15.0648\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4467 - val_loss: 15.0648\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4486 - val_loss: 15.0648\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4396 - val_loss: 15.0648\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4474 - val_loss: 15.0648\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4502 - val_loss: 15.0648\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4489 - val_loss: 15.0648\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 13.4554 - val_loss: 15.0648\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4528 - val_loss: 15.0648\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4498 - val_loss: 15.0648\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4455 - val_loss: 15.0648\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4497 - val_loss: 15.0648\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4501 - val_loss: 15.0648\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4483 - val_loss: 15.0648\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4468 - val_loss: 15.0648\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.4445 - val_loss: 15.0648\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 151us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 143us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 150us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 145us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 192/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4463 - val_loss: 15.0648\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4451 - val_loss: 15.0648\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4470 - val_loss: 15.0648\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4493 - val_loss: 15.0648\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 160us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.4455 - val_loss: 15.0648\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 136us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4529 - val_loss: 15.0648\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 149us/step - loss: 13.4478 - val_loss: 15.0648\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4512 - val_loss: 15.0648\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 238/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4455 - val_loss: 15.0648\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4448 - val_loss: 15.0648\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4443 - val_loss: 15.0648\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4458 - val_loss: 15.0648\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4499 - val_loss: 15.0648\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4463 - val_loss: 15.0648\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 248/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "wm\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/300\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 83.1399 - val_loss: 17.0875\n",
      "Epoch 2/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 15.5730 - val_loss: 5.7701\n",
      "Epoch 3/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 7.4662 - val_loss: 4.5463\n",
      "Epoch 4/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.1663 - val_loss: 4.3702\n",
      "Epoch 5/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 5.5833 - val_loss: 4.3453\n",
      "Epoch 6/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 5.3496 - val_loss: 4.3410\n",
      "Epoch 7/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 5.1049 - val_loss: 4.3410\n",
      "Epoch 8/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 5.0757 - val_loss: 4.3410\n",
      "Epoch 9/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.9750 - val_loss: 4.3410\n",
      "Epoch 10/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.8889 - val_loss: 4.3410\n",
      "Epoch 11/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.9104 - val_loss: 4.3410\n",
      "Epoch 12/300\n",
      "378/378 [==============================] - 0s 110us/step - loss: 4.8698 - val_loss: 4.3410\n",
      "Epoch 13/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.8947 - val_loss: 4.3410\n",
      "Epoch 14/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.8756 - val_loss: 4.3410\n",
      "Epoch 15/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.8541 - val_loss: 4.3410\n",
      "Epoch 16/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.8294 - val_loss: 4.3410\n",
      "Epoch 17/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.7914 - val_loss: 4.3410\n",
      "Epoch 18/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.8266 - val_loss: 4.3410\n",
      "Epoch 19/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7914 - val_loss: 4.3410\n",
      "Epoch 20/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.8456 - val_loss: 4.3410\n",
      "Epoch 21/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.8388 - val_loss: 4.3410\n",
      "Epoch 22/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.8020 - val_loss: 4.3410\n",
      "Epoch 23/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7891 - val_loss: 4.3410\n",
      "Epoch 24/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7675 - val_loss: 4.3410\n",
      "Epoch 25/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7936 - val_loss: 4.3410\n",
      "Epoch 26/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7624 - val_loss: 4.3410\n",
      "Epoch 27/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.8092 - val_loss: 4.3410\n",
      "Epoch 28/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7803 - val_loss: 4.3410\n",
      "Epoch 29/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7942 - val_loss: 4.3410\n",
      "Epoch 30/300\n",
      "378/378 [==============================] - 0s 134us/step - loss: 4.7814 - val_loss: 4.3410\n",
      "Epoch 31/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7552 - val_loss: 4.3410\n",
      "Epoch 32/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7570 - val_loss: 4.3410\n",
      "Epoch 33/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7664 - val_loss: 4.3410\n",
      "Epoch 34/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7657 - val_loss: 4.3410\n",
      "Epoch 35/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7832 - val_loss: 4.3410\n",
      "Epoch 36/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7741 - val_loss: 4.3410\n",
      "Epoch 37/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7536 - val_loss: 4.3410\n",
      "Epoch 38/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7773 - val_loss: 4.3410\n",
      "Epoch 39/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7866 - val_loss: 4.3410\n",
      "Epoch 40/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7525 - val_loss: 4.3410\n",
      "Epoch 41/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7605 - val_loss: 4.3410\n",
      "Epoch 42/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7473 - val_loss: 4.3410\n",
      "Epoch 43/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7483 - val_loss: 4.3410\n",
      "Epoch 44/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7674 - val_loss: 4.3410\n",
      "Epoch 45/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.7554 - val_loss: 4.3410\n",
      "Epoch 46/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7519 - val_loss: 4.3410\n",
      "Epoch 47/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7615 - val_loss: 4.3410\n",
      "Epoch 48/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7456 - val_loss: 4.3410\n",
      "Epoch 49/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7627 - val_loss: 4.3410\n",
      "Epoch 50/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7313 - val_loss: 4.3410\n",
      "Epoch 51/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7510 - val_loss: 4.3410\n",
      "Epoch 52/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7439 - val_loss: 4.3410\n",
      "Epoch 53/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7563 - val_loss: 4.3410\n",
      "Epoch 54/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7474 - val_loss: 4.3410\n",
      "Epoch 55/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7449 - val_loss: 4.3410\n",
      "Epoch 56/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 57/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7409 - val_loss: 4.3410\n",
      "Epoch 58/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7396 - val_loss: 4.3410\n",
      "Epoch 59/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7391 - val_loss: 4.3410\n",
      "Epoch 60/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 61/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7497 - val_loss: 4.3410\n",
      "Epoch 62/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7366 - val_loss: 4.3410\n",
      "Epoch 63/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7472 - val_loss: 4.3410\n",
      "Epoch 64/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7406 - val_loss: 4.3410\n",
      "Epoch 65/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7340 - val_loss: 4.3410\n",
      "Epoch 66/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7343 - val_loss: 4.3410\n",
      "Epoch 67/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7482 - val_loss: 4.3410\n",
      "Epoch 68/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7369 - val_loss: 4.3410\n",
      "Epoch 69/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7344 - val_loss: 4.3410\n",
      "Epoch 70/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7479 - val_loss: 4.3410\n",
      "Epoch 71/300\n",
      "378/378 [==============================] - 0s 172us/step - loss: 4.7350 - val_loss: 4.3410\n",
      "Epoch 72/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7327 - val_loss: 4.3410\n",
      "Epoch 73/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7509 - val_loss: 4.3410\n",
      "Epoch 74/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 75/300\n",
      "378/378 [==============================] - 0s 137us/step - loss: 4.7389 - val_loss: 4.3410\n",
      "Epoch 76/300\n",
      "378/378 [==============================] - 0s 150us/step - loss: 4.7433 - val_loss: 4.3410\n",
      "Epoch 77/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7436 - val_loss: 4.3410\n",
      "Epoch 78/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7427 - val_loss: 4.3410\n",
      "Epoch 79/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7372 - val_loss: 4.3410\n",
      "Epoch 80/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7403 - val_loss: 4.3410\n",
      "Epoch 81/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 82/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7329 - val_loss: 4.3410\n",
      "Epoch 83/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7414 - val_loss: 4.3410\n",
      "Epoch 84/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7337 - val_loss: 4.3410\n",
      "Epoch 85/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 86/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7585 - val_loss: 4.3410\n",
      "Epoch 87/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7378 - val_loss: 4.3410\n",
      "Epoch 88/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7478 - val_loss: 4.3410\n",
      "Epoch 89/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7342 - val_loss: 4.3410\n",
      "Epoch 90/300\n",
      "378/378 [==============================] - 0s 135us/step - loss: 4.7374 - val_loss: 4.3410\n",
      "Epoch 91/300\n",
      "378/378 [==============================] - 0s 152us/step - loss: 4.7334 - val_loss: 4.3410\n",
      "Epoch 92/300\n",
      "378/378 [==============================] - 0s 144us/step - loss: 4.7333 - val_loss: 4.3410\n",
      "Epoch 93/300\n",
      "378/378 [==============================] - 0s 150us/step - loss: 4.7343 - val_loss: 4.3410\n",
      "Epoch 94/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7344 - val_loss: 4.3410\n",
      "Epoch 95/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7366 - val_loss: 4.3410\n",
      "Epoch 96/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7488 - val_loss: 4.3410\n",
      "Epoch 97/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7440 - val_loss: 4.3410\n",
      "Epoch 98/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7370 - val_loss: 4.3410\n",
      "Epoch 99/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 100/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7309 - val_loss: 4.3410\n",
      "Epoch 101/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 102/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7432 - val_loss: 4.3410\n",
      "Epoch 103/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7407 - val_loss: 4.3410\n",
      "Epoch 104/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7382 - val_loss: 4.3410\n",
      "Epoch 105/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7352 - val_loss: 4.3410\n",
      "Epoch 106/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7358 - val_loss: 4.3410\n",
      "Epoch 107/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7355 - val_loss: 4.3410\n",
      "Epoch 108/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 109/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7343 - val_loss: 4.3410\n",
      "Epoch 110/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 111/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7345 - val_loss: 4.3410\n",
      "Epoch 112/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7404 - val_loss: 4.3410\n",
      "Epoch 113/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7414 - val_loss: 4.3410\n",
      "Epoch 114/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7340 - val_loss: 4.3410\n",
      "Epoch 115/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 116/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 117/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7311 - val_loss: 4.3410\n",
      "Epoch 118/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7399 - val_loss: 4.3410\n",
      "Epoch 119/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7329 - val_loss: 4.3410\n",
      "Epoch 120/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7376 - val_loss: 4.3410\n",
      "Epoch 121/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 122/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7389 - val_loss: 4.3410\n",
      "Epoch 123/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7396 - val_loss: 4.3410\n",
      "Epoch 124/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7347 - val_loss: 4.3410\n",
      "Epoch 125/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7354 - val_loss: 4.3410\n",
      "Epoch 126/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7355 - val_loss: 4.3410\n",
      "Epoch 127/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7345 - val_loss: 4.3410\n",
      "Epoch 128/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 129/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7309 - val_loss: 4.3410\n",
      "Epoch 130/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 131/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7368 - val_loss: 4.3410\n",
      "Epoch 132/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7370 - val_loss: 4.3410\n",
      "Epoch 133/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7357 - val_loss: 4.3410\n",
      "Epoch 134/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 135/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7378 - val_loss: 4.3410\n",
      "Epoch 136/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7373 - val_loss: 4.3410\n",
      "Epoch 137/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7344 - val_loss: 4.3410\n",
      "Epoch 138/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 139/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 140/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7361 - val_loss: 4.3410\n",
      "Epoch 141/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 142/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 143/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7351 - val_loss: 4.3410\n",
      "Epoch 144/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 145/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7349 - val_loss: 4.3410\n",
      "Epoch 146/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7370 - val_loss: 4.3410\n",
      "Epoch 147/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7321 - val_loss: 4.3410\n",
      "Epoch 148/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 149/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7323 - val_loss: 4.3410\n",
      "Epoch 150/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7361 - val_loss: 4.3410\n",
      "Epoch 151/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 152/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 153/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7338 - val_loss: 4.3410\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 119us/step - loss: 4.7265 - val_loss: 4.3410\n",
      "Epoch 155/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 156/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7379 - val_loss: 4.3410\n",
      "Epoch 157/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 158/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7332 - val_loss: 4.3410\n",
      "Epoch 159/300\n",
      "378/378 [==============================] - 0s 136us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 160/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7411 - val_loss: 4.3410\n",
      "Epoch 161/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7391 - val_loss: 4.3410\n",
      "Epoch 162/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7347 - val_loss: 4.3410\n",
      "Epoch 163/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7316 - val_loss: 4.3410\n",
      "Epoch 164/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7332 - val_loss: 4.3410\n",
      "Epoch 165/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 166/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 167/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 168/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7354 - val_loss: 4.3410\n",
      "Epoch 169/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 170/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 171/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 172/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 173/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 174/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7351 - val_loss: 4.3410\n",
      "Epoch 175/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 176/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 177/300\n",
      "378/378 [==============================] - 0s 156us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 178/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7360 - val_loss: 4.3410\n",
      "Epoch 179/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 180/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7310 - val_loss: 4.3410\n",
      "Epoch 181/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 182/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 183/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7335 - val_loss: 4.3410\n",
      "Epoch 184/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7312 - val_loss: 4.3410\n",
      "Epoch 185/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 186/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7309 - val_loss: 4.3410\n",
      "Epoch 187/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7325 - val_loss: 4.3410\n",
      "Epoch 188/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 189/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 190/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 191/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 192/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 193/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 194/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 195/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 196/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 197/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7332 - val_loss: 4.3410\n",
      "Epoch 198/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 199/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 200/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 201/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7308 - val_loss: 4.3410\n",
      "Epoch 202/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 203/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 204/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 205/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7337 - val_loss: 4.3410\n",
      "Epoch 206/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 207/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7333 - val_loss: 4.3410\n",
      "Epoch 208/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 209/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7316 - val_loss: 4.3410\n",
      "Epoch 210/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 211/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 212/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 213/300\n",
      "378/378 [==============================] - 0s 139us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 214/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 215/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 216/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7325 - val_loss: 4.3410\n",
      "Epoch 217/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 218/300\n",
      "378/378 [==============================] - 0s 170us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 219/300\n",
      "378/378 [==============================] - 0s 147us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 220/300\n",
      "378/378 [==============================] - 0s 179us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 221/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 222/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 223/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7399 - val_loss: 4.3410\n",
      "Epoch 224/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 225/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 226/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7334 - val_loss: 4.3410\n",
      "Epoch 227/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 228/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 229/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7323 - val_loss: 4.3410\n",
      "Epoch 230/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 231/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 232/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7362 - val_loss: 4.3410\n",
      "Epoch 233/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 234/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 235/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7362 - val_loss: 4.3410\n",
      "Epoch 236/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 237/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 238/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7316 - val_loss: 4.3410\n",
      "Epoch 239/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 240/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 241/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 4.554 - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 242/300\n",
      "378/378 [==============================] - 0s 135us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 243/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7364 - val_loss: 4.3410\n",
      "Epoch 244/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7325 - val_loss: 4.3410\n",
      "Epoch 245/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 246/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 247/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 248/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 249/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 250/300\n",
      "378/378 [==============================] - 0s 136us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 251/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 252/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 253/300\n",
      "378/378 [==============================] - 0s 141us/step - loss: 4.7333 - val_loss: 4.3410\n",
      "Epoch 254/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 255/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 6.104 - 0s 123us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 256/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 257/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 258/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 259/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7338 - val_loss: 4.3410\n",
      "Epoch 260/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7337 - val_loss: 4.3410\n",
      "Epoch 261/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 262/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 263/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 264/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7322 - val_loss: 4.3410\n",
      "Epoch 265/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 266/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7461 - val_loss: 4.3410\n",
      "Epoch 267/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7322 - val_loss: 4.3410\n",
      "Epoch 268/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 269/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 270/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 271/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 272/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 273/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7331 - val_loss: 4.3410\n",
      "Epoch 274/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7338 - val_loss: 4.3410\n",
      "Epoch 275/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 276/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 277/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 278/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 279/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 280/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 281/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 282/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 283/300\n",
      "378/378 [==============================] - 0s 142us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 284/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 285/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7379 - val_loss: 4.3410\n",
      "Epoch 286/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 287/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 288/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 289/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 290/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 291/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 292/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 293/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 294/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 295/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7327 - val_loss: 4.3410\n",
      "Epoch 296/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 297/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 298/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 299/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7340 - val_loss: 4.3410\n",
      "Epoch 300/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7322 - val_loss: 4.3410\n",
      "oven\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 85.4946 - val_loss: 24.5830\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 23.2607 - val_loss: 14.3820\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 16.4812 - val_loss: 13.7477\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 15.5363 - val_loss: 13.6055\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 15.1210 - val_loss: 13.5485\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.8384 - val_loss: 13.5376\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 14.7746 - val_loss: 13.5344\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.6760 - val_loss: 13.5317\n",
      "Epoch 9/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 134us/step - loss: 14.6388 - val_loss: 13.5292\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.5970 - val_loss: 13.5281\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.5895 - val_loss: 13.5295\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 14.5554 - val_loss: 13.5289\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.5966 - val_loss: 13.5239\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.5681 - val_loss: 13.5176\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.5617 - val_loss: 13.5187\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 14.5268 - val_loss: 13.5165\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.5224 - val_loss: 13.5165\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.5293 - val_loss: 13.5165\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4986 - val_loss: 13.5171\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 14.5334 - val_loss: 13.5167\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 14.5318 - val_loss: 13.5176\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 14.4993 - val_loss: 13.5195\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.4883 - val_loss: 13.5187\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.5015 - val_loss: 13.5181\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.5031 - val_loss: 13.5165\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4649 - val_loss: 13.5165\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.5126 - val_loss: 13.5165\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4765 - val_loss: 13.5165\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.5286 - val_loss: 13.5165\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4686 - val_loss: 13.5165\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4754 - val_loss: 13.5165\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4791 - val_loss: 13.5165\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4849 - val_loss: 13.5165\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4909 - val_loss: 13.5165\n",
      "Epoch 35/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.4872 - val_loss: 13.5165\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4868 - val_loss: 13.5165\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.4879 - val_loss: 13.5165\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 136us/step - loss: 14.5085 - val_loss: 13.5165\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 147us/step - loss: 14.4830 - val_loss: 13.5165\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4634 - val_loss: 13.5165\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4808 - val_loss: 13.5165\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4824 - val_loss: 13.5165\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.4680 - val_loss: 13.5165\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.5001 - val_loss: 13.5165\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 8.339 - 0s 129us/step - loss: 14.4864 - val_loss: 13.5165\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 8.701 - 0s 133us/step - loss: 14.4744 - val_loss: 13.5165\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 14.4934 - val_loss: 13.5165\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4646 - val_loss: 13.5165\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4704 - val_loss: 13.5165\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 14.4709 - val_loss: 13.5165\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4679 - val_loss: 13.5165\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 14.4801 - val_loss: 13.5165\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4552 - val_loss: 13.5165\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 14.4637 - val_loss: 13.5165\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.4611 - val_loss: 13.5165\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4682 - val_loss: 13.5165\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4561 - val_loss: 13.5165\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 14.4487 - val_loss: 13.5165\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4685 - val_loss: 13.5165\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.4620 - val_loss: 13.5165\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4613 - val_loss: 13.5165\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4651 - val_loss: 13.5171\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.4619 - val_loss: 13.5205\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4586 - val_loss: 13.5199\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4815 - val_loss: 13.5173\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4579 - val_loss: 13.5165\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4718 - val_loss: 13.5165\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4608 - val_loss: 13.5165\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4773 - val_loss: 13.5165\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4585 - val_loss: 13.5165\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4619 - val_loss: 13.5165\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.4703 - val_loss: 13.5165\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4532 - val_loss: 13.5165\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4636 - val_loss: 13.5165\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4611 - val_loss: 13.5165\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.4746 - val_loss: 13.5165\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 14.4516 - val_loss: 13.5165\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 165us/step - loss: 14.4638 - val_loss: 13.5165\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 14.4767 - val_loss: 13.5165\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 154us/step - loss: 14.4590 - val_loss: 13.5165\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4584 - val_loss: 13.5165\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.4599 - val_loss: 13.5165\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.4577 - val_loss: 13.5165\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 149us/step - loss: 14.4513 - val_loss: 13.5165\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 14.4619 - val_loss: 13.5165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4617 - val_loss: 13.5180\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4544 - val_loss: 13.5179\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.4448 - val_loss: 13.5183\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4630 - val_loss: 13.5165\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4542 - val_loss: 13.5165\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4567 - val_loss: 13.5181\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4473 - val_loss: 13.5075\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4660 - val_loss: 13.5007\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4361 - val_loss: 13.5165\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4503 - val_loss: 13.5090\n",
      "Epoch 96/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4592 - val_loss: 13.4720\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4329 - val_loss: 13.4639\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4308 - val_loss: 13.4675\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.3995 - val_loss: 13.4251\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 14.4174 - val_loss: 13.3731\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.3545 - val_loss: 13.3941\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.3508 - val_loss: 13.3359\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.3103 - val_loss: 13.3279\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.2802 - val_loss: 13.3228\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.2609 - val_loss: 13.2721\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.2139 - val_loss: 13.2903\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.1983 - val_loss: 13.2771\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.0828 - val_loss: 13.1795\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.0993 - val_loss: 13.0846\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 14.0076 - val_loss: 13.1016\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 14.0304 - val_loss: 13.1182\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 13.9632 - val_loss: 13.0712\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.9006 - val_loss: 13.0824\n",
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.9263 - val_loss: 13.0582\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.7625 - val_loss: 13.1548\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.8256 - val_loss: 13.0120\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.7829 - val_loss: 13.0370\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.8275 - val_loss: 12.9281\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.8204 - val_loss: 12.9648\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.7946 - val_loss: 12.9704\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.7651 - val_loss: 12.9854\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.7233 - val_loss: 12.9734\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.6040 - val_loss: 12.9451\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 13.6852 - val_loss: 12.8959\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.6510 - val_loss: 12.8554\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 211us/step - loss: 13.5999 - val_loss: 12.8074\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.5885 - val_loss: 12.8861\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.4836 - val_loss: 12.8323\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4353 - val_loss: 12.7927\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.6071 - val_loss: 12.7419\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.5183 - val_loss: 12.6590\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4677 - val_loss: 12.6050\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4859 - val_loss: 12.6885\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.5135 - val_loss: 12.6759\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.3991 - val_loss: 12.7643\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4379 - val_loss: 12.7539\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4065 - val_loss: 12.7252\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.3669 - val_loss: 12.7693\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.3049 - val_loss: 12.7138\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.3928 - val_loss: 12.7738\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4343 - val_loss: 12.6631\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.2826 - val_loss: 12.6483\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.3228 - val_loss: 12.5174\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.2871 - val_loss: 12.3914\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.2982 - val_loss: 12.5352\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 13.2078 - val_loss: 12.2640\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.1396 - val_loss: 12.2060\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.2084 - val_loss: 12.2880\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1740 - val_loss: 12.5510\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.1966 - val_loss: 12.2309\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.1939 - val_loss: 12.3188\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.1223 - val_loss: 12.3875\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.3229 - val_loss: 12.6054\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.1322 - val_loss: 12.4197\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.0788 - val_loss: 12.3705\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1723 - val_loss: 12.4310\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.0013 - val_loss: 12.2281\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.0248 - val_loss: 12.4465\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.2175 - val_loss: 12.5020\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.0156 - val_loss: 12.6371\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1961 - val_loss: 12.2693\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.1523 - val_loss: 12.6401\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1433 - val_loss: 12.6641\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 13.0366 - val_loss: 12.6070\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.1367 - val_loss: 12.6366\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.1739 - val_loss: 12.5923\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.0676 - val_loss: 12.5381\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.0624 - val_loss: 12.5364\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.0203 - val_loss: 12.5293\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.9440 - val_loss: 12.6299\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.1113 - val_loss: 12.6174\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.0804 - val_loss: 12.5971\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.9573 - val_loss: 12.5656\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.1154 - val_loss: 12.5035\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.1325 - val_loss: 12.6030\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 145us/step - loss: 13.1203 - val_loss: 12.5818\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.0066 - val_loss: 12.3141\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.9833 - val_loss: 12.4126\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.0624 - val_loss: 12.4718\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 13.0250 - val_loss: 12.5041\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 144us/step - loss: 12.9819 - val_loss: 12.3958\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 151us/step - loss: 12.9655 - val_loss: 12.6193\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 12.9404 - val_loss: 12.5503\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 12.9704 - val_loss: 12.6115\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.0110 - val_loss: 12.5220\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 20.68 - 0s 129us/step - loss: 12.9959 - val_loss: 12.2808\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 12.9206 - val_loss: 12.1808\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.0088 - val_loss: 12.2347\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.0576 - val_loss: 12.1633\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.9613 - val_loss: 12.1796\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9055 - val_loss: 12.2568\n",
      "Epoch 192/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 12.9760 - val_loss: 12.3352\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 12.9435 - val_loss: 12.2705\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 105us/step - loss: 12.9510 - val_loss: 12.1830\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.9667 - val_loss: 12.2900\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9616 - val_loss: 12.3646\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 12.9087 - val_loss: 12.4651\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9797 - val_loss: 12.4514\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.0087 - val_loss: 12.4321\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 12.9954 - val_loss: 12.2824\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.8830 - val_loss: 12.4137\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 12.9623 - val_loss: 12.5426\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.8850 - val_loss: 12.6690\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.8357 - val_loss: 12.6236\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9581 - val_loss: 12.4853\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.9478 - val_loss: 12.2952\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 12.9523 - val_loss: 12.4060\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 12.8466 - val_loss: 12.3185\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.9324 - val_loss: 12.3965\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 12.8792 - val_loss: 12.6624\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9159 - val_loss: 12.4546\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 12.9178 - val_loss: 12.4683\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.8867 - val_loss: 12.4595\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8817 - val_loss: 12.3445\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.8515 - val_loss: 12.4234\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.9086 - val_loss: 12.3110\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8702 - val_loss: 12.3821\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 12.9218 - val_loss: 12.2241\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7628 - val_loss: 12.2175\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.9640 - val_loss: 12.3532\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8357 - val_loss: 12.4899\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 12.8780 - val_loss: 12.3274\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 12.7763 - val_loss: 12.2431\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8570 - val_loss: 12.4958\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 12.7729 - val_loss: 12.3801\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 12.7953 - val_loss: 12.5323\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 12.7385 - val_loss: 12.4820\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.8622 - val_loss: 12.6100\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.7570 - val_loss: 12.4748\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8744 - val_loss: 12.4058\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 12.7637 - val_loss: 12.5549\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7855 - val_loss: 12.4922\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 12.7410 - val_loss: 12.4675\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.8480 - val_loss: 12.3852\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.7787 - val_loss: 12.2245\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.7565 - val_loss: 12.2052\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7496 - val_loss: 12.2602\n",
      "Epoch 238/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 131us/step - loss: 12.7644 - val_loss: 12.3515\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 12.8034 - val_loss: 12.4435\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.6870 - val_loss: 12.4177\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.7014 - val_loss: 12.5220\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 12.6575 - val_loss: 12.5753\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.7996 - val_loss: 12.5444\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 10.83 - 0s 128us/step - loss: 12.7116 - val_loss: 12.3564\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 12.6985 - val_loss: 12.5121\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.6630 - val_loss: 12.5820\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.6963 - val_loss: 12.7727\n",
      "Epoch 248/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 12.7422 - val_loss: 12.8242\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 12.7765 - val_loss: 12.7122\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7599 - val_loss: 12.5652\n"
     ]
    }
   ],
   "source": [
    "#pred_appliance = {}\n",
    "sequence_length=24\n",
    "num_iterations_dictionary = {'hvac':400,'fridge':500,'mw':250,'dw':250,'oven':250, 'wm':300}\n",
    "for appliance in APPLIANCES_ORDER[2:]:\n",
    "\n",
    "\n",
    "    print(appliance)\n",
    "    print(\"*\"*20)\n",
    "    np.random.seed(0)\n",
    "    from keras.layers.merge import Subtract, Minimum\n",
    "    model = Sequential()\n",
    "    filters=20\n",
    "    kernel_size=2\n",
    "    model.add(InputLayer(input_shape=(sequence_length,1)))\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ,name='C1'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Conv1D(filters=20,\n",
    "                     kernel_size=5,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Conv1D(filters=25,\n",
    "                     kernel_size=3,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Conv1D(filters=30,\n",
    "                     kernel_size=2,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(sequence_length, activation='relu'))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "    model.compile('adam','mean_absolute_error')\n",
    "    model.fit(train_agg.reshape(-1, 24, 1), train_appliance[appliance], epochs=num_iterations_dictionary[appliance], validation_split=0.1)\n",
    "    pred_appliance[appliance] = model.predict(test_agg.reshape(-1,24,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('per-appliance.pdf','wb') as f:\n",
    "    f.write(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='pdf'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 24, 1)             0         \n",
      "_________________________________________________________________\n",
      "C1 (Conv1D)                  (None, 24, 20)            220       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 12, 20)            0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 12, 20)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 12, 20)            2020      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 6, 20)             0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 6, 20)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 6, 25)             1525      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 3, 25)             0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 3, 25)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 3, 30)             1530      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1, 30)             0         \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 1, 30)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 24)                744       \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 6,039\n",
      "Trainable params: 6,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = {}\n",
    "for appliance in APPLIANCES_ORDER[1:]:\n",
    "    try:\n",
    "        mae[appliance] = mean_absolute_error(test_appliance[appliance], pred_appliance[appliance])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dw         14.499116\n",
       "fridge     34.620154\n",
       "hvac      331.035001\n",
       "mw          6.300214\n",
       "oven       18.633957\n",
       "wm          5.617521\n",
       "dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dw         14.499116\n",
       "fridge     34.620154\n",
       "hvac      331.035001\n",
       "mw          6.300214\n",
       "oven       18.633957\n",
       "wm          5.617521\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a32620588>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAA8CAYAAAB2H0HmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCFDhi2CLKBnyJAhwxbB/wG6pPMOssjkuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a259777f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_agg.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a32611080>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAEFCAYAAADdWD2lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN4hR0mAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a25bf0e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(test_appliance['dw'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a3699e668>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a368b6320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(test_appliance['hvac'][14]).plot(label='GT')\n",
    "\n",
    "pd.Series(pred_appliance['hvac'][14]).plot(label='Pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0c3c11c20fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_hvac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_agg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_hvac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_fridge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "pred_hvac = model.predict(test_agg)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(pred_hvac, test_fridge))\n",
    "print(mean_absolute_error(pred_hvac, test_agg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.907349e-06\n",
       "1    -3.877686e+00\n",
       "2    -9.150000e+00\n",
       "3     0.000000e+00\n",
       "4     0.000000e+00\n",
       "5    -9.633333e+00\n",
       "6     0.000000e+00\n",
       "7     2.861023e-06\n",
       "8    -8.683333e+00\n",
       "9     9.536743e-07\n",
       "10    0.000000e+00\n",
       "11    9.536743e-07\n",
       "12   -9.583333e+00\n",
       "13   -9.516666e+00\n",
       "14   -4.711666e+01\n",
       "15    3.099442e-06\n",
       "16   -4.685000e+01\n",
       "17    9.536743e-07\n",
       "18   -7.310000e+01\n",
       "19   -7.350000e+01\n",
       "20   -4.180000e+01\n",
       "21    0.000000e+00\n",
       "22   -9.616667e+00\n",
       "23   -9.533334e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.DataFrame(pred_hvac) - pd.DataFrame(test_agg)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f56c5e38e529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#pd.Series(test_agg[1, :]).plot(label='GT')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_agg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.Series(test_mw[1, :]).plot(label='GT')\n",
    "#pd.Series(test_agg[1, :]).plot(label='GT')\n",
    "\n",
    "\n",
    "pd.Series(model.predict(test_agg[1:2])[0, :24]).plot(label='Pred')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
