{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from common import APPLIANCES_ORDER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = np.load('../1H-input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subset_dataset(tensor):\n",
    "    t_subset = tensor[:, :, 180:194, :]\n",
    "    all_indices = np.array(list(range(320)))\n",
    "    for i in range(1, 7):\n",
    "        valid_homes = pd.DataFrame(t_subset[:, i, :].reshape(320, 14*24)).dropna().index\n",
    "        all_indices = np.intersect1d(all_indices, valid_homes)\n",
    "    t_subset = t_subset[all_indices, :, :, :].reshape(52, 7, 14, 24)\n",
    "    \n",
    "    # Create artificial aggregate\n",
    "    t_subset[:, 0, :,:] = 0.0\n",
    "    for i in range(1, 7):\n",
    "        t_subset[:, 0, :,:] = t_subset[:, 0, :,:] + t_subset[:, i, :,:]\n",
    "    # t_subset is of shape (#home, appliance, days*hours)\n",
    "    return t_subset, all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 14, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all, valid_homes = create_subset_dataset(tensor)\n",
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 14, 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_objective(y_pred, y_true):\n",
    "    with tf.name_scope(None):\n",
    "        return tf.losses.absolute_difference(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/nipun/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Lambda\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Conv1D, Dense, Flatten, MaxPool1D, InputLayer, Activation, Dropout, MaxPooling1D\n",
    "\n",
    "\n",
    "import keras\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "n_movies = 3\n",
    "n_users=3\n",
    "n_latent_factors=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a32b09160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb0AAADeCAYAAACpOd4EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmfUZ/x9LiBHIqwUoZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2a3e4f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(6,3))\n",
    "cbar_ax = fig.add_axes([.95, 0.15, .02, .7])\n",
    "home = 4\n",
    "appliance_num = 6\n",
    "sns.heatmap(t_all[home, 0, :, :],cmap='Greens',ax=ax[0],cbar_ax=None,cbar=False)\n",
    "sns.heatmap(t_all[home, appliance_num, :, :],cmap='Greens',ax=ax[1],cbar_ax=cbar_ax,vmax=t_all[home, 0, :, :].max())\n",
    "ax[0].set_title(\"Aggregate\")\n",
    "ax[1].set_title(APPLIANCES_ORDER[appliance_num])\n",
    "ax[0].set_ylabel(\"Day\")\n",
    "fig.text(0.5, 0, \"Hour of day\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_days = 14\n",
    "num_hours = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "aggregate = Input(shape=(num_days, num_hours, 1),name='Aggregate')  # adapt this if using `channels_first` image data format\n",
    "def scale(x, con):\n",
    "    return x/con\n",
    "\n",
    "aggregate_norm = Lambda(scale, arguments={'con':5000})(aggregate)\n",
    "\n",
    "hvac_conv = Conv2D(8, (14, 1), activation='relu', padding='same',name='HVAC-CONV')(aggregate_norm)\n",
    "hvac_deconv = Conv2D(1, (1,24), activation='relu', padding='same',name='HVAC-deconv')(hvac_conv)\n",
    "\n",
    "autoencoder = Model(aggregate, hvac_deconv)\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Aggregate (InputLayer)       (None, 14, 24, 1)         0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 14, 24, 1)         0         \n",
      "_________________________________________________________________\n",
      "HVAC-CONV (Conv2D)           (None, 14, 24, 8)         120       \n",
      "_________________________________________________________________\n",
      "HVAC-deconv (Conv2D)         (None, 14, 24, 1)         193       \n",
      "=================================================================\n",
      "Total params: 313\n",
      "Trainable params: 313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"296pt\" viewBox=\"0.00 0.00 329.11 296.00\" width=\"329pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 292)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-292 325.109,-292 325.109,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 112377872624 -->\n",
       "<g class=\"node\" id=\"node1\"><title>112377872624</title>\n",
       "<polygon fill=\"none\" points=\"4.95264,-243.5 4.95264,-287.5 316.157,-287.5 316.157,-243.5 4.95264,-243.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76.896\" y=\"-261.3\">Aggregate: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"148.839,-243.5 148.839,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176.674\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"148.839,-265.5 204.508,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176.674\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"204.508,-243.5 204.508,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"260.333\" y=\"-272.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"204.508,-265.5 316.157,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"260.333\" y=\"-250.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 112377875368 -->\n",
       "<g class=\"node\" id=\"node2\"><title>112377875368</title>\n",
       "<polygon fill=\"none\" points=\"14.6631,-162.5 14.6631,-206.5 306.446,-206.5 306.446,-162.5 14.6631,-162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76.896\" y=\"-180.3\">lambda_1: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"139.129,-162.5 139.129,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166.963\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"139.129,-184.5 194.798,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166.963\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"194.798,-162.5 194.798,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"250.622\" y=\"-191.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"194.798,-184.5 306.446,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"250.622\" y=\"-169.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 112377872624&#45;&gt;112377875368 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>112377872624-&gt;112377875368</title>\n",
       "<path d=\"M160.555,-243.329C160.555,-235.183 160.555,-225.699 160.555,-216.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"164.055,-216.729 160.555,-206.729 157.055,-216.729 164.055,-216.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 112518956032 -->\n",
       "<g class=\"node\" id=\"node3\"><title>112518956032</title>\n",
       "<polygon fill=\"none\" points=\"0.895508,-81.5 0.895508,-125.5 320.214,-125.5 320.214,-81.5 0.895508,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76.896\" y=\"-99.3\">HVAC-CONV: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"152.896,-81.5 152.896,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180.731\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"152.896,-103.5 208.565,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180.731\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"208.565,-81.5 208.565,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.39\" y=\"-110.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"208.565,-103.5 320.214,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.39\" y=\"-88.3\">(None, 14, 24, 8)</text>\n",
       "</g>\n",
       "<!-- 112377875368&#45;&gt;112518956032 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>112377875368-&gt;112518956032</title>\n",
       "<path d=\"M160.555,-162.329C160.555,-154.183 160.555,-144.699 160.555,-135.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"164.055,-135.729 160.555,-125.729 157.055,-135.729 164.055,-135.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 112519102536 -->\n",
       "<g class=\"node\" id=\"node4\"><title>112519102536</title>\n",
       "<polygon fill=\"none\" points=\"0,-0.5 0,-44.5 321.109,-44.5 321.109,-0.5 0,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76.896\" y=\"-18.3\">HVAC-deconv: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"153.792,-0.5 153.792,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.626\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"153.792,-22.5 209.461,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.626\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"209.461,-0.5 209.461,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265.285\" y=\"-29.3\">(None, 14, 24, 8)</text>\n",
       "<polyline fill=\"none\" points=\"209.461,-22.5 321.109,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265.285\" y=\"-7.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 112518956032&#45;&gt;112519102536 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>112518956032-&gt;112519102536</title>\n",
       "<path d=\"M160.555,-81.3294C160.555,-73.1826 160.555,-63.6991 160.555,-54.7971\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"164.055,-54.729 160.555,-44.729 157.055,-54.729 164.055,-54.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(autoencoder,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvac\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Model(aggregate, hvac_deconv)\n",
    "appliance_num=1\n",
    "print(APPLIANCES_ORDER[appliance_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxs = {appliance_num:t_all[:30, appliance_num, :, :].max() for appliance_num in range(7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5000.416633605957,\n",
       " 1: 3963.88330078125,\n",
       " 2: 288.85000610351562,\n",
       " 3: 960.7833251953125,\n",
       " 4: 1122.300048828125,\n",
       " 5: 498.43331909179688,\n",
       " 6: 2914.800048828125}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-11d960cdd6c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'agg' is not defined"
     ]
    }
   ],
   "source": [
    "agg.shape, maxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27 samples, validate on 3 samples\n",
      "Epoch 1/500\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1259 - val_loss: 0.1510\n",
      "Epoch 2/500\n",
      "27/27 [==============================] - 0s 513us/step - loss: 0.1259 - val_loss: 0.1510\n",
      "Epoch 3/500\n",
      "27/27 [==============================] - 0s 399us/step - loss: 0.1259 - val_loss: 0.1509\n",
      "Epoch 4/500\n",
      "27/27 [==============================] - 0s 487us/step - loss: 0.1259 - val_loss: 0.1509\n",
      "Epoch 5/500\n",
      "27/27 [==============================] - 0s 445us/step - loss: 0.1259 - val_loss: 0.1508\n",
      "Epoch 6/500\n",
      "27/27 [==============================] - 0s 468us/step - loss: 0.1259 - val_loss: 0.1508\n",
      "Epoch 7/500\n",
      "27/27 [==============================] - 0s 460us/step - loss: 0.1259 - val_loss: 0.1507\n",
      "Epoch 8/500\n",
      "27/27 [==============================] - 0s 454us/step - loss: 0.1259 - val_loss: 0.1507\n",
      "Epoch 9/500\n",
      "27/27 [==============================] - 0s 474us/step - loss: 0.1259 - val_loss: 0.1507\n",
      "Epoch 10/500\n",
      "27/27 [==============================] - 0s 405us/step - loss: 0.1259 - val_loss: 0.1507\n",
      "Epoch 11/500\n",
      "27/27 [==============================] - 0s 473us/step - loss: 0.1259 - val_loss: 0.1507\n",
      "Epoch 12/500\n",
      "27/27 [==============================] - 0s 477us/step - loss: 0.1259 - val_loss: 0.1507\n",
      "Epoch 13/500\n",
      "27/27 [==============================] - 0s 432us/step - loss: 0.1259 - val_loss: 0.1507\n",
      "Epoch 14/500\n",
      "27/27 [==============================] - 0s 447us/step - loss: 0.1259 - val_loss: 0.1507\n",
      "Epoch 15/500\n",
      "27/27 [==============================] - 0s 434us/step - loss: 0.1259 - val_loss: 0.1507\n",
      "Epoch 16/500\n",
      "27/27 [==============================] - 0s 451us/step - loss: 0.1258 - val_loss: 0.1507\n",
      "Epoch 17/500\n",
      "27/27 [==============================] - 0s 498us/step - loss: 0.1258 - val_loss: 0.1507\n",
      "Epoch 18/500\n",
      "27/27 [==============================] - 0s 481us/step - loss: 0.1258 - val_loss: 0.1507\n",
      "Epoch 19/500\n",
      "27/27 [==============================] - 0s 543us/step - loss: 0.1258 - val_loss: 0.1507\n",
      "Epoch 20/500\n",
      "27/27 [==============================] - 0s 482us/step - loss: 0.1258 - val_loss: 0.1506\n",
      "Epoch 21/500\n",
      "27/27 [==============================] - 0s 541us/step - loss: 0.1258 - val_loss: 0.1506\n",
      "Epoch 22/500\n",
      "27/27 [==============================] - 0s 462us/step - loss: 0.1258 - val_loss: 0.1506\n",
      "Epoch 23/500\n",
      "27/27 [==============================] - 0s 467us/step - loss: 0.1258 - val_loss: 0.1506\n",
      "Epoch 24/500\n",
      "27/27 [==============================] - 0s 450us/step - loss: 0.1258 - val_loss: 0.1505\n",
      "Epoch 25/500\n",
      "27/27 [==============================] - 0s 496us/step - loss: 0.1258 - val_loss: 0.1505\n",
      "Epoch 26/500\n",
      "27/27 [==============================] - 0s 501us/step - loss: 0.1258 - val_loss: 0.1505\n",
      "Epoch 27/500\n",
      "27/27 [==============================] - 0s 585us/step - loss: 0.1258 - val_loss: 0.1505\n",
      "Epoch 28/500\n",
      "27/27 [==============================] - 0s 601us/step - loss: 0.1258 - val_loss: 0.1505\n",
      "Epoch 29/500\n",
      "27/27 [==============================] - 0s 673us/step - loss: 0.1258 - val_loss: 0.1504\n",
      "Epoch 30/500\n",
      "27/27 [==============================] - 0s 631us/step - loss: 0.1258 - val_loss: 0.1504\n",
      "Epoch 31/500\n",
      "27/27 [==============================] - 0s 592us/step - loss: 0.1258 - val_loss: 0.1504\n",
      "Epoch 32/500\n",
      "27/27 [==============================] - 0s 491us/step - loss: 0.1258 - val_loss: 0.1504\n",
      "Epoch 33/500\n",
      "27/27 [==============================] - 0s 519us/step - loss: 0.1258 - val_loss: 0.1504\n",
      "Epoch 34/500\n",
      "27/27 [==============================] - 0s 496us/step - loss: 0.1258 - val_loss: 0.1504\n",
      "Epoch 35/500\n",
      "27/27 [==============================] - 0s 528us/step - loss: 0.1258 - val_loss: 0.1504\n",
      "Epoch 36/500\n",
      "27/27 [==============================] - 0s 479us/step - loss: 0.1258 - val_loss: 0.1504\n",
      "Epoch 37/500\n",
      "27/27 [==============================] - 0s 532us/step - loss: 0.1258 - val_loss: 0.1504\n",
      "Epoch 38/500\n",
      "27/27 [==============================] - 0s 545us/step - loss: 0.1258 - val_loss: 0.1504\n",
      "Epoch 39/500\n",
      "27/27 [==============================] - 0s 519us/step - loss: 0.1257 - val_loss: 0.1503\n",
      "Epoch 40/500\n",
      "27/27 [==============================] - 0s 553us/step - loss: 0.1257 - val_loss: 0.1503\n",
      "Epoch 41/500\n",
      "27/27 [==============================] - 0s 545us/step - loss: 0.1257 - val_loss: 0.1503\n",
      "Epoch 42/500\n",
      "27/27 [==============================] - 0s 530us/step - loss: 0.1257 - val_loss: 0.1502\n",
      "Epoch 43/500\n",
      "27/27 [==============================] - 0s 573us/step - loss: 0.1257 - val_loss: 0.1502\n",
      "Epoch 44/500\n",
      "27/27 [==============================] - 0s 583us/step - loss: 0.1257 - val_loss: 0.1502\n",
      "Epoch 45/500\n",
      "27/27 [==============================] - 0s 567us/step - loss: 0.1257 - val_loss: 0.1502\n",
      "Epoch 46/500\n",
      "27/27 [==============================] - 0s 598us/step - loss: 0.1257 - val_loss: 0.1502\n",
      "Epoch 47/500\n",
      "27/27 [==============================] - 0s 600us/step - loss: 0.1257 - val_loss: 0.1502\n",
      "Epoch 48/500\n",
      "27/27 [==============================] - 0s 562us/step - loss: 0.1257 - val_loss: 0.1502\n",
      "Epoch 49/500\n",
      "27/27 [==============================] - 0s 526us/step - loss: 0.1257 - val_loss: 0.1501\n",
      "Epoch 50/500\n",
      "27/27 [==============================] - 0s 546us/step - loss: 0.1257 - val_loss: 0.1501\n",
      "Epoch 51/500\n",
      "27/27 [==============================] - 0s 532us/step - loss: 0.1257 - val_loss: 0.1501\n",
      "Epoch 52/500\n",
      "27/27 [==============================] - 0s 530us/step - loss: 0.1257 - val_loss: 0.1501\n",
      "Epoch 53/500\n",
      "27/27 [==============================] - 0s 554us/step - loss: 0.1257 - val_loss: 0.1501\n",
      "Epoch 54/500\n",
      "27/27 [==============================] - 0s 575us/step - loss: 0.1257 - val_loss: 0.1501\n",
      "Epoch 55/500\n",
      "27/27 [==============================] - 0s 542us/step - loss: 0.1257 - val_loss: 0.1500\n",
      "Epoch 56/500\n",
      "27/27 [==============================] - 0s 514us/step - loss: 0.1257 - val_loss: 0.1500\n",
      "Epoch 57/500\n",
      "27/27 [==============================] - 0s 525us/step - loss: 0.1257 - val_loss: 0.1500\n",
      "Epoch 58/500\n",
      "27/27 [==============================] - 0s 610us/step - loss: 0.1257 - val_loss: 0.1500\n",
      "Epoch 59/500\n",
      "27/27 [==============================] - 0s 473us/step - loss: 0.1257 - val_loss: 0.1500\n",
      "Epoch 60/500\n",
      "27/27 [==============================] - 0s 546us/step - loss: 0.1257 - val_loss: 0.1500\n",
      "Epoch 61/500\n",
      "27/27 [==============================] - 0s 526us/step - loss: 0.1257 - val_loss: 0.1500\n",
      "Epoch 62/500\n",
      "27/27 [==============================] - 0s 519us/step - loss: 0.1257 - val_loss: 0.1500\n",
      "Epoch 63/500\n",
      "27/27 [==============================] - 0s 487us/step - loss: 0.1257 - val_loss: 0.1500\n",
      "Epoch 64/500\n",
      "27/27 [==============================] - 0s 496us/step - loss: 0.1257 - val_loss: 0.1500\n",
      "Epoch 65/500\n",
      "27/27 [==============================] - 0s 502us/step - loss: 0.1257 - val_loss: 0.1499\n",
      "Epoch 66/500\n",
      "27/27 [==============================] - 0s 490us/step - loss: 0.1257 - val_loss: 0.1499\n",
      "Epoch 67/500\n",
      "27/27 [==============================] - 0s 491us/step - loss: 0.1257 - val_loss: 0.1499\n",
      "Epoch 68/500\n",
      "27/27 [==============================] - 0s 529us/step - loss: 0.1257 - val_loss: 0.1499\n",
      "Epoch 69/500\n",
      "27/27 [==============================] - 0s 519us/step - loss: 0.1256 - val_loss: 0.1498\n",
      "Epoch 70/500\n",
      "27/27 [==============================] - 0s 516us/step - loss: 0.1256 - val_loss: 0.1498\n",
      "Epoch 71/500\n",
      "27/27 [==============================] - 0s 545us/step - loss: 0.1256 - val_loss: 0.1498\n",
      "Epoch 72/500\n",
      "27/27 [==============================] - 0s 508us/step - loss: 0.1256 - val_loss: 0.1498\n",
      "Epoch 73/500\n",
      "27/27 [==============================] - 0s 522us/step - loss: 0.1256 - val_loss: 0.1498\n",
      "Epoch 74/500\n",
      "27/27 [==============================] - 0s 544us/step - loss: 0.1256 - val_loss: 0.1498\n",
      "Epoch 75/500\n",
      "27/27 [==============================] - 0s 486us/step - loss: 0.1256 - val_loss: 0.1498\n",
      "Epoch 76/500\n",
      "27/27 [==============================] - 0s 493us/step - loss: 0.1256 - val_loss: 0.1498\n",
      "Epoch 77/500\n",
      "27/27 [==============================] - 0s 421us/step - loss: 0.1256 - val_loss: 0.1497\n",
      "Epoch 78/500\n",
      "27/27 [==============================] - 0s 438us/step - loss: 0.1256 - val_loss: 0.1497\n",
      "Epoch 79/500\n",
      "27/27 [==============================] - 0s 477us/step - loss: 0.1256 - val_loss: 0.1497\n",
      "Epoch 80/500\n",
      "27/27 [==============================] - 0s 410us/step - loss: 0.1256 - val_loss: 0.1497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "27/27 [==============================] - 0s 472us/step - loss: 0.1256 - val_loss: 0.1497\n",
      "Epoch 82/500\n",
      "27/27 [==============================] - 0s 498us/step - loss: 0.1256 - val_loss: 0.1497\n",
      "Epoch 83/500\n",
      "27/27 [==============================] - 0s 547us/step - loss: 0.1256 - val_loss: 0.1497\n",
      "Epoch 84/500\n",
      "27/27 [==============================] - 0s 540us/step - loss: 0.1256 - val_loss: 0.1496\n",
      "Epoch 85/500\n",
      "27/27 [==============================] - 0s 571us/step - loss: 0.1256 - val_loss: 0.1496\n",
      "Epoch 86/500\n",
      "27/27 [==============================] - 0s 552us/step - loss: 0.1256 - val_loss: 0.1496\n",
      "Epoch 87/500\n",
      "27/27 [==============================] - 0s 543us/step - loss: 0.1256 - val_loss: 0.1496\n",
      "Epoch 88/500\n",
      "27/27 [==============================] - 0s 493us/step - loss: 0.1256 - val_loss: 0.1496\n",
      "Epoch 89/500\n",
      "27/27 [==============================] - 0s 525us/step - loss: 0.1256 - val_loss: 0.1496\n",
      "Epoch 90/500\n",
      "27/27 [==============================] - 0s 540us/step - loss: 0.1256 - val_loss: 0.1496\n",
      "Epoch 91/500\n",
      "27/27 [==============================] - 0s 537us/step - loss: 0.1256 - val_loss: 0.1496\n",
      "Epoch 92/500\n",
      "27/27 [==============================] - 0s 503us/step - loss: 0.1256 - val_loss: 0.1496\n",
      "Epoch 93/500\n",
      "27/27 [==============================] - 0s 495us/step - loss: 0.1256 - val_loss: 0.1496\n",
      "Epoch 94/500\n",
      "27/27 [==============================] - 0s 438us/step - loss: 0.1256 - val_loss: 0.1495\n",
      "Epoch 95/500\n",
      "27/27 [==============================] - 0s 450us/step - loss: 0.1256 - val_loss: 0.1495\n",
      "Epoch 96/500\n",
      "27/27 [==============================] - 0s 509us/step - loss: 0.1256 - val_loss: 0.1495\n",
      "Epoch 97/500\n",
      "27/27 [==============================] - 0s 470us/step - loss: 0.1256 - val_loss: 0.1495\n",
      "Epoch 98/500\n",
      "27/27 [==============================] - 0s 519us/step - loss: 0.1256 - val_loss: 0.1495\n",
      "Epoch 99/500\n",
      "27/27 [==============================] - 0s 502us/step - loss: 0.1256 - val_loss: 0.1495\n",
      "Epoch 100/500\n",
      "27/27 [==============================] - 0s 468us/step - loss: 0.1256 - val_loss: 0.1495\n",
      "Epoch 101/500\n",
      "27/27 [==============================] - 0s 407us/step - loss: 0.1256 - val_loss: 0.1495\n",
      "Epoch 102/500\n",
      "27/27 [==============================] - 0s 471us/step - loss: 0.1256 - val_loss: 0.1495\n",
      "Epoch 103/500\n",
      "27/27 [==============================] - 0s 459us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 104/500\n",
      "27/27 [==============================] - 0s 493us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 105/500\n",
      "27/27 [==============================] - 0s 490us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 106/500\n",
      "27/27 [==============================] - 0s 483us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 107/500\n",
      "27/27 [==============================] - 0s 459us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 108/500\n",
      "27/27 [==============================] - 0s 465us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 109/500\n",
      "27/27 [==============================] - 0s 426us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 110/500\n",
      "27/27 [==============================] - 0s 480us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 111/500\n",
      "27/27 [==============================] - 0s 479us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 112/500\n",
      "27/27 [==============================] - 0s 470us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 113/500\n",
      "27/27 [==============================] - 0s 482us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 114/500\n",
      "27/27 [==============================] - 0s 483us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 115/500\n",
      "27/27 [==============================] - 0s 476us/step - loss: 0.1256 - val_loss: 0.1494\n",
      "Epoch 116/500\n",
      "27/27 [==============================] - 0s 461us/step - loss: 0.1256 - val_loss: 0.1493\n",
      "Epoch 117/500\n",
      "27/27 [==============================] - 0s 469us/step - loss: 0.1256 - val_loss: 0.1493\n",
      "Epoch 118/500\n",
      "27/27 [==============================] - 0s 441us/step - loss: 0.1256 - val_loss: 0.1493\n",
      "Epoch 119/500\n",
      "27/27 [==============================] - 0s 561us/step - loss: 0.1256 - val_loss: 0.1493\n",
      "Epoch 120/500\n",
      "27/27 [==============================] - 0s 477us/step - loss: 0.1256 - val_loss: 0.1493\n",
      "Epoch 121/500\n",
      "27/27 [==============================] - 0s 498us/step - loss: 0.1255 - val_loss: 0.1493\n",
      "Epoch 122/500\n",
      "27/27 [==============================] - 0s 520us/step - loss: 0.1255 - val_loss: 0.1493\n",
      "Epoch 123/500\n",
      "27/27 [==============================] - 0s 546us/step - loss: 0.1255 - val_loss: 0.1493\n",
      "Epoch 124/500\n",
      "27/27 [==============================] - 0s 533us/step - loss: 0.1255 - val_loss: 0.1493\n",
      "Epoch 125/500\n",
      "27/27 [==============================] - 0s 532us/step - loss: 0.1255 - val_loss: 0.1493\n",
      "Epoch 126/500\n",
      "27/27 [==============================] - 0s 584us/step - loss: 0.1255 - val_loss: 0.1493\n",
      "Epoch 127/500\n",
      "27/27 [==============================] - 0s 532us/step - loss: 0.1255 - val_loss: 0.1493\n",
      "Epoch 128/500\n",
      "27/27 [==============================] - 0s 481us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 129/500\n",
      "27/27 [==============================] - 0s 525us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 130/500\n",
      "27/27 [==============================] - 0s 540us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 131/500\n",
      "27/27 [==============================] - 0s 541us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 132/500\n",
      "27/27 [==============================] - 0s 476us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 133/500\n",
      "27/27 [==============================] - 0s 410us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 134/500\n",
      "27/27 [==============================] - 0s 468us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 135/500\n",
      "27/27 [==============================] - 0s 520us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 136/500\n",
      "27/27 [==============================] - 0s 501us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 137/500\n",
      "27/27 [==============================] - 0s 464us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 138/500\n",
      "27/27 [==============================] - 0s 469us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 139/500\n",
      "27/27 [==============================] - 0s 461us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 140/500\n",
      "27/27 [==============================] - 0s 457us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 141/500\n",
      "27/27 [==============================] - 0s 469us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 142/500\n",
      "27/27 [==============================] - 0s 471us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 143/500\n",
      "27/27 [==============================] - 0s 475us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 144/500\n",
      "27/27 [==============================] - 0s 485us/step - loss: 0.1255 - val_loss: 0.1492\n",
      "Epoch 145/500\n",
      "27/27 [==============================] - 0s 484us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 146/500\n",
      "27/27 [==============================] - 0s 508us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 147/500\n",
      "27/27 [==============================] - 0s 535us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 148/500\n",
      "27/27 [==============================] - 0s 452us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 149/500\n",
      "27/27 [==============================] - 0s 480us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 150/500\n",
      "27/27 [==============================] - 0s 482us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 151/500\n",
      "27/27 [==============================] - 0s 485us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 152/500\n",
      "27/27 [==============================] - 0s 497us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 153/500\n",
      "27/27 [==============================] - 0s 519us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 154/500\n",
      "27/27 [==============================] - 0s 509us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 155/500\n",
      "27/27 [==============================] - 0s 544us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 156/500\n",
      "27/27 [==============================] - 0s 533us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 157/500\n",
      "27/27 [==============================] - 0s 481us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 158/500\n",
      "27/27 [==============================] - 0s 495us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 159/500\n",
      "27/27 [==============================] - 0s 487us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 160/500\n",
      "27/27 [==============================] - 0s 543us/step - loss: 0.1255 - val_loss: 0.1491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/500\n",
      "27/27 [==============================] - 0s 519us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 162/500\n",
      "27/27 [==============================] - 0s 501us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 163/500\n",
      "27/27 [==============================] - 0s 503us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 164/500\n",
      "27/27 [==============================] - 0s 504us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 165/500\n",
      "27/27 [==============================] - 0s 469us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 166/500\n",
      "27/27 [==============================] - 0s 471us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 167/500\n",
      "27/27 [==============================] - 0s 464us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 168/500\n",
      "27/27 [==============================] - 0s 441us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 169/500\n",
      "27/27 [==============================] - 0s 482us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 170/500\n",
      "27/27 [==============================] - 0s 490us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 171/500\n",
      "27/27 [==============================] - 0s 500us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 172/500\n",
      "27/27 [==============================] - 0s 484us/step - loss: 0.1255 - val_loss: 0.1491\n",
      "Epoch 173/500\n",
      "27/27 [==============================] - 0s 470us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 174/500\n",
      "27/27 [==============================] - 0s 502us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 175/500\n",
      "27/27 [==============================] - 0s 396us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 176/500\n",
      "27/27 [==============================] - 0s 439us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 177/500\n",
      "27/27 [==============================] - 0s 456us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 178/500\n",
      "27/27 [==============================] - 0s 488us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 179/500\n",
      "27/27 [==============================] - 0s 485us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 180/500\n",
      "27/27 [==============================] - 0s 502us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 181/500\n",
      "27/27 [==============================] - 0s 492us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 182/500\n",
      "27/27 [==============================] - 0s 606us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 183/500\n",
      "27/27 [==============================] - 0s 579us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 184/500\n",
      "27/27 [==============================] - 0s 532us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 185/500\n",
      "27/27 [==============================] - 0s 683us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 186/500\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 187/500\n",
      "27/27 [==============================] - 0s 613us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 188/500\n",
      "27/27 [==============================] - 0s 474us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 189/500\n",
      "27/27 [==============================] - 0s 481us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 190/500\n",
      "27/27 [==============================] - 0s 541us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 191/500\n",
      "27/27 [==============================] - 0s 489us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 192/500\n",
      "27/27 [==============================] - 0s 479us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 193/500\n",
      "27/27 [==============================] - 0s 533us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 194/500\n",
      "27/27 [==============================] - 0s 444us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 195/500\n",
      "27/27 [==============================] - 0s 505us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 196/500\n",
      "27/27 [==============================] - 0s 505us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 197/500\n",
      "27/27 [==============================] - 0s 507us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 198/500\n",
      "27/27 [==============================] - 0s 507us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 199/500\n",
      "27/27 [==============================] - 0s 454us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 200/500\n",
      "27/27 [==============================] - 0s 480us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 201/500\n",
      "27/27 [==============================] - 0s 425us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 202/500\n",
      "27/27 [==============================] - 0s 581us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 203/500\n",
      "27/27 [==============================] - 0s 555us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 204/500\n",
      "27/27 [==============================] - 0s 487us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 205/500\n",
      "27/27 [==============================] - 0s 529us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 206/500\n",
      "27/27 [==============================] - 0s 498us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 207/500\n",
      "27/27 [==============================] - 0s 456us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 208/500\n",
      "27/27 [==============================] - 0s 489us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 209/500\n",
      "27/27 [==============================] - 0s 468us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 210/500\n",
      "27/27 [==============================] - 0s 513us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 211/500\n",
      "27/27 [==============================] - 0s 506us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 212/500\n",
      "27/27 [==============================] - 0s 578us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 213/500\n",
      "27/27 [==============================] - 0s 552us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 214/500\n",
      "27/27 [==============================] - 0s 549us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 215/500\n",
      "27/27 [==============================] - 0s 555us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 216/500\n",
      "27/27 [==============================] - 0s 582us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 217/500\n",
      "27/27 [==============================] - 0s 615us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 218/500\n",
      "27/27 [==============================] - 0s 545us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 219/500\n",
      "27/27 [==============================] - 0s 601us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 220/500\n",
      "27/27 [==============================] - 0s 500us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 221/500\n",
      "27/27 [==============================] - 0s 497us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 222/500\n",
      "27/27 [==============================] - 0s 461us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 223/500\n",
      "27/27 [==============================] - 0s 487us/step - loss: 0.1255 - val_loss: 0.1490\n",
      "Epoch 224/500\n",
      "27/27 [==============================] - 0s 497us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 225/500\n",
      "27/27 [==============================] - 0s 492us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 226/500\n",
      "27/27 [==============================] - 0s 448us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 227/500\n",
      "27/27 [==============================] - 0s 466us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 228/500\n",
      "27/27 [==============================] - 0s 520us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 229/500\n",
      "27/27 [==============================] - 0s 526us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 230/500\n",
      "27/27 [==============================] - 0s 511us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 231/500\n",
      "27/27 [==============================] - 0s 499us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 232/500\n",
      "27/27 [==============================] - 0s 504us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 233/500\n",
      "27/27 [==============================] - 0s 530us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 234/500\n",
      "27/27 [==============================] - 0s 510us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 235/500\n",
      "27/27 [==============================] - 0s 542us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 236/500\n",
      "27/27 [==============================] - 0s 530us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 237/500\n",
      "27/27 [==============================] - 0s 534us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 238/500\n",
      "27/27 [==============================] - 0s 520us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 239/500\n",
      "27/27 [==============================] - 0s 569us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 240/500\n",
      "27/27 [==============================] - 0s 726us/step - loss: 0.1255 - val_loss: 0.1489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/500\n",
      "27/27 [==============================] - 0s 587us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 242/500\n",
      "27/27 [==============================] - 0s 556us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 243/500\n",
      "27/27 [==============================] - 0s 510us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 244/500\n",
      "27/27 [==============================] - 0s 503us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 245/500\n",
      "27/27 [==============================] - 0s 505us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 246/500\n",
      "27/27 [==============================] - 0s 485us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 247/500\n",
      "27/27 [==============================] - 0s 491us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 248/500\n",
      "27/27 [==============================] - 0s 485us/step - loss: 0.1255 - val_loss: 0.1489\n",
      "Epoch 249/500\n",
      "27/27 [==============================] - 0s 543us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 250/500\n",
      "27/27 [==============================] - 0s 573us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 251/500\n",
      "27/27 [==============================] - 0s 470us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 252/500\n",
      "27/27 [==============================] - 0s 499us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 253/500\n",
      "27/27 [==============================] - 0s 559us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 254/500\n",
      "27/27 [==============================] - 0s 502us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 255/500\n",
      "27/27 [==============================] - 0s 526us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 256/500\n",
      "27/27 [==============================] - 0s 502us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 257/500\n",
      "27/27 [==============================] - 0s 519us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 258/500\n",
      "27/27 [==============================] - 0s 555us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 259/500\n",
      "27/27 [==============================] - 0s 525us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 260/500\n",
      "27/27 [==============================] - 0s 540us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 261/500\n",
      "27/27 [==============================] - 0s 546us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 262/500\n",
      "27/27 [==============================] - 0s 534us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 263/500\n",
      "27/27 [==============================] - 0s 513us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 264/500\n",
      "27/27 [==============================] - 0s 552us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 265/500\n",
      "27/27 [==============================] - 0s 544us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 266/500\n",
      "27/27 [==============================] - 0s 494us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 267/500\n",
      "27/27 [==============================] - 0s 466us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 268/500\n",
      "27/27 [==============================] - 0s 529us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 269/500\n",
      "27/27 [==============================] - 0s 472us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 270/500\n",
      "27/27 [==============================] - 0s 469us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 271/500\n",
      "27/27 [==============================] - 0s 466us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 272/500\n",
      "27/27 [==============================] - 0s 464us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 273/500\n",
      "27/27 [==============================] - 0s 450us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 274/500\n",
      "27/27 [==============================] - 0s 530us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 275/500\n",
      "27/27 [==============================] - 0s 513us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 276/500\n",
      "27/27 [==============================] - 0s 497us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 277/500\n",
      "27/27 [==============================] - 0s 526us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 278/500\n",
      "27/27 [==============================] - 0s 522us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 279/500\n",
      "27/27 [==============================] - 0s 457us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 280/500\n",
      "27/27 [==============================] - 0s 528us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 281/500\n",
      "27/27 [==============================] - 0s 542us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 282/500\n",
      "27/27 [==============================] - 0s 510us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 283/500\n",
      "27/27 [==============================] - 0s 520us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 284/500\n",
      "27/27 [==============================] - 0s 513us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 285/500\n",
      "27/27 [==============================] - 0s 550us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 286/500\n",
      "27/27 [==============================] - 0s 498us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 287/500\n",
      "27/27 [==============================] - 0s 522us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 288/500\n",
      "27/27 [==============================] - 0s 515us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 289/500\n",
      "27/27 [==============================] - 0s 431us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 290/500\n",
      "27/27 [==============================] - 0s 459us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 291/500\n",
      "27/27 [==============================] - 0s 513us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 292/500\n",
      "27/27 [==============================] - 0s 515us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 293/500\n",
      "27/27 [==============================] - 0s 528us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 294/500\n",
      "27/27 [==============================] - 0s 499us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 295/500\n",
      "27/27 [==============================] - 0s 582us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 296/500\n",
      "27/27 [==============================] - 0s 517us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 297/500\n",
      "27/27 [==============================] - 0s 504us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 298/500\n",
      "27/27 [==============================] - 0s 586us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 299/500\n",
      "27/27 [==============================] - 0s 486us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 300/500\n",
      "27/27 [==============================] - 0s 580us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 301/500\n",
      "27/27 [==============================] - 0s 524us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 302/500\n",
      "27/27 [==============================] - 0s 563us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 303/500\n",
      "27/27 [==============================] - 0s 548us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 304/500\n",
      "27/27 [==============================] - 0s 487us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 305/500\n",
      "27/27 [==============================] - 0s 481us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 306/500\n",
      "27/27 [==============================] - 0s 546us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 307/500\n",
      "27/27 [==============================] - 0s 540us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 308/500\n",
      "27/27 [==============================] - 0s 558us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 309/500\n",
      "27/27 [==============================] - 0s 533us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 310/500\n",
      "27/27 [==============================] - 0s 523us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 311/500\n",
      "27/27 [==============================] - 0s 550us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 312/500\n",
      "27/27 [==============================] - 0s 515us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 313/500\n",
      "27/27 [==============================] - 0s 493us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 314/500\n",
      "27/27 [==============================] - 0s 526us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 315/500\n",
      "27/27 [==============================] - 0s 509us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 316/500\n",
      "27/27 [==============================] - 0s 501us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 317/500\n",
      "27/27 [==============================] - 0s 511us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 318/500\n",
      "27/27 [==============================] - 0s 511us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 319/500\n",
      "27/27 [==============================] - 0s 536us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 320/500\n",
      "27/27 [==============================] - 0s 503us/step - loss: 0.1254 - val_loss: 0.1488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/500\n",
      "27/27 [==============================] - 0s 539us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 322/500\n",
      "27/27 [==============================] - 0s 520us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 323/500\n",
      "27/27 [==============================] - 0s 499us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 324/500\n",
      "27/27 [==============================] - 0s 462us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 325/500\n",
      "27/27 [==============================] - 0s 485us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 326/500\n",
      "27/27 [==============================] - 0s 468us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 327/500\n",
      "27/27 [==============================] - 0s 465us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 328/500\n",
      "27/27 [==============================] - 0s 478us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 329/500\n",
      "27/27 [==============================] - 0s 489us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 330/500\n",
      "27/27 [==============================] - 0s 507us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 331/500\n",
      "27/27 [==============================] - 0s 507us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 332/500\n",
      "27/27 [==============================] - 0s 496us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 333/500\n",
      "27/27 [==============================] - 0s 465us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 334/500\n",
      "27/27 [==============================] - 0s 511us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 335/500\n",
      "27/27 [==============================] - 0s 456us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 336/500\n",
      "27/27 [==============================] - 0s 474us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 337/500\n",
      "27/27 [==============================] - 0s 530us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 338/500\n",
      "27/27 [==============================] - 0s 529us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 339/500\n",
      "27/27 [==============================] - 0s 543us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 340/500\n",
      "27/27 [==============================] - 0s 517us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 341/500\n",
      "27/27 [==============================] - 0s 498us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 342/500\n",
      "27/27 [==============================] - 0s 499us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 343/500\n",
      "27/27 [==============================] - 0s 460us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 344/500\n",
      "27/27 [==============================] - 0s 517us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 345/500\n",
      "27/27 [==============================] - 0s 477us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 346/500\n",
      "27/27 [==============================] - 0s 485us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 347/500\n",
      "27/27 [==============================] - 0s 499us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 348/500\n",
      "27/27 [==============================] - 0s 470us/step - loss: 0.1254 - val_loss: 0.1489\n",
      "Epoch 349/500\n",
      "27/27 [==============================] - 0s 523us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 350/500\n",
      "27/27 [==============================] - 0s 422us/step - loss: 0.1254 - val_loss: 0.1487\n",
      "Epoch 351/500\n",
      "27/27 [==============================] - 0s 508us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 352/500\n",
      "27/27 [==============================] - 0s 512us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 353/500\n",
      "27/27 [==============================] - 0s 436us/step - loss: 0.1254 - val_loss: 0.1487\n",
      "Epoch 354/500\n",
      "27/27 [==============================] - 0s 457us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 355/500\n",
      "27/27 [==============================] - 0s 461us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 356/500\n",
      "27/27 [==============================] - 0s 474us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 357/500\n",
      "27/27 [==============================] - 0s 484us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 358/500\n",
      "27/27 [==============================] - 0s 474us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 359/500\n",
      "27/27 [==============================] - 0s 489us/step - loss: 0.1254 - val_loss: 0.1488\n",
      "Epoch 360/500\n",
      "27/27 [==============================] - 0s 446us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 361/500\n",
      "27/27 [==============================] - 0s 439us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 362/500\n",
      "27/27 [==============================] - 0s 474us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 363/500\n",
      "27/27 [==============================] - 0s 484us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 364/500\n",
      "27/27 [==============================] - 0s 456us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 365/500\n",
      "27/27 [==============================] - 0s 556us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 366/500\n",
      "27/27 [==============================] - 0s 489us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 367/500\n",
      "27/27 [==============================] - 0s 476us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 368/500\n",
      "27/27 [==============================] - 0s 470us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 369/500\n",
      "27/27 [==============================] - 0s 463us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 370/500\n",
      "27/27 [==============================] - 0s 493us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 371/500\n",
      "27/27 [==============================] - 0s 503us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 372/500\n",
      "27/27 [==============================] - 0s 473us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 373/500\n",
      "27/27 [==============================] - 0s 500us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 374/500\n",
      "27/27 [==============================] - 0s 496us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 375/500\n",
      "27/27 [==============================] - 0s 546us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 376/500\n",
      "27/27 [==============================] - 0s 531us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 377/500\n",
      "27/27 [==============================] - 0s 512us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 378/500\n",
      "27/27 [==============================] - 0s 553us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 379/500\n",
      "27/27 [==============================] - 0s 471us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 380/500\n",
      "27/27 [==============================] - 0s 587us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 381/500\n",
      "27/27 [==============================] - 0s 535us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 382/500\n",
      "27/27 [==============================] - 0s 543us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 383/500\n",
      "27/27 [==============================] - 0s 510us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 384/500\n",
      "27/27 [==============================] - 0s 500us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 385/500\n",
      "27/27 [==============================] - 0s 485us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 386/500\n",
      "27/27 [==============================] - 0s 428us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 387/500\n",
      "27/27 [==============================] - 0s 441us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 388/500\n",
      "27/27 [==============================] - 0s 454us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 389/500\n",
      "27/27 [==============================] - 0s 464us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 390/500\n",
      "27/27 [==============================] - 0s 482us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 391/500\n",
      "27/27 [==============================] - 0s 428us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 392/500\n",
      "27/27 [==============================] - 0s 402us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 393/500\n",
      "27/27 [==============================] - 0s 455us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 394/500\n",
      "27/27 [==============================] - 0s 452us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 395/500\n",
      "27/27 [==============================] - 0s 482us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 396/500\n",
      "27/27 [==============================] - 0s 504us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 397/500\n",
      "27/27 [==============================] - 0s 536us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 398/500\n",
      "27/27 [==============================] - 0s 471us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 399/500\n",
      "27/27 [==============================] - 0s 384us/step - loss: 0.1253 - val_loss: 0.1488\n",
      "Epoch 400/500\n",
      "27/27 [==============================] - 0s 433us/step - loss: 0.1253 - val_loss: 0.1487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/500\n",
      "27/27 [==============================] - 0s 454us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 402/500\n",
      "27/27 [==============================] - 0s 493us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 403/500\n",
      "27/27 [==============================] - 0s 465us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 404/500\n",
      "27/27 [==============================] - 0s 460us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 405/500\n",
      "27/27 [==============================] - 0s 491us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 406/500\n",
      "27/27 [==============================] - 0s 446us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 407/500\n",
      "27/27 [==============================] - 0s 452us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 408/500\n",
      "27/27 [==============================] - 0s 486us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 409/500\n",
      "27/27 [==============================] - 0s 473us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 410/500\n",
      "27/27 [==============================] - 0s 489us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 411/500\n",
      "27/27 [==============================] - 0s 529us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 412/500\n",
      "27/27 [==============================] - 0s 510us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 413/500\n",
      "27/27 [==============================] - 0s 518us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 414/500\n",
      "27/27 [==============================] - 0s 494us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 415/500\n",
      "27/27 [==============================] - 0s 508us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 416/500\n",
      "27/27 [==============================] - 0s 551us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 417/500\n",
      "27/27 [==============================] - 0s 474us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 418/500\n",
      "27/27 [==============================] - 0s 483us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 419/500\n",
      "27/27 [==============================] - 0s 478us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 420/500\n",
      "27/27 [==============================] - 0s 502us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 421/500\n",
      "27/27 [==============================] - 0s 480us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 422/500\n",
      "27/27 [==============================] - 0s 503us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 423/500\n",
      "27/27 [==============================] - 0s 463us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 424/500\n",
      "27/27 [==============================] - 0s 452us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 425/500\n",
      "27/27 [==============================] - 0s 486us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 426/500\n",
      "27/27 [==============================] - 0s 418us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 427/500\n",
      "27/27 [==============================] - 0s 485us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 428/500\n",
      "27/27 [==============================] - 0s 517us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 429/500\n",
      "27/27 [==============================] - 0s 518us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 430/500\n",
      "27/27 [==============================] - 0s 475us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 431/500\n",
      "27/27 [==============================] - 0s 503us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 432/500\n",
      "27/27 [==============================] - 0s 481us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 433/500\n",
      "27/27 [==============================] - 0s 488us/step - loss: 0.1253 - val_loss: 0.1487\n",
      "Epoch 434/500\n",
      "27/27 [==============================] - 0s 540us/step - loss: 0.1252 - val_loss: 0.1487\n",
      "Epoch 435/500\n",
      "27/27 [==============================] - 0s 522us/step - loss: 0.1252 - val_loss: 0.1487\n",
      "Epoch 436/500\n",
      "27/27 [==============================] - 0s 506us/step - loss: 0.1252 - val_loss: 0.1487\n",
      "Epoch 437/500\n",
      "27/27 [==============================] - 0s 499us/step - loss: 0.1252 - val_loss: 0.1487\n",
      "Epoch 438/500\n",
      "27/27 [==============================] - 0s 522us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 439/500\n",
      "27/27 [==============================] - 0s 488us/step - loss: 0.1252 - val_loss: 0.1487\n",
      "Epoch 440/500\n",
      "27/27 [==============================] - 0s 626us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 441/500\n",
      "27/27 [==============================] - 0s 425us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 442/500\n",
      "27/27 [==============================] - 0s 457us/step - loss: 0.1252 - val_loss: 0.1487\n",
      "Epoch 443/500\n",
      "27/27 [==============================] - 0s 496us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 444/500\n",
      "27/27 [==============================] - 0s 457us/step - loss: 0.1252 - val_loss: 0.1487\n",
      "Epoch 445/500\n",
      "27/27 [==============================] - 0s 467us/step - loss: 0.1252 - val_loss: 0.1487\n",
      "Epoch 446/500\n",
      "27/27 [==============================] - 0s 518us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 447/500\n",
      "27/27 [==============================] - 0s 523us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 448/500\n",
      "27/27 [==============================] - 0s 530us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 449/500\n",
      "27/27 [==============================] - 0s 517us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 450/500\n",
      "27/27 [==============================] - 0s 572us/step - loss: 0.1252 - val_loss: 0.1487\n",
      "Epoch 451/500\n",
      "27/27 [==============================] - 0s 544us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 452/500\n",
      "27/27 [==============================] - 0s 486us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 453/500\n",
      "27/27 [==============================] - 0s 508us/step - loss: 0.1252 - val_loss: 0.1487\n",
      "Epoch 454/500\n",
      "27/27 [==============================] - 0s 469us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 455/500\n",
      "27/27 [==============================] - 0s 515us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 456/500\n",
      "27/27 [==============================] - 0s 535us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 457/500\n",
      "27/27 [==============================] - 0s 415us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 458/500\n",
      "27/27 [==============================] - 0s 467us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 459/500\n",
      "27/27 [==============================] - 0s 482us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 460/500\n",
      "27/27 [==============================] - 0s 490us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 461/500\n",
      "27/27 [==============================] - 0s 504us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 462/500\n",
      "27/27 [==============================] - 0s 513us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 463/500\n",
      "27/27 [==============================] - 0s 673us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 464/500\n",
      "27/27 [==============================] - 0s 715us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 465/500\n",
      "27/27 [==============================] - 0s 982us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 466/500\n",
      "27/27 [==============================] - 0s 871us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 467/500\n",
      "27/27 [==============================] - 0s 495us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 468/500\n",
      "27/27 [==============================] - 0s 508us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 469/500\n",
      "27/27 [==============================] - 0s 424us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 470/500\n",
      "27/27 [==============================] - 0s 470us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 471/500\n",
      "27/27 [==============================] - 0s 466us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 472/500\n",
      "27/27 [==============================] - 0s 408us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 473/500\n",
      "27/27 [==============================] - 0s 363us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 474/500\n",
      "27/27 [==============================] - 0s 386us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 475/500\n",
      "27/27 [==============================] - 0s 418us/step - loss: 0.1252 - val_loss: 0.1485\n",
      "Epoch 476/500\n",
      "27/27 [==============================] - 0s 420us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 477/500\n",
      "27/27 [==============================] - 0s 414us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 478/500\n",
      "27/27 [==============================] - 0s 382us/step - loss: 0.1252 - val_loss: 0.1485\n",
      "Epoch 479/500\n",
      "27/27 [==============================] - 0s 440us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 480/500\n",
      "27/27 [==============================] - 0s 394us/step - loss: 0.1252 - val_loss: 0.1485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/500\n",
      "27/27 [==============================] - 0s 397us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 482/500\n",
      "27/27 [==============================] - 0s 433us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 483/500\n",
      "27/27 [==============================] - 0s 410us/step - loss: 0.1252 - val_loss: 0.1485\n",
      "Epoch 484/500\n",
      "27/27 [==============================] - 0s 423us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 485/500\n",
      "27/27 [==============================] - 0s 393us/step - loss: 0.1252 - val_loss: 0.1485\n",
      "Epoch 486/500\n",
      "27/27 [==============================] - 0s 411us/step - loss: 0.1252 - val_loss: 0.1486\n",
      "Epoch 487/500\n",
      "27/27 [==============================] - 0s 398us/step - loss: 0.1252 - val_loss: 0.1485\n",
      "Epoch 488/500\n",
      "27/27 [==============================] - 0s 401us/step - loss: 0.1251 - val_loss: 0.1486\n",
      "Epoch 489/500\n",
      "27/27 [==============================] - 0s 406us/step - loss: 0.1251 - val_loss: 0.1485\n",
      "Epoch 490/500\n",
      "27/27 [==============================] - 0s 428us/step - loss: 0.1251 - val_loss: 0.1485\n",
      "Epoch 491/500\n",
      "27/27 [==============================] - 0s 457us/step - loss: 0.1251 - val_loss: 0.1485\n",
      "Epoch 492/500\n",
      "27/27 [==============================] - 0s 402us/step - loss: 0.1251 - val_loss: 0.1485\n",
      "Epoch 493/500\n",
      "27/27 [==============================] - 0s 423us/step - loss: 0.1251 - val_loss: 0.1486\n",
      "Epoch 494/500\n",
      "27/27 [==============================] - 0s 372us/step - loss: 0.1251 - val_loss: 0.1485\n",
      "Epoch 495/500\n",
      "27/27 [==============================] - 0s 443us/step - loss: 0.1251 - val_loss: 0.1486\n",
      "Epoch 496/500\n",
      "27/27 [==============================] - 0s 447us/step - loss: 0.1251 - val_loss: 0.1485\n",
      "Epoch 497/500\n",
      "27/27 [==============================] - 0s 450us/step - loss: 0.1251 - val_loss: 0.1486\n",
      "Epoch 498/500\n",
      "27/27 [==============================] - 0s 409us/step - loss: 0.1251 - val_loss: 0.1485\n",
      "Epoch 499/500\n",
      "27/27 [==============================] - 0s 464us/step - loss: 0.1251 - val_loss: 0.1485\n",
      "Epoch 500/500\n",
      "27/27 [==============================] - 0s 514us/step - loss: 0.1251 - val_loss: 0.1485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a38e4d8d0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = Model(aggregate, hvac_deconv)\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "agg = (t_all[:30, 0, :, :].reshape(-1, num_days, num_hours,1))/maxs[0]\n",
    "appl = (t_all[:30, appliance_num, :, :].reshape(-1, num_days, num_hours,1) /maxs[appliance_num])\n",
    "autoencoder.fit(agg, appl\n",
    "                ,validation_split=0.1,epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = autoencoder.predict(t_all[30:, 0, :, :].reshape(-1, num_days, num_hours,1)).reshape(-1, num_days, num_hours)\n",
    "gt =t_all[30:, appliance_num, :, :]\n",
    "\n",
    "\n",
    "pred_fl = pred.flatten()\n",
    "gt_fl = gt.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22, 14, 24), (30, 14, 24, 1))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape, (t_all[:30, 0, :, :].reshape(-1, num_days, num_hours,1)/maxs[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "932.12410719348748"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(gt_fl, pred_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3dcf5be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_overall_appliance = (1*autoencoder.predict(t_all[30:, 0, :, :].reshape(-1, num_days, num_hours,1)/maxs[0])).reshape(-1,14,24)\n",
    "ax = pd.DataFrame(pred_overall_appliance[1, 3, :]).squeeze().plot(label='Predicted HVAC')\n",
    "pd.DataFrame(gt[1, 3, :]).squeeze().plot(ax=ax,label=\"GT HVAC\")\n",
    "#pd.DataFrame(t_all[31,4, 3, :]).squeeze().plot(ax=ax,label=\"GT DW\")\n",
    "plt.legend()\n",
    "plt.savefig(\"/Users/nipun/Desktop/hvac-cnn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a463ea630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_overall_appliance = (maxs[appliance_num]*autoencoder.predict(t_all[30:, 0, :, :].reshape(-1, num_days, num_hours,1)/maxs[0])).reshape(-1,14,24)\n",
    "ax = pd.DataFrame(pred_overall_appliance[1, 3, :]).squeeze().plot(label='Predicted HVAC')\n",
    "pd.DataFrame(gt[1, 3, :]).squeeze().plot(ax=ax,label=\"GT HVAC\")\n",
    "pd.DataFrame(t_all[31,4, 3, :]).squeeze().plot(ax=ax,label=\"GT DW\")\n",
    "plt.legend()\n",
    "plt.savefig(\"/Users/nipun/Desktop/hvac-dw-cnn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a42001ef0>]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKaVs6v8DpJPza5aLVqIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a41cbcc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(autoencoder.layers[2].get_weights()[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg = t_all[:30, 0, :].reshape(30*14, 24)\n",
    "train_appliance = {}\n",
    "test_appliance = {}\n",
    "for appliance_num, appliance in enumerate(APPLIANCES_ORDER[1:]):\n",
    "    train_appliance[appliance] = t_all[:30, appliance_num+1, :].reshape(30*14, 24)\n",
    "    test_appliance[appliance] = t_all[30:, appliance_num+1, :].reshape(22*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_hvac = t_all[30:, 1, :].reshape(22*14, 24)\n",
    "test_fridge = t_all[30:, 2, :].reshape(22*14, 24)\n",
    "\n",
    "test_mw = t_all[30:, 3, :].reshape(22*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "test_agg = t_all[30:, 0, :].reshape(22*14, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_hvac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-122380d9c71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_hvac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_hvac' is not defined"
     ]
    }
   ],
   "source": [
    "train_hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_hvac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-26ef9ccd3f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_hvac_fridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_hvac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fridge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_hvac_fridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_hvac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_fridge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_hvac' is not defined"
     ]
    }
   ],
   "source": [
    "train_hvac_fridge = np.hstack([train_hvac, train_fridge])\n",
    "test_hvac_fridge = np.hstack([test_hvac, test_fridge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fridge\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/500\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 104.0335 - val_loss: 80.1024\n",
      "Epoch 2/500\n",
      "378/378 [==============================] - 0s 128us/step - loss: 73.5439 - val_loss: 82.9650\n",
      "Epoch 3/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 69.5276 - val_loss: 71.1884\n",
      "Epoch 4/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 65.7872 - val_loss: 66.9487\n",
      "Epoch 5/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 61.5952 - val_loss: 60.5597\n",
      "Epoch 6/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 57.9371 - val_loss: 58.2805\n",
      "Epoch 7/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 54.5536 - val_loss: 53.5956\n",
      "Epoch 8/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 52.0083 - val_loss: 48.1487\n",
      "Epoch 9/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 49.8050 - val_loss: 47.3854\n",
      "Epoch 10/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 48.4076 - val_loss: 50.2753\n",
      "Epoch 11/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 48.1385 - val_loss: 44.8737\n",
      "Epoch 12/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 47.0126 - val_loss: 49.4019\n",
      "Epoch 13/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 46.6314 - val_loss: 44.6897\n",
      "Epoch 14/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 45.7859 - val_loss: 47.2255\n",
      "Epoch 15/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 45.4930 - val_loss: 45.9505\n",
      "Epoch 16/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 44.4689 - val_loss: 46.0596\n",
      "Epoch 17/500\n",
      "378/378 [==============================] - 0s 148us/step - loss: 43.9302 - val_loss: 43.3321\n",
      "Epoch 18/500\n",
      "378/378 [==============================] - 0s 140us/step - loss: 44.4852 - val_loss: 46.0947\n",
      "Epoch 19/500\n",
      "378/378 [==============================] - 0s 149us/step - loss: 44.3549 - val_loss: 44.9310\n",
      "Epoch 20/500\n",
      "378/378 [==============================] - 0s 174us/step - loss: 44.6102 - val_loss: 43.7336\n",
      "Epoch 21/500\n",
      "378/378 [==============================] - 0s 160us/step - loss: 44.0030 - val_loss: 48.1959\n",
      "Epoch 22/500\n",
      "378/378 [==============================] - 0s 158us/step - loss: 43.8427 - val_loss: 46.0852\n",
      "Epoch 23/500\n",
      "378/378 [==============================] - 0s 164us/step - loss: 43.7722 - val_loss: 45.0606\n",
      "Epoch 24/500\n",
      "378/378 [==============================] - 0s 156us/step - loss: 43.6814 - val_loss: 45.7697\n",
      "Epoch 25/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 43.6771 - val_loss: 46.1566\n",
      "Epoch 26/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 43.5886 - val_loss: 47.4341\n",
      "Epoch 27/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 42.9120 - val_loss: 43.5555\n",
      "Epoch 28/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 42.4706 - val_loss: 47.4463\n",
      "Epoch 29/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 42.8407 - val_loss: 44.2160\n",
      "Epoch 30/500\n",
      "378/378 [==============================] - 0s 149us/step - loss: 42.2783 - val_loss: 42.4520\n",
      "Epoch 31/500\n",
      "378/378 [==============================] - 0s 160us/step - loss: 43.3057 - val_loss: 46.7205\n",
      "Epoch 32/500\n",
      "378/378 [==============================] - 0s 184us/step - loss: 42.4322 - val_loss: 45.1091\n",
      "Epoch 33/500\n",
      "378/378 [==============================] - 0s 142us/step - loss: 42.2389 - val_loss: 40.7218\n",
      "Epoch 34/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 42.0600 - val_loss: 49.7435\n",
      "Epoch 35/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 42.2471 - val_loss: 44.0191\n",
      "Epoch 36/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 42.5494 - val_loss: 45.9218\n",
      "Epoch 37/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 41.5170 - val_loss: 41.8289\n",
      "Epoch 38/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 42.4506 - val_loss: 47.9723\n",
      "Epoch 39/500\n",
      "378/378 [==============================] - 0s 165us/step - loss: 42.6541 - val_loss: 45.8354\n",
      "Epoch 40/500\n",
      "378/378 [==============================] - 0s 177us/step - loss: 42.0716 - val_loss: 41.1500\n",
      "Epoch 41/500\n",
      "378/378 [==============================] - 0s 136us/step - loss: 41.4746 - val_loss: 48.8116\n",
      "Epoch 42/500\n",
      "378/378 [==============================] - 0s 136us/step - loss: 41.2513 - val_loss: 42.3331\n",
      "Epoch 43/500\n",
      "378/378 [==============================] - 0s 140us/step - loss: 41.2785 - val_loss: 45.9130\n",
      "Epoch 44/500\n",
      "378/378 [==============================] - 0s 141us/step - loss: 41.6845 - val_loss: 44.5290\n",
      "Epoch 45/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 41.2962 - val_loss: 42.9995\n",
      "Epoch 46/500\n",
      "378/378 [==============================] - 0s 137us/step - loss: 40.9711 - val_loss: 45.8175\n",
      "Epoch 47/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 40.8149 - val_loss: 41.8467\n",
      "Epoch 48/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 41.4019 - val_loss: 45.1568\n",
      "Epoch 49/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 40.9754 - val_loss: 43.5708\n",
      "Epoch 50/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 40.2788 - val_loss: 47.1195\n",
      "Epoch 51/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 41.0625 - val_loss: 44.8752\n",
      "Epoch 52/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 40.3319 - val_loss: 44.8103\n",
      "Epoch 53/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 41.1924 - val_loss: 44.8948\n",
      "Epoch 54/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 41.0538 - val_loss: 42.7822\n",
      "Epoch 55/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 40.2997 - val_loss: 44.7457\n",
      "Epoch 56/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 40.9821 - val_loss: 44.7372\n",
      "Epoch 57/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 40.7127 - val_loss: 41.2518\n",
      "Epoch 58/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 39.8744 - val_loss: 45.4890\n",
      "Epoch 59/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.5597 - val_loss: 43.3057\n",
      "Epoch 60/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 40.1709 - val_loss: 43.7829\n",
      "Epoch 61/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 41.0494 - val_loss: 43.0961\n",
      "Epoch 62/500\n",
      "378/378 [==============================] - 0s 130us/step - loss: 39.2322 - val_loss: 44.9770\n",
      "Epoch 63/500\n",
      "378/378 [==============================] - 0s 130us/step - loss: 39.1229 - val_loss: 41.1290\n",
      "Epoch 64/500\n",
      "378/378 [==============================] - 0s 137us/step - loss: 40.3158 - val_loss: 44.3470\n",
      "Epoch 65/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 39.8204 - val_loss: 45.1032\n",
      "Epoch 66/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 39.9711 - val_loss: 41.7657\n",
      "Epoch 67/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 40.3331 - val_loss: 43.3147\n",
      "Epoch 68/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 39.7473 - val_loss: 42.9658\n",
      "Epoch 69/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 38.9953 - val_loss: 41.1763\n",
      "Epoch 70/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 39.6315 - val_loss: 45.8329\n",
      "Epoch 71/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 39.7593 - val_loss: 42.4869\n",
      "Epoch 72/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 39.4382 - val_loss: 41.9857\n",
      "Epoch 73/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 39.6509 - val_loss: 46.6549\n",
      "Epoch 74/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 40.6982 - val_loss: 41.7623\n",
      "Epoch 75/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.1051 - val_loss: 40.3273\n",
      "Epoch 76/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 39.8424 - val_loss: 42.8496\n",
      "Epoch 77/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 38.8710 - val_loss: 42.2651\n",
      "Epoch 78/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.0188 - val_loss: 40.2959\n",
      "Epoch 79/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.6100 - val_loss: 43.2281\n",
      "Epoch 80/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 38.8302 - val_loss: 42.8166\n",
      "Epoch 81/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 39.6735 - val_loss: 42.1837\n",
      "Epoch 82/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 39.6392 - val_loss: 42.4519\n",
      "Epoch 83/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 39.2762 - val_loss: 41.7199\n",
      "Epoch 84/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.3054 - val_loss: 42.9781\n",
      "Epoch 85/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.9147 - val_loss: 41.6997\n",
      "Epoch 86/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 38.7012 - val_loss: 42.5982\n",
      "Epoch 87/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 39.7174 - val_loss: 44.3348\n",
      "Epoch 88/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 38.1136 - val_loss: 41.5925\n",
      "Epoch 89/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.2501 - val_loss: 41.0288\n",
      "Epoch 90/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 38.9085 - val_loss: 42.4723\n",
      "Epoch 91/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.5742 - val_loss: 44.1792\n",
      "Epoch 92/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.1642 - val_loss: 42.0135\n",
      "Epoch 93/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 37.9017 - val_loss: 39.7521\n",
      "Epoch 94/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.5336 - val_loss: 43.5638\n",
      "Epoch 95/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 38.6539 - val_loss: 39.9789\n",
      "Epoch 96/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.2280 - val_loss: 43.8354\n",
      "Epoch 97/500\n",
      "378/378 [==============================] - 0s 131us/step - loss: 37.6934 - val_loss: 41.9996\n",
      "Epoch 98/500\n",
      "378/378 [==============================] - 0s 136us/step - loss: 38.3388 - val_loss: 40.3986\n",
      "Epoch 99/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 38.4684 - val_loss: 43.0004\n",
      "Epoch 100/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 37.7812 - val_loss: 40.8004\n",
      "Epoch 101/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 38.3084 - val_loss: 42.5884\n",
      "Epoch 102/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 38.0219 - val_loss: 40.1661\n",
      "Epoch 103/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.8136 - val_loss: 42.0351\n",
      "Epoch 104/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.4676 - val_loss: 41.8278\n",
      "Epoch 105/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 38.0989 - val_loss: 39.3695\n",
      "Epoch 106/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 38.5142 - val_loss: 40.6178\n",
      "Epoch 107/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 37.7047 - val_loss: 42.8883\n",
      "Epoch 108/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.3515 - val_loss: 41.1268\n",
      "Epoch 109/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.6724 - val_loss: 42.5482\n",
      "Epoch 110/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.7970 - val_loss: 44.6775\n",
      "Epoch 111/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.0464 - val_loss: 41.4212\n",
      "Epoch 112/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.3740 - val_loss: 40.7406\n",
      "Epoch 113/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.1242 - val_loss: 42.7961\n",
      "Epoch 114/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.1698 - val_loss: 42.5217\n",
      "Epoch 115/500\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.6577 - val_loss: 38.7924\n",
      "Epoch 116/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 38.2747 - val_loss: 42.1709\n",
      "Epoch 117/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.2091 - val_loss: 42.9062\n",
      "Epoch 118/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 37.9202 - val_loss: 40.6951\n",
      "Epoch 119/500\n",
      "378/378 [==============================] - 0s 138us/step - loss: 37.7985 - val_loss: 40.9422\n",
      "Epoch 120/500\n",
      "378/378 [==============================] - 0s 144us/step - loss: 37.1773 - val_loss: 41.2419\n",
      "Epoch 121/500\n",
      "378/378 [==============================] - 0s 139us/step - loss: 37.9636 - val_loss: 41.1232\n",
      "Epoch 122/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 37.3918 - val_loss: 41.6713\n",
      "Epoch 123/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.3932 - val_loss: 41.4056\n",
      "Epoch 124/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.7046 - val_loss: 42.2162\n",
      "Epoch 125/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 37.0647 - val_loss: 42.7995\n",
      "Epoch 126/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.5751 - val_loss: 39.7770\n",
      "Epoch 127/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.5386 - val_loss: 43.7344\n",
      "Epoch 128/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 37.2338 - val_loss: 40.3577\n",
      "Epoch 129/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.8634 - val_loss: 42.2646\n",
      "Epoch 130/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 37.5268 - val_loss: 41.8290\n",
      "Epoch 131/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.6206 - val_loss: 40.4287\n",
      "Epoch 132/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.8056 - val_loss: 43.1433\n",
      "Epoch 133/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.6017 - val_loss: 41.4761\n",
      "Epoch 134/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 37.0967 - val_loss: 40.5701\n",
      "Epoch 135/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.5803 - val_loss: 43.6633\n",
      "Epoch 136/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.5535 - val_loss: 40.9257\n",
      "Epoch 137/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 36.9403 - val_loss: 41.8853\n",
      "Epoch 138/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 37.1445 - val_loss: 42.3455\n",
      "Epoch 139/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.7953 - val_loss: 42.5648\n",
      "Epoch 140/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 37.2671 - val_loss: 41.5168\n",
      "Epoch 141/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.3948 - val_loss: 41.5488\n",
      "Epoch 142/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.7204 - val_loss: 43.5029\n",
      "Epoch 143/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.8382 - val_loss: 42.0986\n",
      "Epoch 144/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.7946 - val_loss: 42.4184\n",
      "Epoch 145/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.5353 - val_loss: 40.5995\n",
      "Epoch 146/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.3156 - val_loss: 41.9622\n",
      "Epoch 147/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.2562 - val_loss: 41.3804\n",
      "Epoch 148/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.0928 - val_loss: 43.0357\n",
      "Epoch 149/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.3101 - val_loss: 44.4459\n",
      "Epoch 150/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.7756 - val_loss: 40.4137\n",
      "Epoch 151/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.3767 - val_loss: 44.1017\n",
      "Epoch 152/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4884 - val_loss: 40.0036\n",
      "Epoch 153/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.3941 - val_loss: 43.1410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.6051 - val_loss: 40.9227\n",
      "Epoch 155/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.8613 - val_loss: 37.9792\n",
      "Epoch 156/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.0987 - val_loss: 39.7679\n",
      "Epoch 157/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.8294 - val_loss: 40.1168\n",
      "Epoch 158/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.9135 - val_loss: 40.3052\n",
      "Epoch 159/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.9357 - val_loss: 41.5186\n",
      "Epoch 160/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.4006 - val_loss: 42.6061\n",
      "Epoch 161/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.0867 - val_loss: 39.4884\n",
      "Epoch 162/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.2109 - val_loss: 40.9371\n",
      "Epoch 163/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 35.8086 - val_loss: 40.4950\n",
      "Epoch 164/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.3686 - val_loss: 41.0350\n",
      "Epoch 165/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.5588 - val_loss: 40.5976\n",
      "Epoch 166/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.3359 - val_loss: 41.4256\n",
      "Epoch 167/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 35.9847 - val_loss: 42.4337\n",
      "Epoch 168/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 35.3983 - val_loss: 41.2303\n",
      "Epoch 169/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 35.9816 - val_loss: 41.5328\n",
      "Epoch 170/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.5666 - val_loss: 43.9853\n",
      "Epoch 171/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 35.7636 - val_loss: 39.1470\n",
      "Epoch 172/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.1742 - val_loss: 42.7128\n",
      "Epoch 173/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.5819 - val_loss: 39.3321\n",
      "Epoch 174/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.7133 - val_loss: 39.9044\n",
      "Epoch 175/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 35.7240 - val_loss: 40.8036\n",
      "Epoch 176/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 35.6581 - val_loss: 38.8584\n",
      "Epoch 177/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.0380 - val_loss: 39.5968\n",
      "Epoch 178/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.1719 - val_loss: 39.5981\n",
      "Epoch 179/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 35.2089 - val_loss: 40.9771\n",
      "Epoch 180/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.0237 - val_loss: 39.9841\n",
      "Epoch 181/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.6023 - val_loss: 40.0825\n",
      "Epoch 182/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.5747 - val_loss: 40.2889\n",
      "Epoch 183/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.0117 - val_loss: 40.0691\n",
      "Epoch 184/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 34.9315 - val_loss: 40.7599\n",
      "Epoch 185/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.4915 - val_loss: 39.3586\n",
      "Epoch 186/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.4378 - val_loss: 37.8337\n",
      "Epoch 187/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.6471 - val_loss: 40.5622\n",
      "Epoch 188/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 34.3123 - val_loss: 41.8082\n",
      "Epoch 189/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 35.4651 - val_loss: 38.2440\n",
      "Epoch 190/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.1621 - val_loss: 38.5763\n",
      "Epoch 191/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.6551 - val_loss: 38.7860\n",
      "Epoch 192/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 35.0629 - val_loss: 41.1443\n",
      "Epoch 193/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.5129 - val_loss: 39.1493\n",
      "Epoch 194/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.2658 - val_loss: 41.4198\n",
      "Epoch 195/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 34.7779 - val_loss: 40.7980\n",
      "Epoch 196/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 34.8862 - val_loss: 40.9140\n",
      "Epoch 197/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.8743 - val_loss: 41.8909\n",
      "Epoch 198/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 34.9811 - val_loss: 40.3892\n",
      "Epoch 199/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 34.7969 - val_loss: 39.4014\n",
      "Epoch 200/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.0890 - val_loss: 43.0319\n",
      "Epoch 201/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 34.5145 - val_loss: 40.5962\n",
      "Epoch 202/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 33.5780 - val_loss: 38.8348\n",
      "Epoch 203/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 35.1359 - val_loss: 41.8587\n",
      "Epoch 204/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.4528 - val_loss: 42.7902\n",
      "Epoch 205/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 34.6293 - val_loss: 40.3214\n",
      "Epoch 206/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 34.3915 - val_loss: 41.8413\n",
      "Epoch 207/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 33.8281 - val_loss: 39.3678\n",
      "Epoch 208/500\n",
      "378/378 [==============================] - 0s 131us/step - loss: 34.6807 - val_loss: 39.3762\n",
      "Epoch 209/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 34.6764 - val_loss: 40.1510\n",
      "Epoch 210/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 34.6784 - val_loss: 39.5043\n",
      "Epoch 211/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.5211 - val_loss: 39.0586\n",
      "Epoch 212/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.9104 - val_loss: 40.3789\n",
      "Epoch 213/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 34.3465 - val_loss: 40.7001\n",
      "Epoch 214/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 34.0345 - val_loss: 37.1951\n",
      "Epoch 215/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.1767 - val_loss: 36.8480\n",
      "Epoch 216/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 33.4834 - val_loss: 39.8700\n",
      "Epoch 217/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.5928 - val_loss: 37.8554\n",
      "Epoch 218/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 33.8481 - val_loss: 39.6575\n",
      "Epoch 219/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 33.7444 - val_loss: 39.6737\n",
      "Epoch 220/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.6347 - val_loss: 39.5478\n",
      "Epoch 221/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 33.2341 - val_loss: 37.9035\n",
      "Epoch 222/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.6352 - val_loss: 40.9090\n",
      "Epoch 223/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.3126 - val_loss: 39.1993\n",
      "Epoch 224/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.8085 - val_loss: 40.9065\n",
      "Epoch 225/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 34.0013 - val_loss: 39.4138\n",
      "Epoch 226/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 33.1372 - val_loss: 38.9036\n",
      "Epoch 227/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 33.5136 - val_loss: 39.4523\n",
      "Epoch 228/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 33.1981 - val_loss: 39.3788\n",
      "Epoch 229/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 34.0994 - val_loss: 40.0920\n",
      "Epoch 230/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.7761 - val_loss: 38.8550\n",
      "Epoch 231/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.1654 - val_loss: 40.7713\n",
      "Epoch 232/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 33.2325 - val_loss: 39.1207\n",
      "Epoch 233/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.4129 - val_loss: 41.0991\n",
      "Epoch 234/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 33.4390 - val_loss: 40.5542\n",
      "Epoch 235/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.3369 - val_loss: 40.3075\n",
      "Epoch 236/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 33.1326 - val_loss: 41.1542\n",
      "Epoch 237/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.1350 - val_loss: 40.2384\n",
      "Epoch 238/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.3292 - val_loss: 38.4436\n",
      "Epoch 239/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.5944 - val_loss: 41.0797\n",
      "Epoch 240/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 32.7899 - val_loss: 39.7161\n",
      "Epoch 241/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.7902 - val_loss: 40.3507\n",
      "Epoch 242/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.6278 - val_loss: 40.6046\n",
      "Epoch 243/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 32.6562 - val_loss: 38.6116\n",
      "Epoch 244/500\n",
      "378/378 [==============================] - 0s 104us/step - loss: 32.3921 - val_loss: 38.9213\n",
      "Epoch 245/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.9833 - val_loss: 37.8780\n",
      "Epoch 246/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.6613 - val_loss: 38.0931\n",
      "Epoch 247/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 32.2458 - val_loss: 38.0734\n",
      "Epoch 248/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.2783 - val_loss: 38.4257\n",
      "Epoch 249/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.8058 - val_loss: 37.4787\n",
      "Epoch 250/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 32.3892 - val_loss: 38.5760\n",
      "Epoch 251/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 31.8504 - val_loss: 38.8264\n",
      "Epoch 252/500\n",
      "378/378 [==============================] - 0s 147us/step - loss: 32.6496 - val_loss: 39.7328\n",
      "Epoch 253/500\n",
      "378/378 [==============================] - 0s 158us/step - loss: 32.8848 - val_loss: 38.6988\n",
      "Epoch 254/500\n",
      "378/378 [==============================] - 0s 143us/step - loss: 32.1924 - val_loss: 37.5768\n",
      "Epoch 255/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.1575 - val_loss: 38.1733\n",
      "Epoch 256/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.2895 - val_loss: 38.2265\n",
      "Epoch 257/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.4456 - val_loss: 39.3512\n",
      "Epoch 258/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.7110 - val_loss: 40.1061\n",
      "Epoch 259/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 32.6290 - val_loss: 39.3175\n",
      "Epoch 260/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.6322 - val_loss: 39.9267\n",
      "Epoch 261/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.0425 - val_loss: 39.3842\n",
      "Epoch 262/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.8770 - val_loss: 37.8634\n",
      "Epoch 263/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 32.2401 - val_loss: 38.3843\n",
      "Epoch 264/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.9199 - val_loss: 40.1153\n",
      "Epoch 265/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 32.0201 - val_loss: 39.0634\n",
      "Epoch 266/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.1032 - val_loss: 38.9554\n",
      "Epoch 267/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.9224 - val_loss: 38.0711\n",
      "Epoch 268/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.9938 - val_loss: 38.6844\n",
      "Epoch 269/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 32.0690 - val_loss: 40.0462\n",
      "Epoch 270/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.6396 - val_loss: 39.5965\n",
      "Epoch 271/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 32.0912 - val_loss: 40.2744\n",
      "Epoch 272/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.9187 - val_loss: 39.5437\n",
      "Epoch 273/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.0368 - val_loss: 40.3475\n",
      "Epoch 274/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.6007 - val_loss: 40.0427\n",
      "Epoch 275/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 32.3510 - val_loss: 40.2399\n",
      "Epoch 276/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.9359 - val_loss: 39.6656\n",
      "Epoch 277/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.5143 - val_loss: 40.0709\n",
      "Epoch 278/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.5180 - val_loss: 38.3469\n",
      "Epoch 279/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.4433 - val_loss: 40.1589\n",
      "Epoch 280/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.8190 - val_loss: 39.7440\n",
      "Epoch 281/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.2347 - val_loss: 40.3970\n",
      "Epoch 282/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.8447 - val_loss: 40.1331\n",
      "Epoch 283/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.4352 - val_loss: 38.7765\n",
      "Epoch 284/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.5202 - val_loss: 38.9538\n",
      "Epoch 285/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.7205 - val_loss: 40.0717\n",
      "Epoch 286/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.8096 - val_loss: 39.4729\n",
      "Epoch 287/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.4904 - val_loss: 40.7404\n",
      "Epoch 288/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.0193 - val_loss: 40.5975\n",
      "Epoch 289/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.8245 - val_loss: 41.0510\n",
      "Epoch 290/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.0844 - val_loss: 38.8660\n",
      "Epoch 291/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.2984 - val_loss: 39.3991\n",
      "Epoch 292/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.8212 - val_loss: 40.5605\n",
      "Epoch 293/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.0310 - val_loss: 40.6840\n",
      "Epoch 294/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.5821 - val_loss: 38.9809\n",
      "Epoch 295/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.7669 - val_loss: 38.2291\n",
      "Epoch 296/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.6155 - val_loss: 38.1854\n",
      "Epoch 297/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.4318 - val_loss: 39.0626\n",
      "Epoch 298/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.7502 - val_loss: 40.4638\n",
      "Epoch 299/500\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.5481 - val_loss: 40.0488\n",
      "Epoch 300/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6654 - val_loss: 39.5720\n",
      "Epoch 301/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.8151 - val_loss: 38.1565\n",
      "Epoch 302/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.0689 - val_loss: 39.8381\n",
      "Epoch 303/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.6052 - val_loss: 39.1679\n",
      "Epoch 304/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 31.3682 - val_loss: 39.7888\n",
      "Epoch 305/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.7615 - val_loss: 38.7943\n",
      "Epoch 306/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 112us/step - loss: 31.1762 - val_loss: 39.1932\n",
      "Epoch 307/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 31.3601 - val_loss: 40.4758\n",
      "Epoch 308/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.9723 - val_loss: 39.7988\n",
      "Epoch 309/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.2240 - val_loss: 39.2697\n",
      "Epoch 310/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.3497 - val_loss: 39.5279\n",
      "Epoch 311/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 31.1582 - val_loss: 39.6884\n",
      "Epoch 312/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.5861 - val_loss: 40.0486\n",
      "Epoch 313/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.2570 - val_loss: 38.6425\n",
      "Epoch 314/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.3038 - val_loss: 40.4669\n",
      "Epoch 315/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.9954 - val_loss: 41.9334\n",
      "Epoch 316/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.3679 - val_loss: 41.2329\n",
      "Epoch 317/500\n",
      "378/378 [==============================] - 0s 137us/step - loss: 31.5218 - val_loss: 40.7655\n",
      "Epoch 318/500\n",
      "378/378 [==============================] - 0s 139us/step - loss: 31.6142 - val_loss: 39.9809\n",
      "Epoch 319/500\n",
      "378/378 [==============================] - 0s 143us/step - loss: 31.4562 - val_loss: 40.3262\n",
      "Epoch 320/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 31.4370 - val_loss: 41.6874\n",
      "Epoch 321/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.4294 - val_loss: 38.9304\n",
      "Epoch 322/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.0041 - val_loss: 39.5018\n",
      "Epoch 323/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 31.4846 - val_loss: 39.0897\n",
      "Epoch 324/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.9111 - val_loss: 39.8756\n",
      "Epoch 325/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.1426 - val_loss: 39.9551\n",
      "Epoch 326/500\n",
      "378/378 [==============================] - 0s 140us/step - loss: 31.4511 - val_loss: 42.4207\n",
      "Epoch 327/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.7267 - val_loss: 40.2815\n",
      "Epoch 328/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 31.3319 - val_loss: 40.4035\n",
      "Epoch 329/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.9088 - val_loss: 40.7895\n",
      "Epoch 330/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.8061 - val_loss: 40.8743\n",
      "Epoch 331/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.0416 - val_loss: 42.1785\n",
      "Epoch 332/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.0934 - val_loss: 41.2613\n",
      "Epoch 333/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.4310 - val_loss: 40.6687\n",
      "Epoch 334/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8968 - val_loss: 40.0269\n",
      "Epoch 335/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.2072 - val_loss: 39.6861\n",
      "Epoch 336/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.3627 - val_loss: 39.9449\n",
      "Epoch 337/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.5941 - val_loss: 40.3691\n",
      "Epoch 338/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.8323 - val_loss: 40.6163\n",
      "Epoch 339/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.0530 - val_loss: 40.5178\n",
      "Epoch 340/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.3706 - val_loss: 41.5034\n",
      "Epoch 341/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.9251 - val_loss: 41.8167\n",
      "Epoch 342/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.3378 - val_loss: 41.0410\n",
      "Epoch 343/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.0038 - val_loss: 42.0460\n",
      "Epoch 344/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.5542 - val_loss: 38.7011\n",
      "Epoch 345/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.9995 - val_loss: 40.5179\n",
      "Epoch 346/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.8664 - val_loss: 39.7197\n",
      "Epoch 347/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.7968 - val_loss: 39.8042\n",
      "Epoch 348/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.3351 - val_loss: 40.9780\n",
      "Epoch 349/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.0728 - val_loss: 39.5267\n",
      "Epoch 350/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.6935 - val_loss: 41.0507\n",
      "Epoch 351/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.2417 - val_loss: 39.2725\n",
      "Epoch 352/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.7594 - val_loss: 42.9680\n",
      "Epoch 353/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.1339 - val_loss: 40.1520\n",
      "Epoch 354/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.2462 - val_loss: 39.7720\n",
      "Epoch 355/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.0445 - val_loss: 40.7611\n",
      "Epoch 356/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6748 - val_loss: 39.9286\n",
      "Epoch 357/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.8844 - val_loss: 41.1739\n",
      "Epoch 358/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.8852 - val_loss: 39.9802\n",
      "Epoch 359/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.3380 - val_loss: 40.3607\n",
      "Epoch 360/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.6271 - val_loss: 39.8885\n",
      "Epoch 361/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.7246 - val_loss: 39.4884\n",
      "Epoch 362/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.5408 - val_loss: 39.2351\n",
      "Epoch 363/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.4051 - val_loss: 40.5264\n",
      "Epoch 364/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.6440 - val_loss: 39.8891\n",
      "Epoch 365/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.2815 - val_loss: 40.2606\n",
      "Epoch 366/500\n",
      "378/378 [==============================] - 0s 131us/step - loss: 31.0982 - val_loss: 41.5259\n",
      "Epoch 367/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 31.3395 - val_loss: 40.8794\n",
      "Epoch 368/500\n",
      "378/378 [==============================] - 0s 153us/step - loss: 30.7414 - val_loss: 38.3555\n",
      "Epoch 369/500\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.4409 - val_loss: 39.9452\n",
      "Epoch 370/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.7530 - val_loss: 40.7835\n",
      "Epoch 371/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.7052 - val_loss: 41.7158\n",
      "Epoch 372/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.4735 - val_loss: 40.1631\n",
      "Epoch 373/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.3573 - val_loss: 39.7269\n",
      "Epoch 374/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 30.9443 - val_loss: 39.9814\n",
      "Epoch 375/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4814 - val_loss: 40.2537\n",
      "Epoch 376/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.3785 - val_loss: 40.2174\n",
      "Epoch 377/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.7557 - val_loss: 39.7024\n",
      "Epoch 378/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.3892 - val_loss: 39.6333\n",
      "Epoch 379/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.9077 - val_loss: 40.5202\n",
      "Epoch 380/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.9289 - val_loss: 40.3811\n",
      "Epoch 381/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1997 - val_loss: 42.2661\n",
      "Epoch 382/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.6856 - val_loss: 40.5898\n",
      "Epoch 383/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.2197 - val_loss: 40.3579\n",
      "Epoch 384/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.1481 - val_loss: 38.9558\n",
      "Epoch 385/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.6162 - val_loss: 39.1890\n",
      "Epoch 386/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.5506 - val_loss: 38.9824\n",
      "Epoch 387/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.0495 - val_loss: 38.4847\n",
      "Epoch 388/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6081 - val_loss: 39.1477\n",
      "Epoch 389/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.6526 - val_loss: 39.9162\n",
      "Epoch 390/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.9777 - val_loss: 39.2943\n",
      "Epoch 391/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.2468 - val_loss: 41.0840\n",
      "Epoch 392/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.0274 - val_loss: 39.0025\n",
      "Epoch 393/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.2719 - val_loss: 38.3878\n",
      "Epoch 394/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.9318 - val_loss: 40.7006\n",
      "Epoch 395/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.2238 - val_loss: 39.7894\n",
      "Epoch 396/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2653 - val_loss: 39.3546\n",
      "Epoch 397/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.3448 - val_loss: 40.1611\n",
      "Epoch 398/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.5298 - val_loss: 40.2074\n",
      "Epoch 399/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.8110 - val_loss: 39.4222\n",
      "Epoch 400/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2634 - val_loss: 38.2084\n",
      "Epoch 401/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8555 - val_loss: 40.3814\n",
      "Epoch 402/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.7609 - val_loss: 39.2130\n",
      "Epoch 403/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.1720 - val_loss: 39.8305\n",
      "Epoch 404/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1160 - val_loss: 39.7367\n",
      "Epoch 405/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.9318 - val_loss: 40.1310\n",
      "Epoch 406/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6782 - val_loss: 41.4852\n",
      "Epoch 407/500\n",
      "378/378 [==============================] - 0s 106us/step - loss: 30.6017 - val_loss: 41.2509\n",
      "Epoch 408/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.4026 - val_loss: 42.1216\n",
      "Epoch 409/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.4075 - val_loss: 40.1767\n",
      "Epoch 410/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.2170 - val_loss: 41.5693\n",
      "Epoch 411/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1166 - val_loss: 39.1333\n",
      "Epoch 412/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.0525 - val_loss: 39.4924\n",
      "Epoch 413/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.6204 - val_loss: 42.4384\n",
      "Epoch 414/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6252 - val_loss: 39.4593\n",
      "Epoch 415/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2652 - val_loss: 38.2620\n",
      "Epoch 416/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.5431 - val_loss: 39.5074\n",
      "Epoch 417/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.4167 - val_loss: 41.1863\n",
      "Epoch 418/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.3338 - val_loss: 40.9955\n",
      "Epoch 419/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.5837 - val_loss: 40.8305\n",
      "Epoch 420/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.2243 - val_loss: 39.9133\n",
      "Epoch 421/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.3353 - val_loss: 39.7060\n",
      "Epoch 422/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8174 - val_loss: 38.4152\n",
      "Epoch 423/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3533 - val_loss: 40.2639\n",
      "Epoch 424/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.2485 - val_loss: 39.0004\n",
      "Epoch 425/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.5796 - val_loss: 39.4051\n",
      "Epoch 426/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.5380 - val_loss: 40.9850\n",
      "Epoch 427/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.1968 - val_loss: 39.8298\n",
      "Epoch 428/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.4565 - val_loss: 39.1908\n",
      "Epoch 429/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.7562 - val_loss: 39.7028\n",
      "Epoch 430/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.8779 - val_loss: 40.3042\n",
      "Epoch 431/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.6204 - val_loss: 39.3020\n",
      "Epoch 432/500\n",
      "378/378 [==============================] - 0s 141us/step - loss: 30.6257 - val_loss: 37.8392\n",
      "Epoch 433/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.4159 - val_loss: 37.8130\n",
      "Epoch 434/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.1642 - val_loss: 38.9769\n",
      "Epoch 435/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.6226 - val_loss: 38.8080\n",
      "Epoch 436/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.0750 - val_loss: 41.5153\n",
      "Epoch 437/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.3290 - val_loss: 39.2093\n",
      "Epoch 438/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.0403 - val_loss: 39.2562\n",
      "Epoch 439/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.6607 - val_loss: 40.1348\n",
      "Epoch 440/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.8219 - val_loss: 39.7103\n",
      "Epoch 441/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4798 - val_loss: 40.2748\n",
      "Epoch 442/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.3315 - val_loss: 40.6481\n",
      "Epoch 443/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.9586 - val_loss: 41.7228\n",
      "Epoch 444/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.5015 - val_loss: 39.1771\n",
      "Epoch 445/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.3006 - val_loss: 42.6020\n",
      "Epoch 446/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.5700 - val_loss: 40.0667\n",
      "Epoch 447/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.3689 - val_loss: 41.5413\n",
      "Epoch 448/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.5158 - val_loss: 39.0737\n",
      "Epoch 449/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.5732 - val_loss: 40.6396\n",
      "Epoch 450/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.5456 - val_loss: 40.4941\n",
      "Epoch 451/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.6499 - val_loss: 40.1168\n",
      "Epoch 452/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.6815 - val_loss: 38.7778\n",
      "Epoch 453/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.9414 - val_loss: 39.3367\n",
      "Epoch 454/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.2941 - val_loss: 38.7558\n",
      "Epoch 455/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.4684 - val_loss: 39.9339\n",
      "Epoch 456/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8758 - val_loss: 41.2318\n",
      "Epoch 457/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.0184 - val_loss: 40.6614\n",
      "Epoch 458/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 117us/step - loss: 30.3796 - val_loss: 40.3136\n",
      "Epoch 459/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.4188 - val_loss: 39.4738\n",
      "Epoch 460/500\n",
      "378/378 [==============================] - 0s 134us/step - loss: 30.3670 - val_loss: 41.0653\n",
      "Epoch 461/500\n",
      "378/378 [==============================] - 0s 139us/step - loss: 31.0115 - val_loss: 40.8251\n",
      "Epoch 462/500\n",
      "378/378 [==============================] - 0s 148us/step - loss: 29.9184 - val_loss: 39.8697\n",
      "Epoch 463/500\n",
      "378/378 [==============================] - 0s 142us/step - loss: 30.1677 - val_loss: 40.4435\n",
      "Epoch 464/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.2846 - val_loss: 39.8982\n",
      "Epoch 465/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.1463 - val_loss: 40.1937\n",
      "Epoch 466/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.7215 - val_loss: 39.5725\n",
      "Epoch 467/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.6679 - val_loss: 40.0594\n",
      "Epoch 468/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2605 - val_loss: 40.2787\n",
      "Epoch 469/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.7998 - val_loss: 40.6502\n",
      "Epoch 470/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.2017 - val_loss: 40.5727\n",
      "Epoch 471/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.5701 - val_loss: 40.0939\n",
      "Epoch 472/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2308 - val_loss: 41.4846\n",
      "Epoch 473/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.5639 - val_loss: 40.9715\n",
      "Epoch 474/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.9164 - val_loss: 40.4180\n",
      "Epoch 475/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.1598 - val_loss: 39.9764\n",
      "Epoch 476/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.3017 - val_loss: 40.2078\n",
      "Epoch 477/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.2758 - val_loss: 40.3070\n",
      "Epoch 478/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.0853 - val_loss: 40.0043\n",
      "Epoch 479/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.8395 - val_loss: 39.9165\n",
      "Epoch 480/500\n",
      "378/378 [==============================] - 0s 107us/step - loss: 30.2189 - val_loss: 41.9646\n",
      "Epoch 481/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.1160 - val_loss: 42.3742\n",
      "Epoch 482/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.0229 - val_loss: 41.2193\n",
      "Epoch 483/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.6877 - val_loss: 39.6180\n",
      "Epoch 484/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.5883 - val_loss: 40.9395\n",
      "Epoch 485/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.8305 - val_loss: 41.0310\n",
      "Epoch 486/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.7814 - val_loss: 40.0478\n",
      "Epoch 487/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.7570 - val_loss: 40.6354\n",
      "Epoch 488/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.1819 - val_loss: 42.2014\n",
      "Epoch 489/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.2726 - val_loss: 41.4132\n",
      "Epoch 490/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.4976 - val_loss: 40.4997\n",
      "Epoch 491/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.5545 - val_loss: 40.1144\n",
      "Epoch 492/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.3070 - val_loss: 41.5619\n",
      "Epoch 493/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.2813 - val_loss: 40.8578\n",
      "Epoch 494/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.7680 - val_loss: 40.8972\n",
      "Epoch 495/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.4461 - val_loss: 41.0887\n",
      "Epoch 496/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.7840 - val_loss: 41.8095\n",
      "Epoch 497/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.0379 - val_loss: 41.4529\n",
      "Epoch 498/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.6094 - val_loss: 41.3542\n",
      "Epoch 499/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.9991 - val_loss: 41.5048\n",
      "Epoch 500/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2172 - val_loss: 40.0301\n",
      "mw\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 83.8219 - val_loss: 17.9068\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 158us/step - loss: 15.9060 - val_loss: 9.1364\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 8.6676 - val_loss: 8.4402\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 7.5378 - val_loss: 8.3536\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.9806 - val_loss: 8.3394\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.8319 - val_loss: 8.3364\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.6353 - val_loss: 8.3336\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.5548 - val_loss: 8.3308\n",
      "Epoch 9/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.5012 - val_loss: 8.3275\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3987 - val_loss: 8.3269\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.4453 - val_loss: 8.3271\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.4317 - val_loss: 8.3288\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.4610 - val_loss: 8.3305\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.4278 - val_loss: 8.3305\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.4111 - val_loss: 8.3299\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3986 - val_loss: 8.3305\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3594 - val_loss: 8.3305\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3660 - val_loss: 8.3305\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3618 - val_loss: 8.3305\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.4251 - val_loss: 8.3305\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3732 - val_loss: 8.3305\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3477 - val_loss: 8.3305\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3357 - val_loss: 8.3305\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3571 - val_loss: 8.3305\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3493 - val_loss: 8.3305\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3137 - val_loss: 8.3305\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3493 - val_loss: 8.3305\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3213 - val_loss: 8.3305\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3604 - val_loss: 8.3305\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3377 - val_loss: 8.3305\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3254 - val_loss: 8.3305\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3174 - val_loss: 8.3305\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3316 - val_loss: 8.3305\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3302 - val_loss: 8.3305\n",
      "Epoch 35/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 121us/step - loss: 6.3622 - val_loss: 8.3305\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3444 - val_loss: 8.3305\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3139 - val_loss: 8.3305\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3362 - val_loss: 8.3305\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3247 - val_loss: 8.3305\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3242 - val_loss: 8.3305\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3202 - val_loss: 8.3305\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3161 - val_loss: 8.3305\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3093 - val_loss: 8.3305\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3236 - val_loss: 8.3305\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - 0s 180us/step - loss: 6.3189 - val_loss: 8.3305\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - 0s 149us/step - loss: 6.3081 - val_loss: 8.3305\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 6.3318 - val_loss: 8.3305\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3134 - val_loss: 8.3305\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3074 - val_loss: 8.3305\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3299 - val_loss: 8.3305\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3110 - val_loss: 8.3305\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3149 - val_loss: 8.3305\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3224 - val_loss: 8.3305\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3119 - val_loss: 8.3305\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 6.3101 - val_loss: 8.3305\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 6.3043 - val_loss: 8.3305\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3084 - val_loss: 8.3305\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3243 - val_loss: 8.3305\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 6.3036 - val_loss: 8.3305\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3057 - val_loss: 8.3305\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3265 - val_loss: 8.3305\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3062 - val_loss: 8.3305\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3051 - val_loss: 8.3305\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3094 - val_loss: 8.3305\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3042 - val_loss: 8.3305\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3096 - val_loss: 8.3305\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3021 - val_loss: 8.3305\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3027 - val_loss: 8.3305\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3144 - val_loss: 8.3305\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3022 - val_loss: 8.3305\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3075 - val_loss: 8.3305\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3030 - val_loss: 8.3305\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3057 - val_loss: 8.3305\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3083 - val_loss: 8.3305\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3121 - val_loss: 8.3305\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3060 - val_loss: 8.3305\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.2999 - val_loss: 8.3305\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3070 - val_loss: 8.3305\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.2971 - val_loss: 8.3305\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3019 - val_loss: 8.3305\n",
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3153 - val_loss: 8.3305\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3059 - val_loss: 8.3305\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3069 - val_loss: 8.3305\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3007 - val_loss: 8.3305\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3034 - val_loss: 8.3305\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3056 - val_loss: 8.3305\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3047 - val_loss: 8.3305\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3082 - val_loss: 8.3305\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.2995 - val_loss: 8.3305\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3089 - val_loss: 8.3305\n",
      "Epoch 96/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3063 - val_loss: 8.3305\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3115 - val_loss: 8.3305\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3056 - val_loss: 8.3305\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3052 - val_loss: 8.3305\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3070 - val_loss: 8.3305\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.2973 - val_loss: 8.3305\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3007 - val_loss: 8.3305\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3049 - val_loss: 8.3305\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3045 - val_loss: 8.3305\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3040 - val_loss: 8.3305\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3008 - val_loss: 8.3305\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3032 - val_loss: 8.3305\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3068 - val_loss: 8.3305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3000 - val_loss: 8.3305\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 111us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3038 - val_loss: 8.3305\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3047 - val_loss: 8.3305\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3041 - val_loss: 8.3305\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3045 - val_loss: 8.3305\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3047 - val_loss: 8.3305\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3055 - val_loss: 8.3305\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3039 - val_loss: 8.3305\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3027 - val_loss: 8.3305\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3031 - val_loss: 8.3305\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3086 - val_loss: 8.3305\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3023 - val_loss: 8.3305\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3041 - val_loss: 8.3305\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3063 - val_loss: 8.3305\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3061 - val_loss: 8.3305\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3008 - val_loss: 8.3305\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3019 - val_loss: 8.3305\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 6.3036 - val_loss: 8.3305\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3004 - val_loss: 8.3305\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3049 - val_loss: 8.3305\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3036 - val_loss: 8.3305\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.2990 - val_loss: 8.3305\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 159us/step - loss: 6.3033 - val_loss: 8.3305\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 159us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3090 - val_loss: 8.3305\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.2981 - val_loss: 8.3305\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3005 - val_loss: 8.3305\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3094 - val_loss: 8.3305\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3125 - val_loss: 8.3305\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3011 - val_loss: 8.3305\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3069 - val_loss: 8.3305\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3057 - val_loss: 8.3305\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3042 - val_loss: 8.3305\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.2989 - val_loss: 8.3305\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3043 - val_loss: 8.3305\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3019 - val_loss: 8.3305\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3020 - val_loss: 8.3305\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3006 - val_loss: 8.3305\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3030 - val_loss: 8.3305\n",
      "Epoch 192/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 129us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3023 - val_loss: 8.3305\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.2995 - val_loss: 8.3305\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3016 - val_loss: 8.3305\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3010 - val_loss: 8.3305\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3031 - val_loss: 8.3305\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3073 - val_loss: 8.3305\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 106us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3020 - val_loss: 8.3305\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3010 - val_loss: 8.3305\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3011 - val_loss: 8.3305\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3023 - val_loss: 8.3305\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3010 - val_loss: 8.3305\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3021 - val_loss: 8.3305\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3021 - val_loss: 8.3305\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3008 - val_loss: 8.3305\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 6.3050 - val_loss: 8.3305\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3016 - val_loss: 8.3305\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 180us/step - loss: 6.3084 - val_loss: 8.3305\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 238/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3007 - val_loss: 8.3305\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - 0s 145us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3029 - val_loss: 8.3305\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 248/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 178us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "dw\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 88.8070 - val_loss: 27.0537\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 23.8474 - val_loss: 16.5754\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 16.2885 - val_loss: 15.3362\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 14.8806 - val_loss: 15.1313\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.3470 - val_loss: 15.0873\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.0234 - val_loss: 15.0768\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.8086 - val_loss: 15.0720\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.7636 - val_loss: 15.0704\n",
      "Epoch 9/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.6758 - val_loss: 15.0676\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.6207 - val_loss: 15.0651\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.6369 - val_loss: 15.0648\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.5958 - val_loss: 15.0648\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.6300 - val_loss: 15.0648\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.5852 - val_loss: 15.0648\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.5750 - val_loss: 15.0648\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.5561 - val_loss: 15.0648\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4897 - val_loss: 15.0648\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.5354 - val_loss: 15.0648\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4976 - val_loss: 15.0648\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.5699 - val_loss: 15.0648\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.5366 - val_loss: 15.0648\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.5273 - val_loss: 15.0648\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4944 - val_loss: 15.0648\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.5043 - val_loss: 15.0648\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.5022 - val_loss: 15.0648\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4721 - val_loss: 15.0648\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4894 - val_loss: 15.0648\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4748 - val_loss: 15.0648\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4939 - val_loss: 15.0648\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4882 - val_loss: 15.0648\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 147us/step - loss: 13.4665 - val_loss: 15.0648\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 136us/step - loss: 13.4645 - val_loss: 15.0648\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4709 - val_loss: 15.0648\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4623 - val_loss: 15.0648\n",
      "Epoch 35/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4907 - val_loss: 15.0648\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4793 - val_loss: 15.0648\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4697 - val_loss: 15.0648\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.4852 - val_loss: 15.0648\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.5081 - val_loss: 15.0648\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4590 - val_loss: 15.0648\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 13.4690 - val_loss: 15.0648\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4574 - val_loss: 15.0648\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4556 - val_loss: 15.0648\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 111us/step - loss: 13.4849 - val_loss: 15.0648\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.4650 - val_loss: 15.0648\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 13.4597 - val_loss: 15.0648\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4779 - val_loss: 15.0648\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4569 - val_loss: 15.0648\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4761 - val_loss: 15.0648\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4626 - val_loss: 15.0648\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4567 - val_loss: 15.0648\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.4476 - val_loss: 15.0648\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4640 - val_loss: 15.0648\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 155us/step - loss: 13.4589 - val_loss: 15.0648\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 147us/step - loss: 13.4535 - val_loss: 15.0648\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 13.4530 - val_loss: 15.0648\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4541 - val_loss: 15.0648\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4690 - val_loss: 15.0648\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.4498 - val_loss: 15.0648\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4676 - val_loss: 15.0648\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4530 - val_loss: 15.0648\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 109us/step - loss: 13.4593 - val_loss: 15.0648\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4552 - val_loss: 15.0648\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4494 - val_loss: 15.0648\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4624 - val_loss: 15.0648\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4461 - val_loss: 15.0648\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4506 - val_loss: 15.0648\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4569 - val_loss: 15.0648\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4472 - val_loss: 15.0648\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4620 - val_loss: 15.0648\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4461 - val_loss: 15.0648\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4514 - val_loss: 15.0648\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4516 - val_loss: 15.0648\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4563 - val_loss: 15.0648\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4503 - val_loss: 15.0648\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4501 - val_loss: 15.0648\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4505 - val_loss: 15.0648\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4466 - val_loss: 15.0648\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4519 - val_loss: 15.0648\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4495 - val_loss: 15.0648\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4659 - val_loss: 15.0648\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4515 - val_loss: 15.0648\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4526 - val_loss: 15.0648\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4509 - val_loss: 15.0648\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4507 - val_loss: 15.0648\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.4443 - val_loss: 15.0648\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4499 - val_loss: 15.0648\n",
      "Epoch 96/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 126us/step - loss: 13.4612 - val_loss: 15.0648\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4559 - val_loss: 15.0648\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4526 - val_loss: 15.0648\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4521 - val_loss: 15.0648\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4508 - val_loss: 15.0648\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4481 - val_loss: 15.0648\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4490 - val_loss: 15.0648\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4481 - val_loss: 15.0648\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4506 - val_loss: 15.0648\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4491 - val_loss: 15.0648\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4482 - val_loss: 15.0648\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4517 - val_loss: 15.0648\n",
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4460 - val_loss: 15.0648\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4479 - val_loss: 15.0648\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4490 - val_loss: 15.0648\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4505 - val_loss: 15.0648\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4533 - val_loss: 15.0648\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4464 - val_loss: 15.0648\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4518 - val_loss: 15.0648\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4489 - val_loss: 15.0648\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4477 - val_loss: 15.0648\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4468 - val_loss: 15.0648\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4483 - val_loss: 15.0648\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4512 - val_loss: 15.0648\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4502 - val_loss: 15.0648\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4456 - val_loss: 15.0648\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4516 - val_loss: 15.0648\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4519 - val_loss: 15.0648\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4465 - val_loss: 15.0648\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4463 - val_loss: 15.0648\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4470 - val_loss: 15.0648\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4492 - val_loss: 15.0648\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4468 - val_loss: 15.0648\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4467 - val_loss: 15.0648\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4486 - val_loss: 15.0648\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4396 - val_loss: 15.0648\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4474 - val_loss: 15.0648\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4502 - val_loss: 15.0648\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4489 - val_loss: 15.0648\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 13.4554 - val_loss: 15.0648\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4528 - val_loss: 15.0648\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4498 - val_loss: 15.0648\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4455 - val_loss: 15.0648\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4497 - val_loss: 15.0648\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4501 - val_loss: 15.0648\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4483 - val_loss: 15.0648\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4468 - val_loss: 15.0648\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.4445 - val_loss: 15.0648\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 151us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 143us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 150us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 145us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 192/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4463 - val_loss: 15.0648\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4451 - val_loss: 15.0648\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4470 - val_loss: 15.0648\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4493 - val_loss: 15.0648\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 160us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.4455 - val_loss: 15.0648\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 136us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4529 - val_loss: 15.0648\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 149us/step - loss: 13.4478 - val_loss: 15.0648\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4512 - val_loss: 15.0648\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 238/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4455 - val_loss: 15.0648\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4448 - val_loss: 15.0648\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4443 - val_loss: 15.0648\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4458 - val_loss: 15.0648\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4499 - val_loss: 15.0648\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4463 - val_loss: 15.0648\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 248/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "wm\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/300\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 83.1399 - val_loss: 17.0875\n",
      "Epoch 2/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 15.5730 - val_loss: 5.7701\n",
      "Epoch 3/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 7.4662 - val_loss: 4.5463\n",
      "Epoch 4/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.1663 - val_loss: 4.3702\n",
      "Epoch 5/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 5.5833 - val_loss: 4.3453\n",
      "Epoch 6/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 5.3496 - val_loss: 4.3410\n",
      "Epoch 7/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 5.1049 - val_loss: 4.3410\n",
      "Epoch 8/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 5.0757 - val_loss: 4.3410\n",
      "Epoch 9/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.9750 - val_loss: 4.3410\n",
      "Epoch 10/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.8889 - val_loss: 4.3410\n",
      "Epoch 11/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.9104 - val_loss: 4.3410\n",
      "Epoch 12/300\n",
      "378/378 [==============================] - 0s 110us/step - loss: 4.8698 - val_loss: 4.3410\n",
      "Epoch 13/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.8947 - val_loss: 4.3410\n",
      "Epoch 14/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.8756 - val_loss: 4.3410\n",
      "Epoch 15/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.8541 - val_loss: 4.3410\n",
      "Epoch 16/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.8294 - val_loss: 4.3410\n",
      "Epoch 17/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.7914 - val_loss: 4.3410\n",
      "Epoch 18/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.8266 - val_loss: 4.3410\n",
      "Epoch 19/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7914 - val_loss: 4.3410\n",
      "Epoch 20/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.8456 - val_loss: 4.3410\n",
      "Epoch 21/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.8388 - val_loss: 4.3410\n",
      "Epoch 22/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.8020 - val_loss: 4.3410\n",
      "Epoch 23/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7891 - val_loss: 4.3410\n",
      "Epoch 24/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7675 - val_loss: 4.3410\n",
      "Epoch 25/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7936 - val_loss: 4.3410\n",
      "Epoch 26/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7624 - val_loss: 4.3410\n",
      "Epoch 27/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.8092 - val_loss: 4.3410\n",
      "Epoch 28/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7803 - val_loss: 4.3410\n",
      "Epoch 29/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7942 - val_loss: 4.3410\n",
      "Epoch 30/300\n",
      "378/378 [==============================] - 0s 134us/step - loss: 4.7814 - val_loss: 4.3410\n",
      "Epoch 31/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7552 - val_loss: 4.3410\n",
      "Epoch 32/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7570 - val_loss: 4.3410\n",
      "Epoch 33/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7664 - val_loss: 4.3410\n",
      "Epoch 34/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7657 - val_loss: 4.3410\n",
      "Epoch 35/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7832 - val_loss: 4.3410\n",
      "Epoch 36/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7741 - val_loss: 4.3410\n",
      "Epoch 37/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7536 - val_loss: 4.3410\n",
      "Epoch 38/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7773 - val_loss: 4.3410\n",
      "Epoch 39/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7866 - val_loss: 4.3410\n",
      "Epoch 40/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7525 - val_loss: 4.3410\n",
      "Epoch 41/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7605 - val_loss: 4.3410\n",
      "Epoch 42/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7473 - val_loss: 4.3410\n",
      "Epoch 43/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7483 - val_loss: 4.3410\n",
      "Epoch 44/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7674 - val_loss: 4.3410\n",
      "Epoch 45/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.7554 - val_loss: 4.3410\n",
      "Epoch 46/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7519 - val_loss: 4.3410\n",
      "Epoch 47/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7615 - val_loss: 4.3410\n",
      "Epoch 48/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7456 - val_loss: 4.3410\n",
      "Epoch 49/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7627 - val_loss: 4.3410\n",
      "Epoch 50/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7313 - val_loss: 4.3410\n",
      "Epoch 51/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7510 - val_loss: 4.3410\n",
      "Epoch 52/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7439 - val_loss: 4.3410\n",
      "Epoch 53/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7563 - val_loss: 4.3410\n",
      "Epoch 54/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7474 - val_loss: 4.3410\n",
      "Epoch 55/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7449 - val_loss: 4.3410\n",
      "Epoch 56/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 57/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7409 - val_loss: 4.3410\n",
      "Epoch 58/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7396 - val_loss: 4.3410\n",
      "Epoch 59/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7391 - val_loss: 4.3410\n",
      "Epoch 60/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 61/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7497 - val_loss: 4.3410\n",
      "Epoch 62/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7366 - val_loss: 4.3410\n",
      "Epoch 63/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7472 - val_loss: 4.3410\n",
      "Epoch 64/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7406 - val_loss: 4.3410\n",
      "Epoch 65/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7340 - val_loss: 4.3410\n",
      "Epoch 66/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7343 - val_loss: 4.3410\n",
      "Epoch 67/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7482 - val_loss: 4.3410\n",
      "Epoch 68/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7369 - val_loss: 4.3410\n",
      "Epoch 69/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7344 - val_loss: 4.3410\n",
      "Epoch 70/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7479 - val_loss: 4.3410\n",
      "Epoch 71/300\n",
      "378/378 [==============================] - 0s 172us/step - loss: 4.7350 - val_loss: 4.3410\n",
      "Epoch 72/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7327 - val_loss: 4.3410\n",
      "Epoch 73/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7509 - val_loss: 4.3410\n",
      "Epoch 74/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 75/300\n",
      "378/378 [==============================] - 0s 137us/step - loss: 4.7389 - val_loss: 4.3410\n",
      "Epoch 76/300\n",
      "378/378 [==============================] - 0s 150us/step - loss: 4.7433 - val_loss: 4.3410\n",
      "Epoch 77/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7436 - val_loss: 4.3410\n",
      "Epoch 78/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7427 - val_loss: 4.3410\n",
      "Epoch 79/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7372 - val_loss: 4.3410\n",
      "Epoch 80/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7403 - val_loss: 4.3410\n",
      "Epoch 81/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 82/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7329 - val_loss: 4.3410\n",
      "Epoch 83/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7414 - val_loss: 4.3410\n",
      "Epoch 84/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7337 - val_loss: 4.3410\n",
      "Epoch 85/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 86/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7585 - val_loss: 4.3410\n",
      "Epoch 87/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7378 - val_loss: 4.3410\n",
      "Epoch 88/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7478 - val_loss: 4.3410\n",
      "Epoch 89/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7342 - val_loss: 4.3410\n",
      "Epoch 90/300\n",
      "378/378 [==============================] - 0s 135us/step - loss: 4.7374 - val_loss: 4.3410\n",
      "Epoch 91/300\n",
      "378/378 [==============================] - 0s 152us/step - loss: 4.7334 - val_loss: 4.3410\n",
      "Epoch 92/300\n",
      "378/378 [==============================] - 0s 144us/step - loss: 4.7333 - val_loss: 4.3410\n",
      "Epoch 93/300\n",
      "378/378 [==============================] - 0s 150us/step - loss: 4.7343 - val_loss: 4.3410\n",
      "Epoch 94/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7344 - val_loss: 4.3410\n",
      "Epoch 95/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7366 - val_loss: 4.3410\n",
      "Epoch 96/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7488 - val_loss: 4.3410\n",
      "Epoch 97/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7440 - val_loss: 4.3410\n",
      "Epoch 98/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7370 - val_loss: 4.3410\n",
      "Epoch 99/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 100/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7309 - val_loss: 4.3410\n",
      "Epoch 101/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 102/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7432 - val_loss: 4.3410\n",
      "Epoch 103/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7407 - val_loss: 4.3410\n",
      "Epoch 104/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7382 - val_loss: 4.3410\n",
      "Epoch 105/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7352 - val_loss: 4.3410\n",
      "Epoch 106/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7358 - val_loss: 4.3410\n",
      "Epoch 107/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7355 - val_loss: 4.3410\n",
      "Epoch 108/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 109/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7343 - val_loss: 4.3410\n",
      "Epoch 110/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 111/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7345 - val_loss: 4.3410\n",
      "Epoch 112/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7404 - val_loss: 4.3410\n",
      "Epoch 113/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7414 - val_loss: 4.3410\n",
      "Epoch 114/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7340 - val_loss: 4.3410\n",
      "Epoch 115/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 116/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 117/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7311 - val_loss: 4.3410\n",
      "Epoch 118/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7399 - val_loss: 4.3410\n",
      "Epoch 119/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7329 - val_loss: 4.3410\n",
      "Epoch 120/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7376 - val_loss: 4.3410\n",
      "Epoch 121/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 122/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7389 - val_loss: 4.3410\n",
      "Epoch 123/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7396 - val_loss: 4.3410\n",
      "Epoch 124/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7347 - val_loss: 4.3410\n",
      "Epoch 125/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7354 - val_loss: 4.3410\n",
      "Epoch 126/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7355 - val_loss: 4.3410\n",
      "Epoch 127/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7345 - val_loss: 4.3410\n",
      "Epoch 128/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 129/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7309 - val_loss: 4.3410\n",
      "Epoch 130/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 131/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7368 - val_loss: 4.3410\n",
      "Epoch 132/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7370 - val_loss: 4.3410\n",
      "Epoch 133/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7357 - val_loss: 4.3410\n",
      "Epoch 134/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 135/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7378 - val_loss: 4.3410\n",
      "Epoch 136/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7373 - val_loss: 4.3410\n",
      "Epoch 137/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7344 - val_loss: 4.3410\n",
      "Epoch 138/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 139/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 140/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7361 - val_loss: 4.3410\n",
      "Epoch 141/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 142/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 143/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7351 - val_loss: 4.3410\n",
      "Epoch 144/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 145/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7349 - val_loss: 4.3410\n",
      "Epoch 146/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7370 - val_loss: 4.3410\n",
      "Epoch 147/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7321 - val_loss: 4.3410\n",
      "Epoch 148/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 149/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7323 - val_loss: 4.3410\n",
      "Epoch 150/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7361 - val_loss: 4.3410\n",
      "Epoch 151/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 152/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 153/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7338 - val_loss: 4.3410\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 119us/step - loss: 4.7265 - val_loss: 4.3410\n",
      "Epoch 155/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 156/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7379 - val_loss: 4.3410\n",
      "Epoch 157/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 158/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7332 - val_loss: 4.3410\n",
      "Epoch 159/300\n",
      "378/378 [==============================] - 0s 136us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 160/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7411 - val_loss: 4.3410\n",
      "Epoch 161/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7391 - val_loss: 4.3410\n",
      "Epoch 162/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7347 - val_loss: 4.3410\n",
      "Epoch 163/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7316 - val_loss: 4.3410\n",
      "Epoch 164/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7332 - val_loss: 4.3410\n",
      "Epoch 165/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 166/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 167/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 168/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7354 - val_loss: 4.3410\n",
      "Epoch 169/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 170/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 171/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 172/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 173/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 174/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7351 - val_loss: 4.3410\n",
      "Epoch 175/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 176/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 177/300\n",
      "378/378 [==============================] - 0s 156us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 178/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7360 - val_loss: 4.3410\n",
      "Epoch 179/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 180/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7310 - val_loss: 4.3410\n",
      "Epoch 181/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 182/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 183/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7335 - val_loss: 4.3410\n",
      "Epoch 184/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7312 - val_loss: 4.3410\n",
      "Epoch 185/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 186/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7309 - val_loss: 4.3410\n",
      "Epoch 187/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7325 - val_loss: 4.3410\n",
      "Epoch 188/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 189/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 190/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 191/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 192/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 193/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 194/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 195/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 196/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 197/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7332 - val_loss: 4.3410\n",
      "Epoch 198/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 199/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 200/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 201/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7308 - val_loss: 4.3410\n",
      "Epoch 202/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 203/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 204/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 205/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7337 - val_loss: 4.3410\n",
      "Epoch 206/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 207/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7333 - val_loss: 4.3410\n",
      "Epoch 208/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 209/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7316 - val_loss: 4.3410\n",
      "Epoch 210/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 211/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 212/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 213/300\n",
      "378/378 [==============================] - 0s 139us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 214/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 215/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 216/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7325 - val_loss: 4.3410\n",
      "Epoch 217/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 218/300\n",
      "378/378 [==============================] - 0s 170us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 219/300\n",
      "378/378 [==============================] - 0s 147us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 220/300\n",
      "378/378 [==============================] - 0s 179us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 221/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 222/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 223/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7399 - val_loss: 4.3410\n",
      "Epoch 224/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 225/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 226/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7334 - val_loss: 4.3410\n",
      "Epoch 227/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 228/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 229/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7323 - val_loss: 4.3410\n",
      "Epoch 230/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 231/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 232/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7362 - val_loss: 4.3410\n",
      "Epoch 233/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 234/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 235/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7362 - val_loss: 4.3410\n",
      "Epoch 236/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 237/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 238/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7316 - val_loss: 4.3410\n",
      "Epoch 239/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 240/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 241/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 4.554 - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 242/300\n",
      "378/378 [==============================] - 0s 135us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 243/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7364 - val_loss: 4.3410\n",
      "Epoch 244/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7325 - val_loss: 4.3410\n",
      "Epoch 245/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 246/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 247/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 248/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 249/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 250/300\n",
      "378/378 [==============================] - 0s 136us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 251/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 252/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 253/300\n",
      "378/378 [==============================] - 0s 141us/step - loss: 4.7333 - val_loss: 4.3410\n",
      "Epoch 254/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 255/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 6.104 - 0s 123us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 256/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 257/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 258/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 259/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7338 - val_loss: 4.3410\n",
      "Epoch 260/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7337 - val_loss: 4.3410\n",
      "Epoch 261/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 262/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 263/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 264/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7322 - val_loss: 4.3410\n",
      "Epoch 265/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 266/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7461 - val_loss: 4.3410\n",
      "Epoch 267/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7322 - val_loss: 4.3410\n",
      "Epoch 268/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 269/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 270/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 271/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 272/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 273/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7331 - val_loss: 4.3410\n",
      "Epoch 274/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7338 - val_loss: 4.3410\n",
      "Epoch 275/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 276/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 277/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 278/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 279/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 280/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 281/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 282/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 283/300\n",
      "378/378 [==============================] - 0s 142us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 284/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 285/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7379 - val_loss: 4.3410\n",
      "Epoch 286/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 287/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 288/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 289/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 290/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 291/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 292/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 293/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 294/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 295/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7327 - val_loss: 4.3410\n",
      "Epoch 296/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 297/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 298/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 299/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7340 - val_loss: 4.3410\n",
      "Epoch 300/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7322 - val_loss: 4.3410\n",
      "oven\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 85.4946 - val_loss: 24.5830\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 23.2607 - val_loss: 14.3820\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 16.4812 - val_loss: 13.7477\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 15.5363 - val_loss: 13.6055\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 15.1210 - val_loss: 13.5485\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.8384 - val_loss: 13.5376\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 14.7746 - val_loss: 13.5344\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.6760 - val_loss: 13.5317\n",
      "Epoch 9/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 134us/step - loss: 14.6388 - val_loss: 13.5292\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.5970 - val_loss: 13.5281\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.5895 - val_loss: 13.5295\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 14.5554 - val_loss: 13.5289\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.5966 - val_loss: 13.5239\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.5681 - val_loss: 13.5176\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.5617 - val_loss: 13.5187\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 14.5268 - val_loss: 13.5165\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.5224 - val_loss: 13.5165\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.5293 - val_loss: 13.5165\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4986 - val_loss: 13.5171\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 14.5334 - val_loss: 13.5167\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 14.5318 - val_loss: 13.5176\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 14.4993 - val_loss: 13.5195\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.4883 - val_loss: 13.5187\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.5015 - val_loss: 13.5181\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.5031 - val_loss: 13.5165\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4649 - val_loss: 13.5165\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.5126 - val_loss: 13.5165\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4765 - val_loss: 13.5165\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.5286 - val_loss: 13.5165\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4686 - val_loss: 13.5165\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4754 - val_loss: 13.5165\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4791 - val_loss: 13.5165\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4849 - val_loss: 13.5165\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4909 - val_loss: 13.5165\n",
      "Epoch 35/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.4872 - val_loss: 13.5165\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4868 - val_loss: 13.5165\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.4879 - val_loss: 13.5165\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 136us/step - loss: 14.5085 - val_loss: 13.5165\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 147us/step - loss: 14.4830 - val_loss: 13.5165\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4634 - val_loss: 13.5165\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4808 - val_loss: 13.5165\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4824 - val_loss: 13.5165\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.4680 - val_loss: 13.5165\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.5001 - val_loss: 13.5165\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 8.339 - 0s 129us/step - loss: 14.4864 - val_loss: 13.5165\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 8.701 - 0s 133us/step - loss: 14.4744 - val_loss: 13.5165\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 14.4934 - val_loss: 13.5165\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4646 - val_loss: 13.5165\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4704 - val_loss: 13.5165\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 14.4709 - val_loss: 13.5165\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4679 - val_loss: 13.5165\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 14.4801 - val_loss: 13.5165\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4552 - val_loss: 13.5165\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 14.4637 - val_loss: 13.5165\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.4611 - val_loss: 13.5165\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4682 - val_loss: 13.5165\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4561 - val_loss: 13.5165\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 14.4487 - val_loss: 13.5165\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4685 - val_loss: 13.5165\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.4620 - val_loss: 13.5165\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4613 - val_loss: 13.5165\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4651 - val_loss: 13.5171\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.4619 - val_loss: 13.5205\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4586 - val_loss: 13.5199\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4815 - val_loss: 13.5173\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4579 - val_loss: 13.5165\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4718 - val_loss: 13.5165\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4608 - val_loss: 13.5165\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4773 - val_loss: 13.5165\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4585 - val_loss: 13.5165\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4619 - val_loss: 13.5165\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.4703 - val_loss: 13.5165\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4532 - val_loss: 13.5165\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4636 - val_loss: 13.5165\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4611 - val_loss: 13.5165\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.4746 - val_loss: 13.5165\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 14.4516 - val_loss: 13.5165\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 165us/step - loss: 14.4638 - val_loss: 13.5165\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 14.4767 - val_loss: 13.5165\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 154us/step - loss: 14.4590 - val_loss: 13.5165\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4584 - val_loss: 13.5165\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.4599 - val_loss: 13.5165\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.4577 - val_loss: 13.5165\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 149us/step - loss: 14.4513 - val_loss: 13.5165\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 14.4619 - val_loss: 13.5165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4617 - val_loss: 13.5180\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4544 - val_loss: 13.5179\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.4448 - val_loss: 13.5183\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4630 - val_loss: 13.5165\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4542 - val_loss: 13.5165\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4567 - val_loss: 13.5181\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4473 - val_loss: 13.5075\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4660 - val_loss: 13.5007\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4361 - val_loss: 13.5165\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4503 - val_loss: 13.5090\n",
      "Epoch 96/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4592 - val_loss: 13.4720\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4329 - val_loss: 13.4639\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4308 - val_loss: 13.4675\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.3995 - val_loss: 13.4251\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 14.4174 - val_loss: 13.3731\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.3545 - val_loss: 13.3941\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.3508 - val_loss: 13.3359\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.3103 - val_loss: 13.3279\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.2802 - val_loss: 13.3228\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.2609 - val_loss: 13.2721\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.2139 - val_loss: 13.2903\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.1983 - val_loss: 13.2771\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.0828 - val_loss: 13.1795\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.0993 - val_loss: 13.0846\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 14.0076 - val_loss: 13.1016\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 14.0304 - val_loss: 13.1182\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 13.9632 - val_loss: 13.0712\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.9006 - val_loss: 13.0824\n",
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.9263 - val_loss: 13.0582\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.7625 - val_loss: 13.1548\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.8256 - val_loss: 13.0120\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.7829 - val_loss: 13.0370\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.8275 - val_loss: 12.9281\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.8204 - val_loss: 12.9648\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.7946 - val_loss: 12.9704\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.7651 - val_loss: 12.9854\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.7233 - val_loss: 12.9734\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.6040 - val_loss: 12.9451\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 13.6852 - val_loss: 12.8959\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.6510 - val_loss: 12.8554\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 211us/step - loss: 13.5999 - val_loss: 12.8074\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.5885 - val_loss: 12.8861\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.4836 - val_loss: 12.8323\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4353 - val_loss: 12.7927\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.6071 - val_loss: 12.7419\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.5183 - val_loss: 12.6590\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4677 - val_loss: 12.6050\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4859 - val_loss: 12.6885\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.5135 - val_loss: 12.6759\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.3991 - val_loss: 12.7643\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4379 - val_loss: 12.7539\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4065 - val_loss: 12.7252\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.3669 - val_loss: 12.7693\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.3049 - val_loss: 12.7138\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.3928 - val_loss: 12.7738\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4343 - val_loss: 12.6631\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.2826 - val_loss: 12.6483\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.3228 - val_loss: 12.5174\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.2871 - val_loss: 12.3914\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.2982 - val_loss: 12.5352\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 13.2078 - val_loss: 12.2640\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.1396 - val_loss: 12.2060\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.2084 - val_loss: 12.2880\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1740 - val_loss: 12.5510\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.1966 - val_loss: 12.2309\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.1939 - val_loss: 12.3188\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.1223 - val_loss: 12.3875\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.3229 - val_loss: 12.6054\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.1322 - val_loss: 12.4197\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.0788 - val_loss: 12.3705\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1723 - val_loss: 12.4310\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.0013 - val_loss: 12.2281\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.0248 - val_loss: 12.4465\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.2175 - val_loss: 12.5020\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.0156 - val_loss: 12.6371\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1961 - val_loss: 12.2693\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.1523 - val_loss: 12.6401\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1433 - val_loss: 12.6641\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 13.0366 - val_loss: 12.6070\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.1367 - val_loss: 12.6366\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.1739 - val_loss: 12.5923\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.0676 - val_loss: 12.5381\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.0624 - val_loss: 12.5364\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.0203 - val_loss: 12.5293\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.9440 - val_loss: 12.6299\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.1113 - val_loss: 12.6174\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.0804 - val_loss: 12.5971\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.9573 - val_loss: 12.5656\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.1154 - val_loss: 12.5035\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.1325 - val_loss: 12.6030\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 145us/step - loss: 13.1203 - val_loss: 12.5818\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.0066 - val_loss: 12.3141\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.9833 - val_loss: 12.4126\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.0624 - val_loss: 12.4718\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 13.0250 - val_loss: 12.5041\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 144us/step - loss: 12.9819 - val_loss: 12.3958\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 151us/step - loss: 12.9655 - val_loss: 12.6193\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 12.9404 - val_loss: 12.5503\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 12.9704 - val_loss: 12.6115\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.0110 - val_loss: 12.5220\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 20.68 - 0s 129us/step - loss: 12.9959 - val_loss: 12.2808\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 12.9206 - val_loss: 12.1808\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.0088 - val_loss: 12.2347\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.0576 - val_loss: 12.1633\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.9613 - val_loss: 12.1796\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9055 - val_loss: 12.2568\n",
      "Epoch 192/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 12.9760 - val_loss: 12.3352\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 12.9435 - val_loss: 12.2705\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 105us/step - loss: 12.9510 - val_loss: 12.1830\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.9667 - val_loss: 12.2900\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9616 - val_loss: 12.3646\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 12.9087 - val_loss: 12.4651\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9797 - val_loss: 12.4514\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.0087 - val_loss: 12.4321\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 12.9954 - val_loss: 12.2824\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.8830 - val_loss: 12.4137\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 12.9623 - val_loss: 12.5426\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.8850 - val_loss: 12.6690\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.8357 - val_loss: 12.6236\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9581 - val_loss: 12.4853\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.9478 - val_loss: 12.2952\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 12.9523 - val_loss: 12.4060\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 12.8466 - val_loss: 12.3185\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.9324 - val_loss: 12.3965\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 12.8792 - val_loss: 12.6624\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9159 - val_loss: 12.4546\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 12.9178 - val_loss: 12.4683\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.8867 - val_loss: 12.4595\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8817 - val_loss: 12.3445\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.8515 - val_loss: 12.4234\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.9086 - val_loss: 12.3110\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8702 - val_loss: 12.3821\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 12.9218 - val_loss: 12.2241\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7628 - val_loss: 12.2175\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.9640 - val_loss: 12.3532\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8357 - val_loss: 12.4899\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 12.8780 - val_loss: 12.3274\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 12.7763 - val_loss: 12.2431\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8570 - val_loss: 12.4958\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 12.7729 - val_loss: 12.3801\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 12.7953 - val_loss: 12.5323\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 12.7385 - val_loss: 12.4820\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.8622 - val_loss: 12.6100\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.7570 - val_loss: 12.4748\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8744 - val_loss: 12.4058\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 12.7637 - val_loss: 12.5549\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7855 - val_loss: 12.4922\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 12.7410 - val_loss: 12.4675\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.8480 - val_loss: 12.3852\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.7787 - val_loss: 12.2245\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.7565 - val_loss: 12.2052\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7496 - val_loss: 12.2602\n",
      "Epoch 238/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 131us/step - loss: 12.7644 - val_loss: 12.3515\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 12.8034 - val_loss: 12.4435\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.6870 - val_loss: 12.4177\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.7014 - val_loss: 12.5220\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 12.6575 - val_loss: 12.5753\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.7996 - val_loss: 12.5444\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 10.83 - 0s 128us/step - loss: 12.7116 - val_loss: 12.3564\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 12.6985 - val_loss: 12.5121\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.6630 - val_loss: 12.5820\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.6963 - val_loss: 12.7727\n",
      "Epoch 248/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 12.7422 - val_loss: 12.8242\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 12.7765 - val_loss: 12.7122\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7599 - val_loss: 12.5652\n"
     ]
    }
   ],
   "source": [
    "#pred_appliance = {}\n",
    "sequence_length=24\n",
    "num_iterations_dictionary = {'hvac':400,'fridge':500,'mw':250,'dw':250,'oven':250, 'wm':300}\n",
    "for appliance in APPLIANCES_ORDER[2:]:\n",
    "\n",
    "\n",
    "    print(appliance)\n",
    "    print(\"*\"*20)\n",
    "    np.random.seed(0)\n",
    "    from keras.layers.merge import Subtract, Minimum\n",
    "    model = Sequential()\n",
    "    filters=20\n",
    "    kernel_size=2\n",
    "    model.add(InputLayer(input_shape=(sequence_length,1)))\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ,name='C1'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Conv1D(filters=20,\n",
    "                     kernel_size=5,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Conv1D(filters=25,\n",
    "                     kernel_size=3,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Conv1D(filters=30,\n",
    "                     kernel_size=2,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(sequence_length, activation='relu'))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "    model.compile('adam','mean_absolute_error')\n",
    "    model.fit(train_agg.reshape(-1, 24, 1), train_appliance[appliance], epochs=num_iterations_dictionary[appliance], validation_split=0.1)\n",
    "    pred_appliance[appliance] = model.predict(test_agg.reshape(-1,24,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('per-appliance.pdf','wb') as f:\n",
    "    f.write(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='pdf'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 24, 1)             0         \n",
      "_________________________________________________________________\n",
      "C1 (Conv1D)                  (None, 24, 20)            220       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 12, 20)            0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 12, 20)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 12, 20)            2020      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 6, 20)             0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 6, 20)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 6, 25)             1525      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 3, 25)             0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 3, 25)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 3, 30)             1530      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1, 30)             0         \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 1, 30)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 24)                744       \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 6,039\n",
      "Trainable params: 6,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = {}\n",
    "for appliance in APPLIANCES_ORDER[1:]:\n",
    "    try:\n",
    "        mae[appliance] = mean_absolute_error(test_appliance[appliance], pred_appliance[appliance])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dw         14.499116\n",
       "fridge     34.620154\n",
       "hvac      331.035001\n",
       "mw          6.300214\n",
       "oven       18.633957\n",
       "wm          5.617521\n",
       "dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dw         14.499116\n",
       "fridge     34.620154\n",
       "hvac      331.035001\n",
       "mw          6.300214\n",
       "oven       18.633957\n",
       "wm          5.617521\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a32620588>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAA8CAYAAAB2H0HmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCFDhi2CLKBnyJAhwxbB/wG6pPMOssjkuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a259777f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_agg.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a32611080>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAEFCAYAAADdWD2lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN4hR0mAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a25bf0e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(test_appliance['dw'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a3699e668>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd8VFX6+PHPmZn03gghhQChhQARQhNQRFSssNjQVbCX\nVdd1/X13dddVd1d31W1f185iwRVEvlYs2CgC0gQM0hISWiopkEoySSZzfn/cGRhCIMn0zJz365XX\nTO7cuXNmCPPcc895ziOklCiKoij+SefpBiiKoiieo4KAoiiKH1NBQFEUxY+pIKAoiuLHVBBQFEXx\nYyoIKIqi+DEVBBRFUfyYCgKKoih+TAUBRVEUP2bwdAO6Eh8fL9PT0z3dDEVRlF5l27Zt1VLKhK72\n8/ogkJ6eztatWz3dDEVRlF5FCHG4O/upy0GKoih+TAUBRVEUP6aCgKIoih/z+jEBRVEUe7S1tVFS\nUoLRaPR0U1wqODiYlJQUAgIC7Hq+CgKKovikkpISIiIiSE9PRwjh6ea4hJSSo0ePUlJSwoABA+w6\nhrocpCiKTzIajcTFxflsAAAQQhAXF+dQb0cFAUVRfJYvBwArR9+jCgKK2xRWNvLV7iOeboaiKDZU\nEFDc5uXVhdz9322s3Fvh6aYoiltUVFRw4403MnDgQMaOHcukSZN47733yM7OJjs7m/DwcIYOHUp2\ndjbz5s3zSBtVEFDcpqS2GYCH3sul+FiTh1ujKK4lpWT27Nmcd955HDhwgG3btrF06VIqKyvJzc0l\nNzeXnJwcFi9eTG5uLm+//bZH2qmCgOI2ZbXNjEuPQQK/WLydFlO7p5ukKC6zatUqAgMDueeee05s\n69+/Pw888IAHW3U6NUVUcYt2s+RInZFZ2f24c+pA7vrvNv782R6emj3S001T/MAfP93NnrJ6px4z\ns18kT1w54oyP7969mzFjxjj1NV2hy56AEOINIUSlEGKXzba/CSHyhBA/CSE+EkJE2zz2qBCiUAiR\nL4S4xGb7WCHETstj/xb+MGyvnFDZYMRklvSLDuHiEX2567yBvLOpiE9ySz3dNEVxi/vuu4/Ro0cz\nbtw4TzflFN3pCbwFvAjYXrD6BnhUSmkSQjwLPAr8VgiRCcwFRgD9gG+FEEOklO3AK8CdwGbgC2Am\nsMJZb0TxbmWW8YB+0SEA/M8lQ/mxqIZHP9xJZlIkgxMjPNm8Xk9Kye6yerKSozzdFK90tjN2Vxkx\nYgQffPDBid9feuklqqurycnJcXtbzqbLnoCUci1wrMO2r6WUJsuvm4AUy/1ZwFIpZYuU8iBQCIwX\nQiQBkVLKTVJKiRZQZjvrTSjer7RWS2ZJtgSBAL2OF28cQ2ignnsXb+d4i+lsT1e6sOXgMa54YT2r\n8tTMK28xffp0jEYjr7zyyoltTU3eNyHCGQPDt3HyjD4ZKLZ5rMSyLdlyv+N2xU907AkAJEYG8/zc\nc9hf1cjvP9qJdn6g2GN/1XEAPtyuLq95CyEEH3/8Md999x0DBgxg/PjxzJ8/n2effdbTTTuFQwPD\nQojfAyZgsXOac+K4dwF3AaSlpTnz0IqHlNU2ExUSQHjQqX9ykzPi+fWMIfzjm33kpMdy08T+Hmph\n71ZSo51hfru3gsYW02mfs+IZSUlJLF269IyPr1mzxn2NOQO7ewJCiFuAK4Cfy5OncKVAqs1uKZZt\npZy8ZGS7vVNSygVSyhwpZU5CQpfV0ZReoLSm+ZRegK37Lshg2tAE/vTpHnaW1Lm5Zb6hpKYZg05g\nbDPztcrKVnrAriAghJgJ/Aa4Skppe5FrOTBXCBEkhBgADAa2SCnLgXohxETLrKB5wCcOtl3pRUpr\nm0mODu70MZ1O8K/rsokPD+Texduoa2pzc+t6v5KaJnLSY0iJCeGT3DJPN0fpRbozRfRdYCMwVAhR\nIoS4HW22UATwjRAiVwjxKoCUcjewDNgDfAncZ5kZBPALYCHaYPF+1Mwgv1JWe+aeAEBMWCAv/nwM\nFfVGHv6/XMxmNT7QEyU1zaTGhHLV6H6sL6ymurHF001SeonuzA66QUqZJKUMkFKmSClfl1JmSClT\npZTZlp97bPZ/Wko5SEo5VEq5wmb7VillluWx+6UaBfQbDcY26o2mswYBgDFpMfzusuF8u7eSBesO\nuKl1vZ+xrZ3KhhZSYkKZfU4y7WbJ5z+Ve7pZSi+hlo1QXK68Tpse2lUQALjl3HQuH5nE377KZ/OB\no65umk+wfr4pMSEMSYxgWN8IlYSndJsKAorLldZo00OTuxEEhBA8c/VI+seGcv+7P1LZ4NulAZ3B\nOjMoJUb7fGdlJ7O9qJaio943J13xPioIKC5XWtv9IAAQERzAyzeNocHYxoPv5mJqN7uyeb1eiSXI\npsSGAnBVdj8Alu9QvQFP0+v1ZGdnk5WVxbXXXutQstiaNWu44oornNg6jQoCisuV1WrTFxMigrr9\nnGF9I3lq9kg2HjjKv77d58LW9X4lNU0YdIJEy+ebHB3C+PRYPs4tUwl4HhYSEkJubi67du0iMDCQ\nV1999ZTHpZSYzZ49yVFBQHG5stpm+kYFo9f1bM3Aa8amMHdcKi+t3q+WQziLkhrt8zXoT/53viq7\nH4WVjewpd+7KmYr9pk6dSmFhIYcOHWLo0KHMmzePrKwsiouL+frrr5k0aRJjxozh2muvpbGxEYAv\nv/ySYcOGMWbMGD788EOXtEulFSouV1Zr7NagcGeevGoEP5XU8dB7O/jiwandvqTkT0pqmk+MB1hd\nPjKJJ5fvZnluGSP6qUXlWPEIHNnp3GP2HQmXPtOtXU0mEytWrGDmzJkAFBQUsGjRIiZOnEh1dTVP\nPfUU3377LWFhYTz77LP885//5De/+Q133nknq1atIiMjg+uvv9657bdQPQHF5Uprm0mx88s7OEDP\nyz8fQ11zGx9sK+n6CX6opKaJlJjQU7bFhAVy/pAElu8oUzkXHtTc3Ex2djY5OTmkpaVx++23A1px\nmYkTJwKwadMm9uzZw+TJk8nOzmbRokUcPnyYvLw8BgwYwODBgxFCcNNNN7mkjaonoLiUqd3MkXr7\newIA6fFh9I0M5rCa7XKaFlM7FfUtp/UEQLsktDKvki2HjjFxYJwHWudFunnG7mzWMYGOwsLCTtyX\nUnLRRRfx7rvvnrJPZ89zBdUTUFyqsqGFdksxGUekxYaqusSdKKu15giEnvbYRZmJhAbq1TISXm7i\nxIl8//33FBYWAnD8+HH27dvHsGHDOHToEPv37wc4LUg4iwoCikudXEK683WDuis1NpQiFQRO0zFH\nwFZooIGLMxP5Ymc5rSY1zdZbJSQk8NZbb3HDDTcwatQoJk2aRF5eHsHBwSxYsIDLL7+cMWPG0KdP\nH5e8vrocpLhUT3MEziQtNpQPG4wY29oJDtA7o2k+wZqI11kQAC1x7OPcMr7bV8VFmYnubJoCJ2b5\n2EpPT2fXrl2nbJs+fTo//PDDafvOnDmTvLw8l7UPVE9AcTHr5QqHLwfFhSDlyaCiaEpqmtHrBH0j\nO+9pTRkcT2xYoFpGQjkjFQQUlyqtbSI6NIAwB4ucpFmyYdUloVOV1DSR1CFHwFaAXsflI5NOFJtR\nlI5UEFBcqqzWSL8ox+f2p1qCgBocPlVnOQIdzcru57fFZvwhY9rR96iCgOJSXdUR6K6E8CCCA3Rq\nUbQOtCBw+swgW2P7+2exmeDgYI4ePerTgUBKydGjRwkOtn/ihRoYVlyqtLaZCQNiHT6OEII0NUPo\nFC2mdioajF0OugshuGp0P15be4Dqxhbiw7u/hlNvlpKSQklJCVVVVZ5uiksFBweTkpLS9Y5noIKA\n4jL1xjYaulFMprtSY1QQsFVea0TKM88MsjUrO5mX1+zn85/KmX9uuusb5wUCAgIYMGCAp5vh9dTl\nIMVlrDkCyd34kuqOVEvCmC9373vixBLSXVwOAhjaVxWbUTqngoDiMicTxZwTBNJiQzne2s6x461O\nOV5vd7ZEsc6oYjNKZ1QQUFym1JIj4KyVP9U00VNZcwSSoro3KHjl6CRAFZtRTqWCgOIyZbXNBOgF\nCU4aiEyLs0wTrVEJY6ANuveNPHOOQEcpMaGq2IxyGhUEFJexFpPR9bCYzJmkxqhcAVvaEtI962Wp\nYjNKRyoIKC5TWtPs1CIwIYF6EiKC1DVti+7kCHR0+cgkDDrBcj/LGVDOrMsgIIR4QwhRKYTYZbMt\nVgjxjRCiwHIbY/PYo0KIQiFEvhDiEpvtY4UQOy2P/VsI4ZzTQ8VrOStRzJbKFdC0mrQ6DT3tCahi\nM0pH3ekJvAXM7LDtEWCllHIwsNLyO0KITGAuMMLynJeFENYlH18B7gQGW346HlPxIdZiMs4uB6mC\ngKa8rrnbOQIdXZXdj/I6I1sOHXNBy5TepssgIKVcC3T8a5kFLLLcXwTMttm+VErZIqU8CBQC44UQ\nSUCklHKT1Eak3rZ5juKDKhpaMEvnTQ+1So0Npbyu2e/Xx7fmCNiTg6GKzSi27B0TSJRSllvuHwGs\nC5UnA8U2+5VYtiVb7nfcrvgoZ+cIWKXGhGCWJ4/vr6w5Aqk9HBMAVWxGOZXDA8OWM3unXlwUQtwl\nhNgqhNjq6+t++CprsRNXXA4ClStQUtOMTkDfbuYIdDQrO5m65ja+26f+f/k7e4NAheUSD5bbSsv2\nUiDVZr8Uy7ZSy/2O2zslpVwgpcyRUuYkJCTY2UTFk0qdVFayI2uugAoCzSRFhRDQzRyBjlSxGcXK\n3iCwHJhvuT8f+MRm+1whRJAQYgDaAPAWy6WjeiHERMusoHk2z1F8UFltMzGhAYQGOneNwsSIYAL1\nOopr/DsIlNY0O7Qmkyo2o1h1Z4rou8BGYKgQokQIcTvwDHCREKIAmGH5HSnlbmAZsAf4ErhPStlu\nOdQvgIVog8X7gRVOfi+KF3HF9FAAnU6QEhvi9wlj9iSKdeTPxWaUk7o8TZNS3nCGhy48w/5PA093\nsn0rkNWj1im9Vlmt8cSlG2fz92miJ3MEHPt8bYvNzBlj/3r0Su+mMoYVlyirdW62sK202FC/zho+\nUmfEbGeOgC1rsZn1hdVUN7Y4qXVKb6OCgOJ0dc1tNLSYXBoE6o0m6praXHJ8b9fTJaTPZlZ2Mu1m\nyec/lXe9s+KTVBBQnM5VOQJWqX4+TdSaKGZPjkBH1mIzn+5QiWP+SgUBxenKXDQ91MrfcwVKapoc\nyhHoaMbwRH4srqXe6J89K3+ngoDidCfKSqqegEuU1Gh1BOzNEehockY87WbJ5gNqLSF/pIKA4nSl\ntUYC9TrinVRMpqPwIAOxYYF+HQQcnRlka0z/aIIDdHxfWO20Yyq9hwoCitOV1jaTFO28YjKdsRad\n90fOyBGwFWTQM35AHOtVEPBLKggoTldW20y/KNdcCrJKiw31y6zhtnb76gh0ZUpGHIWVjRypMzr1\nuIr3U0FAcTpXZQvbSosNobSmGVO7f62CeTJHwLmJeFMytDW61CUh/6OCgOJUbe1mKuqNJLtoZpBV\nWmwoJrOk3M/OXIudmCNga1jfCOLCAlUQ8EMqCChOVVFvdEkxmY6sM4T8bVzAmiPg7J6ATic4NyOe\n9YXVaKvDK/5CBQHFqUodqHjVE/6aK+BoHYGzmZIRR2VDC4WVjU4/tuK9VBBQnKqszrXZwlZJUSEY\ndMIPg0ATfSODCTQ4/7/u5Ix4ADVLyM+oIKA4VVmtdo3e1bOD9DpBSkyIHwYB5+YI2EqJCSU9LpT1\nBSoI+BMVBBSnKq1tJjYskJBAvctfyx9zBRwtJtOVyRnxbDpwlDY/m3Xlz1QQUJxKmx7q2plBVql+\nVlegrd1MeV2z02cG2ZqSEc/x1nZ2FNe67DUU76KCgOJUrqwj0FFabCg1TW00+MnCZ86qI3A2kwbF\nIYQaF/AnKggoTiOlpLTG9YliVmknpok2u+X1PM1V00NtRYcGMjI5SuUL+BEVBBSnqW82cby13a09\nAfCfaaLOLCZzNlMy4vmxqFYVoPcTKggoTlPq4mIyHflbwlhJTTNCaNNjXWlKRjwms2TLwaMufR3F\nO6ggoDiNqyuKdRQVEkBUSIAf9QSaXZYjYGtM/xiCDDrWF6gg4A9UEFCc5mSimHtmB4Gl6LzfBAHn\nLiF9JsEBesYPiFXjAn5CBQHFaUprmgk06IgPc00xmc6k+VGugCsTxTqanBFPfkUDlfX+tUCfP3Io\nCAghHhJC7BZC7BJCvCuECBZCxAohvhFCFFhuY2z2f1QIUSiEyBdCXOJ48xVvUlrbTL8o1xaT6Sg1\nNpSSmmbazb696JnJRXUEzmSKZQmJ7/er3oCvszsICCGSgV8COVLKLEAPzAUeAVZKKQcDKy2/I4TI\ntDw+ApgJvCyEcH1aqeI27qgj0FFabCitluWrfVl5nZF2s3RbEMhMiiQ6NECNC/gBRy8HGYAQIYQB\nCAXKgFnAIsvji4DZlvuzgKVSyhYp5UGgEBjv4OsrXqSs1uj2IJAaq72er48LWHMEkqPdczlIpxNM\nHhTP92ppaZ9ndxCQUpYCfweKgHKgTkr5NZAopSy37HYESLTcTwaKbQ5RYtl2GiHEXUKIrUKIrVVV\nVfY2UXGjtnYzFQ3uDwJpfjJN1F05ArYmZ8RzpN7I/qrjbntNxf0cuRwUg3Z2PwDoB4QJIW6y3Udq\npxA9Po2QUi6QUuZIKXMSEhLsbaLiRkfqjEgJKW4OAv2iQ9AJ3w8CpbWWHAE3zrw6MS6gZgn5NEcu\nB80ADkopq6SUbcCHwLlAhRAiCcByW2nZvxRItXl+imWb4gPcnShmFaDX0S/a95eULqlpJjEimCCD\n+4bR0uJCSY0NUesI+ThHgkARMFEIESqEEMCFwF5gOTDfss984BPL/eXAXCFEkBBiADAY2OLA6yte\n5GSimPvOVK38IVfAXTkCHU3JSGDT/qOY1NLSPsuRMYHNwPvAdmCn5VgLgGeAi4QQBWi9hWcs++8G\nlgF7gC+B+6SU7Q61XvEa7s4WtqUFAd9eRE7LEfBEEIinocXET6V1bn9txT0MjjxZSvkE8ESHzS1o\nvYLO9n8aeNqR11S8U2mtkbiwQIID3D/rNzU2lOrGFppaTYQGOvQn7ZVM7WbK64xuSxSzZV1a+vuC\nasakxXT9BKXXURnDilOU1bq24tXZ+PqS0kfq3ZsjYCs2LJAR/SJZp8YFfJYKAopTaNnCng0Cvjou\n4I46AmczOSOeH4tqOK6WlvZJKggoDpNSeiRb2Mp/goBnPt8pGfG0tUu2HDrmkddXXEsFAcVhdc1t\nNLW2e2RmEEB0aADhQQafzRUoqWlye46ArXHpsQQadHxfoC4J+SIVBBSHWXME3FVRrCMhBKk+vJpo\nSU0zfSKC3JojYCs4QE9O/xiVL+CjVBBQHFZWqy3e5qnLQQBpsb6bMFbqxiWkz2RyRjx5Rxqoamjx\naDsU51NBQHFYqWVdG0/NDoKTCWO+uNhZSa1nEsVsWZeQ2KCWlvY5KggoDiurMxJo0BEXFuixNqTF\nhtJiMvvcmaqp3Ux5rfvqCJxJVnIUUSEBah0hH6SCgOKw0tpmkqND0FYP8YxUH50hVNHQgsksPX45\nSK8TnDsojvUFamlpX6OCgOIwbXqoZ2auWPnqNNGSY+5fQvpMJmfEU1Zn5GC1Wlral6ggoDiszIOJ\nYlbJMSEI4YNBwMOJYrbU0tK+SQUBxSGtJjOVDS0eHRQGCDLoSYoM9tkg4OmeFkD/uFCSo9XS0r5G\nBQHFIdZiMp6cHmrli7kCJTVNJEZ6LkfAlhCCKRnxbNh/lHazGhfwFSoIKA7xdKKYLS0I+NYiciVe\nkCNga/LgeBqMJnaqpaV9hgoCikM8WUego7TYUI7UGzG2+U6ZipLaJq8IsFbnDooD1LiAL1FBQHGI\nNQgkRXn+mrV1hpD1Onpv126WXpEjYCs+PIjhSZGsV+sI+QwVBBSHlNY2Ex8e5JFiMh2lnqgr4Bvj\nAhX1Rq/IEehoSkYc2w7X0NzqOz0uf6aCgOIQLVHM870A8L1cAU8vIX0mUwYn0Npu5ge1tLRPUEFA\ncYgn6wh0FB8eSEiA3oeCgPckitkalx5DoF6nxgV8hAoCit20YjJGrwkCQogTC8n5gpM5At7x+VqF\nBhoY0z+adWpcwCeoIKDYrbapjea2dq/6kvKlXIGSmib6RHjHeEtHUzLi2VNez9FG31qwzx+pIKDY\n7WSOgHeMCYBvLSmt5Qh4T4C1NfnE0tJHPdwSxVEOBQEhRLQQ4n0hRJ4QYq8QYpIQIlYI8Y0QosBy\nG2Oz/6NCiEIhRL4Q4hLHm6940skg4D2zV9JiQ2hqbefY8VZPN8Vh3pYoZmtkchQRwQY1LuADHO0J\nPA98KaUcBowG9gKPACullIOBlZbfEUJkAnOBEcBM4GUhhPf1c5VuO5ko5j09AV9ZUrrdLCmr9d6e\ngEGvY9LAONappaV7PbuDgBAiCjgPeB1AStkqpawFZgGLLLstAmZb7s8ClkopW6SUB4FCYLy9r694\nXlltM0EGHbEeLCbTka9ME61s8M4cAVtTBsdTWtvc6z9rf+dIT2AAUAW8KYT4UQixUAgRBiRKKcst\n+xwBEi33k4Fim+eXWLYpvVRZrdHjxWQ6sn5p9vbBYevMILesztpmhJbGHj/t/CEJCAF//HQPbe1m\nFzRMcQdHgoABGAO8IqU8BziO5dKPldT6iT3uKwoh7hJCbBVCbK2qqnKgiYorlXpRjoBVSKCePhFB\nvf7s1G05Ao1V8Np58MIYKMvt0VP7x4Xxp1lZrMqr5KH3ctXKor2UwYHnlgAlUsrNlt/fRwsCFUKI\nJClluRAiCai0PF4KpNo8P8Wy7TRSygXAAoCcnBz1l+WlSmubmT60j6ebcRpfyBUoOeaG1VmbjsHb\ns6C2CEJj4c3L4Lq3YfCMbh/i5on9Od5i4pkVeYQHGfjrnJFe1TO09f62Ep5ZsZdAvY6I4ADCgw2E\nBxkIDzYQab0fFEBEsLYtwvJYRLC2bWB8mNe+N0fYHQSklEeEEMVCiKFSynzgQmCP5Wc+8Izl9hPL\nU5YDS4QQ/wT6AYOBLY40XvGcFlM7VQ0tXtcTAC0IbD7Yu5c0KKlpJsGVOQLNtfDfn8HRQrjxPUgY\nBkuuhSXXwZX/C2PmdftQ95w/iEajiRdXFxIeZOD3lw/3ui/L4mNN/OHjXQzqE8awvpE0Gk00tpio\nbWqluKaJRqOJBqOJ5rOsQHvb5AE8fmWmG1vtHo70BAAeABYLIQKBA8CtaJeYlgkhbgcOA9cBSCl3\nCyGWoQUJE3CflFKtQNVLHakzAt41M8gqNTaUj3JLaTWZCTT0zlSYktom110KammAxddCxW6YuwQG\nXaBtv3UFLJsPyx+AuhKY9ih088v84YuH0NhiYuH6g0QEB/DgjMGuabsdpJT89oOf0OsEC27OOeuJ\ni6ndzPGWduqNbTS2aIGiwdjG+9tKeGfTYe4+fyCJkd73N+8Ih4KAlDIXyOnkoQvPsP/TwNOOvKbi\nHbypmExHabGhSKm1cUB8mKebY5eSmmZGpUQ7/8CtTbDkeijdBtctgiEXn3wsKELrFXz2K/juWS0Q\nXPk86AO6PKwQgsevyKTBaOJf3+4jLEjPHVMHOr/9dnh3SzEb9h/lLz8b2WXP1aDXERWqIyr01Pc8\nuE8EX+2u4PX1B/ndZcNd2Vy3652nSYrHldVaewJeGATievc0UZflCLQZYemNULQR5iyA4Veevo8+\nAK56UesF5C7WegzG+m4dXqcTPHv1SC7N6stTn+/lvR+KnNt+O5TVNvOXL/Zy7qA4bhif2vUTziA1\nNpQrRiWxeNNh6pranNhCz1NBQLFLqWUKY5IXXg5K6+V1BSobjLS1S+cGAVMrLJsHB1bDrJdg5DVn\n3lcImPaItt+hddqAcX1Zt17GoNfxv3OzOW9IAo98uJNPd3Tvea4gpeR3H+2k3Sx59upRDo9T3Dtt\nEMdb23l74yGntM9bqCCg2KWsVhu49IYC6B0lhAcRaND12iBwso6AkxLF2k3wwW1Q8BVc8S/IvrF7\nzzvnJrhxGdQchIUXQeXebj0tyKDntZvGMq5/LA+9l8uqvAoHGm+/D7eXsia/it/OHHoik9wRw/pG\nMn1YH97ccMinCuqoIKDYpazO+3IErHQ6QWpMSK+9HFTqzGIy5nb46G7Y+ynMfAZybuvZ8zMu1AaM\nzSZ4/RI4uK5bTwsJ1LPwlhyGJ0Vy7zvb2ejmheYq64388dPd5PSPYd6kdKcd995pgzh2vJVlW4u7\n3rmXUEFAsYs3VRTrTG/OFbAmijk86G42w/Jfwq734cInYOK99h0naRTc8S1EJsE7c2Dn+916WmRw\nAItuG09abCh3LPqB3OJa+16/h6SU/P7jXbSYzDx3zSh0OudNVx2XHsu49BgWrD3gM1nSKggoPaYV\nk2mmX5R39gTAEgSO9s4lpUtqnFC3WUr44v9B7jtw/iMw9deONSo6FW77ElLGwwe3w/p/aa/Rhdiw\nQN65YwJx4UHMf2MLeUe6N8jsiM9+KuebPRX8+qIhDEwId/rx7502iNLaZo+OdziTCgJKj9U0tWFs\nM3vt5SDQZnM0tJioa+59MzkcriMgJXz1e9j6Okx+UBvkdYaQGLj5Q8i6Gr59Ej5/WLvc1IXEyGAW\n3zGB4AAdNy3cwsHq485pTyeONrbwxPLdjE6NdtkU1QuG9mFY3wheWbMfsw8slaGCgNJjpe5c3MxO\nvXk10ZIaBxLFpISVf4JNL8GEe2DGH7ud8NUthiCYs1ALLltfh6U/79YU0tTYUBbfMQGzlNy0cPOJ\nZcid7clP99BgbONv14xC78TLQLaEENw7bRAFlY2szKvs+gleTgUBpce8OVHMqrfmCpjNktJaB4rJ\nrP0brP8njL1FGwh2xfINOh1c9Ce47O/ajKMXc2DHe11eHsroE8Hbt42nvrmNmxZuptrJpSm/2n2E\nT3eU8cvpgxmSGOHUY3d0+cgkUmNDeHlNYa+85GhLBQGlx04Wk/HeIJAa0zuDQGVDi/05At8/D6uf\nhtE3wOX/ck0AsDX+TsuAcTJ8dJeWT3Bk11mfkpUcxZu3jqO8zsh1r21k2+EapzSltqmVxz7eRWZS\nJPdMG+T0m2YwAAAgAElEQVSUY56NQa/jrvMG8WNRba9fp0oFAaXHymqbCQ7QERPa9XICnhIWZCA+\nPJDiY6657OAqdi0hXXMY/u8W+OZxGDFHy/jVuem/dvJYuGOltrxEVZ62LPWK32oL1J1BTnosb9wy\njubWdq55dQOPfbzT4bGbP3+2l5rjrTx3zSgC9O5579eOTSE+PJBX1ux3y+u5igoCSo9ZcwS8baXI\njlJjQ3tdwliPEsVaGmHVU/DiOMj/UlvqYc4C0Du6LmQP6XTa5acHtmm3m1/TLhH9uFibptqJSYPi\n+ObX53PLueks2VzEjH9+x+c/ldt1aWV1fiUfbC/h3mmDyEqOcuy99EBwgJ5bJw/gu31V7C6rc9vr\nOpsKAkqPldY0e/V4gFVqTO/LFehWT8Bshh1LtS/atX+DzKvgga3aLKBuLPbmMqGxcMU/4a41EJMO\nn/wC3rgEynd0unt4kIEnrhzBx/dNpk9EEPct2c7ti7b2KHDXG9v43Yc7GdwnnPunZzjlbfTEzZP6\nExFk6NW9ARUElB4rtZSV9HZpsaGU1jZj6kVJPaW1XeQIFP8Ar8/QsoAj+sJtX8PVCyEqxb0NPZt+\n2Vq7Zr0Exw7AgmnadNKmzq+dj0qJ5pP7JvPY5cPZdOAoF/9rLQvW7u/Wv9tfv8ijot7Ic9eM8sgS\nJpHBAfx8Yn++2FnOIRdOfXUlFQSUHjG2tVPd6J3FZDpKiw2l3Swpt9Q+6A3OmCNQVwof3KkFgLpS\nmP0K3LEK0ia4v5HdodNpaw89sA3G3Qlb39B6LtsWdXqJyKDXccfUgXz90HmcOyiOv3yRx1Uvfn/W\nLOPvC6t5d0sRd0wdyDlpMa58N2d125R0DHodr6094LE2OEIFAaVHrNese0MQSO1luQJms2RPWT0D\nbWsgtDbBmme1L9A9n8DUh7Uv1uwb3Tf464iQaLjsObh7LcQPgU9/qQWy0u2d7p4SE8rC+Tm88vMx\nVDe28LOXv+fJ5btpMJ46cHy8xcQjH/7EgPgwfn3REHe8kzPqExHMtWNT+GBbCZX1veeEw8rNI0hK\nb7fpgLYQ2DlpLih44mS2uQKT3fGCLQ3aSpsVu7SCLBFJEJ0GUanabdDZlzDYWVrH0eOtnDckQZtz\nv+sD+OYJqC+BzFna3PyYdHe8E+frO1JbiO6nZfDNH+A/07X3lDZJu3yUmHXi8xFCcOnIJCYPjucf\nX+WzaOMhvtx1hCevGsHMrL4A/O2rfEpqmll29yTXleDsgbvOG8i7W4p4/fuDPHpp7yo6o4KA0iPr\nC6pJigo+9WzVS/WNDCZAL5zfEzCbteWVK3ZpJRordmv3aw7Z7CSADjNdQmK1NXii0yC6/8ngEJ0G\n0amszq9ECLggshTeuA2KN2lfnnNeg/Qpzn0PniAEjL4ehl6qVS776T3Y87H1Qa2n0C8bkkZDUjaR\nfUfyx1lZzD4nmUc/3Mk972zjosxErhrdj0UbDzF/Ujrj0mM9+Y5O6B8XxuWj+rF4UxG/mJZBVIj3\nTp/uSAUBpdvazZIN+6u5ZERfr58eCqDXCVIcnSHUXAMVe05+0Vfs0s722yzHFDqIy4B+52jXwBOz\nIHGElkB1vApqi6H2MNQWQV2xdlu1Dwq+BdOpOQx3iHDmhMYR9d8iCIuHK/+tHVPn+TNdpwqOhEue\nhoufgoZybfZQWa52e3CtFhys4jI4Jymbz3NG8UV1In/a1sw3eypIiQnhfy4Z6rn30Il7zx/EpzvK\neGfTYe67wP0zleylgoDSbTtL66g3mpgyON7TTem2U3IFTC3aDJXmY9qXu/X+KdtqTm5rOgpN1ScP\nFhKjfcmPma990SeOgD7DIeAM4yMRfbWf1HGnPyYlHK+GuiKoLeJ4xQE+XL2RKfFGLeFrykMQ7L45\n7x4hBET2036GXnpye0OFFhDKLYGhaBP6Xe9zJXClHo6GJNM86nbCAqZ5quWdykyKYPYgQf66D2gT\negKqLCcOdaUQNxD6ZNr8DNdmdHnByZQKAkq3rS+oAmByRi8JAlIy17Sc0dVL4ekmaDvLFD5DsHa5\nJiRGm++eMFS7jRlw8uw+oq/z/tMKAeEJ2k/yWL5uLeHxtmEsv3oyuKLAfG8SkQgRF8OQi09uO159\nIijEFa6CzX+CMss01HgPnHWbWqF6n/Ylf2Sn9lOxi/9tshTPWYN2mS9xJAy8AI7th0PrT+3lBEVq\nwaDPcOhjOaHokwlhcW59KyoIKN22rqCazKRI4sODPN2Urpla4NNfcVnZEja0ZxIzdiqhUQmnftGH\nxJ68DXRSKUc7rc6rIj48kKx+Pn72b6+weMiYof1M+bWWLPflb+HVyTD9MZj4C9deNivdDkUbtbWR\njuzUlsgwW2Ys6YMgMROGXoZMzOLxzYItTf347IHLT1/CorkGKvOgco/lZy/s/hi2vWXzXvtox+uT\nCVnXQMpY170vVBBQuul4i4ntRTXcNnmAp5vStcYqeO/nULyZstEP8vPN4/jPoPHMyEz0dMs61W6W\nrC2o4sJhiU6tguWzhIDsG2DgNPj81/D1Y9r02VkvQ4ITp4tKqZ29f/csHLKU1QxP1HqGGRdqg/aJ\nWdqYkGWpDgGcH1nBf9/eymc/lfGzczok8YXEQP9J2o/t6zQcORkUrAFi65tafsWN72nv1UUcDgJC\nCD2wFSiVUl4hhIgF3gPSgUPAdVLKGsu+jwK3A+3AL6WUXzn6+op7bDl4jLZ26f3jAUd2wrs3aJcP\nrnmTmCGz0P3wFTtKar02COQW11Lb1MYFwxI83ZTeJTIJ5i7Ryl2u+B94dQpc8ChMesCx9ZOkhAOr\n4bvntLP/8ES4+GkYdR2E9+ny6dOH9WFoolZ0Ztbo5K4DuxDae4lM0oKLVWMVLLoSlsy1BILz7X9P\nZ+GMbJMHgb02vz8CrJRSDgZWWn5HCJEJzAVGADOBly0BROkF1hVUE2jQec2UvE7t/Uwrhm5uh9tW\nQNYcQgL1DE2McFt9W3usya9ErxNMzVBBoMeEgFHXwi82a2MI3z4Jr1+knVH3lJSw72tYOAP++zNt\nddZLn4MHd8C593crAADodIJ7pg1kX0Ujq/MdKDoTngDzP4WY/rDkeji4zv5jnYVDQUAIkQJcDiy0\n2TwLWGS5vwiYbbN9qZSyRUp5ECgExjvy+or7rC+sYnx6rFck5pxGSlj7d+0SUJ9hcNdqbcqmRXZa\nNDuKa722FODq/ErGpEUT5cVLc3u9iES47r9wzZvalNzXztMW12vvxhLVUkLe59oaR0uuhcYKuPyf\n8GAuTLj7zLO/zuKKUf1Ijg7h5TX7HSs6Yw0E0Wmw5Drt8pSTOdoT+F/gN4DtYiCJUspyy/0jgLUP\nngwU2+xXYtmmeLmKeiP7Khq981JQWzN8cAes+jOMvA5u+VybxWMjOyWaeqOJQ0e9b4Gvynoju0rr\nmTa0e2eZylkIAVlz4L4tMOxybZnthReeudCN2Qy7P9IuIy29EYy1cNUL8MB2GHe7VkrTTgF6HXef\nP5Bth2v44ZCDhXPC+2iBICoFFl8Lh7537Hgd2B0EhBBXAJVSym1n2kdqIbDHYVAIcZcQYqsQYmtV\nVZW9TVScZH2BNld+irdNDa0v16pZ7XofLnxcW0u/k7O20analMsdJd53SWjNPu3v+wIVBJwnLB6u\nfQuuexvqy7Qz/DXPnuwVmNu1cYRXJmnFeExGmP0q3L8NxswDQ6BTmnHt2FTiwgJ5eU2h4weLSIT5\nn50MBIc3On5MC0d6ApOBq4QQh4ClwHQhxDtAhRAiCcBya70oVgqk2jw/xbLtNFLKBVLKHCllTkKC\nuk7qad8XVhMXFkhmUqSnm3JS6Xb4zwVQla8NDk59+Ixz+DP6hBMWqGdHsfcV/vguv4rEyCCGJ7m2\nJq5fypyljRWMmA1r/gILLoBNr8JL4+GD27V9rn5d6zlk3+D0YjwhgXpumzKANflVPPbxTo63mBw7\nYESi1iOITILF10DRJqe00+4gIKV8VEqZIqVMRxvwXSWlvAlYDsy37DYf+MRyfzkwVwgRJIQYAAwG\nttjdcsUtpJSsL6zm3Ix475m+uOsDePNS0AXA7V9rXf+z0OsEI1Oi+NHLBofb2s2sLajigqF9esUy\nHL1SWJxWb2HuEjheqeUWGILh2kVw70YYeY1L8wvunDqQO6YMYPHmImY+v5aN+486dsCIvlqPIKIv\nvHM1FG12uI2uWIv2GeAiIUQBMMPyO1LK3cAyYA/wJXCflLLdBa+vONG+ikYqG1qYkuHeLMZOmc2w\n6ml4/zZt4PfOVdA3q1tPHZ0azd6yelpM3vMnt/1wDQ1GE9OGqt6uyw27XDvjv3MV3L1O6x24YSnu\nQIOOx67IZNndk9ALwQ3/2cSTy3fT1OpAryAySQsE4YlaICh27FzaKZ+ClHKNlPIKy/2jUsoLpZSD\npZQzpJTHbPZ7Wko5SEo5VEq5whmvrbjWOstSEVMGe/iLqvU4/N98WPscZN8E8z7RZk50U3ZKNK3t\nZvLKG1zYyJ5ZnV+FQSd6zzIcvV1INCSP9UgdhnHpsax48DxunZzOWxsOcenz69hysPNKa90SmQS3\nfKb9H/jvHK3inJ16QVUKxZPWF1YzMD7Ms+UkjfXa5Z+8z+CSv8CsF3s8c8MbB4fX5FcyLj2WiGA1\nNdQfhATqeeLKESy9ayJSwvULNvKnT/fQ3Gpn7zSyn9YjCIuHd+ZAyVa7DqOCgHJGLaZ2Nh845tmp\noWYzfHyvNs1v7hKYdJ9di7glRQXTJyKI3CLvCALldc3kHWlQWcJ+aOLAOL781VTmTezPG98f5LJ/\nr2PrITt7BVHJWo8gNFZLcCs542TNM1JBQDmj7YdraW5r9+zU0PX/0HoAFz916nLDPSSEYHRqNLle\n0hNYk6+mhvqz0EADf5yVxZI7J9DWbuba1zby1Gd7MLbZ0SuIStF6BCExWiA4Q+nOM1FBQDmj9YVV\n6HWCiYM8NChc8I02EDzyOph4r8OHy06N5kDVceqau5FF6mKr8ypJjg4ho8/ZS04qvu3cQfF89avz\n+PmENBauP8hlz69je5EdyWXRqVqPICQK/jsbyn7s9lO9PghU1Bspr2vuekfF6dYXVJOdGk2kJ65Z\nH92vzeXumwVXPu+UdfxHW9bp31ni2XyBFlM73xdWM21ogpoaqhAWZOCp2SNZfMcEWkxmrnllA39d\nsbfnvYLoNC1jPjgK3p7d9f4WXh8EKhtamPLsau56eytr91V57fovvqa2qZWfSus8cymopRHeu0kr\n3Xj9O05b639UqrZWf26xg2n8Dtp6qIbjre3qUpByiskZ8Xz5q6lcPy6N1747wBUvrGd3WQ9PWKLT\ntEtDfUd2+yleHwSGJkZw59SBbD1cw7w3tjD9H2tYsHY/NcdbPd00n7Zx/1GkhKnuHhSWEpbfrxXt\nuOYNiEl32qEjgwMYlBBGroczh9fkVxKo13GuN+ReKF4lIjiAv84Zydu3jafB2Mb9S36kvacnvjH9\ntUtD3eT1QSDQoOORS4ex8dHpPD83m4SIIP7yRR4T/rqSXy/LZXtRjWOr9CmdWldYTXiQ4cTUSrfZ\n8G9tUa8Ln4BB051++NGp0eQW13r0b2Z1fhUTBsYSGqhqOimdO29IAk9eOYKD1cf57Kcyl76W1wcB\nqyCDnlnZyfzfPedqXaacVL7adYQ5L2/g8n+vZ8nmIsfX5lBOWF9QzcSBcaeXx3Ol/au09eAzZ8Pk\nB13yEtmp0VQ3tlBeZ3TJ8btSfKyJwspGtWqo0qVLRvRlSGI4L60udOll8F4TBGwN6xvJn2dnsfn3\nM3hqdhZmKfndRzuZ+JeVPP7JLvZVeE9WaG9UdLSJomNN7r0UVHNIWw4iYZhWPNxFA6bZlp6Np4rM\nrLEUGblALRWhdEGnE9x3QQb7Khr5avcR172Oy47sBuFBBm6a2J8VD07lg3snMSMzkaVbirn4X2u5\n7a0faDWZuz6Icpp1hdocdrctZ9DapA0ES7M2EBzkummTw/pGEqjXscNDQWB1fhX940IZEB/mkddX\nepcrRvVjYHwYL6wqdNklzF4dBKyEEIztH8u/rs9m0+8u5FczBrMqr5L/rDvg6ab1SusLqkmKCmZQ\nghu+qKSETx/UMoLnLIS4QS59uUCDjsx+kR7pCRjb2tmwv1qtGqp0m14n+MUFGewpr2dVngOlKs/C\nJ4KArdiwQH41YwiXZvXl3ysLKDra5Okm9SrtZsmG/UeZkhHvni+qTa/AzmUw/fdajVg3yE6NZmdp\nXc9nXTho88FjGNvMatVQpUdmZfcjNTaEf7uoN+BzQcDq8SszMegEf/hkl5o91AM7S+uoa25zz3pB\nB9fB14/BsCtgysOufz2L7NRomlrbKah079jR6rxKggw6Jg5UU0OV7gvQ67j3/Ax2FNeyzlLlz5l8\nNggkRYXw8MVD+W5fFZ/vLO/6CQoA6wvcNB5QW6yV9osbBLNfcevyvidWFHXzJaE1+ZWcOyiO4ADX\nFTFRfNPVY5NJigrmhVUFTj+p9dkgADBvUn+ykiP506d7qDd6fr2Y3mBdQTWZSZHEh9tfZLtLbUZY\ndjOYWrSVQYPdW7YyPS6UyGCDW5PGDlYf59DRJi4YpqaGKj0XZNBzz/mD+OFQDZsOOFCHoBM+HQQM\neh1Pzx5JVWML//gq39PN8XrHW0xsL6px7dRQKeHzX2sLXM15DeIHu+61zsC6oqg7ewLWqaHThqgg\noNjn+nGpJEQE8cKqAqce16eDAGhd/3kT+/P2psMemxbYlR+Lathf1ejpZrDl0DHa2qVrxwN+WAi5\ni+H833ZZG9iVzkmNJr+iwf6CHj20Or+KQQlhpMU5Zx0kxf8EB+i5+7yBbNh/lG2Hndcb8PkgAPDw\nJUNJCA/idx/txNTuXbkDK/dWcO2rG7nu1Y0eXy11fUE1gQYd49JjXfMCh9bDl4/A4Evg/Edc8xrd\nNDo1mnazZFdPF+iyQ1OriU0HjqosYcVhN05IIzYskH+vLHTaMf0iCEQGB/D4lZnsLqvn7Y2HPd2c\nEzbsr+bexdsZnBiBsa2dXyze7tEEt/UF1YxPj3XNwOXuj7Wi2DEDYM4Cj9R5tTUqxX2Dwxv3H6XV\nZFarhioOCw00cMfUAXy3r8ppf7t+EQQALh+ZxPlDEvjH1/keP+MG2F5Uwx2LtpIeF8qSOybw3DWj\n+bGolqc+3+OR9lTWG8mvaHD+pSAp4fvntSLxfUfBbV9qBb89LCEiiOToELckja3OryQ0UM+4ATEu\nfy3F982blE5USAAvrHJOb8BvgoAQgj/PysJklvzpU8980VrtLa/nlje2kBARxDu3TyAmLJDLRyVx\n59QBvL3xMB9uL3F7m9YXavOPnVo/oN0Enz0E3zyuLQo3f7lWFNtLZKdFuzwISClZk1/F5Ix4ggxq\naqjiuPAgA7dNHsC3eyvYU1bv8PH8JggApMWF8ssLB7Ni1xFW5VV4pA0Hqhq5+fUthAYaeOf2CfSJ\nDD7x2G9nDmPiwFh+99FOp/zj9sT6gmpiwwLJTHLSdE1jPSy5Dra9CVMegmvehIAQ5xzbSbJToimp\naaa6scVlr7G/qpGSmmZ1KUhxqlsmpxMRZODF1Y7PFLI7CAghUoUQq4UQe4QQu4UQD1q2xwohvhFC\nFFhuY2ye86gQolAIkS+EuMTh1tvhzqkDGdwnnD98vJumVvcuPV1a28xNCzcjpeSdOyaQGnvqTBGD\nXscLN4whKiSAe97ZRl2Te3IbpJSsL6zm3EFx6HROWCqirgTemAkH1milIWc86fExgM5Yk8Z+cmHx\n+dV5WvKdWipCcaaokADmndufFbuOUODgqsmO/M80AQ9LKTOBicB9QohM4BFgpZRyMLDS8juWx+YC\nI4CZwMtCCLf3jwMNOp6anUVpbbNTR9i7UtXQwk0LN9PQYmLRbePPWGA8ISKIl38+lvK6Zh5aluuW\ncpr7KhqpbGhxTn5A+Q5YOAPqiuGm92HsLY4f00WykiPR64RLk8ZW51cyNDGCftHe1QtSer/bpwwk\nJEDPS6sd+x6zOwhIKcullNst9xuAvUAyMAtYZNltEWCteDwLWCqlbJFSHgQKgfH2vr4jJgyM49qx\nKSxcd4D8I65fP6a2qZWbX9/MkTojb906jqzkqLPuP7Z/DH+4IpNVeZW86OA/cHessywVMWWwg2er\n+V/CG5eC0MNtX7mkMpgzhQYaGJIY4bJxgcYWEz8cOsa0YaoXoDhfbFggN03sz/IdZRysPm73cZzS\nRxdCpAPnAJuBRCmldbGeI0Ci5X4yUGzztBLLts6Od5cQYqsQYmtVVZUzmniaRy8bTkSwgd99tNOl\nZ9uNLSZuefMHDlQdZ8G8sYzt3705+DdP7M/PzknmX9/uO5Ft6irrC6sZGB9GsiNnq5sXwNIbtAzg\nO1dCYqbzGuhC2alR7HBRucnvC6tpa5dqPEBxmTumDiBAr+NlB04WHQ4CQohw4APgV1LKU0YzpfY/\nq8f/u6SUC6SUOVLKnIQE15xFxYYF8rvLhrPtcA3LthZ3/QQ7GNvauXPRVnaW1vHCjecwtQdn2kII\n/vKzkQxNjODBpbkUH3PNktitJjObDxyzf2qouR2+fBRW/A8MmQm3fgERfZ3bSBcanRJNXXMbh12w\n5Pia/EoiggyM7a+mhiqu0ScimBvGp/HRj6V2f0c4FASEEAFoAWCxlPJDy+YKIUSS5fEkwHoaWwqk\n2jw9xbLNY64Zm8L4AbH8dUWe02eItLWbuX/JdjYeOMrfrx3FJSN6/sUYEqjntZvHYpaSe97ZhrHN\n+UscbC+qobmt3b6poa3H4b2bYdPLMOFerSpYYO+qmHViRVEnDw5LKVmdV8WUwfHurdOs+J17zh+E\nTghe+W6/Xc93ZHaQAF4H9kop/2nz0HJgvuX+fOATm+1zhRBBQogBwGBgi72v7wza2XYWTa0m/vL5\nXqcdt90seXjZDr7dW8mfZ2fxs3NS7D5W/7gw/vf6bHaX1fPYx86vjbC+oBq9TjBxUA/XuG+ogLcu\nh30r4NLn4NJnQNf75sEPSYwgNFDPj0XODQJ5Rxo4Um9Ul4IUl+sbFcy1OSm8v7XErkRYR05RJgM3\nA9OFELmWn8uAZ4CLhBAFwAzL70gpdwPLgD3Al8B9UsquT23ry6DZdVP4MvpEcPd5g/jwx1I2FDpe\nsEFKyWMf72T5jjJ+O3MYN0/s7/AxLxyeyC+nZ/D+thLe3eLcS1frCqvJTo0mMjig+0+q2AMLL4Sq\nfG0p6Al3O7VN7qTXCbKSo5zeE1htGcc5X00NVdzg3mmDMEvJa9/1vKSuI7OD1ksphZRylJQy2/Lz\nhZTyqJTyQinlYCnlDCnlMZvnPC2lHCSlHCqlXNGtF2qshH+fA5tfA1Orvc09q/unZ5AWG8pjH++i\nxWT/JRcpJX/5Yi/vbinmvgsGce8059XLfXDGEM4bksCTy3c7bTZLXVMbO0tqe3Yp6PAGeOMSaG+D\nW1fA0Eud0hZPyk6NZndZvVPXbVqTX8WIfpEk2iQDKoqrpMSEMmdMMu9uKaKywdij53r/xcqEodA3\nC1b8Bl6eCHs/1dajcaLgAD1/np3FgerjvLrG/uL0L6wq5D/rDjJ/Un/+38VDndhC7Yz133Oz6RMZ\nxC/e2cZRJ4xhbNhfjVnS/fyA/avgv3MgPBHu+Bb6ZTvcBm8wOiWaVpPZadOF65rb2Ha4Rl0KUtzq\nF9MyaGs385+1PfsOM7ioPc4TEALzlkPBN/DNH+C9myBtElz8FKTkOO1lzh+SwBWjknhpTSFJUcGY\npaSxxURTazvHW000tWi3x63bLLeNNr+3mMxcPSaFJ64c4ZIi7dGhgbx601jmvLKBXy79kUW3jsfg\nwKDjusJqwoMMJwZHzyp/BSybB3GDYd7HEO47X3DZadr7zy2uYWTK2XM4uuOr3UdoN0uVJay4VXp8\nGFeN7sc7m4q45/zuX4Xw/iAAIAQMuVhLPvrxv7D6ae2adNbVcOHjEJPulJd5/IpM1hdW85sPfjpl\ne6BBR3iQgdBAPWGBBkKDtNuEiKBTfk+OCeHG8WnOWXrhDLKSo3hqdha/ef8n/vHNPn47c5jdx1pf\nUM3EgXFdz17Z9SF8eKe2CuhNH0Coi+oNeEi/qGDiw4PILa7j5kmOHavB2Mbfv8pnVEoUY9LU1FDF\nve6fnsEnO8p44/uD3X5O7wgCVnoD5NwKI6/Rlife8KJ2eWjC3TD1YQhx7D9dn8hgvvt/F1DVaCQs\nyEBooPbF721T/K7LSeXHolpeWbOf0SnRzMzq+fTToqNNFB1r4rbJ6WffMXcJfHIfpE6AG5e5vR6w\nOwghtKQxJwwOP/9tAVWNLfxnXo5LTwYUpTMZfSK4LCuJRRu6XzeldwUBq6AImP4YjL1V6xVseBF+\nfEcrWZhzOxgC7T50VGgAUaE9mCnjIU9elcmesjoeei+X19dHEhEcQESwgfAgw4n71p/wIJvfLfe/\n26fNXjnrUhE/LITPH4aB07RZQL0sB6AnRqdEszKvknpjW89mStnIO1LPmxsOMXdcWvcusSmKC9w/\nPYO1Bd1faUG4Il3emXJycuTWrVvPvlP5T/D1Y3DwO61y1UV/hOFXaZeRfFh5XTPPrMijsr6FhpY2\nGowmGo0mGowmWrtRRjMpKpgNj0zvfPxiwwvaZzrkUrj2LQjw7Vkua/dVMe+NLSy+YwKT7Uick1Jy\n/WubKKhsYNXD04gJs/9ERFEc1dzaTmiQYZuUssuB097ZE+goaRTM+wQKv9W+uKwDmGEJ2peXIUQb\nYD5x/yy3AaHQJxOiU7t+XQ9Ligrh+bnndPqYsU0btD4ZGNqoN5os27SAMbZ/zOkBQEr47jlY8xet\nEMzVC0Hv/T0jR41OsQ4O19oVBD7OLWXLoWM8M2ekCgCKx4UEdj9x0zeCAGhn/YMvgoEXQO47sPcz\naGsGYx20VYCpGdqMp96eTcIw7XgZF2mzkRy4xOQJwQF6ggP0xIcHdf9JUsK3T2jjLaNvhKte0MZh\n/J4mvhsAAAfzSURBVEBUaAAD48Psqttab2zj6c/zGJ0azXU53n/yoCi2fO9/uN6grWHf1Tr2UoKp\n5fTg0HocijdD4Tew6VXtskhguHZd3BoUojpd/LR3M5u1XIwf/qONq1z2d68sBONKo1Oj2bC/51nj\n//x6H0ePt/DmLePUYLDS6/heEOguISyXf4Kh4wrKqePg3PuhpREOroWCr7VLTXmfaY/3GQGDZ8Dg\ni7VZM739com5HZb/UutBTbpfy8Hw8fGUzoxOieKjH0spr2smKap7y2rvKavn7Y2H+PmENKfkGCiK\nu/lvEOiOoHAYdpn2IyVU5WlJawVfw8aXtMsmQZEnewmDpkNkcu/6Am1vg4/uhl0fwPmPwLRHelf7\nnSjbMq9/R3Ftt4KA2Sx5/JNdRIcGOj1DXFHcRQWB7hIC+gzXfib/UiukfvA7S1D4BvYu1/YLiT25\nX5/hkGC59cYEK1ML/N+tkP85zPgjTPmVp1vkUcOTIgjQa+UmZ2Yldbn/hz+WsvVwDc9dM4ro0N41\nZqQoVioI2Cs4EoZfqf1ICZV74OA67bYqD35aBi02NXbC+0KfYdrMoxPBYZiW8+BOUmo/bU2w7GZt\nPaBL/wYT7nJvO7xQkEFPZlJktwaH65ra+OsXexmTFs01Y+xfKlxRPE0FAWcQAhJHaD9WUkJ9KVTm\naYGhci9U7YWtb546MykqTQsGITHapRlzG7SboL315H1zm/b7ifttJ/c1m0CaLT+WL3jr79jctz5+\nSqE3AVe9CGNudtMH5f1Gp0bzwbYS2s0S/VkGef/xTT41Ta0sum28GgxWejUVBFxFCIhK0X4Gzzi5\n3WyG2kOnBofKvVrvQRcA+kBthtOJ+wFgCNJu9YGgM5x6X2fQirkInfaD0F5b6GxubR/TnXwsdQIM\nPN9DH5B3yk6N5u2Nh9lf1ciQxM57abtK63hn02FuntifrGQ1GKz0bioIuJtOB7EDtZ9hl3m6NUoH\n1uUecotrOw0CZrPkD5/sIjYskF+rwWDFB/jXRHBF6cKAuDAigg1nHBd4f1sJPxbV8uilw4kK6eVT\ngxUFFQQU5RQ6nWB0SnSn1dtqm1p55ss8xqXHMGeMDyYMKn5JBQFF6SA7NZq8Iw0Y204tNfq3r/Kp\na27jT7OyXFI0SFE8QQUBRelgdGo07WbJ7rK6E9t+KqllyZYi5k3qz/Ak36upoPgvFQQUpYPRluUf\ncou1IGA2S/7w8S7iw4N46KIhnmyaojidCgKK0kGfyGD6RQWfGBdY+kMxO0rq+P1lw+0uOKMo3srt\nQUAIMVMIkS+EKBRCPOLu11eU7shOi2ZHcS3Hjrfy3Fd5jB8Qy6zsfp5ulqI4nVuDgBBCD7wEXApk\nAjcIITLd2QZF6Y7RKdEUHWvidx/upMFo4s9qMFjxUe7uCYwHCqWUB6SUrcBSYJab26AoXbImjX25\n+wi3npvO0L5uXuNJUdzE3UEgGSi2+b3Esk1RvMrI5Ch0AvpEBPHgjMGebo6iuIxXLhshhLgLuAsg\nLS3Nw61R/FFYkIFHLh3GyORoItRgsOLD3B0ESgHbIqwplm2nkFIuABYA5OTkyI6PK4o73HXeIE83\nQVFczt2Xg34ABgshBgghAoG5wHI3t0FRFEWxcGtPQEppEkLcD3wF6IE3pJS73dkGRVEU5SS3jwlI\nKb8AvnD36yqKoiinUxnDiqIofkwFAUVRFD+mgoCiKIofU0FAURTFj6kgoCiK4seElN6diyWEaADy\nPd0OLxYPVHu6EV5OfUZdU59R13rbZ9RfSpnQ1U5euWxEB/lSyhxPN8JbCSG2qs/n7NRn1DX1GXXN\nVz8jdTlIURTFj6kgoCiK4sd6QxBY4OkGeDn1+XRNfUZdU59R13zyM/L6gWFFURTFdXpDT0BRFEVx\nEa8NAqogfdeEEIeEEDuFELlCiK2ebo83EEK8IYSoFELsstkWK4T4RghRYLmN8WQbPe0Mn9GTQohS\ny99SrhDiMk+20ZOEEKlCiNVCiD1CiN1CiAct233y78grg4AqSN8jF0gps31x6pqd3gJmdtj2CLBS\nSjkYWGn53Z+9xemfEcC/LH9L2ZbVfv2VCXhYSpkJTATus3z/+OTfkVcGAVRBesVOUsq1wLEOm2cB\niyz3FwGz3dooL3OGz0ixkFKWSym3W+43AHvRaqH75N+RtwYBVZC+eyTwrRBim6Uus9K5RCllueX+\nESDRk43xYg+I/9/e3bJEEIVRHP+fYFI/gEXET7D2RTbZLVajwWK2mIyKzSBGFQRfv4LJqoJVg8hu\ntLuP4c7iwqIuiM5l7vmVGWYYuAwPe4Zn7s6V7qp2USNaHb8laQ5YAG5paB3lGgI2nnZEtEhts3VJ\ni3UPKHeRpsN5StyofWAeaAGvwE69w6mfpCngDNiIiLfhc02qo1xDYKwF6UsXES/VtgdckNpoNqor\naQag2vZqHk92IqIbEe8R0QcOKLyWJE2QAuAoIs6rw42so1xDwAvS/0DSpKTpwT6wBDx8f1WxroHV\nan8VuKpxLFka/LhVlim4liQJOAQeI2J36FQj6yjbP4tVU9T2+FyQfrvmIWVF0jzp6R/ShwCPfY9A\n0gnQIX3xsQtsAZfAKTALPAMrEVHsi9Ev7lGH1AoK4AlYG+p/F0VSG7gB7oF+dXiT9F6gcXWUbQiY\nmdnfy7UdZGZm/8AhYGZWMIeAmVnBHAJmZgVzCJiZFcwhYGZWMIeAmVnBHAJmZgX7AIwrbA96rprz\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a368b6320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(test_appliance['hvac'][14]).plot(label='GT')\n",
    "\n",
    "pd.Series(pred_appliance['hvac'][14]).plot(label='Pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0c3c11c20fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_hvac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_agg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_hvac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_fridge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "pred_hvac = model.predict(test_agg)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(pred_hvac, test_fridge))\n",
    "print(mean_absolute_error(pred_hvac, test_agg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.907349e-06\n",
       "1    -3.877686e+00\n",
       "2    -9.150000e+00\n",
       "3     0.000000e+00\n",
       "4     0.000000e+00\n",
       "5    -9.633333e+00\n",
       "6     0.000000e+00\n",
       "7     2.861023e-06\n",
       "8    -8.683333e+00\n",
       "9     9.536743e-07\n",
       "10    0.000000e+00\n",
       "11    9.536743e-07\n",
       "12   -9.583333e+00\n",
       "13   -9.516666e+00\n",
       "14   -4.711666e+01\n",
       "15    3.099442e-06\n",
       "16   -4.685000e+01\n",
       "17    9.536743e-07\n",
       "18   -7.310000e+01\n",
       "19   -7.350000e+01\n",
       "20   -4.180000e+01\n",
       "21    0.000000e+00\n",
       "22   -9.616667e+00\n",
       "23   -9.533334e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.DataFrame(pred_hvac) - pd.DataFrame(test_agg)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f56c5e38e529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#pd.Series(test_agg[1, :]).plot(label='GT')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_agg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.Series(test_mw[1, :]).plot(label='GT')\n",
    "#pd.Series(test_agg[1, :]).plot(label='GT')\n",
    "\n",
    "\n",
    "pd.Series(model.predict(test_agg[1:2])[0, :24]).plot(label='Pred')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
