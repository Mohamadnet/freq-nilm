{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from common import APPLIANCES_ORDER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = np.load('../1H-input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subset_dataset(tensor):\n",
    "    t_subset = tensor[:, :, 180:194, :]\n",
    "    all_indices = np.array(list(range(320)))\n",
    "    for i in range(1, 7):\n",
    "        valid_homes = pd.DataFrame(t_subset[:, i, :].reshape(320, 14*24)).dropna().index\n",
    "        all_indices = np.intersect1d(all_indices, valid_homes)\n",
    "    t_subset = t_subset[all_indices, :, :, :].reshape(52, 7, 14, 24)\n",
    "    \n",
    "    # Create artificial aggregate\n",
    "    t_subset[:, 0, :,:] = 0.0\n",
    "    for i in range(1, 7):\n",
    "        t_subset[:, 0, :,:] = t_subset[:, 0, :,:] + t_subset[:, i, :,:]\n",
    "    # t_subset is of shape (#home, appliance, days*hours)\n",
    "    return t_subset, all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 14, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all, valid_homes = create_subset_dataset(tensor)\n",
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 14, 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_objective(y_pred, y_true):\n",
    "    with tf.name_scope(None):\n",
    "        return tf.losses.absolute_difference(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/nipun/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Conv1D, Dense, Flatten, MaxPool1D, InputLayer, Activation, Dropout, MaxPooling1D\n",
    "\n",
    "\n",
    "import keras\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "n_movies = 3\n",
    "n_users=3\n",
    "n_latent_factors=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10dff3f28>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb0AAADeCAYAAACpOd4EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmfUZ/x9LiBHIqwUoZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d9e5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(6,3))\n",
    "cbar_ax = fig.add_axes([.95, 0.15, .02, .7])\n",
    "home = 4\n",
    "appliance_num = 6\n",
    "sns.heatmap(t_all[home, 0, :, :],cmap='Greens',ax=ax[0],cbar_ax=None,cbar=False)\n",
    "sns.heatmap(t_all[home, appliance_num, :, :],cmap='Greens',ax=ax[1],cbar_ax=cbar_ax,vmax=t_all[home, 0, :, :].max())\n",
    "ax[0].set_title(\"Aggregate\")\n",
    "ax[1].set_title(APPLIANCES_ORDER[appliance_num])\n",
    "ax[0].set_ylabel(\"Day\")\n",
    "fig.text(0.5, 0, \"Hour of day\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_days = 14\n",
    "num_hours = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxs = {appliance_num:t_all[:30, appliance_num, :, :].max() for appliance_num in range(7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENERGY_MEAN = {APPLIANCES_ORDER[i]:np.mean(t_all[:, i, :]) for i in list(range(1, 7))}\n",
    "ENERGY_MEAN = pd.Series(ENERGY_MEAN)\n",
    "ENERGY_MEAN.sort_values(inplace=True, ascending=False)\n",
    "appliance_orders_subtract = [APPLIANCES_ORDER.index(x) for x in ENERGY_MEAN.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvac      894.701044\n",
       "fridge     89.265252\n",
       "oven       16.646041\n",
       "dw         13.984549\n",
       "mw          6.417846\n",
       "wm          5.083840\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENERGY_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5000.416633605957,\n",
       " 3002.5000457763672,\n",
       " 2914.800048828125,\n",
       " 1126.2667155265808,\n",
       " 960.7833251953125,\n",
       " 498.43331909179688,\n",
       " 0.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_maxes = [t_all[:30, 0, :, :].max()]\n",
    "for i in range(6):\n",
    "    appliances_considered = appliance_orders_subtract[:i+1]\n",
    "    input_maxes.append((t_all[:30, 0, :, :]-t_all[:30,appliances_considered , :, :].sum(axis=1)).max())\n",
    "input_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3963.88330078125,\n",
       " 288.85000610351562,\n",
       " 2914.800048828125,\n",
       " 1122.300048828125,\n",
       " 960.7833251953125,\n",
       " 498.43331909179688]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appliance_maxes = [t_all[:30, i, :, :].max() for i in appliance_orders_subtract]\n",
    "appliance_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000.416633605957"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all[:30, 0, :,:].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3963.88330078125,\n",
       " 288.85000610351562,\n",
       " 2914.800048828125,\n",
       " 1122.300048828125,\n",
       " 960.7833251953125,\n",
       " 498.43331909179688]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appliance_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "appliance_num=1\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Lambda, Subtract, Merge\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "agg = Input(shape=(num_days, num_hours, 1),name='Aggregate')  # adapt this if using `channels_first` image data format\n",
    "agg_norm = Lambda(lambda x: x/4000.,name='Norm')(agg)\n",
    "\n",
    "hvac_c = Conv2D(6, (14, 1), activation='relu', padding='same',name='Conv-HVAC')(agg_norm)\n",
    "\n",
    "hvac_d = Conv2D(1, (1,24), activation='relu', padding='same',name='HVAC')(hvac_c)\n",
    "\n",
    "hvac_scaled = Lambda(lambda x: x*4000.)(hvac_d)\n",
    "agg_minus_hvac = Subtract()([agg, hvac_scaled])\n",
    "\n",
    "#agg_minus_hvac = Subtract()([agg, hvac_d])\n",
    "\n",
    "agg_minus_hvac_norm = Lambda(lambda x: x/4000.,name='Norm-hvac')(agg_minus_hvac)\n",
    "fridge_c = Conv2D(6, (14, 1), activation='relu', padding='same',name='Conv-Fridge')(agg_minus_hvac_norm)\n",
    "\n",
    "fridge_d = Conv2D(1, (1,24), activation='relu', padding='same',name='Fridge')(fridge_c)\n",
    "\n",
    "fridge_scaled = Lambda(lambda x: x*4000.)(fridge_d)\n",
    "agg_minus_oven = Subtract()([agg_minus_hvac, fridge_scaled ])\n",
    "\n",
    "\n",
    "agg_minus_oven_norm = Lambda(lambda x: x/4000.,name='Norm-hvac-oven')(agg_minus_oven)\n",
    "oven_c = Conv2D(6, (14, 1), activation='relu', padding='same',name='Conv-Oven')(agg_minus_oven_norm)\n",
    "\n",
    "oven_d = Conv2D(1, (1,24), activation='relu', padding='same',name='Oven')(oven_c)\n",
    "\n",
    "oven_scaled = Lambda(lambda x: x*4000.)(oven_d)\n",
    "agg_minus_dw = Subtract()([agg_minus_oven, oven_scaled ])\n",
    "\n",
    "\n",
    "agg_minus_dw_norm = Lambda(lambda x: x/4000.,name='Norm-hvac-oven-dw')(agg_minus_dw)\n",
    "dw_c = Conv2D(6, (14, 1), activation='relu', padding='same',name='Conv-dw')(agg_minus_dw_norm)\n",
    "\n",
    "dw_d = Conv2D(1, (1,24), activation='relu', padding='same',name='DW')(dw_c)\n",
    "\n",
    "#hvac_fridge = Merge(mode='concat',concat_axis=1)([hvac_d, fridge_d])\n",
    "\n",
    "autoencoder = Model(agg, [hvac_d, fridge_d, oven_d, dw_d])\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Aggregate (InputLayer)          (None, 14, 24, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Norm (Lambda)                   (None, 14, 24, 1)    0           Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Conv-HVAC (Conv2D)              (None, 14, 24, 6)    90          Norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "HVAC (Conv2D)                   (None, 14, 24, 1)    145         Conv-HVAC[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 14, 24, 1)    0           HVAC[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 14, 24, 1)    0           Aggregate[0][0]                  \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Norm-hvac (Lambda)              (None, 14, 24, 1)    0           subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv-Fridge (Conv2D)            (None, 14, 24, 6)    90          Norm-hvac[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Fridge (Conv2D)                 (None, 14, 24, 1)    145         Conv-Fridge[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 14, 24, 1)    0           Fridge[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 14, 24, 1)    0           subtract_1[0][0]                 \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Norm-hvac-oven (Lambda)         (None, 14, 24, 1)    0           subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv-Oven (Conv2D)              (None, 14, 24, 6)    90          Norm-hvac-oven[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Oven (Conv2D)                   (None, 14, 24, 1)    145         Conv-Oven[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 14, 24, 1)    0           Oven[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 14, 24, 1)    0           subtract_2[0][0]                 \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Norm-hvac-oven-dw (Lambda)      (None, 14, 24, 1)    0           subtract_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv-dw (Conv2D)                (None, 14, 24, 6)    90          Norm-hvac-oven-dw[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "DW (Conv2D)                     (None, 14, 24, 1)    145         Conv-dw[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 940\n",
      "Trainable params: 940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1511pt\" viewBox=\"0.00 0.00 482.93 1511.00\" width=\"483pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1507)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-1507 478.928,-1507 478.928,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 4529798224 -->\n",
       "<g class=\"node\" id=\"node1\"><title>4529798224</title>\n",
       "<polygon fill=\"none\" points=\"111.5,-1458.5 111.5,-1502.5 422.704,-1502.5 422.704,-1458.5 111.5,-1458.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.443\" y=\"-1476.3\">Aggregate: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"255.387,-1458.5 255.387,-1502.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283.221\" y=\"-1487.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"255.387,-1480.5 311.056,-1480.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283.221\" y=\"-1465.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"311.056,-1458.5 311.056,-1502.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.88\" y=\"-1487.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"311.056,-1480.5 422.704,-1480.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.88\" y=\"-1465.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4529798504 -->\n",
       "<g class=\"node\" id=\"node2\"><title>4529798504</title>\n",
       "<polygon fill=\"none\" points=\"40.4829,-1377.5 40.4829,-1421.5 309.721,-1421.5 309.721,-1377.5 40.4829,-1377.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"91.4434\" y=\"-1395.3\">Norm: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"142.404,-1377.5 142.404,-1421.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.238\" y=\"-1406.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"142.404,-1399.5 198.073,-1399.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.238\" y=\"-1384.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"198.073,-1377.5 198.073,-1421.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253.897\" y=\"-1406.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"198.073,-1399.5 309.721,-1399.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253.897\" y=\"-1384.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4529798224&#45;&gt;4529798504 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>4529798224-&gt;4529798504</title>\n",
       "<path d=\"M242.448,-1458.33C231.723,-1449.12 219.009,-1438.2 207.532,-1428.35\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"209.691,-1425.59 199.824,-1421.73 205.131,-1430.9 209.691,-1425.59\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4531506648 -->\n",
       "<g class=\"node\" id=\"node6\"><title>4531506648</title>\n",
       "<polygon fill=\"none\" points=\"57.2759,-1053.5 57.2759,-1097.5 464.928,-1097.5 464.928,-1053.5 57.2759,-1053.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.457\" y=\"-1071.3\">subtract_1: Subtract</text>\n",
       "<polyline fill=\"none\" points=\"185.638,-1053.5 185.638,-1097.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"213.473\" y=\"-1082.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"185.638,-1075.5 241.307,-1075.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"213.473\" y=\"-1060.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"241.307,-1053.5 241.307,-1097.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353.118\" y=\"-1082.3\">[(None, 14, 24, 1), (None, 14, 24, 1)]</text>\n",
       "<polyline fill=\"none\" points=\"241.307,-1075.5 464.928,-1075.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"352.631\" y=\"-1060.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4529798224&#45;&gt;4531506648 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>4529798224-&gt;4531506648</title>\n",
       "<path d=\"M290.437,-1458.41C300.481,-1448.26 311.612,-1435.39 319.102,-1422 342.273,-1380.57 349.102,-1366.97 349.102,-1319.5 349.102,-1319.5 349.102,-1319.5 349.102,-1236.5 349.102,-1190 351.683,-1174.08 328.102,-1134 321.478,-1122.74 311.84,-1112.61 301.928,-1104.1\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"303.939,-1101.22 293.972,-1097.63 299.523,-1106.65 303.939,-1101.22\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4529798056 -->\n",
       "<g class=\"node\" id=\"node3\"><title>4529798056</title>\n",
       "<polygon fill=\"none\" points=\"9.59229,-1296.5 9.59229,-1340.5 320.612,-1340.5 320.612,-1296.5 9.59229,-1296.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.4434\" y=\"-1314.3\">Conv-HVAC: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"153.294,-1296.5 153.294,-1340.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.129\" y=\"-1325.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"153.294,-1318.5 208.963,-1318.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.129\" y=\"-1303.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"208.963,-1296.5 208.963,-1340.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.788\" y=\"-1325.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"208.963,-1318.5 320.612,-1318.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.788\" y=\"-1303.3\">(None, 14, 24, 6)</text>\n",
       "</g>\n",
       "<!-- 4529798504&#45;&gt;4529798056 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>4529798504-&gt;4529798056</title>\n",
       "<path d=\"M172.422,-1377.33C171.391,-1369.18 170.191,-1359.7 169.064,-1350.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"172.517,-1350.21 167.789,-1340.73 165.573,-1351.09 172.517,-1350.21\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4529798448 -->\n",
       "<g class=\"node\" id=\"node4\"><title>4529798448</title>\n",
       "<polygon fill=\"none\" points=\"30.0923,-1215.5 30.0923,-1259.5 306.112,-1259.5 306.112,-1215.5 30.0923,-1215.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"84.4434\" y=\"-1233.3\">HVAC: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"138.794,-1215.5 138.794,-1259.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166.629\" y=\"-1244.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"138.794,-1237.5 194.463,-1237.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166.629\" y=\"-1222.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"194.463,-1215.5 194.463,-1259.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"250.288\" y=\"-1244.3\">(None, 14, 24, 6)</text>\n",
       "<polyline fill=\"none\" points=\"194.463,-1237.5 306.112,-1237.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"250.288\" y=\"-1222.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4529798056&#45;&gt;4529798448 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>4529798056-&gt;4529798448</title>\n",
       "<path d=\"M165.906,-1296.33C166.215,-1288.18 166.576,-1278.7 166.914,-1269.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"170.414,-1269.85 167.296,-1259.73 163.419,-1269.59 170.414,-1269.85\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4529798280 -->\n",
       "<g class=\"node\" id=\"node5\"><title>4529798280</title>\n",
       "<polygon fill=\"none\" points=\"27.2104,-1134.5 27.2104,-1178.5 318.994,-1178.5 318.994,-1134.5 27.2104,-1134.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"89.4434\" y=\"-1152.3\">lambda_1: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"151.676,-1134.5 151.676,-1178.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179.511\" y=\"-1163.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"151.676,-1156.5 207.345,-1156.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179.511\" y=\"-1141.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"207.345,-1134.5 207.345,-1178.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.169\" y=\"-1163.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"207.345,-1156.5 318.994,-1156.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.169\" y=\"-1141.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4529798448&#45;&gt;4529798280 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>4529798448-&gt;4529798280</title>\n",
       "<path d=\"M169.442,-1215.33C169.958,-1207.18 170.558,-1197.7 171.121,-1188.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"174.62,-1188.93 171.758,-1178.73 167.634,-1188.49 174.62,-1188.93\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4529798280&#45;&gt;4531506648 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>4529798280-&gt;4531506648</title>\n",
       "<path d=\"M196.684,-1134.33C206.844,-1125.21 218.871,-1114.41 229.765,-1104.63\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"232.351,-1107.01 237.455,-1097.73 227.675,-1101.8 232.351,-1107.01\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4531232720 -->\n",
       "<g class=\"node\" id=\"node7\"><title>4531232720</title>\n",
       "<polygon fill=\"none\" points=\"19.938,-972.5 19.938,-1016.5 320.266,-1016.5 320.266,-972.5 19.938,-972.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"86.4434\" y=\"-990.3\">Norm-hvac: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"152.949,-972.5 152.949,-1016.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180.783\" y=\"-1001.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"152.949,-994.5 208.618,-994.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180.783\" y=\"-979.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"208.618,-972.5 208.618,-1016.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.442\" y=\"-1001.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"208.618,-994.5 320.266,-994.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.442\" y=\"-979.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4531506648&#45;&gt;4531232720 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>4531506648-&gt;4531232720</title>\n",
       "<path d=\"M236.716,-1053.33C226.107,-1044.12 213.531,-1033.2 202.18,-1023.35\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"204.402,-1020.64 194.556,-1016.73 199.813,-1025.93 204.402,-1020.64\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4531908736 -->\n",
       "<g class=\"node\" id=\"node11\"><title>4531908736</title>\n",
       "<polygon fill=\"none\" points=\"59.2759,-648.5 59.2759,-692.5 466.928,-692.5 466.928,-648.5 59.2759,-648.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123.457\" y=\"-666.3\">subtract_2: Subtract</text>\n",
       "<polyline fill=\"none\" points=\"187.638,-648.5 187.638,-692.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"215.473\" y=\"-677.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"187.638,-670.5 243.307,-670.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"215.473\" y=\"-655.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"243.307,-648.5 243.307,-692.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355.118\" y=\"-677.3\">[(None, 14, 24, 1), (None, 14, 24, 1)]</text>\n",
       "<polyline fill=\"none\" points=\"243.307,-670.5 466.928,-670.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"354.631\" y=\"-655.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4531506648&#45;&gt;4531908736 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>4531506648-&gt;4531908736</title>\n",
       "<path d=\"M294.396,-1053.43C307.071,-1043.79 320.515,-1031.32 329.102,-1017 353.063,-977.04 351.102,-961.093 351.102,-914.5 351.102,-914.5 351.102,-914.5 351.102,-831.5 351.102,-784.998 353.683,-769.079 330.102,-729 323.478,-717.742 313.84,-707.607 303.928,-699.095\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"305.939,-696.22 295.972,-692.627 301.523,-701.651 305.939,-696.22\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4529797552 -->\n",
       "<g class=\"node\" id=\"node8\"><title>4529797552</title>\n",
       "<polygon fill=\"none\" points=\"13.2485,-891.5 13.2485,-935.5 322.956,-935.5 322.956,-891.5 13.2485,-891.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"84.4434\" y=\"-909.3\">Conv-Fridge: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"155.638,-891.5 155.638,-935.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.473\" y=\"-920.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"155.638,-913.5 211.307,-913.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.473\" y=\"-898.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"211.307,-891.5 211.307,-935.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.131\" y=\"-920.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"211.307,-913.5 322.956,-913.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.131\" y=\"-898.3\">(None, 14, 24, 6)</text>\n",
       "</g>\n",
       "<!-- 4531232720&#45;&gt;4529797552 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>4531232720-&gt;4529797552</title>\n",
       "<path d=\"M169.566,-972.329C169.36,-964.183 169.12,-954.699 168.894,-945.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"172.392,-945.637 168.639,-935.729 165.394,-945.814 172.392,-945.637\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4531371536 -->\n",
       "<g class=\"node\" id=\"node9\"><title>4531371536</title>\n",
       "<polygon fill=\"none\" points=\"33.7485,-810.5 33.7485,-854.5 308.456,-854.5 308.456,-810.5 33.7485,-810.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87.4434\" y=\"-828.3\">Fridge: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"141.138,-810.5 141.138,-854.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.973\" y=\"-839.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"141.138,-832.5 196.807,-832.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.973\" y=\"-817.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"196.807,-810.5 196.807,-854.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.631\" y=\"-839.3\">(None, 14, 24, 6)</text>\n",
       "<polyline fill=\"none\" points=\"196.807,-832.5 308.456,-832.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.631\" y=\"-817.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4529797552&#45;&gt;4531371536 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>4529797552-&gt;4531371536</title>\n",
       "<path d=\"M168.906,-891.329C169.215,-883.183 169.576,-873.699 169.914,-864.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"173.414,-864.855 170.296,-854.729 166.419,-864.589 173.414,-864.855\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4531232552 -->\n",
       "<g class=\"node\" id=\"node10\"><title>4531232552</title>\n",
       "<polygon fill=\"none\" points=\"29.2104,-729.5 29.2104,-773.5 320.994,-773.5 320.994,-729.5 29.2104,-729.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"91.4434\" y=\"-747.3\">lambda_2: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"153.676,-729.5 153.676,-773.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.511\" y=\"-758.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"153.676,-751.5 209.345,-751.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.511\" y=\"-736.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"209.345,-729.5 209.345,-773.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265.169\" y=\"-758.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"209.345,-751.5 320.994,-751.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265.169\" y=\"-736.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4531371536&#45;&gt;4531232552 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>4531371536-&gt;4531232552</title>\n",
       "<path d=\"M172.174,-810.329C172.586,-802.183 173.067,-792.699 173.517,-783.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"177.017,-783.893 174.027,-773.729 170.026,-783.539 177.017,-783.893\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4531232552&#45;&gt;4531908736 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>4531232552-&gt;4531908736</title>\n",
       "<path d=\"M198.684,-729.329C208.844,-720.209 220.871,-709.412 231.765,-699.632\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"234.351,-702.014 239.455,-692.729 229.675,-696.805 234.351,-702.014\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4531845608 -->\n",
       "<g class=\"node\" id=\"node12\"><title>4531845608</title>\n",
       "<polygon fill=\"none\" points=\"0,-567.5 0,-611.5 332.204,-611.5 332.204,-567.5 0,-567.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.4434\" y=\"-585.3\">Norm-hvac-oven: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"164.887,-567.5 164.887,-611.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.721\" y=\"-596.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"164.887,-589.5 220.556,-589.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.721\" y=\"-574.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"220.556,-567.5 220.556,-611.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"276.38\" y=\"-596.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"220.556,-589.5 332.204,-589.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"276.38\" y=\"-574.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4531908736&#45;&gt;4531845608 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>4531908736-&gt;4531845608</title>\n",
       "<path d=\"M237.108,-648.329C225.691,-639.031 212.137,-627.992 199.946,-618.064\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"202.132,-615.33 192.168,-611.729 197.712,-620.758 202.132,-615.33\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4532698304 -->\n",
       "<g class=\"node\" id=\"node16\"><title>4532698304</title>\n",
       "<polygon fill=\"none\" points=\"67.2759,-243.5 67.2759,-287.5 474.928,-287.5 474.928,-243.5 67.2759,-243.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.457\" y=\"-261.3\">subtract_3: Subtract</text>\n",
       "<polyline fill=\"none\" points=\"195.638,-243.5 195.638,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.473\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"195.638,-265.5 251.307,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.473\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"251.307,-243.5 251.307,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363.118\" y=\"-272.3\">[(None, 14, 24, 1), (None, 14, 24, 1)]</text>\n",
       "<polyline fill=\"none\" points=\"251.307,-265.5 474.928,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"362.631\" y=\"-250.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4531908736&#45;&gt;4532698304 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>4531908736-&gt;4532698304</title>\n",
       "<path d=\"M303.122,-648.443C317.203,-639.1 331.817,-626.829 341.102,-612 365.69,-572.731 360.102,-555.832 360.102,-509.5 360.102,-509.5 360.102,-509.5 360.102,-426.5 360.102,-379.907 361.917,-364.047 338.102,-324 331.425,-312.773 321.77,-302.648 311.858,-294.137\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"313.871,-291.263 303.906,-287.667 309.453,-296.693 313.871,-291.263\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4531559224 -->\n",
       "<g class=\"node\" id=\"node13\"><title>4531559224</title>\n",
       "<polygon fill=\"none\" points=\"21.3623,-486.5 21.3623,-530.5 324.842,-530.5 324.842,-486.5 21.3623,-486.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"89.4434\" y=\"-504.3\">Conv-Oven: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"157.524,-486.5 157.524,-530.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185.359\" y=\"-515.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"157.524,-508.5 213.193,-508.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185.359\" y=\"-493.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"213.193,-486.5 213.193,-530.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"269.018\" y=\"-515.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"213.193,-508.5 324.842,-508.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"269.018\" y=\"-493.3\">(None, 14, 24, 6)</text>\n",
       "</g>\n",
       "<!-- 4531845608&#45;&gt;4531559224 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>4531845608-&gt;4531559224</title>\n",
       "<path d=\"M167.978,-567.329C168.7,-559.183 169.54,-549.699 170.329,-540.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"173.825,-540.999 171.221,-530.729 166.852,-540.381 173.825,-540.999\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4531846616 -->\n",
       "<g class=\"node\" id=\"node14\"><title>4531846616</title>\n",
       "<polygon fill=\"none\" points=\"42.8623,-405.5 42.8623,-449.5 311.342,-449.5 311.342,-405.5 42.8623,-405.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"93.4434\" y=\"-423.3\">Oven: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"144.024,-405.5 144.024,-449.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"171.859\" y=\"-434.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"144.024,-427.5 199.693,-427.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"171.859\" y=\"-412.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"199.693,-405.5 199.693,-449.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255.518\" y=\"-434.3\">(None, 14, 24, 6)</text>\n",
       "<polyline fill=\"none\" points=\"199.693,-427.5 311.342,-427.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255.518\" y=\"-412.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4531559224&#45;&gt;4531846616 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>4531559224-&gt;4531846616</title>\n",
       "<path d=\"M174.174,-486.329C174.586,-478.183 175.067,-468.699 175.517,-459.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"179.017,-459.893 176.027,-449.729 172.026,-459.539 179.017,-459.893\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4528575488 -->\n",
       "<g class=\"node\" id=\"node15\"><title>4528575488</title>\n",
       "<polygon fill=\"none\" points=\"37.2104,-324.5 37.2104,-368.5 328.994,-368.5 328.994,-324.5 37.2104,-324.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"99.4434\" y=\"-342.3\">lambda_3: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"161.676,-324.5 161.676,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.511\" y=\"-353.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"161.676,-346.5 217.345,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.511\" y=\"-331.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"217.345,-324.5 217.345,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"273.169\" y=\"-353.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"217.345,-346.5 328.994,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"273.169\" y=\"-331.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4531846616&#45;&gt;4528575488 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>4531846616-&gt;4528575488</title>\n",
       "<path d=\"M178.71,-405.329C179.329,-397.183 180.049,-387.699 180.725,-378.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"184.222,-378.965 181.49,-368.729 177.242,-378.435 184.222,-378.965\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4528575488&#45;&gt;4532698304 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>4528575488-&gt;4532698304</title>\n",
       "<path d=\"M206.684,-324.329C216.844,-315.209 228.871,-304.412 239.765,-294.632\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"242.351,-297.014 247.455,-287.729 237.675,-291.805 242.351,-297.014\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4532759240 -->\n",
       "<g class=\"node\" id=\"node17\"><title>4532759240</title>\n",
       "<polygon fill=\"none\" points=\"94.1138,-162.5 94.1138,-206.5 448.09,-206.5 448.09,-162.5 94.1138,-162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187.443\" y=\"-180.3\">Norm-hvac-oven-dw: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"280.773,-162.5 280.773,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"308.607\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"280.773,-184.5 336.442,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"308.607\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"336.442,-162.5 336.442,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"392.266\" y=\"-191.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"336.442,-184.5 448.09,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"392.266\" y=\"-169.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4532698304&#45;&gt;4532759240 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>4532698304-&gt;4532759240</title>\n",
       "<path d=\"M271.102,-243.329C271.102,-235.183 271.102,-225.699 271.102,-216.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"274.602,-216.729 271.102,-206.729 267.602,-216.729 274.602,-216.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4529239880 -->\n",
       "<g class=\"node\" id=\"node18\"><title>4529239880</title>\n",
       "<polygon fill=\"none\" points=\"125.969,-81.5 125.969,-125.5 416.235,-125.5 416.235,-81.5 125.969,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187.443\" y=\"-99.3\">Conv-dw: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"248.917,-81.5 248.917,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"276.752\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"248.917,-103.5 304.586,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"276.752\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"304.586,-81.5 304.586,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360.411\" y=\"-110.3\">(None, 14, 24, 1)</text>\n",
       "<polyline fill=\"none\" points=\"304.586,-103.5 416.235,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360.411\" y=\"-88.3\">(None, 14, 24, 6)</text>\n",
       "</g>\n",
       "<!-- 4532759240&#45;&gt;4529239880 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>4532759240-&gt;4529239880</title>\n",
       "<path d=\"M271.102,-162.329C271.102,-154.183 271.102,-144.699 271.102,-135.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"274.602,-135.729 271.102,-125.729 267.602,-135.729 274.602,-135.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4532697520 -->\n",
       "<g class=\"node\" id=\"node19\"><title>4532697520</title>\n",
       "<polygon fill=\"none\" points=\"140.619,-0.5 140.619,-44.5 401.585,-44.5 401.585,-0.5 140.619,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187.443\" y=\"-18.3\">DW: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"234.268,-0.5 234.268,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.103\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"234.268,-22.5 289.937,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.103\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"289.937,-0.5 289.937,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345.761\" y=\"-29.3\">(None, 14, 24, 6)</text>\n",
       "<polyline fill=\"none\" points=\"289.937,-22.5 401.585,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345.761\" y=\"-7.3\">(None, 14, 24, 1)</text>\n",
       "</g>\n",
       "<!-- 4529239880&#45;&gt;4532697520 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>4529239880-&gt;4532697520</title>\n",
       "<path d=\"M271.102,-81.3294C271.102,-73.1826 271.102,-63.6991 271.102,-54.7971\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"274.602,-54.729 271.102,-44.729 267.602,-54.729 274.602,-54.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(autoencoder,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvac\n"
     ]
    }
   ],
   "source": [
    "appliance_num=1\n",
    "print(APPLIANCES_ORDER[appliance_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 5000.416633605957,\n",
       "  1: 3963.88330078125,\n",
       "  2: 288.85000610351562,\n",
       "  3: 960.7833251953125,\n",
       "  4: 1122.300048828125,\n",
       "  5: 498.43331909179688,\n",
       "  6: 2914.800048828125},\n",
       " ['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxs, APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_hvac = t_all[:30, 1, :, :].reshape(-1, num_days, num_hours,1) /4000.\n",
    "train_fridge = t_all[:30, 2, :, :].reshape(-1, num_days, num_hours,1) /4000.\n",
    "train_oven = t_all[:30, 6, :, :].reshape(-1, num_days, num_hours,1) /4000.\n",
    "train_dw = t_all[:30, 4, :, :].reshape(-1, num_days, num_hours,1) /4000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33.46529688 125.692656222 6.3002141955 14.4991161736\n",
      "1 33.0148366535 72.8441482975 6.3002141955 14.4991161736\n",
      "2 32.8252944023 89.7934506007 6.3002141955 14.4991161736\n",
      "3 32.8117162599 97.1607570471 6.3002141955 14.4991161736\n",
      "4 32.8881643732 85.2619380215 6.3002141955 14.4991161736\n",
      "5 33.03410384 73.0478036168 6.3002141955 14.4991161736\n",
      "6 33.1978095698 81.9145613197 6.3002141955 14.4991161736\n",
      "7 33.2593777495 90.4346546955 6.3002141955 14.4991161736\n",
      "8 33.1987483651 81.7994707397 6.3002141955 14.4991161736\n",
      "9 33.0760563819 72.8728634337 6.3002141955 14.4991161736\n",
      "10 32.9685061303 75.6728858417 6.3002141955 14.4991161736\n",
      "11 32.9085889859 79.9622196926 6.3002141955 14.4991161736\n",
      "12 32.8970342524 79.0907905376 6.3002141955 14.4991161736\n",
      "13 32.9247126244 74.5730876943 6.3002141955 14.4991161736\n",
      "14 32.9850819106 72.709032698 6.3002141955 14.4991161736\n",
      "15 33.0601998599 77.2492095145 6.3002141955 14.4991161736\n",
      "16 33.1213591894 80.2919852302 6.3002141955 14.4991161736\n",
      "17 33.1345214189 76.7998401095 6.3002141955 14.4991161736\n",
      "18 33.0967261229 72.7771789831 6.3002141955 14.4991161736\n",
      "19 33.0354018578 73.1722368981 6.3002141955 14.4991161736\n",
      "20 32.9800909696 74.8543022759 6.3002141955 14.4991161736\n",
      "21 32.9515210731 74.5592082717 6.3002141955 14.4991161736\n",
      "22 32.9503599024 72.8598448542 6.3002141955 14.4991161736\n",
      "23 32.9725062068 72.7768491689 6.3002141955 14.4991161736\n",
      "24 33.0045384881 75.1937085905 6.3002141955 14.4991161736\n",
      "25 33.0352075296 76.2180239669 6.3002141955 14.4991161736\n",
      "26 33.0491192135 74.2318921739 6.3002141955 14.4991161736\n",
      "27 33.0425386243 72.4992507364 6.3002141955 14.4991161736\n",
      "28 33.0226683266 72.7060408244 6.3002141955 14.4991161736\n",
      "29 33.0021114252 73.1803923056 6.3002141955 14.4991161736\n",
      "30 32.9892334268 72.7236389499 6.3002141955 14.4991161736\n",
      "31 32.9883007028 72.3715747084 6.3002141955 14.4991161736\n",
      "32 32.9965061863 73.3056572783 6.3002141955 14.4991161736\n",
      "33 33.0078420252 74.411724341 6.3002141955 14.4991161736\n",
      "34 33.0147452541 73.7693070386 6.3002141955 14.4991161736\n",
      "35 33.0130849809 72.5403643502 6.3002141955 14.4991161736\n",
      "36 33.006168527 72.2832562319 6.3002141955 14.4991161736\n",
      "37 32.9995890516 72.4749316218 6.3002141955 14.4991161736\n",
      "38 32.9977952392 72.3171904675 6.3002141955 14.4991161736\n",
      "39 32.9984128992 72.2700470751 6.3002141955 14.4991161736\n",
      "40 32.9973565696 72.8958751767 6.3002141955 14.4991161736\n",
      "41 32.9917329731 73.4326459873 6.3002141955 14.4991161736\n",
      "42 32.9825978341 72.9376719759 6.3002141955 14.4991161736\n",
      "43 32.9748521626 72.2715345813 6.3002141955 14.4991161736\n",
      "44 32.9752917827 72.1295912642 6.3002141955 14.4991161736\n",
      "45 32.9836697986 72.1408297354 6.3002141955 14.4991161736\n",
      "46 32.9986391759 72.0991671547 6.3002141955 14.4991161736\n",
      "47 33.007370312 72.3793653932 6.3002141955 14.4991161736\n",
      "48 33.0010863935 72.8361200064 6.3002141955 14.4991161736\n",
      "49 32.980687705 72.7154487958 6.3002141955 14.4991161736\n",
      "50 32.9574936077 72.2365664455 6.3002141955 14.4991161736\n",
      "51 32.9442829978 72.0071498521 6.3002141955 14.4991161736\n",
      "52 32.951392356 71.983574982 6.3002141955 14.4991161736\n",
      "53 32.9727930042 72.0155901979 6.3002141955 14.4991161736\n",
      "54 32.9955837866 72.2585077352 6.3002141955 14.4991161736\n",
      "55 32.9996454494 72.4870644268 6.3002141955 14.4991161736\n",
      "56 32.9804489417 72.3504308113 6.3002141955 14.4991161736\n",
      "57 32.9510341248 72.0730352931 6.3002141955 14.4991161736\n",
      "58 32.9320274912 71.9505958729 6.3002141955 14.4991161736\n",
      "59 32.9313488146 71.95003164 6.3002141955 14.4991161736\n",
      "60 32.9470493551 72.0539064124 6.3002141955 14.4991161736\n",
      "61 32.9648024247 72.24968863 6.3002141955 14.4991161736\n",
      "62 32.9637475117 72.2450124767 6.3002141955 14.4991161736\n",
      "63 32.9433429143 72.0373360051 6.3002141955 14.4991161736\n",
      "64 32.9136070968 71.8981245127 6.3002141955 14.4991161736\n",
      "65 32.8925316649 71.8781686176 6.3002141955 14.4991161736\n",
      "66 32.885648262 71.9818906162 6.3002141955 14.4991161736\n",
      "67 32.8993886741 72.2718071222 6.3002141955 14.4991161736\n",
      "68 32.9128032087 72.426414204 6.3002141955 14.4991161736\n",
      "69 32.9129589941 72.2231072603 6.3002141955 14.4991161736\n",
      "70 32.9002680681 71.9685748622 6.3002141955 14.4991161736\n",
      "71 32.8952399766 71.8922025389 6.3002141955 14.4991161736\n",
      "72 32.901914995 71.9808791829 6.3002141955 14.4991161736\n",
      "73 32.8904089918 72.1204610247 6.3002141955 14.4991161736\n",
      "74 32.8568640277 72.1107590684 6.3002141955 14.4991161736\n",
      "75 32.8413429126 72.0063630609 6.3002141955 14.4991161736\n",
      "76 32.8489414954 71.9198058135 6.3002141955 14.4991161736\n",
      "77 32.8651381866 71.8624540303 6.3002141955 14.4991161736\n",
      "78 32.8612614489 71.8115317005 6.3002141955 14.4991161736\n",
      "79 32.8393499495 71.798106908 6.3002141955 14.4991161736\n",
      "80 32.8185088676 71.855299053 6.3002141955 14.4991161736\n",
      "81 32.8255425496 71.9857333815 6.3002141955 14.4991161736\n",
      "82 32.8316823368 72.0003981858 6.3002141955 14.4991161736\n",
      "83 32.8108957712 71.8244932953 6.3002141955 14.4991161736\n",
      "84 32.8066196808 71.7366290876 6.3002141955 14.4991161736\n",
      "85 32.8243186944 71.7851154882 6.3002141955 14.4991161736\n",
      "86 32.8149529964 71.8352076559 6.3002141955 14.4991161736\n",
      "87 32.7834018113 71.8027869924 6.3002141955 14.4991161736\n",
      "88 32.7874484579 71.8195987997 6.3002141955 14.4991161736\n",
      "89 32.7957765974 71.79342564 6.3002141955 14.4991161736\n",
      "90 32.784337903 71.7141445999 6.3002141955 14.4991161736\n",
      "91 32.7692121056 71.6915765656 6.3002141955 14.4991161736\n",
      "92 32.768159181 71.7706827327 6.3002141955 14.4991161736\n",
      "93 32.7626709139 71.8224392629 6.3002141955 14.4991161736\n",
      "94 32.7473937356 71.7747008999 6.3002141955 14.4991161736\n",
      "95 32.7533692163 71.7502947798 6.3002141955 14.4991161736\n",
      "96 32.7473684111 71.7172785628 6.3002141955 14.4991161736\n",
      "97 32.7418215492 71.7353073249 6.3002141955 14.4991161736\n",
      "98 32.7333846087 71.767896433 6.3002141955 14.4991161736\n",
      "99 32.7207397145 71.7670557496 6.3002141955 14.4991161736\n",
      "100 32.7324985638 71.8073473906 6.3002141955 14.4991161736\n",
      "101 32.690481059 71.6845484201 6.3002141955 14.4991161736\n",
      "102 32.7515646916 71.8830352978 6.3002141955 14.4991161736\n",
      "103 32.6817819154 71.6473338208 6.3002141955 14.4991161736\n",
      "104 32.6868030876 71.719189694 6.3002141955 14.4991161736\n",
      "105 32.715298469 71.8882724449 6.3002141955 14.4991161736\n",
      "106 32.6491980956 71.6613751633 6.3002141955 14.4991161736\n",
      "107 32.6743119899 71.7994579281 6.3002141955 14.4991161736\n",
      "108 32.6662835693 71.8179996735 6.3002141955 14.4991161736\n",
      "109 32.6280666719 71.7255314367 6.3002141955 14.4991161736\n",
      "110 32.6613410252 71.8545399696 6.3002141955 14.4991161736\n",
      "111 32.6177522755 71.7327882261 6.3002141955 14.4991161736\n",
      "112 32.6364191786 71.8109484012 6.3002141955 14.4991161736\n",
      "113 32.5941284008 71.7135909867 6.3002141955 14.4991161736\n",
      "114 32.6440201054 71.9425725766 6.3002141955 14.4991161736\n",
      "115 32.5336885484 71.6010281961 6.3002141955 14.4991161736\n",
      "116 32.6724228441 72.1655410886 6.3002141955 14.4991161736\n",
      "117 32.5302524918 71.5841664553 6.3002141955 14.4991161736\n",
      "118 32.5988750861 71.8648851082 6.3002141955 14.4991161736\n",
      "119 32.5835968458 71.8969210738 6.3002141955 14.4991161736\n",
      "120 32.5052572923 71.6990486031 6.3002141955 14.4991161736\n",
      "121 32.6257086203 72.3178781962 6.3002141955 14.4991161736\n",
      "122 32.472383077 71.6491888996 6.3002141955 14.4991161736\n",
      "123 32.5821554785 72.1109702273 6.3002141955 14.4991161736\n",
      "124 32.5177290435 71.876403401 6.3002141955 14.4991161736\n",
      "125 32.4924105226 71.8339152361 6.3002141955 14.4991161736\n",
      "126 32.5746237406 72.2544559019 6.3002141955 14.4991161736\n",
      "127 32.4318517834 71.7057550864 6.3002141955 14.4991161736\n",
      "128 32.5904150474 72.4668281038 6.3002141955 14.4991161736\n",
      "129 32.4027634952 71.6485883933 6.3002141955 14.4991161736\n",
      "130 32.5992555474 72.553575241 6.3002141955 14.4991161736\n",
      "131 32.3839276725 71.6170733096 6.3002141955 14.4991161736\n",
      "132 32.5708261259 72.5253858882 6.3002141955 14.4991161736\n",
      "133 32.4155236818 71.788925037 6.3002141955 14.4991161736\n",
      "134 32.4697730877 72.077895714 6.3002141955 14.4991161736\n",
      "135 32.4752112158 72.107460881 6.3002141955 14.4991161736\n",
      "136 32.4118829501 71.7931096382 6.3002141955 14.4991161736\n",
      "137 32.5279737876 72.3196985253 6.3002141955 14.4991161736\n",
      "138 32.3683431933 71.6327690062 6.3002141955 14.4991161736\n",
      "139 32.5887035307 72.6892319943 6.3002141955 14.4991161736\n",
      "140 32.31263334 71.5310138694 6.3002141955 14.4991161736\n",
      "141 32.6954111935 73.6354466196 6.3002141955 14.4991161736\n",
      "142 32.2978557029 71.5428897747 6.3002141955 14.4991161736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 32.5576302995 72.787133889 6.3002141955 14.4991161736\n",
      "144 32.3864749134 71.8507236457 6.3002141955 14.4991161736\n",
      "145 32.3647905403 71.7736194663 6.3002141955 14.4991161736\n",
      "146 32.6020511157 73.0385347244 6.3002141955 14.4991161736\n",
      "147 32.2892537231 71.618427494 6.3002141955 14.4991161736\n",
      "148 32.6100351124 73.1763051519 6.3002141955 14.4991161736\n",
      "149 32.3355355557 71.7345238319 6.3002141955 14.4991161736\n",
      "150 32.3820104924 71.9459025675 6.3002141955 14.4991161736\n",
      "151 32.5055700453 72.6195345957 6.3002141955 14.4991161736\n",
      "152 32.2836255259 71.6077017552 6.3002141955 14.4991161736\n",
      "153 32.5301586116 72.786661986 6.3002141955 14.4991161736\n",
      "154 32.3268607081 71.757928046 6.3002141955 14.4991161736\n",
      "155 32.3723677913 71.9855681584 6.3002141955 14.4991161736\n",
      "156 32.4291344061 72.3013489928 6.3002141955 14.4991161736\n",
      "157 32.2990593962 71.7023277554 6.3002141955 14.4991161736\n",
      "158 32.4989947514 72.6845439453 6.3002141955 14.4991161736\n",
      "159 32.2613385441 71.6150898668 6.3002141955 14.4991161736\n",
      "160 32.5015820177 72.7930397197 6.3002141955 14.4991161736\n",
      "161 32.2695805975 71.6819121532 6.3002141955 14.4991161736\n",
      "162 32.4312297978 72.4902749674 6.3002141955 14.4991161736\n",
      "163 32.3078832596 71.8780372612 6.3002141955 14.4991161736\n",
      "164 32.3716053034 72.1937717872 6.3002141955 14.4991161736\n",
      "165 32.3393799237 72.0279791713 6.3002141955 14.4991161736\n",
      "166 32.3574645571 72.100214018 6.3002141955 14.4991161736\n",
      "167 32.3473117182 72.0378772244 6.3002141955 14.4991161736\n",
      "168 32.3518051978 72.0527481676 6.3002141955 14.4991161736\n",
      "169 32.3427231271 72.0225765254 6.3002141955 14.4991161736\n",
      "170 32.3942551474 72.2737208687 6.3002141955 14.4991161736\n",
      "171 32.2692535827 71.7893293608 6.3002141955 14.4991161736\n",
      "172 32.5998413228 73.3997827942 6.3002141955 14.4991161736\n",
      "173 32.2011064477 71.853550287 6.3002141955 14.4991161736\n",
      "174 33.0956734894 76.2285482338 6.3002141955 14.4991161736\n",
      "175 32.1996867848 72.0015115204 6.3002141955 14.4991161736\n",
      "176 32.4112303923 72.3934235263 6.3002141955 14.4991161736\n",
      "177 32.6442689571 73.5329265404 6.3002141955 14.4991161736\n",
      "178 32.2002586493 72.1266431284 6.3002141955 14.4991161736\n",
      "179 32.5417102643 73.0768348973 6.3002141955 14.4991161736\n",
      "180 32.4753184443 72.7721179419 6.3002141955 14.4991161736\n",
      "181 32.1892436181 71.9138122388 6.3002141955 14.4991161736\n",
      "182 32.5367399191 73.139929852 6.3002141955 14.4991161736\n",
      "183 32.401938439 72.4156239623 6.3002141955 14.4991161736\n",
      "184 32.191992687 71.7520218612 6.3002141955 14.4991161736\n",
      "185 32.4868545037 72.7906082444 6.3002141955 14.4991161736\n",
      "186 32.3652168203 72.1998408145 6.3002141955 14.4991161736\n",
      "187 32.2018702774 71.7083654256 6.3002141955 14.4991161736\n",
      "188 32.4348407112 72.6014014648 6.3002141955 14.4991161736\n",
      "189 32.3555400692 72.257525839 6.3002141955 14.4991161736\n",
      "190 32.2106425329 71.7202664321 6.3002141955 14.4991161736\n",
      "191 32.4310972283 72.6421617187 6.3002141955 14.4991161736\n",
      "192 32.3118332671 72.0388846786 6.3002141955 14.4991161736\n",
      "193 32.2346867334 71.7336186616 6.3002141955 14.4991161736\n",
      "194 32.401125926 72.3634639847 6.3002141955 14.4991161736\n",
      "195 32.2938007437 71.9127429158 6.3002141955 14.4991161736\n",
      "196 32.2492155178 71.7779545306 6.3002141955 14.4991161736\n",
      "197 32.3990811998 72.3880440585 6.3002141955 14.4991161736\n",
      "198 32.2725683606 71.9138023263 6.3002141955 14.4991161736\n",
      "199 32.2850679495 71.9915961131 6.3002141955 14.4991161736\n",
      "200 32.3917152779 72.4553492869 6.3002141955 14.4991161736\n",
      "201 32.2528816292 71.9100673888 6.3002141955 14.4991161736\n",
      "202 32.3723383709 72.3679430114 6.3002141955 14.4991161736\n",
      "203 32.3176224088 72.1399242634 6.3002141955 14.4991161736\n",
      "204 32.2827810999 72.0206584628 6.3002141955 14.4991161736\n",
      "205 32.4129457526 72.5235336347 6.3002141955 14.4991161736\n",
      "206 32.2531413594 71.9573421997 6.3002141955 14.4991161736\n",
      "207 32.4342653618 72.6021073028 6.3002141955 14.4991161736\n",
      "208 32.2525517673 71.9551605533 6.3002141955 14.4991161736\n",
      "209 32.4356154854 72.5662333489 6.3002141955 14.4991161736\n",
      "210 32.2587308698 71.9577925541 6.3002141955 14.4991161736\n",
      "211 32.4109429684 72.4640476461 6.3002141955 14.4991161736\n",
      "212 32.2812983675 72.0460228226 6.3002141955 14.4991161736\n",
      "213 32.3757366226 72.3813768504 6.3002141955 14.4991161736\n",
      "214 32.3025265215 72.1621203466 6.3002141955 14.4991161736\n",
      "215 32.3680226003 72.4165173619 6.3002141955 14.4991161736\n",
      "216 32.308100558 72.2213673738 6.3002141955 14.4991161736\n",
      "217 32.3667280272 72.430190308 6.3002141955 14.4991161736\n",
      "218 32.2991647241 72.1846958913 6.3002141955 14.4991161736\n",
      "219 32.3888352212 72.4922223981 6.3002141955 14.4991161736\n",
      "220 32.2509528229 72.0286139005 6.3002141955 14.4991161736\n",
      "221 32.5381456113 73.0442684142 6.3002141955 14.4991161736\n",
      "222 32.1765758558 71.9858802137 6.3002141955 14.4991161736\n",
      "223 32.7826655025 74.0695926374 6.3002141955 14.4991161736\n",
      "224 32.177286642 72.1835288602 6.3002141955 14.4991161736\n",
      "225 32.6255477373 73.4051936031 6.3002141955 14.4991161736\n",
      "226 32.2788512222 72.1855855337 6.3002141955 14.4991161736\n",
      "227 32.2689893881 72.1553664211 6.3002141955 14.4991161736\n",
      "228 32.5780085233 73.227371358 6.3002141955 14.4991161736\n",
      "229 32.180304436 72.0335416074 6.3002141955 14.4991161736\n",
      "230 32.5397033723 73.1184554757 6.3002141955 14.4991161736\n",
      "231 32.2961277663 72.2142000961 6.3002141955 14.4991161736\n",
      "232 32.275768936 72.1583497004 6.3002141955 14.4991161736\n",
      "233 32.5567776172 73.154347719 6.3002141955 14.4991161736\n",
      "234 32.1897578116 72.0610245997 6.3002141955 14.4991161736\n",
      "235 32.5269328738 72.9662345928 6.3002141955 14.4991161736\n",
      "236 32.3034188655 72.2240572064 6.3002141955 14.4991161736\n",
      "237 32.2815582247 72.1741391832 6.3002141955 14.4991161736\n",
      "238 32.5258422079 72.9666404176 6.3002141955 14.4991161736\n",
      "239 32.2043012433 72.0560583889 6.3002141955 14.4991161736\n",
      "240 32.4927374636 72.8735055265 6.3002141955 14.4991161736\n",
      "241 32.2861045166 72.1590887657 6.3002141955 14.4991161736\n",
      "242 32.3253935375 72.2764656037 6.3002141955 14.4991161736\n",
      "243 32.4211213424 72.6181686919 6.3002141955 14.4991161736\n",
      "244 32.2401992073 72.0670075738 6.3002141955 14.4991161736\n",
      "245 32.5007599856 72.9220127523 6.3002141955 14.4991161736\n",
      "246 32.2290818707 72.0760458626 6.3002141955 14.4991161736\n",
      "247 32.4507176489 72.7386439662 6.3002141955 14.4991161736\n",
      "248 32.3154041664 72.2907802848 6.3002141955 14.4991161736\n",
      "249 32.324374956 72.3217697471 6.3002141955 14.4991161736\n",
      "250 32.421818982 72.6522220695 6.3002141955 14.4991161736\n",
      "251 32.2546717992 72.1427108064 6.3002141955 14.4991161736\n",
      "252 32.4895696919 72.905242252 6.3002141955 14.4991161736\n",
      "253 32.2290508381 72.0858797172 6.3002141955 14.4991161736\n",
      "254 32.500392293 72.9263261841 6.3002141955 14.4991161736\n",
      "255 32.234047626 72.1042886683 6.3002141955 14.4991161736\n",
      "256 32.4872482814 72.8887833111 6.3002141955 14.4991161736\n",
      "257 32.2371947735 72.1604282108 6.3002141955 14.4991161736\n",
      "258 32.4637819782 72.8558784109 6.3002141955 14.4991161736\n",
      "259 32.2688795313 72.2591536433 6.3002141955 14.4991161736\n",
      "260 32.4084231193 72.7013019085 6.3002141955 14.4991161736\n",
      "261 32.2947060799 72.3377323714 6.3002141955 14.4991161736\n",
      "262 32.4115569551 72.7276386434 6.3002141955 14.4991161736\n",
      "263 32.2710460002 72.2796437482 6.3002141955 14.4991161736\n",
      "264 32.4948634059 73.0159927888 6.3002141955 14.4991161736\n",
      "265 32.2125876606 72.1976111925 6.3002141955 14.4991161736\n",
      "266 32.6453899089 73.5699962275 6.3002141955 14.4991161736\n",
      "267 32.1824890623 72.2685743048 6.3002141955 14.4991161736\n",
      "268 32.6804698123 73.6669155599 6.3002141955 14.4991161736\n",
      "269 32.1926852057 72.3216130514 6.3002141955 14.4991161736\n",
      "270 32.473075232 72.9304522301 6.3002141955 14.4991161736\n",
      "271 32.3453422961 72.5436442917 6.3002141955 14.4991161736\n",
      "272 32.2614055351 72.3284421658 6.3002141955 14.4991161736\n",
      "273 32.5393061938 73.2027160516 6.3002141955 14.4991161736\n",
      "274 32.1818216936 72.1822325692 6.3002141955 14.4991161736\n",
      "275 32.628705322 73.6025417683 6.3002141955 14.4991161736\n",
      "276 32.2094143862 72.2530146773 6.3002141955 14.4991161736\n",
      "277 32.4627512018 72.9843014546 6.3002141955 14.4991161736\n",
      "278 32.319369487 72.5174704941 6.3002141955 14.4991161736\n",
      "279 32.3485193815 72.5785467213 6.3002141955 14.4991161736\n",
      "280 32.4016650358 72.6917240438 6.3002141955 14.4991161736\n",
      "281 32.2573304342 72.3105763163 6.3002141955 14.4991161736\n",
      "282 32.4886987174 72.9344490376 6.3002141955 14.4991161736\n",
      "283 32.1950858145 72.178475293 6.3002141955 14.4991161736\n",
      "284 32.5386187767 73.1708571477 6.3002141955 14.4991161736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 32.203093969 72.1893989324 6.3002141955 14.4991161736\n",
      "286 32.4608831117 72.9897910178 6.3002141955 14.4991161736\n",
      "287 32.2587932436 72.339012296 6.3002141955 14.4991161736\n",
      "288 32.4044833096 72.7723788052 6.3002141955 14.4991161736\n",
      "289 32.2674012747 72.3402996996 6.3002141955 14.4991161736\n",
      "290 32.412631408 72.7218347055 6.3002141955 14.4991161736\n",
      "291 32.2483375073 72.2847642086 6.3002141955 14.4991161736\n",
      "292 32.4740143979 72.8754015998 6.3002141955 14.4991161736\n",
      "293 32.2077096211 72.2535974929 6.3002141955 14.4991161736\n",
      "294 32.5500785598 73.1839416449 6.3002141955 14.4991161736\n",
      "295 32.206430861 72.2853826583 6.3002141955 14.4991161736\n",
      "296 32.5438776264 73.2220419917 6.3002141955 14.4991161736\n",
      "297 32.2108262747 72.2877962753 6.3002141955 14.4991161736\n",
      "298 32.5283083261 73.1597086771 6.3002141955 14.4991161736\n",
      "299 32.2185713043 72.2883328782 6.3002141955 14.4991161736\n",
      "300 32.4993728338 73.0195801312 6.3002141955 14.4991161736\n",
      "301 32.2309849743 72.3060624641 6.3002141955 14.4991161736\n",
      "302 32.4840383515 72.9512116491 6.3002141955 14.4991161736\n",
      "303 32.2582592696 72.3562027968 6.3002141955 14.4991161736\n",
      "304 32.4608784027 72.8694757271 6.3002141955 14.4991161736\n",
      "305 32.2597928553 72.3500460935 6.3002141955 14.4991161736\n",
      "306 32.5065967036 72.9894838526 6.3002141955 14.4991161736\n",
      "307 32.2195197742 72.2860291198 6.3002141955 14.4991161736\n",
      "308 32.643762459 73.4115647871 6.3002141955 14.4991161736\n",
      "309 32.1923530546 72.3144907661 6.3002141955 14.4991161736\n",
      "310 32.6657635149 73.4634849815 6.3002141955 14.4991161736\n",
      "311 32.2212171447 72.3440344678 6.3002141955 14.4991161736\n",
      "312 32.5125446975 72.9806657903 6.3002141955 14.4991161736\n",
      "313 32.3034834418 72.4291433865 6.3002141955 14.4991161736\n",
      "314 32.3877785851 72.5885296431 6.3002141955 14.4991161736\n",
      "315 32.3908082877 72.5843944564 6.3002141955 14.4991161736\n",
      "316 32.2910936158 72.3495284364 6.3002141955 14.4991161736\n",
      "317 32.5170561654 72.9683629999 6.3002141955 14.4991161736\n",
      "318 32.2129434059 72.2104978988 6.3002141955 14.4991161736\n",
      "319 32.6518662647 73.386767562 6.3002141955 14.4991161736\n",
      "320 32.1961924705 72.2297127359 6.3002141955 14.4991161736\n",
      "321 32.5727006858 73.1049384313 6.3002141955 14.4991161736\n",
      "322 32.2680913303 72.3417259669 6.3002141955 14.4991161736\n",
      "323 32.3923962529 72.591231312 6.3002141955 14.4991161736\n",
      "324 32.4247361597 72.666088439 6.3002141955 14.4991161736\n",
      "325 32.2386253175 72.2541177125 6.3002141955 14.4991161736\n",
      "326 32.6195165729 73.2294722233 6.3002141955 14.4991161736\n",
      "327 32.1910205424 72.1441737348 6.3002141955 14.4991161736\n",
      "328 32.5974612696 73.142595806 6.3002141955 14.4991161736\n",
      "329 32.2329693914 72.1952198497 6.3002141955 14.4991161736\n",
      "330 32.4142332408 72.6101118588 6.3002141955 14.4991161736\n",
      "331 32.3504921276 72.4729134404 6.3002141955 14.4991161736\n",
      "332 32.3365985305 72.4594731459 6.3002141955 14.4991161736\n",
      "333 32.4310594163 72.7031287397 6.3002141955 14.4991161736\n",
      "334 32.2508438408 72.2798802664 6.3002141955 14.4991161736\n",
      "335 32.5698507875 73.0925246268 6.3002141955 14.4991161736\n",
      "336 32.2013156399 72.1831254137 6.3002141955 14.4991161736\n",
      "337 32.6327364316 73.1832855366 6.3002141955 14.4991161736\n",
      "338 32.1989606619 72.1875696192 6.3002141955 14.4991161736\n",
      "339 32.5687888582 72.9982262681 6.3002141955 14.4991161736\n",
      "340 32.2521738243 72.2694356284 6.3002141955 14.4991161736\n",
      "341 32.4406956151 72.6824102066 6.3002141955 14.4991161736\n",
      "342 32.332265729 72.4398061415 6.3002141955 14.4991161736\n",
      "343 32.3750199204 72.5328944086 6.3002141955 14.4991161736\n",
      "344 32.3971518237 72.587822237 6.3002141955 14.4991161736\n",
      "345 32.295553192 72.3494386683 6.3002141955 14.4991161736\n",
      "346 32.5303725794 72.9489022746 6.3002141955 14.4991161736\n",
      "347 32.1970753536 72.2019162926 6.3002141955 14.4991161736\n",
      "348 32.6941438841 73.377788802 6.3002141955 14.4991161736\n",
      "349 32.1861684926 72.2372463712 6.3002141955 14.4991161736\n",
      "350 32.6030804384 73.1251302203 6.3002141955 14.4991161736\n",
      "351 32.2598655487 72.3125498843 6.3002141955 14.4991161736\n",
      "352 32.4100493286 72.6179896783 6.3002141955 14.4991161736\n",
      "353 32.4116213012 72.6233563315 6.3002141955 14.4991161736\n",
      "354 32.2755489458 72.3266763078 6.3002141955 14.4991161736\n",
      "355 32.5636084421 73.0290238466 6.3002141955 14.4991161736\n",
      "356 32.201955126 72.2261939187 6.3002141955 14.4991161736\n",
      "357 32.6521267974 73.211409746 6.3002141955 14.4991161736\n",
      "358 32.2152733968 72.2184572548 6.3002141955 14.4991161736\n",
      "359 32.4671736182 72.665471595 6.3002141955 14.4991161736\n",
      "360 32.3382893502 72.3663875818 6.3002141955 14.4991161736\n",
      "361 32.3215297482 72.3472920858 6.3002141955 14.4991161736\n",
      "362 32.4622403461 72.7152031473 6.3002141955 14.4991161736\n",
      "363 32.2334048686 72.2333525538 6.3002141955 14.4991161736\n",
      "364 32.6072647784 73.1953647328 6.3002141955 14.4991161736\n",
      "365 32.2120436915 72.2380886885 6.3002141955 14.4991161736\n",
      "366 32.5423847644 72.9733157652 6.3002141955 14.4991161736\n",
      "367 32.2662827933 72.2861690274 6.3002141955 14.4991161736\n",
      "368 32.4345410077 72.6154653306 6.3002141955 14.4991161736\n",
      "369 32.3160423067 72.3422457837 6.3002141955 14.4991161736\n",
      "370 32.4082285966 72.5205445598 6.3002141955 14.4991161736\n",
      "371 32.3170378766 72.3314260442 6.3002141955 14.4991161736\n",
      "372 32.4239646909 72.5856393313 6.3002141955 14.4991161736\n",
      "373 32.3009524955 72.3220898463 6.3002141955 14.4991161736\n",
      "374 32.4876121789 72.7916237523 6.3002141955 14.4991161736\n",
      "375 32.232702553 72.2083479486 6.3002141955 14.4991161736\n",
      "376 32.650767843 73.2122468641 6.3002141955 14.4991161736\n",
      "377 32.1866705779 72.2077606516 6.3002141955 14.4991161736\n",
      "378 32.689020899 73.2859158326 6.3002141955 14.4991161736\n",
      "379 32.2103848256 72.2437116316 6.3002141955 14.4991161736\n",
      "380 32.5028132505 72.7808038357 6.3002141955 14.4991161736\n",
      "381 32.3127030006 72.3646571214 6.3002141955 14.4991161736\n",
      "382 32.3963540019 72.54050879 6.3002141955 14.4991161736\n",
      "383 32.3902898423 72.543064356 6.3002141955 14.4991161736\n",
      "384 32.340583485 72.4422740633 6.3002141955 14.4991161736\n",
      "385 32.4365198066 72.672219987 6.3002141955 14.4991161736\n",
      "386 32.2846330854 72.3214613988 6.3002141955 14.4991161736\n",
      "387 32.5432395564 72.9147886394 6.3002141955 14.4991161736\n",
      "388 32.2085743245 72.2275129263 6.3002141955 14.4991161736\n",
      "389 32.6718223596 73.2125080323 6.3002141955 14.4991161736\n",
      "390 32.1912099082 72.2175743121 6.3002141955 14.4991161736\n",
      "391 32.6029560112 73.0168101501 6.3002141955 14.4991161736\n",
      "392 32.2553219739 72.2602345728 6.3002141955 14.4991161736\n",
      "393 32.4646756624 72.7015546272 6.3002141955 14.4991161736\n",
      "394 32.3098790723 72.3610364139 6.3002141955 14.4991161736\n",
      "395 32.4408169144 72.6638174003 6.3002141955 14.4991161736\n",
      "396 32.305256198 72.3335662427 6.3002141955 14.4991161736\n",
      "397 32.4646743701 72.6828596523 6.3002141955 14.4991161736\n",
      "398 32.2521910162 72.2150803861 6.3002141955 14.4991161736\n",
      "399 32.5791347174 72.9530608074 6.3002141955 14.4991161736\n",
      "400 32.202522538 72.2068161489 6.3002141955 14.4991161736\n",
      "401 32.6649909987 73.2151235477 6.3002141955 14.4991161736\n",
      "402 32.2058627812 72.252585358 6.3002141955 14.4991161736\n",
      "403 32.5408345717 72.8879108376 6.3002141955 14.4991161736\n",
      "404 32.2945319941 72.3294454472 6.3002141955 14.4991161736\n",
      "405 32.3917818901 72.5156690003 6.3002141955 14.4991161736\n",
      "406 32.3997332689 72.5301437892 6.3002141955 14.4991161736\n",
      "407 32.2918063568 72.2922616866 6.3002141955 14.4991161736\n",
      "408 32.5498991524 72.9031928448 6.3002141955 14.4991161736\n",
      "409 32.198987869 72.1909489476 6.3002141955 14.4991161736\n",
      "410 32.717391218 73.3489918241 6.3002141955 14.4991161736\n",
      "411 32.186722049 72.2175898149 6.3002141955 14.4991161736\n",
      "412 32.6186011031 73.0392867838 6.3002141955 14.4991161736\n",
      "413 32.2503532068 72.2534458159 6.3002141955 14.4991161736\n",
      "414 32.4056644579 72.530864936 6.3002141955 14.4991161736\n",
      "415 32.4132364124 72.5457807138 6.3002141955 14.4991161736\n",
      "416 32.2624743558 72.241300306 6.3002141955 14.4991161736\n",
      "417 32.5903243534 72.9930845262 6.3002141955 14.4991161736\n",
      "418 32.195081859 72.1830450554 6.3002141955 14.4991161736\n",
      "419 32.6810430629 73.265242667 6.3002141955 14.4991161736\n",
      "420 32.2077668875 72.191303938 6.3002141955 14.4991161736\n",
      "421 32.5004324364 72.7471255671 6.3002141955 14.4991161736\n",
      "422 32.280577027 72.2543316499 6.3002141955 14.4991161736\n",
      "423 32.4160332285 72.5302322857 6.3002141955 14.4991161736\n",
      "424 32.329102863 72.3493453764 6.3002141955 14.4991161736\n",
      "425 32.3978523463 72.5036906656 6.3002141955 14.4991161736\n",
      "426 32.3204644878 72.3367576901 6.3002141955 14.4991161736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427 32.4608434909 72.6297194613 6.3002141955 14.4991161736\n",
      "428 32.2335093935 72.1624651799 6.3002141955 14.4991161736\n",
      "429 32.6101014408 72.9762217116 6.3002141955 14.4991161736\n",
      "430 32.1847045804 72.1504467655 6.3002141955 14.4991161736\n",
      "431 32.6762418654 73.1682907515 6.3002141955 14.4991161736\n",
      "432 32.1977505684 72.1825102742 6.3002141955 14.4991161736\n",
      "433 32.5621103511 72.8947844603 6.3002141955 14.4991161736\n",
      "434 32.2726299164 72.2614351449 6.3002141955 14.4991161736\n",
      "435 32.4313544335 72.5560929618 6.3002141955 14.4991161736\n",
      "436 32.3144637272 72.3028050851 6.3002141955 14.4991161736\n",
      "437 32.4385950875 72.5369773825 6.3002141955 14.4991161736\n",
      "438 32.2550442653 72.1603103366 6.3002141955 14.4991161736\n",
      "439 32.5440967432 72.7892086821 6.3002141955 14.4991161736\n",
      "440 32.1908322547 72.1073232811 6.3002141955 14.4991161736\n",
      "441 32.6760734632 73.1655979745 6.3002141955 14.4991161736\n",
      "442 32.1825240407 72.1518625065 6.3002141955 14.4991161736\n",
      "443 32.624196507 73.0621347837 6.3002141955 14.4991161736\n",
      "444 32.2257023472 72.1874391364 6.3002141955 14.4991161736\n",
      "445 32.4891854716 72.6862814373 6.3002141955 14.4991161736\n",
      "446 32.2602638548 72.1992944231 6.3002141955 14.4991161736\n",
      "447 32.4812910846 72.6166304046 6.3002141955 14.4991161736\n",
      "448 32.2218320522 72.1154916841 6.3002141955 14.4991161736\n",
      "449 32.5800801686 72.8508965678 6.3002141955 14.4991161736\n",
      "450 32.1940751556 72.0976556068 6.3002141955 14.4991161736\n",
      "451 32.5910147047 72.9032446335 6.3002141955 14.4991161736\n",
      "452 32.2190387585 72.1194692818 6.3002141955 14.4991161736\n",
      "453 32.4935745819 72.6734227557 6.3002141955 14.4991161736\n",
      "454 32.24947786 72.1641177668 6.3002141955 14.4991161736\n",
      "455 32.4755112937 72.6264570355 6.3002141955 14.4991161736\n",
      "456 32.2456861495 72.159696939 6.3002141955 14.4991161736\n",
      "457 32.5363239819 72.7847515381 6.3002141955 14.4991161736\n",
      "458 32.2103838699 72.1166777067 6.3002141955 14.4991161736\n",
      "459 32.592149612 72.9026718683 6.3002141955 14.4991161736\n",
      "460 32.2080814578 72.0970875047 6.3002141955 14.4991161736\n",
      "461 32.5361718175 72.7309648679 6.3002141955 14.4991161736\n",
      "462 32.239790603 72.1326618525 6.3002141955 14.4991161736\n",
      "463 32.4864377209 72.618726195 6.3002141955 14.4991161736\n",
      "464 32.265067226 72.2007446348 6.3002141955 14.4991161736\n",
      "465 32.494279606 72.6621434084 6.3002141955 14.4991161736\n",
      "466 32.239337899 72.167719946 6.3002141955 14.4991161736\n",
      "467 32.5709246173 72.8387116604 6.3002141955 14.4991161736\n",
      "468 32.1979584728 72.1225237355 6.3002141955 14.4991161736\n",
      "469 32.6340548931 73.0026393494 6.3002141955 14.4991161736\n",
      "470 32.1965081132 72.1140712384 6.3002141955 14.4991161736\n",
      "471 32.5814504363 72.8524378173 6.3002141955 14.4991161736\n",
      "472 32.2376922676 72.1301967631 6.3002141955 14.4991161736\n",
      "473 32.4713087717 72.5765368243 6.3002141955 14.4991161736\n",
      "474 32.2760631507 72.1954122634 6.3002141955 14.4991161736\n",
      "475 32.4381407714 72.5247798285 6.3002141955 14.4991161736\n",
      "476 32.2755744757 72.2057136114 6.3002141955 14.4991161736\n",
      "477 32.4794987028 72.6204625662 6.3002141955 14.4991161736\n",
      "478 32.2320397618 72.1193610588 6.3002141955 14.4991161736\n",
      "479 32.5763448276 72.8239107807 6.3002141955 14.4991161736\n",
      "480 32.1858354941 72.076343437 6.3002141955 14.4991161736\n",
      "481 32.6675365975 73.0383068138 6.3002141955 14.4991161736\n",
      "482 32.1726620298 72.131366851 6.3002141955 14.4991161736\n",
      "483 32.672504928 73.0666621582 6.3002141955 14.4991161736\n",
      "484 32.1933478804 72.1451483034 6.3002141955 14.4991161736\n",
      "485 32.5571632099 72.7786985988 6.3002141955 14.4991161736\n",
      "486 32.2507545905 72.1626704845 6.3002141955 14.4991161736\n",
      "487 32.410365218 72.4571307666 6.3002141955 14.4991161736\n",
      "488 32.3471054448 72.3356312526 6.3002141955 14.4991161736\n",
      "489 32.3314690768 72.3079292135 6.3002141955 14.4991161736\n",
      "490 32.413846169 72.4802980662 6.3002141955 14.4991161736\n",
      "491 32.2570794579 72.1544714745 6.3002141955 14.4991161736\n",
      "492 32.5477634336 72.784200727 6.3002141955 14.4991161736\n",
      "493 32.1847760224 72.0776878424 6.3002141955 14.4991161736\n",
      "494 32.7446113196 73.2861387768 6.3002141955 14.4991161736\n",
      "495 32.1669899778 72.1875532668 6.3002141955 14.4991161736\n",
      "496 32.6926424741 73.1277172754 6.3002141955 14.4991161736\n",
      "497 32.2130508387 72.1730903131 6.3002141955 14.4991161736\n",
      "498 32.4583261666 72.5430161191 6.3002141955 14.4991161736\n",
      "499 32.3221229743 72.272225303 6.3002141955 14.4991161736\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_absolute_error',loss_weights=[1, 18, 18,18])\n",
    "for e in range(500):\n",
    "    autoencoder.fit((t_all[:30, 0, :, :].reshape(-1, num_days, num_hours,1)),[train_hvac, train_fridge, train_oven, train_dw]\n",
    "                ,validation_split=0.1,epochs=1,verbose=0)\n",
    "    pred = autoencoder.predict(t_all[30:, 0, :, :].reshape(-1, num_days, num_hours,1))\n",
    "    pred_hvac = 4000.*pred[0]\n",
    "    pred_fridge = 4000.*pred[1]\n",
    "    pred_oven = 4000.*pred[2]\n",
    "    pred_dw = 4000.*pred[3]\n",
    "    gt_hvac =t_all[30:, 1, :, :]\n",
    "    gt_fridge =t_all[30:, 2, :, :]\n",
    "    gt_oven =t_all[30:, 3, :, :]\n",
    "    gt_dw =t_all[30:, 4, :, :]\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    print(e, mean_absolute_error(gt_fridge.flatten(), pred_fridge.flatten()),\n",
    "          mean_absolute_error(gt_hvac.flatten(), pred_hvac.flatten()),\n",
    "         mean_absolute_error(gt_oven.flatten(), pred_oven.flatten()),\n",
    "         mean_absolute_error(gt_dw.flatten(), pred_dw.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = autoencoder.predict(t_all[30:, 0, :, :].reshape(-1, num_days, num_hours,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_hvac = maxs[1]*pred[0]\n",
    "pred_fridge = maxs[2]*pred[1]\n",
    "\n",
    "gt_hvac =t_all[30:, 1, :, :]\n",
    "gt_fridge =t_all[30:, 2, :, :]\n",
    "\n",
    "#pred_hvac = 1.*autoencoder.predict(t_all[30:, 0, :, :].reshape(-1, num_days, num_hours,1))[:, :14,:, :]\n",
    "#pred_fridge = 1.*autoencoder.predict(t_all[30:, 0, :, :].reshape(-1, num_days, num_hours,1))[:, 14:28,:, :]\n",
    "\n",
    "gt_hvac =t_all[30:, 1, :, :]\n",
    "gt_fridge =t_all[30:, 2, :, :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0e42e7d0de18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mappliance_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_days\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hours\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_days\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hours\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mt_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappliance_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred_fl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "pred = maxs[appliance_num]*autoencoder.predict(t_all[30:, 0, :, :].reshape(-1, num_days, num_hours,1)).reshape(-1, num_days, num_hours)\n",
    "gt =t_all[30:, appliance_num, :, :]\n",
    "\n",
    "\n",
    "pred_fl = pred.flatten()\n",
    "gt_fl = gt.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-85ed59065747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_days\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hours\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "pred.shape, (t_all[:30, 0, :, :].reshape(-1, num_days, num_hours,1)/maxs[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.00008459583789"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(gt_fridge.flatten(), pred_fridge.flatten())\n",
    "mean_absolute_error(gt_hvac.flatten(), pred_hvac.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ab72a69151de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_overall_appliance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_days\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hours\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_overall_appliance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Predicted HVAC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GT HVAC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#pd.DataFrame(t_all[31,4, 3, :]).squeeze().plot(ax=ax,label=\"GT DW\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "pred_overall_appliance = (1.*autoencoder.predict(t_all[30:, 0, :, :].reshape(-1, num_days, num_hours,1))).reshape(-1,14,24)\n",
    "ax = pd.DataFrame(pred_overall_appliance[1, 3, :]).squeeze().plot(label='Predicted HVAC')\n",
    "pd.DataFrame(gt[1, 3, :]).squeeze().plot(ax=ax,label=\"GT HVAC\")\n",
    "#pd.DataFrame(t_all[31,4, 3, :]).squeeze().plot(ax=ax,label=\"GT DW\")\n",
    "plt.legend()\n",
    "plt.savefig(\"/Users/nipun/Desktop/hvac-cnn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 14, 24)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a34bdadd8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRpXoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a34b0a160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(pred_hvac[0, 1,:,:].flatten()).plot()\n",
    "pd.Series(gt_hvac[0,1,:].flatten()).plot()\n",
    "pd.Series(gt_oven[0,1,:].flatten()).plot()\n",
    "pd.Series(pred_oven[0,1,:].flatten()).plot()\n",
    "pd.Series(pred_dw[0,1,:].flatten()).plot(label='Pred DW')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_oven.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGBNglgSMMSbALAkYY0yA/Q/RkK/2tflW/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3972e940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_overall_appliance = (maxs[appliance_num]*autoencoder.predict(t_all[30:, 0, :, :].reshape(-1, num_days, num_hours,1))).reshape(-1,14,24)\n",
    "ax = pd.DataFrame(pred_overall_appliance[1, 3, :]).squeeze().plot(label='Predicted HVAC')\n",
    "pd.DataFrame(gt[1, 3, :]).squeeze().plot(ax=ax,label=\"GT HVAC\")\n",
    "pd.DataFrame(t_all[31,4, 3, :]).squeeze().plot(ax=ax,label=\"GT DW\")\n",
    "plt.legend()\n",
    "plt.savefig(\"/Users/nipun/Desktop/hvac-dw-cnn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a42001ef0>]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKaVs6v8DpJPza5aLVqIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a41cbcc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(autoencoder.layers[2].get_weights()[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg = t_all[:30, 0, :].reshape(30*14, 24)\n",
    "train_appliance = {}\n",
    "test_appliance = {}\n",
    "for appliance_num, appliance in enumerate(APPLIANCES_ORDER[1:]):\n",
    "    train_appliance[appliance] = t_all[:30, appliance_num+1, :].reshape(30*14, 24)\n",
    "    test_appliance[appliance] = t_all[30:, appliance_num+1, :].reshape(22*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_hvac = t_all[30:, 1, :].reshape(22*14, 24)\n",
    "test_fridge = t_all[30:, 2, :].reshape(22*14, 24)\n",
    "\n",
    "test_mw = t_all[30:, 3, :].reshape(22*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "test_agg = t_all[30:, 0, :].reshape(22*14, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_hvac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-122380d9c71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_hvac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_hvac' is not defined"
     ]
    }
   ],
   "source": [
    "train_hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_hvac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-26ef9ccd3f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_hvac_fridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_hvac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fridge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_hvac_fridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_hvac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_fridge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_hvac' is not defined"
     ]
    }
   ],
   "source": [
    "train_hvac_fridge = np.hstack([train_hvac, train_fridge])\n",
    "test_hvac_fridge = np.hstack([test_hvac, test_fridge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fridge\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/500\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 104.0335 - val_loss: 80.1024\n",
      "Epoch 2/500\n",
      "378/378 [==============================] - 0s 128us/step - loss: 73.5439 - val_loss: 82.9650\n",
      "Epoch 3/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 69.5276 - val_loss: 71.1884\n",
      "Epoch 4/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 65.7872 - val_loss: 66.9487\n",
      "Epoch 5/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 61.5952 - val_loss: 60.5597\n",
      "Epoch 6/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 57.9371 - val_loss: 58.2805\n",
      "Epoch 7/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 54.5536 - val_loss: 53.5956\n",
      "Epoch 8/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 52.0083 - val_loss: 48.1487\n",
      "Epoch 9/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 49.8050 - val_loss: 47.3854\n",
      "Epoch 10/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 48.4076 - val_loss: 50.2753\n",
      "Epoch 11/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 48.1385 - val_loss: 44.8737\n",
      "Epoch 12/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 47.0126 - val_loss: 49.4019\n",
      "Epoch 13/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 46.6314 - val_loss: 44.6897\n",
      "Epoch 14/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 45.7859 - val_loss: 47.2255\n",
      "Epoch 15/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 45.4930 - val_loss: 45.9505\n",
      "Epoch 16/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 44.4689 - val_loss: 46.0596\n",
      "Epoch 17/500\n",
      "378/378 [==============================] - 0s 148us/step - loss: 43.9302 - val_loss: 43.3321\n",
      "Epoch 18/500\n",
      "378/378 [==============================] - 0s 140us/step - loss: 44.4852 - val_loss: 46.0947\n",
      "Epoch 19/500\n",
      "378/378 [==============================] - 0s 149us/step - loss: 44.3549 - val_loss: 44.9310\n",
      "Epoch 20/500\n",
      "378/378 [==============================] - 0s 174us/step - loss: 44.6102 - val_loss: 43.7336\n",
      "Epoch 21/500\n",
      "378/378 [==============================] - 0s 160us/step - loss: 44.0030 - val_loss: 48.1959\n",
      "Epoch 22/500\n",
      "378/378 [==============================] - 0s 158us/step - loss: 43.8427 - val_loss: 46.0852\n",
      "Epoch 23/500\n",
      "378/378 [==============================] - 0s 164us/step - loss: 43.7722 - val_loss: 45.0606\n",
      "Epoch 24/500\n",
      "378/378 [==============================] - 0s 156us/step - loss: 43.6814 - val_loss: 45.7697\n",
      "Epoch 25/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 43.6771 - val_loss: 46.1566\n",
      "Epoch 26/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 43.5886 - val_loss: 47.4341\n",
      "Epoch 27/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 42.9120 - val_loss: 43.5555\n",
      "Epoch 28/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 42.4706 - val_loss: 47.4463\n",
      "Epoch 29/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 42.8407 - val_loss: 44.2160\n",
      "Epoch 30/500\n",
      "378/378 [==============================] - 0s 149us/step - loss: 42.2783 - val_loss: 42.4520\n",
      "Epoch 31/500\n",
      "378/378 [==============================] - 0s 160us/step - loss: 43.3057 - val_loss: 46.7205\n",
      "Epoch 32/500\n",
      "378/378 [==============================] - 0s 184us/step - loss: 42.4322 - val_loss: 45.1091\n",
      "Epoch 33/500\n",
      "378/378 [==============================] - 0s 142us/step - loss: 42.2389 - val_loss: 40.7218\n",
      "Epoch 34/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 42.0600 - val_loss: 49.7435\n",
      "Epoch 35/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 42.2471 - val_loss: 44.0191\n",
      "Epoch 36/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 42.5494 - val_loss: 45.9218\n",
      "Epoch 37/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 41.5170 - val_loss: 41.8289\n",
      "Epoch 38/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 42.4506 - val_loss: 47.9723\n",
      "Epoch 39/500\n",
      "378/378 [==============================] - 0s 165us/step - loss: 42.6541 - val_loss: 45.8354\n",
      "Epoch 40/500\n",
      "378/378 [==============================] - 0s 177us/step - loss: 42.0716 - val_loss: 41.1500\n",
      "Epoch 41/500\n",
      "378/378 [==============================] - 0s 136us/step - loss: 41.4746 - val_loss: 48.8116\n",
      "Epoch 42/500\n",
      "378/378 [==============================] - 0s 136us/step - loss: 41.2513 - val_loss: 42.3331\n",
      "Epoch 43/500\n",
      "378/378 [==============================] - 0s 140us/step - loss: 41.2785 - val_loss: 45.9130\n",
      "Epoch 44/500\n",
      "378/378 [==============================] - 0s 141us/step - loss: 41.6845 - val_loss: 44.5290\n",
      "Epoch 45/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 41.2962 - val_loss: 42.9995\n",
      "Epoch 46/500\n",
      "378/378 [==============================] - 0s 137us/step - loss: 40.9711 - val_loss: 45.8175\n",
      "Epoch 47/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 40.8149 - val_loss: 41.8467\n",
      "Epoch 48/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 41.4019 - val_loss: 45.1568\n",
      "Epoch 49/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 40.9754 - val_loss: 43.5708\n",
      "Epoch 50/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 40.2788 - val_loss: 47.1195\n",
      "Epoch 51/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 41.0625 - val_loss: 44.8752\n",
      "Epoch 52/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 40.3319 - val_loss: 44.8103\n",
      "Epoch 53/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 41.1924 - val_loss: 44.8948\n",
      "Epoch 54/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 41.0538 - val_loss: 42.7822\n",
      "Epoch 55/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 40.2997 - val_loss: 44.7457\n",
      "Epoch 56/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 40.9821 - val_loss: 44.7372\n",
      "Epoch 57/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 40.7127 - val_loss: 41.2518\n",
      "Epoch 58/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 39.8744 - val_loss: 45.4890\n",
      "Epoch 59/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.5597 - val_loss: 43.3057\n",
      "Epoch 60/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 40.1709 - val_loss: 43.7829\n",
      "Epoch 61/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 41.0494 - val_loss: 43.0961\n",
      "Epoch 62/500\n",
      "378/378 [==============================] - 0s 130us/step - loss: 39.2322 - val_loss: 44.9770\n",
      "Epoch 63/500\n",
      "378/378 [==============================] - 0s 130us/step - loss: 39.1229 - val_loss: 41.1290\n",
      "Epoch 64/500\n",
      "378/378 [==============================] - 0s 137us/step - loss: 40.3158 - val_loss: 44.3470\n",
      "Epoch 65/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 39.8204 - val_loss: 45.1032\n",
      "Epoch 66/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 39.9711 - val_loss: 41.7657\n",
      "Epoch 67/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 40.3331 - val_loss: 43.3147\n",
      "Epoch 68/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 39.7473 - val_loss: 42.9658\n",
      "Epoch 69/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 38.9953 - val_loss: 41.1763\n",
      "Epoch 70/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 39.6315 - val_loss: 45.8329\n",
      "Epoch 71/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 39.7593 - val_loss: 42.4869\n",
      "Epoch 72/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 39.4382 - val_loss: 41.9857\n",
      "Epoch 73/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 39.6509 - val_loss: 46.6549\n",
      "Epoch 74/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 40.6982 - val_loss: 41.7623\n",
      "Epoch 75/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.1051 - val_loss: 40.3273\n",
      "Epoch 76/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 39.8424 - val_loss: 42.8496\n",
      "Epoch 77/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 38.8710 - val_loss: 42.2651\n",
      "Epoch 78/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.0188 - val_loss: 40.2959\n",
      "Epoch 79/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.6100 - val_loss: 43.2281\n",
      "Epoch 80/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 38.8302 - val_loss: 42.8166\n",
      "Epoch 81/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 39.6735 - val_loss: 42.1837\n",
      "Epoch 82/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 39.6392 - val_loss: 42.4519\n",
      "Epoch 83/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 39.2762 - val_loss: 41.7199\n",
      "Epoch 84/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.3054 - val_loss: 42.9781\n",
      "Epoch 85/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.9147 - val_loss: 41.6997\n",
      "Epoch 86/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 38.7012 - val_loss: 42.5982\n",
      "Epoch 87/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 39.7174 - val_loss: 44.3348\n",
      "Epoch 88/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 38.1136 - val_loss: 41.5925\n",
      "Epoch 89/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.2501 - val_loss: 41.0288\n",
      "Epoch 90/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 38.9085 - val_loss: 42.4723\n",
      "Epoch 91/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 39.5742 - val_loss: 44.1792\n",
      "Epoch 92/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.1642 - val_loss: 42.0135\n",
      "Epoch 93/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 37.9017 - val_loss: 39.7521\n",
      "Epoch 94/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.5336 - val_loss: 43.5638\n",
      "Epoch 95/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 38.6539 - val_loss: 39.9789\n",
      "Epoch 96/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.2280 - val_loss: 43.8354\n",
      "Epoch 97/500\n",
      "378/378 [==============================] - 0s 131us/step - loss: 37.6934 - val_loss: 41.9996\n",
      "Epoch 98/500\n",
      "378/378 [==============================] - 0s 136us/step - loss: 38.3388 - val_loss: 40.3986\n",
      "Epoch 99/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 38.4684 - val_loss: 43.0004\n",
      "Epoch 100/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 37.7812 - val_loss: 40.8004\n",
      "Epoch 101/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 38.3084 - val_loss: 42.5884\n",
      "Epoch 102/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 38.0219 - val_loss: 40.1661\n",
      "Epoch 103/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.8136 - val_loss: 42.0351\n",
      "Epoch 104/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 38.4676 - val_loss: 41.8278\n",
      "Epoch 105/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 38.0989 - val_loss: 39.3695\n",
      "Epoch 106/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 38.5142 - val_loss: 40.6178\n",
      "Epoch 107/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 37.7047 - val_loss: 42.8883\n",
      "Epoch 108/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.3515 - val_loss: 41.1268\n",
      "Epoch 109/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.6724 - val_loss: 42.5482\n",
      "Epoch 110/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.7970 - val_loss: 44.6775\n",
      "Epoch 111/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.0464 - val_loss: 41.4212\n",
      "Epoch 112/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.3740 - val_loss: 40.7406\n",
      "Epoch 113/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 38.1242 - val_loss: 42.7961\n",
      "Epoch 114/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.1698 - val_loss: 42.5217\n",
      "Epoch 115/500\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.6577 - val_loss: 38.7924\n",
      "Epoch 116/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 38.2747 - val_loss: 42.1709\n",
      "Epoch 117/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.2091 - val_loss: 42.9062\n",
      "Epoch 118/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 37.9202 - val_loss: 40.6951\n",
      "Epoch 119/500\n",
      "378/378 [==============================] - 0s 138us/step - loss: 37.7985 - val_loss: 40.9422\n",
      "Epoch 120/500\n",
      "378/378 [==============================] - 0s 144us/step - loss: 37.1773 - val_loss: 41.2419\n",
      "Epoch 121/500\n",
      "378/378 [==============================] - 0s 139us/step - loss: 37.9636 - val_loss: 41.1232\n",
      "Epoch 122/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 37.3918 - val_loss: 41.6713\n",
      "Epoch 123/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.3932 - val_loss: 41.4056\n",
      "Epoch 124/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.7046 - val_loss: 42.2162\n",
      "Epoch 125/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 37.0647 - val_loss: 42.7995\n",
      "Epoch 126/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.5751 - val_loss: 39.7770\n",
      "Epoch 127/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.5386 - val_loss: 43.7344\n",
      "Epoch 128/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 37.2338 - val_loss: 40.3577\n",
      "Epoch 129/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.8634 - val_loss: 42.2646\n",
      "Epoch 130/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 37.5268 - val_loss: 41.8290\n",
      "Epoch 131/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.6206 - val_loss: 40.4287\n",
      "Epoch 132/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.8056 - val_loss: 43.1433\n",
      "Epoch 133/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.6017 - val_loss: 41.4761\n",
      "Epoch 134/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 37.0967 - val_loss: 40.5701\n",
      "Epoch 135/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.5803 - val_loss: 43.6633\n",
      "Epoch 136/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.5535 - val_loss: 40.9257\n",
      "Epoch 137/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 36.9403 - val_loss: 41.8853\n",
      "Epoch 138/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 37.1445 - val_loss: 42.3455\n",
      "Epoch 139/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.7953 - val_loss: 42.5648\n",
      "Epoch 140/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 37.2671 - val_loss: 41.5168\n",
      "Epoch 141/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.3948 - val_loss: 41.5488\n",
      "Epoch 142/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.7204 - val_loss: 43.5029\n",
      "Epoch 143/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.8382 - val_loss: 42.0986\n",
      "Epoch 144/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.7946 - val_loss: 42.4184\n",
      "Epoch 145/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.5353 - val_loss: 40.5995\n",
      "Epoch 146/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.3156 - val_loss: 41.9622\n",
      "Epoch 147/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.2562 - val_loss: 41.3804\n",
      "Epoch 148/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.0928 - val_loss: 43.0357\n",
      "Epoch 149/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.3101 - val_loss: 44.4459\n",
      "Epoch 150/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.7756 - val_loss: 40.4137\n",
      "Epoch 151/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.3767 - val_loss: 44.1017\n",
      "Epoch 152/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4884 - val_loss: 40.0036\n",
      "Epoch 153/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.3941 - val_loss: 43.1410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.6051 - val_loss: 40.9227\n",
      "Epoch 155/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.8613 - val_loss: 37.9792\n",
      "Epoch 156/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.0987 - val_loss: 39.7679\n",
      "Epoch 157/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.8294 - val_loss: 40.1168\n",
      "Epoch 158/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.9135 - val_loss: 40.3052\n",
      "Epoch 159/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.9357 - val_loss: 41.5186\n",
      "Epoch 160/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.4006 - val_loss: 42.6061\n",
      "Epoch 161/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.0867 - val_loss: 39.4884\n",
      "Epoch 162/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.2109 - val_loss: 40.9371\n",
      "Epoch 163/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 35.8086 - val_loss: 40.4950\n",
      "Epoch 164/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.3686 - val_loss: 41.0350\n",
      "Epoch 165/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.5588 - val_loss: 40.5976\n",
      "Epoch 166/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.3359 - val_loss: 41.4256\n",
      "Epoch 167/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 35.9847 - val_loss: 42.4337\n",
      "Epoch 168/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 35.3983 - val_loss: 41.2303\n",
      "Epoch 169/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 35.9816 - val_loss: 41.5328\n",
      "Epoch 170/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.5666 - val_loss: 43.9853\n",
      "Epoch 171/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 35.7636 - val_loss: 39.1470\n",
      "Epoch 172/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.1742 - val_loss: 42.7128\n",
      "Epoch 173/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.5819 - val_loss: 39.3321\n",
      "Epoch 174/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.7133 - val_loss: 39.9044\n",
      "Epoch 175/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 35.7240 - val_loss: 40.8036\n",
      "Epoch 176/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 35.6581 - val_loss: 38.8584\n",
      "Epoch 177/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.0380 - val_loss: 39.5968\n",
      "Epoch 178/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.1719 - val_loss: 39.5981\n",
      "Epoch 179/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 35.2089 - val_loss: 40.9771\n",
      "Epoch 180/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.0237 - val_loss: 39.9841\n",
      "Epoch 181/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.6023 - val_loss: 40.0825\n",
      "Epoch 182/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.5747 - val_loss: 40.2889\n",
      "Epoch 183/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.0117 - val_loss: 40.0691\n",
      "Epoch 184/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 34.9315 - val_loss: 40.7599\n",
      "Epoch 185/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.4915 - val_loss: 39.3586\n",
      "Epoch 186/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.4378 - val_loss: 37.8337\n",
      "Epoch 187/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.6471 - val_loss: 40.5622\n",
      "Epoch 188/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 34.3123 - val_loss: 41.8082\n",
      "Epoch 189/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 35.4651 - val_loss: 38.2440\n",
      "Epoch 190/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.1621 - val_loss: 38.5763\n",
      "Epoch 191/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.6551 - val_loss: 38.7860\n",
      "Epoch 192/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 35.0629 - val_loss: 41.1443\n",
      "Epoch 193/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 35.5129 - val_loss: 39.1493\n",
      "Epoch 194/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.2658 - val_loss: 41.4198\n",
      "Epoch 195/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 34.7779 - val_loss: 40.7980\n",
      "Epoch 196/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 34.8862 - val_loss: 40.9140\n",
      "Epoch 197/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.8743 - val_loss: 41.8909\n",
      "Epoch 198/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 34.9811 - val_loss: 40.3892\n",
      "Epoch 199/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 34.7969 - val_loss: 39.4014\n",
      "Epoch 200/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.0890 - val_loss: 43.0319\n",
      "Epoch 201/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 34.5145 - val_loss: 40.5962\n",
      "Epoch 202/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 33.5780 - val_loss: 38.8348\n",
      "Epoch 203/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 35.1359 - val_loss: 41.8587\n",
      "Epoch 204/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.4528 - val_loss: 42.7902\n",
      "Epoch 205/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 34.6293 - val_loss: 40.3214\n",
      "Epoch 206/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 34.3915 - val_loss: 41.8413\n",
      "Epoch 207/500\n",
      "378/378 [==============================] - 0s 135us/step - loss: 33.8281 - val_loss: 39.3678\n",
      "Epoch 208/500\n",
      "378/378 [==============================] - 0s 131us/step - loss: 34.6807 - val_loss: 39.3762\n",
      "Epoch 209/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 34.6764 - val_loss: 40.1510\n",
      "Epoch 210/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 34.6784 - val_loss: 39.5043\n",
      "Epoch 211/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.5211 - val_loss: 39.0586\n",
      "Epoch 212/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.9104 - val_loss: 40.3789\n",
      "Epoch 213/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 34.3465 - val_loss: 40.7001\n",
      "Epoch 214/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 34.0345 - val_loss: 37.1951\n",
      "Epoch 215/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.1767 - val_loss: 36.8480\n",
      "Epoch 216/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 33.4834 - val_loss: 39.8700\n",
      "Epoch 217/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.5928 - val_loss: 37.8554\n",
      "Epoch 218/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 33.8481 - val_loss: 39.6575\n",
      "Epoch 219/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 33.7444 - val_loss: 39.6737\n",
      "Epoch 220/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.6347 - val_loss: 39.5478\n",
      "Epoch 221/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 33.2341 - val_loss: 37.9035\n",
      "Epoch 222/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.6352 - val_loss: 40.9090\n",
      "Epoch 223/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.3126 - val_loss: 39.1993\n",
      "Epoch 224/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.8085 - val_loss: 40.9065\n",
      "Epoch 225/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 34.0013 - val_loss: 39.4138\n",
      "Epoch 226/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 33.1372 - val_loss: 38.9036\n",
      "Epoch 227/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 33.5136 - val_loss: 39.4523\n",
      "Epoch 228/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 33.1981 - val_loss: 39.3788\n",
      "Epoch 229/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 34.0994 - val_loss: 40.0920\n",
      "Epoch 230/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.7761 - val_loss: 38.8550\n",
      "Epoch 231/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.1654 - val_loss: 40.7713\n",
      "Epoch 232/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 33.2325 - val_loss: 39.1207\n",
      "Epoch 233/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.4129 - val_loss: 41.0991\n",
      "Epoch 234/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 33.4390 - val_loss: 40.5542\n",
      "Epoch 235/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.3369 - val_loss: 40.3075\n",
      "Epoch 236/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 33.1326 - val_loss: 41.1542\n",
      "Epoch 237/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.1350 - val_loss: 40.2384\n",
      "Epoch 238/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.3292 - val_loss: 38.4436\n",
      "Epoch 239/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.5944 - val_loss: 41.0797\n",
      "Epoch 240/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 32.7899 - val_loss: 39.7161\n",
      "Epoch 241/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.7902 - val_loss: 40.3507\n",
      "Epoch 242/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.6278 - val_loss: 40.6046\n",
      "Epoch 243/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 32.6562 - val_loss: 38.6116\n",
      "Epoch 244/500\n",
      "378/378 [==============================] - 0s 104us/step - loss: 32.3921 - val_loss: 38.9213\n",
      "Epoch 245/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.9833 - val_loss: 37.8780\n",
      "Epoch 246/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.6613 - val_loss: 38.0931\n",
      "Epoch 247/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 32.2458 - val_loss: 38.0734\n",
      "Epoch 248/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.2783 - val_loss: 38.4257\n",
      "Epoch 249/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.8058 - val_loss: 37.4787\n",
      "Epoch 250/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 32.3892 - val_loss: 38.5760\n",
      "Epoch 251/500\n",
      "378/378 [==============================] - 0s 129us/step - loss: 31.8504 - val_loss: 38.8264\n",
      "Epoch 252/500\n",
      "378/378 [==============================] - 0s 147us/step - loss: 32.6496 - val_loss: 39.7328\n",
      "Epoch 253/500\n",
      "378/378 [==============================] - 0s 158us/step - loss: 32.8848 - val_loss: 38.6988\n",
      "Epoch 254/500\n",
      "378/378 [==============================] - 0s 143us/step - loss: 32.1924 - val_loss: 37.5768\n",
      "Epoch 255/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.1575 - val_loss: 38.1733\n",
      "Epoch 256/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.2895 - val_loss: 38.2265\n",
      "Epoch 257/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.4456 - val_loss: 39.3512\n",
      "Epoch 258/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.7110 - val_loss: 40.1061\n",
      "Epoch 259/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 32.6290 - val_loss: 39.3175\n",
      "Epoch 260/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.6322 - val_loss: 39.9267\n",
      "Epoch 261/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.0425 - val_loss: 39.3842\n",
      "Epoch 262/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.8770 - val_loss: 37.8634\n",
      "Epoch 263/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 32.2401 - val_loss: 38.3843\n",
      "Epoch 264/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.9199 - val_loss: 40.1153\n",
      "Epoch 265/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 32.0201 - val_loss: 39.0634\n",
      "Epoch 266/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.1032 - val_loss: 38.9554\n",
      "Epoch 267/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.9224 - val_loss: 38.0711\n",
      "Epoch 268/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.9938 - val_loss: 38.6844\n",
      "Epoch 269/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 32.0690 - val_loss: 40.0462\n",
      "Epoch 270/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.6396 - val_loss: 39.5965\n",
      "Epoch 271/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 32.0912 - val_loss: 40.2744\n",
      "Epoch 272/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.9187 - val_loss: 39.5437\n",
      "Epoch 273/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.0368 - val_loss: 40.3475\n",
      "Epoch 274/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.6007 - val_loss: 40.0427\n",
      "Epoch 275/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 32.3510 - val_loss: 40.2399\n",
      "Epoch 276/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.9359 - val_loss: 39.6656\n",
      "Epoch 277/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.5143 - val_loss: 40.0709\n",
      "Epoch 278/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.5180 - val_loss: 38.3469\n",
      "Epoch 279/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.4433 - val_loss: 40.1589\n",
      "Epoch 280/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.8190 - val_loss: 39.7440\n",
      "Epoch 281/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.2347 - val_loss: 40.3970\n",
      "Epoch 282/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.8447 - val_loss: 40.1331\n",
      "Epoch 283/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.4352 - val_loss: 38.7765\n",
      "Epoch 284/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.5202 - val_loss: 38.9538\n",
      "Epoch 285/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.7205 - val_loss: 40.0717\n",
      "Epoch 286/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.8096 - val_loss: 39.4729\n",
      "Epoch 287/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.4904 - val_loss: 40.7404\n",
      "Epoch 288/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.0193 - val_loss: 40.5975\n",
      "Epoch 289/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.8245 - val_loss: 41.0510\n",
      "Epoch 290/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.0844 - val_loss: 38.8660\n",
      "Epoch 291/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.2984 - val_loss: 39.3991\n",
      "Epoch 292/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.8212 - val_loss: 40.5605\n",
      "Epoch 293/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.0310 - val_loss: 40.6840\n",
      "Epoch 294/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.5821 - val_loss: 38.9809\n",
      "Epoch 295/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.7669 - val_loss: 38.2291\n",
      "Epoch 296/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.6155 - val_loss: 38.1854\n",
      "Epoch 297/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.4318 - val_loss: 39.0626\n",
      "Epoch 298/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.7502 - val_loss: 40.4638\n",
      "Epoch 299/500\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.5481 - val_loss: 40.0488\n",
      "Epoch 300/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6654 - val_loss: 39.5720\n",
      "Epoch 301/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.8151 - val_loss: 38.1565\n",
      "Epoch 302/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.0689 - val_loss: 39.8381\n",
      "Epoch 303/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.6052 - val_loss: 39.1679\n",
      "Epoch 304/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 31.3682 - val_loss: 39.7888\n",
      "Epoch 305/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.7615 - val_loss: 38.7943\n",
      "Epoch 306/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 112us/step - loss: 31.1762 - val_loss: 39.1932\n",
      "Epoch 307/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 31.3601 - val_loss: 40.4758\n",
      "Epoch 308/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.9723 - val_loss: 39.7988\n",
      "Epoch 309/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.2240 - val_loss: 39.2697\n",
      "Epoch 310/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.3497 - val_loss: 39.5279\n",
      "Epoch 311/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 31.1582 - val_loss: 39.6884\n",
      "Epoch 312/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.5861 - val_loss: 40.0486\n",
      "Epoch 313/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.2570 - val_loss: 38.6425\n",
      "Epoch 314/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.3038 - val_loss: 40.4669\n",
      "Epoch 315/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.9954 - val_loss: 41.9334\n",
      "Epoch 316/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.3679 - val_loss: 41.2329\n",
      "Epoch 317/500\n",
      "378/378 [==============================] - 0s 137us/step - loss: 31.5218 - val_loss: 40.7655\n",
      "Epoch 318/500\n",
      "378/378 [==============================] - 0s 139us/step - loss: 31.6142 - val_loss: 39.9809\n",
      "Epoch 319/500\n",
      "378/378 [==============================] - 0s 143us/step - loss: 31.4562 - val_loss: 40.3262\n",
      "Epoch 320/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 31.4370 - val_loss: 41.6874\n",
      "Epoch 321/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.4294 - val_loss: 38.9304\n",
      "Epoch 322/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.0041 - val_loss: 39.5018\n",
      "Epoch 323/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 31.4846 - val_loss: 39.0897\n",
      "Epoch 324/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.9111 - val_loss: 39.8756\n",
      "Epoch 325/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.1426 - val_loss: 39.9551\n",
      "Epoch 326/500\n",
      "378/378 [==============================] - 0s 140us/step - loss: 31.4511 - val_loss: 42.4207\n",
      "Epoch 327/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.7267 - val_loss: 40.2815\n",
      "Epoch 328/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 31.3319 - val_loss: 40.4035\n",
      "Epoch 329/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.9088 - val_loss: 40.7895\n",
      "Epoch 330/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.8061 - val_loss: 40.8743\n",
      "Epoch 331/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.0416 - val_loss: 42.1785\n",
      "Epoch 332/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.0934 - val_loss: 41.2613\n",
      "Epoch 333/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.4310 - val_loss: 40.6687\n",
      "Epoch 334/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8968 - val_loss: 40.0269\n",
      "Epoch 335/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.2072 - val_loss: 39.6861\n",
      "Epoch 336/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.3627 - val_loss: 39.9449\n",
      "Epoch 337/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.5941 - val_loss: 40.3691\n",
      "Epoch 338/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.8323 - val_loss: 40.6163\n",
      "Epoch 339/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.0530 - val_loss: 40.5178\n",
      "Epoch 340/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.3706 - val_loss: 41.5034\n",
      "Epoch 341/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.9251 - val_loss: 41.8167\n",
      "Epoch 342/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.3378 - val_loss: 41.0410\n",
      "Epoch 343/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.0038 - val_loss: 42.0460\n",
      "Epoch 344/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.5542 - val_loss: 38.7011\n",
      "Epoch 345/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.9995 - val_loss: 40.5179\n",
      "Epoch 346/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.8664 - val_loss: 39.7197\n",
      "Epoch 347/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.7968 - val_loss: 39.8042\n",
      "Epoch 348/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.3351 - val_loss: 40.9780\n",
      "Epoch 349/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.0728 - val_loss: 39.5267\n",
      "Epoch 350/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.6935 - val_loss: 41.0507\n",
      "Epoch 351/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.2417 - val_loss: 39.2725\n",
      "Epoch 352/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.7594 - val_loss: 42.9680\n",
      "Epoch 353/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.1339 - val_loss: 40.1520\n",
      "Epoch 354/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.2462 - val_loss: 39.7720\n",
      "Epoch 355/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.0445 - val_loss: 40.7611\n",
      "Epoch 356/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6748 - val_loss: 39.9286\n",
      "Epoch 357/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.8844 - val_loss: 41.1739\n",
      "Epoch 358/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.8852 - val_loss: 39.9802\n",
      "Epoch 359/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.3380 - val_loss: 40.3607\n",
      "Epoch 360/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.6271 - val_loss: 39.8885\n",
      "Epoch 361/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.7246 - val_loss: 39.4884\n",
      "Epoch 362/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.5408 - val_loss: 39.2351\n",
      "Epoch 363/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.4051 - val_loss: 40.5264\n",
      "Epoch 364/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.6440 - val_loss: 39.8891\n",
      "Epoch 365/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.2815 - val_loss: 40.2606\n",
      "Epoch 366/500\n",
      "378/378 [==============================] - 0s 131us/step - loss: 31.0982 - val_loss: 41.5259\n",
      "Epoch 367/500\n",
      "378/378 [==============================] - 0s 145us/step - loss: 31.3395 - val_loss: 40.8794\n",
      "Epoch 368/500\n",
      "378/378 [==============================] - 0s 153us/step - loss: 30.7414 - val_loss: 38.3555\n",
      "Epoch 369/500\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.4409 - val_loss: 39.9452\n",
      "Epoch 370/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.7530 - val_loss: 40.7835\n",
      "Epoch 371/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.7052 - val_loss: 41.7158\n",
      "Epoch 372/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.4735 - val_loss: 40.1631\n",
      "Epoch 373/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.3573 - val_loss: 39.7269\n",
      "Epoch 374/500\n",
      "378/378 [==============================] - 0s 126us/step - loss: 30.9443 - val_loss: 39.9814\n",
      "Epoch 375/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4814 - val_loss: 40.2537\n",
      "Epoch 376/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.3785 - val_loss: 40.2174\n",
      "Epoch 377/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.7557 - val_loss: 39.7024\n",
      "Epoch 378/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.3892 - val_loss: 39.6333\n",
      "Epoch 379/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.9077 - val_loss: 40.5202\n",
      "Epoch 380/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.9289 - val_loss: 40.3811\n",
      "Epoch 381/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1997 - val_loss: 42.2661\n",
      "Epoch 382/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.6856 - val_loss: 40.5898\n",
      "Epoch 383/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.2197 - val_loss: 40.3579\n",
      "Epoch 384/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.1481 - val_loss: 38.9558\n",
      "Epoch 385/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.6162 - val_loss: 39.1890\n",
      "Epoch 386/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.5506 - val_loss: 38.9824\n",
      "Epoch 387/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.0495 - val_loss: 38.4847\n",
      "Epoch 388/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6081 - val_loss: 39.1477\n",
      "Epoch 389/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.6526 - val_loss: 39.9162\n",
      "Epoch 390/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.9777 - val_loss: 39.2943\n",
      "Epoch 391/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.2468 - val_loss: 41.0840\n",
      "Epoch 392/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.0274 - val_loss: 39.0025\n",
      "Epoch 393/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.2719 - val_loss: 38.3878\n",
      "Epoch 394/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.9318 - val_loss: 40.7006\n",
      "Epoch 395/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.2238 - val_loss: 39.7894\n",
      "Epoch 396/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2653 - val_loss: 39.3546\n",
      "Epoch 397/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.3448 - val_loss: 40.1611\n",
      "Epoch 398/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.5298 - val_loss: 40.2074\n",
      "Epoch 399/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.8110 - val_loss: 39.4222\n",
      "Epoch 400/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2634 - val_loss: 38.2084\n",
      "Epoch 401/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8555 - val_loss: 40.3814\n",
      "Epoch 402/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.7609 - val_loss: 39.2130\n",
      "Epoch 403/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.1720 - val_loss: 39.8305\n",
      "Epoch 404/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1160 - val_loss: 39.7367\n",
      "Epoch 405/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.9318 - val_loss: 40.1310\n",
      "Epoch 406/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6782 - val_loss: 41.4852\n",
      "Epoch 407/500\n",
      "378/378 [==============================] - 0s 106us/step - loss: 30.6017 - val_loss: 41.2509\n",
      "Epoch 408/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.4026 - val_loss: 42.1216\n",
      "Epoch 409/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.4075 - val_loss: 40.1767\n",
      "Epoch 410/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.2170 - val_loss: 41.5693\n",
      "Epoch 411/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1166 - val_loss: 39.1333\n",
      "Epoch 412/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.0525 - val_loss: 39.4924\n",
      "Epoch 413/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.6204 - val_loss: 42.4384\n",
      "Epoch 414/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6252 - val_loss: 39.4593\n",
      "Epoch 415/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2652 - val_loss: 38.2620\n",
      "Epoch 416/500\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.5431 - val_loss: 39.5074\n",
      "Epoch 417/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.4167 - val_loss: 41.1863\n",
      "Epoch 418/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.3338 - val_loss: 40.9955\n",
      "Epoch 419/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.5837 - val_loss: 40.8305\n",
      "Epoch 420/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.2243 - val_loss: 39.9133\n",
      "Epoch 421/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.3353 - val_loss: 39.7060\n",
      "Epoch 422/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8174 - val_loss: 38.4152\n",
      "Epoch 423/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3533 - val_loss: 40.2639\n",
      "Epoch 424/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.2485 - val_loss: 39.0004\n",
      "Epoch 425/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.5796 - val_loss: 39.4051\n",
      "Epoch 426/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.5380 - val_loss: 40.9850\n",
      "Epoch 427/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.1968 - val_loss: 39.8298\n",
      "Epoch 428/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.4565 - val_loss: 39.1908\n",
      "Epoch 429/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.7562 - val_loss: 39.7028\n",
      "Epoch 430/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.8779 - val_loss: 40.3042\n",
      "Epoch 431/500\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.6204 - val_loss: 39.3020\n",
      "Epoch 432/500\n",
      "378/378 [==============================] - 0s 141us/step - loss: 30.6257 - val_loss: 37.8392\n",
      "Epoch 433/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.4159 - val_loss: 37.8130\n",
      "Epoch 434/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.1642 - val_loss: 38.9769\n",
      "Epoch 435/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.6226 - val_loss: 38.8080\n",
      "Epoch 436/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.0750 - val_loss: 41.5153\n",
      "Epoch 437/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.3290 - val_loss: 39.2093\n",
      "Epoch 438/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.0403 - val_loss: 39.2562\n",
      "Epoch 439/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.6607 - val_loss: 40.1348\n",
      "Epoch 440/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.8219 - val_loss: 39.7103\n",
      "Epoch 441/500\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4798 - val_loss: 40.2748\n",
      "Epoch 442/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.3315 - val_loss: 40.6481\n",
      "Epoch 443/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.9586 - val_loss: 41.7228\n",
      "Epoch 444/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.5015 - val_loss: 39.1771\n",
      "Epoch 445/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.3006 - val_loss: 42.6020\n",
      "Epoch 446/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.5700 - val_loss: 40.0667\n",
      "Epoch 447/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.3689 - val_loss: 41.5413\n",
      "Epoch 448/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.5158 - val_loss: 39.0737\n",
      "Epoch 449/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.5732 - val_loss: 40.6396\n",
      "Epoch 450/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.5456 - val_loss: 40.4941\n",
      "Epoch 451/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.6499 - val_loss: 40.1168\n",
      "Epoch 452/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.6815 - val_loss: 38.7778\n",
      "Epoch 453/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.9414 - val_loss: 39.3367\n",
      "Epoch 454/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.2941 - val_loss: 38.7558\n",
      "Epoch 455/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.4684 - val_loss: 39.9339\n",
      "Epoch 456/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8758 - val_loss: 41.2318\n",
      "Epoch 457/500\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.0184 - val_loss: 40.6614\n",
      "Epoch 458/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 117us/step - loss: 30.3796 - val_loss: 40.3136\n",
      "Epoch 459/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.4188 - val_loss: 39.4738\n",
      "Epoch 460/500\n",
      "378/378 [==============================] - 0s 134us/step - loss: 30.3670 - val_loss: 41.0653\n",
      "Epoch 461/500\n",
      "378/378 [==============================] - 0s 139us/step - loss: 31.0115 - val_loss: 40.8251\n",
      "Epoch 462/500\n",
      "378/378 [==============================] - 0s 148us/step - loss: 29.9184 - val_loss: 39.8697\n",
      "Epoch 463/500\n",
      "378/378 [==============================] - 0s 142us/step - loss: 30.1677 - val_loss: 40.4435\n",
      "Epoch 464/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.2846 - val_loss: 39.8982\n",
      "Epoch 465/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.1463 - val_loss: 40.1937\n",
      "Epoch 466/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.7215 - val_loss: 39.5725\n",
      "Epoch 467/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.6679 - val_loss: 40.0594\n",
      "Epoch 468/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2605 - val_loss: 40.2787\n",
      "Epoch 469/500\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.7998 - val_loss: 40.6502\n",
      "Epoch 470/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.2017 - val_loss: 40.5727\n",
      "Epoch 471/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.5701 - val_loss: 40.0939\n",
      "Epoch 472/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2308 - val_loss: 41.4846\n",
      "Epoch 473/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.5639 - val_loss: 40.9715\n",
      "Epoch 474/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.9164 - val_loss: 40.4180\n",
      "Epoch 475/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.1598 - val_loss: 39.9764\n",
      "Epoch 476/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.3017 - val_loss: 40.2078\n",
      "Epoch 477/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.2758 - val_loss: 40.3070\n",
      "Epoch 478/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.0853 - val_loss: 40.0043\n",
      "Epoch 479/500\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.8395 - val_loss: 39.9165\n",
      "Epoch 480/500\n",
      "378/378 [==============================] - 0s 107us/step - loss: 30.2189 - val_loss: 41.9646\n",
      "Epoch 481/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.1160 - val_loss: 42.3742\n",
      "Epoch 482/500\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.0229 - val_loss: 41.2193\n",
      "Epoch 483/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.6877 - val_loss: 39.6180\n",
      "Epoch 484/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.5883 - val_loss: 40.9395\n",
      "Epoch 485/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.8305 - val_loss: 41.0310\n",
      "Epoch 486/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.7814 - val_loss: 40.0478\n",
      "Epoch 487/500\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.7570 - val_loss: 40.6354\n",
      "Epoch 488/500\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.1819 - val_loss: 42.2014\n",
      "Epoch 489/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.2726 - val_loss: 41.4132\n",
      "Epoch 490/500\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.4976 - val_loss: 40.4997\n",
      "Epoch 491/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.5545 - val_loss: 40.1144\n",
      "Epoch 492/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.3070 - val_loss: 41.5619\n",
      "Epoch 493/500\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.2813 - val_loss: 40.8578\n",
      "Epoch 494/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.7680 - val_loss: 40.8972\n",
      "Epoch 495/500\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.4461 - val_loss: 41.0887\n",
      "Epoch 496/500\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.7840 - val_loss: 41.8095\n",
      "Epoch 497/500\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.0379 - val_loss: 41.4529\n",
      "Epoch 498/500\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.6094 - val_loss: 41.3542\n",
      "Epoch 499/500\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.9991 - val_loss: 41.5048\n",
      "Epoch 500/500\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2172 - val_loss: 40.0301\n",
      "mw\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 83.8219 - val_loss: 17.9068\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 158us/step - loss: 15.9060 - val_loss: 9.1364\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 8.6676 - val_loss: 8.4402\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 7.5378 - val_loss: 8.3536\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.9806 - val_loss: 8.3394\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.8319 - val_loss: 8.3364\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.6353 - val_loss: 8.3336\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.5548 - val_loss: 8.3308\n",
      "Epoch 9/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.5012 - val_loss: 8.3275\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3987 - val_loss: 8.3269\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.4453 - val_loss: 8.3271\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.4317 - val_loss: 8.3288\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.4610 - val_loss: 8.3305\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.4278 - val_loss: 8.3305\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.4111 - val_loss: 8.3299\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3986 - val_loss: 8.3305\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3594 - val_loss: 8.3305\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3660 - val_loss: 8.3305\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3618 - val_loss: 8.3305\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.4251 - val_loss: 8.3305\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3732 - val_loss: 8.3305\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3477 - val_loss: 8.3305\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3357 - val_loss: 8.3305\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3571 - val_loss: 8.3305\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3493 - val_loss: 8.3305\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3137 - val_loss: 8.3305\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3493 - val_loss: 8.3305\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3213 - val_loss: 8.3305\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3604 - val_loss: 8.3305\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3377 - val_loss: 8.3305\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3254 - val_loss: 8.3305\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3174 - val_loss: 8.3305\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3316 - val_loss: 8.3305\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3302 - val_loss: 8.3305\n",
      "Epoch 35/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 121us/step - loss: 6.3622 - val_loss: 8.3305\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3444 - val_loss: 8.3305\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3139 - val_loss: 8.3305\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3362 - val_loss: 8.3305\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3247 - val_loss: 8.3305\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3242 - val_loss: 8.3305\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3202 - val_loss: 8.3305\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3161 - val_loss: 8.3305\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3093 - val_loss: 8.3305\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3236 - val_loss: 8.3305\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - 0s 180us/step - loss: 6.3189 - val_loss: 8.3305\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - 0s 149us/step - loss: 6.3081 - val_loss: 8.3305\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 6.3318 - val_loss: 8.3305\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3134 - val_loss: 8.3305\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3074 - val_loss: 8.3305\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3299 - val_loss: 8.3305\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3110 - val_loss: 8.3305\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3149 - val_loss: 8.3305\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3224 - val_loss: 8.3305\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3119 - val_loss: 8.3305\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 6.3101 - val_loss: 8.3305\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 6.3043 - val_loss: 8.3305\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3084 - val_loss: 8.3305\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3243 - val_loss: 8.3305\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 6.3036 - val_loss: 8.3305\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3057 - val_loss: 8.3305\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3265 - val_loss: 8.3305\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3062 - val_loss: 8.3305\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3051 - val_loss: 8.3305\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3094 - val_loss: 8.3305\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3042 - val_loss: 8.3305\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3096 - val_loss: 8.3305\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3021 - val_loss: 8.3305\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3027 - val_loss: 8.3305\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3144 - val_loss: 8.3305\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3022 - val_loss: 8.3305\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3075 - val_loss: 8.3305\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3030 - val_loss: 8.3305\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3057 - val_loss: 8.3305\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3083 - val_loss: 8.3305\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3121 - val_loss: 8.3305\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3060 - val_loss: 8.3305\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.2999 - val_loss: 8.3305\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3070 - val_loss: 8.3305\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.2971 - val_loss: 8.3305\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3019 - val_loss: 8.3305\n",
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3153 - val_loss: 8.3305\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3059 - val_loss: 8.3305\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3069 - val_loss: 8.3305\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3007 - val_loss: 8.3305\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3034 - val_loss: 8.3305\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3056 - val_loss: 8.3305\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3047 - val_loss: 8.3305\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3082 - val_loss: 8.3305\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.2995 - val_loss: 8.3305\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3089 - val_loss: 8.3305\n",
      "Epoch 96/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3063 - val_loss: 8.3305\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3115 - val_loss: 8.3305\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3056 - val_loss: 8.3305\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3052 - val_loss: 8.3305\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3070 - val_loss: 8.3305\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.2973 - val_loss: 8.3305\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3007 - val_loss: 8.3305\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3049 - val_loss: 8.3305\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3045 - val_loss: 8.3305\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3040 - val_loss: 8.3305\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3008 - val_loss: 8.3305\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3032 - val_loss: 8.3305\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3068 - val_loss: 8.3305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3000 - val_loss: 8.3305\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 111us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3038 - val_loss: 8.3305\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3047 - val_loss: 8.3305\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3041 - val_loss: 8.3305\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3045 - val_loss: 8.3305\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3047 - val_loss: 8.3305\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3055 - val_loss: 8.3305\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3039 - val_loss: 8.3305\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3027 - val_loss: 8.3305\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3031 - val_loss: 8.3305\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3086 - val_loss: 8.3305\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3023 - val_loss: 8.3305\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3041 - val_loss: 8.3305\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3063 - val_loss: 8.3305\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3061 - val_loss: 8.3305\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3008 - val_loss: 8.3305\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3019 - val_loss: 8.3305\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 6.3036 - val_loss: 8.3305\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3004 - val_loss: 8.3305\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3049 - val_loss: 8.3305\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3036 - val_loss: 8.3305\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.2990 - val_loss: 8.3305\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 159us/step - loss: 6.3033 - val_loss: 8.3305\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 159us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3090 - val_loss: 8.3305\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.2981 - val_loss: 8.3305\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3005 - val_loss: 8.3305\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3094 - val_loss: 8.3305\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3125 - val_loss: 8.3305\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3011 - val_loss: 8.3305\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3069 - val_loss: 8.3305\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3057 - val_loss: 8.3305\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3042 - val_loss: 8.3305\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.2989 - val_loss: 8.3305\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3043 - val_loss: 8.3305\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3019 - val_loss: 8.3305\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3028 - val_loss: 8.3305\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3020 - val_loss: 8.3305\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3006 - val_loss: 8.3305\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3030 - val_loss: 8.3305\n",
      "Epoch 192/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 129us/step - loss: 6.3026 - val_loss: 8.3305\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3023 - val_loss: 8.3305\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.2995 - val_loss: 8.3305\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3016 - val_loss: 8.3305\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3010 - val_loss: 8.3305\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3031 - val_loss: 8.3305\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3073 - val_loss: 8.3305\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 106us/step - loss: 6.3018 - val_loss: 8.3305\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 6.3020 - val_loss: 8.3305\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3010 - val_loss: 8.3305\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3011 - val_loss: 8.3305\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3017 - val_loss: 8.3305\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 6.3013 - val_loss: 8.3305\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 6.3023 - val_loss: 8.3305\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3010 - val_loss: 8.3305\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3021 - val_loss: 8.3305\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 6.3021 - val_loss: 8.3305\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 6.3008 - val_loss: 8.3305\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 6.3050 - val_loss: 8.3305\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3009 - val_loss: 8.3305\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 6.3016 - val_loss: 8.3305\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 180us/step - loss: 6.3084 - val_loss: 8.3305\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 238/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 6.3015 - val_loss: 8.3305\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.3007 - val_loss: 8.3305\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - 0s 145us/step - loss: 6.3014 - val_loss: 8.3305\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 6.3029 - val_loss: 8.3305\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 248/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 6.3025 - val_loss: 8.3305\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 178us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 6.3012 - val_loss: 8.3305\n",
      "dw\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 88.8070 - val_loss: 27.0537\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 23.8474 - val_loss: 16.5754\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 16.2885 - val_loss: 15.3362\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 14.8806 - val_loss: 15.1313\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.3470 - val_loss: 15.0873\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.0234 - val_loss: 15.0768\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.8086 - val_loss: 15.0720\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.7636 - val_loss: 15.0704\n",
      "Epoch 9/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.6758 - val_loss: 15.0676\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.6207 - val_loss: 15.0651\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.6369 - val_loss: 15.0648\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.5958 - val_loss: 15.0648\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.6300 - val_loss: 15.0648\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.5852 - val_loss: 15.0648\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.5750 - val_loss: 15.0648\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.5561 - val_loss: 15.0648\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4897 - val_loss: 15.0648\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.5354 - val_loss: 15.0648\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4976 - val_loss: 15.0648\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.5699 - val_loss: 15.0648\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.5366 - val_loss: 15.0648\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.5273 - val_loss: 15.0648\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4944 - val_loss: 15.0648\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.5043 - val_loss: 15.0648\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.5022 - val_loss: 15.0648\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4721 - val_loss: 15.0648\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4894 - val_loss: 15.0648\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4748 - val_loss: 15.0648\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4939 - val_loss: 15.0648\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4882 - val_loss: 15.0648\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 147us/step - loss: 13.4665 - val_loss: 15.0648\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 136us/step - loss: 13.4645 - val_loss: 15.0648\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4709 - val_loss: 15.0648\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4623 - val_loss: 15.0648\n",
      "Epoch 35/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4907 - val_loss: 15.0648\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4793 - val_loss: 15.0648\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4697 - val_loss: 15.0648\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.4852 - val_loss: 15.0648\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.5081 - val_loss: 15.0648\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4590 - val_loss: 15.0648\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 13.4690 - val_loss: 15.0648\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4574 - val_loss: 15.0648\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4556 - val_loss: 15.0648\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 111us/step - loss: 13.4849 - val_loss: 15.0648\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.4650 - val_loss: 15.0648\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - 0s 114us/step - loss: 13.4597 - val_loss: 15.0648\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4779 - val_loss: 15.0648\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4569 - val_loss: 15.0648\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4761 - val_loss: 15.0648\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4626 - val_loss: 15.0648\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4567 - val_loss: 15.0648\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.4476 - val_loss: 15.0648\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4640 - val_loss: 15.0648\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 155us/step - loss: 13.4589 - val_loss: 15.0648\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 147us/step - loss: 13.4535 - val_loss: 15.0648\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 13.4530 - val_loss: 15.0648\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4541 - val_loss: 15.0648\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4690 - val_loss: 15.0648\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 110us/step - loss: 13.4498 - val_loss: 15.0648\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4676 - val_loss: 15.0648\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 112us/step - loss: 13.4530 - val_loss: 15.0648\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 109us/step - loss: 13.4593 - val_loss: 15.0648\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4552 - val_loss: 15.0648\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4494 - val_loss: 15.0648\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4624 - val_loss: 15.0648\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4461 - val_loss: 15.0648\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4506 - val_loss: 15.0648\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4569 - val_loss: 15.0648\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4472 - val_loss: 15.0648\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4620 - val_loss: 15.0648\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4461 - val_loss: 15.0648\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4514 - val_loss: 15.0648\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4516 - val_loss: 15.0648\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4563 - val_loss: 15.0648\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4503 - val_loss: 15.0648\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4501 - val_loss: 15.0648\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4505 - val_loss: 15.0648\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4466 - val_loss: 15.0648\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4519 - val_loss: 15.0648\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4495 - val_loss: 15.0648\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4659 - val_loss: 15.0648\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4515 - val_loss: 15.0648\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4526 - val_loss: 15.0648\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4509 - val_loss: 15.0648\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4507 - val_loss: 15.0648\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.4443 - val_loss: 15.0648\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4499 - val_loss: 15.0648\n",
      "Epoch 96/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 126us/step - loss: 13.4612 - val_loss: 15.0648\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4559 - val_loss: 15.0648\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4473 - val_loss: 15.0648\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4526 - val_loss: 15.0648\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4521 - val_loss: 15.0648\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4508 - val_loss: 15.0648\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4481 - val_loss: 15.0648\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4490 - val_loss: 15.0648\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4481 - val_loss: 15.0648\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4506 - val_loss: 15.0648\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4491 - val_loss: 15.0648\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4482 - val_loss: 15.0648\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4517 - val_loss: 15.0648\n",
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4460 - val_loss: 15.0648\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4479 - val_loss: 15.0648\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4490 - val_loss: 15.0648\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4505 - val_loss: 15.0648\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4533 - val_loss: 15.0648\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4464 - val_loss: 15.0648\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4518 - val_loss: 15.0648\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4489 - val_loss: 15.0648\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4477 - val_loss: 15.0648\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4468 - val_loss: 15.0648\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4483 - val_loss: 15.0648\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4512 - val_loss: 15.0648\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4502 - val_loss: 15.0648\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4456 - val_loss: 15.0648\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4516 - val_loss: 15.0648\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4519 - val_loss: 15.0648\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4465 - val_loss: 15.0648\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4463 - val_loss: 15.0648\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4470 - val_loss: 15.0648\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4492 - val_loss: 15.0648\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4468 - val_loss: 15.0648\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4467 - val_loss: 15.0648\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4486 - val_loss: 15.0648\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 116us/step - loss: 13.4396 - val_loss: 15.0648\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4474 - val_loss: 15.0648\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4502 - val_loss: 15.0648\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4489 - val_loss: 15.0648\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 115us/step - loss: 13.4554 - val_loss: 15.0648\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4528 - val_loss: 15.0648\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4498 - val_loss: 15.0648\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 117us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4455 - val_loss: 15.0648\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4497 - val_loss: 15.0648\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4501 - val_loss: 15.0648\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4471 - val_loss: 15.0648\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4483 - val_loss: 15.0648\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 13.4468 - val_loss: 15.0648\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.4445 - val_loss: 15.0648\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 151us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 143us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 150us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 145us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 192/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4463 - val_loss: 15.0648\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4451 - val_loss: 15.0648\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4470 - val_loss: 15.0648\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4469 - val_loss: 15.0648\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4457 - val_loss: 15.0648\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4493 - val_loss: 15.0648\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 160us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4462 - val_loss: 15.0648\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.4455 - val_loss: 15.0648\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 136us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4453 - val_loss: 15.0648\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4529 - val_loss: 15.0648\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 149us/step - loss: 13.4478 - val_loss: 15.0648\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.4454 - val_loss: 15.0648\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4475 - val_loss: 15.0648\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4459 - val_loss: 15.0648\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4512 - val_loss: 15.0648\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 238/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4455 - val_loss: 15.0648\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4448 - val_loss: 15.0648\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4443 - val_loss: 15.0648\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.4458 - val_loss: 15.0648\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.4499 - val_loss: 15.0648\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4463 - val_loss: 15.0648\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 248/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 128us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 113us/step - loss: 13.4452 - val_loss: 15.0648\n",
      "wm\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/300\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 83.1399 - val_loss: 17.0875\n",
      "Epoch 2/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 15.5730 - val_loss: 5.7701\n",
      "Epoch 3/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 7.4662 - val_loss: 4.5463\n",
      "Epoch 4/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 6.1663 - val_loss: 4.3702\n",
      "Epoch 5/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 5.5833 - val_loss: 4.3453\n",
      "Epoch 6/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 5.3496 - val_loss: 4.3410\n",
      "Epoch 7/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 5.1049 - val_loss: 4.3410\n",
      "Epoch 8/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 5.0757 - val_loss: 4.3410\n",
      "Epoch 9/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.9750 - val_loss: 4.3410\n",
      "Epoch 10/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.8889 - val_loss: 4.3410\n",
      "Epoch 11/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.9104 - val_loss: 4.3410\n",
      "Epoch 12/300\n",
      "378/378 [==============================] - 0s 110us/step - loss: 4.8698 - val_loss: 4.3410\n",
      "Epoch 13/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.8947 - val_loss: 4.3410\n",
      "Epoch 14/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.8756 - val_loss: 4.3410\n",
      "Epoch 15/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.8541 - val_loss: 4.3410\n",
      "Epoch 16/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.8294 - val_loss: 4.3410\n",
      "Epoch 17/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.7914 - val_loss: 4.3410\n",
      "Epoch 18/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.8266 - val_loss: 4.3410\n",
      "Epoch 19/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7914 - val_loss: 4.3410\n",
      "Epoch 20/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.8456 - val_loss: 4.3410\n",
      "Epoch 21/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.8388 - val_loss: 4.3410\n",
      "Epoch 22/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.8020 - val_loss: 4.3410\n",
      "Epoch 23/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7891 - val_loss: 4.3410\n",
      "Epoch 24/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7675 - val_loss: 4.3410\n",
      "Epoch 25/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7936 - val_loss: 4.3410\n",
      "Epoch 26/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7624 - val_loss: 4.3410\n",
      "Epoch 27/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.8092 - val_loss: 4.3410\n",
      "Epoch 28/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7803 - val_loss: 4.3410\n",
      "Epoch 29/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7942 - val_loss: 4.3410\n",
      "Epoch 30/300\n",
      "378/378 [==============================] - 0s 134us/step - loss: 4.7814 - val_loss: 4.3410\n",
      "Epoch 31/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7552 - val_loss: 4.3410\n",
      "Epoch 32/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7570 - val_loss: 4.3410\n",
      "Epoch 33/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7664 - val_loss: 4.3410\n",
      "Epoch 34/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7657 - val_loss: 4.3410\n",
      "Epoch 35/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7832 - val_loss: 4.3410\n",
      "Epoch 36/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7741 - val_loss: 4.3410\n",
      "Epoch 37/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7536 - val_loss: 4.3410\n",
      "Epoch 38/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7773 - val_loss: 4.3410\n",
      "Epoch 39/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7866 - val_loss: 4.3410\n",
      "Epoch 40/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7525 - val_loss: 4.3410\n",
      "Epoch 41/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7605 - val_loss: 4.3410\n",
      "Epoch 42/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7473 - val_loss: 4.3410\n",
      "Epoch 43/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7483 - val_loss: 4.3410\n",
      "Epoch 44/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7674 - val_loss: 4.3410\n",
      "Epoch 45/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.7554 - val_loss: 4.3410\n",
      "Epoch 46/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7519 - val_loss: 4.3410\n",
      "Epoch 47/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7615 - val_loss: 4.3410\n",
      "Epoch 48/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7456 - val_loss: 4.3410\n",
      "Epoch 49/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7627 - val_loss: 4.3410\n",
      "Epoch 50/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7313 - val_loss: 4.3410\n",
      "Epoch 51/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7510 - val_loss: 4.3410\n",
      "Epoch 52/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7439 - val_loss: 4.3410\n",
      "Epoch 53/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7563 - val_loss: 4.3410\n",
      "Epoch 54/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7474 - val_loss: 4.3410\n",
      "Epoch 55/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7449 - val_loss: 4.3410\n",
      "Epoch 56/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 57/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7409 - val_loss: 4.3410\n",
      "Epoch 58/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7396 - val_loss: 4.3410\n",
      "Epoch 59/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7391 - val_loss: 4.3410\n",
      "Epoch 60/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 61/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7497 - val_loss: 4.3410\n",
      "Epoch 62/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7366 - val_loss: 4.3410\n",
      "Epoch 63/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7472 - val_loss: 4.3410\n",
      "Epoch 64/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7406 - val_loss: 4.3410\n",
      "Epoch 65/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7340 - val_loss: 4.3410\n",
      "Epoch 66/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7343 - val_loss: 4.3410\n",
      "Epoch 67/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7482 - val_loss: 4.3410\n",
      "Epoch 68/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7369 - val_loss: 4.3410\n",
      "Epoch 69/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7344 - val_loss: 4.3410\n",
      "Epoch 70/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7479 - val_loss: 4.3410\n",
      "Epoch 71/300\n",
      "378/378 [==============================] - 0s 172us/step - loss: 4.7350 - val_loss: 4.3410\n",
      "Epoch 72/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7327 - val_loss: 4.3410\n",
      "Epoch 73/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7509 - val_loss: 4.3410\n",
      "Epoch 74/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 75/300\n",
      "378/378 [==============================] - 0s 137us/step - loss: 4.7389 - val_loss: 4.3410\n",
      "Epoch 76/300\n",
      "378/378 [==============================] - 0s 150us/step - loss: 4.7433 - val_loss: 4.3410\n",
      "Epoch 77/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7436 - val_loss: 4.3410\n",
      "Epoch 78/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7427 - val_loss: 4.3410\n",
      "Epoch 79/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7372 - val_loss: 4.3410\n",
      "Epoch 80/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7403 - val_loss: 4.3410\n",
      "Epoch 81/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 82/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7329 - val_loss: 4.3410\n",
      "Epoch 83/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7414 - val_loss: 4.3410\n",
      "Epoch 84/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7337 - val_loss: 4.3410\n",
      "Epoch 85/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 86/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7585 - val_loss: 4.3410\n",
      "Epoch 87/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7378 - val_loss: 4.3410\n",
      "Epoch 88/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7478 - val_loss: 4.3410\n",
      "Epoch 89/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7342 - val_loss: 4.3410\n",
      "Epoch 90/300\n",
      "378/378 [==============================] - 0s 135us/step - loss: 4.7374 - val_loss: 4.3410\n",
      "Epoch 91/300\n",
      "378/378 [==============================] - 0s 152us/step - loss: 4.7334 - val_loss: 4.3410\n",
      "Epoch 92/300\n",
      "378/378 [==============================] - 0s 144us/step - loss: 4.7333 - val_loss: 4.3410\n",
      "Epoch 93/300\n",
      "378/378 [==============================] - 0s 150us/step - loss: 4.7343 - val_loss: 4.3410\n",
      "Epoch 94/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7344 - val_loss: 4.3410\n",
      "Epoch 95/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7366 - val_loss: 4.3410\n",
      "Epoch 96/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7488 - val_loss: 4.3410\n",
      "Epoch 97/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7440 - val_loss: 4.3410\n",
      "Epoch 98/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7370 - val_loss: 4.3410\n",
      "Epoch 99/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 100/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7309 - val_loss: 4.3410\n",
      "Epoch 101/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 102/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7432 - val_loss: 4.3410\n",
      "Epoch 103/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7407 - val_loss: 4.3410\n",
      "Epoch 104/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7382 - val_loss: 4.3410\n",
      "Epoch 105/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7352 - val_loss: 4.3410\n",
      "Epoch 106/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7358 - val_loss: 4.3410\n",
      "Epoch 107/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7355 - val_loss: 4.3410\n",
      "Epoch 108/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 109/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7343 - val_loss: 4.3410\n",
      "Epoch 110/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 111/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7345 - val_loss: 4.3410\n",
      "Epoch 112/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7404 - val_loss: 4.3410\n",
      "Epoch 113/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7414 - val_loss: 4.3410\n",
      "Epoch 114/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7340 - val_loss: 4.3410\n",
      "Epoch 115/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 116/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 117/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7311 - val_loss: 4.3410\n",
      "Epoch 118/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7399 - val_loss: 4.3410\n",
      "Epoch 119/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7329 - val_loss: 4.3410\n",
      "Epoch 120/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7376 - val_loss: 4.3410\n",
      "Epoch 121/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 122/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7389 - val_loss: 4.3410\n",
      "Epoch 123/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7396 - val_loss: 4.3410\n",
      "Epoch 124/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7347 - val_loss: 4.3410\n",
      "Epoch 125/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7354 - val_loss: 4.3410\n",
      "Epoch 126/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7355 - val_loss: 4.3410\n",
      "Epoch 127/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7345 - val_loss: 4.3410\n",
      "Epoch 128/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 129/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7309 - val_loss: 4.3410\n",
      "Epoch 130/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 131/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7368 - val_loss: 4.3410\n",
      "Epoch 132/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7370 - val_loss: 4.3410\n",
      "Epoch 133/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7357 - val_loss: 4.3410\n",
      "Epoch 134/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 135/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7378 - val_loss: 4.3410\n",
      "Epoch 136/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7373 - val_loss: 4.3410\n",
      "Epoch 137/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7344 - val_loss: 4.3410\n",
      "Epoch 138/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 139/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 140/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7361 - val_loss: 4.3410\n",
      "Epoch 141/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 142/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 143/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7351 - val_loss: 4.3410\n",
      "Epoch 144/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 145/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7349 - val_loss: 4.3410\n",
      "Epoch 146/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7370 - val_loss: 4.3410\n",
      "Epoch 147/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7321 - val_loss: 4.3410\n",
      "Epoch 148/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 149/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7323 - val_loss: 4.3410\n",
      "Epoch 150/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7361 - val_loss: 4.3410\n",
      "Epoch 151/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 152/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 153/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7338 - val_loss: 4.3410\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 119us/step - loss: 4.7265 - val_loss: 4.3410\n",
      "Epoch 155/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 156/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7379 - val_loss: 4.3410\n",
      "Epoch 157/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 158/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7332 - val_loss: 4.3410\n",
      "Epoch 159/300\n",
      "378/378 [==============================] - 0s 136us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 160/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7411 - val_loss: 4.3410\n",
      "Epoch 161/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7391 - val_loss: 4.3410\n",
      "Epoch 162/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7347 - val_loss: 4.3410\n",
      "Epoch 163/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7316 - val_loss: 4.3410\n",
      "Epoch 164/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7332 - val_loss: 4.3410\n",
      "Epoch 165/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 166/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 167/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 168/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7354 - val_loss: 4.3410\n",
      "Epoch 169/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 170/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 171/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 172/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 173/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 174/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7351 - val_loss: 4.3410\n",
      "Epoch 175/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 176/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 177/300\n",
      "378/378 [==============================] - 0s 156us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 178/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7360 - val_loss: 4.3410\n",
      "Epoch 179/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 180/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7310 - val_loss: 4.3410\n",
      "Epoch 181/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 182/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 183/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7335 - val_loss: 4.3410\n",
      "Epoch 184/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7312 - val_loss: 4.3410\n",
      "Epoch 185/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 186/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7309 - val_loss: 4.3410\n",
      "Epoch 187/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7325 - val_loss: 4.3410\n",
      "Epoch 188/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 189/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 190/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 191/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 192/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7330 - val_loss: 4.3410\n",
      "Epoch 193/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 194/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 195/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 196/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 197/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 4.7332 - val_loss: 4.3410\n",
      "Epoch 198/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 199/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 200/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 201/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7308 - val_loss: 4.3410\n",
      "Epoch 202/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 203/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 204/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 205/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7337 - val_loss: 4.3410\n",
      "Epoch 206/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 207/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7333 - val_loss: 4.3410\n",
      "Epoch 208/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 209/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7316 - val_loss: 4.3410\n",
      "Epoch 210/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 211/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 212/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 213/300\n",
      "378/378 [==============================] - 0s 139us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 214/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 215/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 216/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7325 - val_loss: 4.3410\n",
      "Epoch 217/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 218/300\n",
      "378/378 [==============================] - 0s 170us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 219/300\n",
      "378/378 [==============================] - 0s 147us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 220/300\n",
      "378/378 [==============================] - 0s 179us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 221/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 222/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 223/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7399 - val_loss: 4.3410\n",
      "Epoch 224/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 225/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 226/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7334 - val_loss: 4.3410\n",
      "Epoch 227/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 228/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 229/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7323 - val_loss: 4.3410\n",
      "Epoch 230/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 231/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 232/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7362 - val_loss: 4.3410\n",
      "Epoch 233/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 234/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 235/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7362 - val_loss: 4.3410\n",
      "Epoch 236/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 237/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 238/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7316 - val_loss: 4.3410\n",
      "Epoch 239/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 240/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 241/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 4.554 - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 242/300\n",
      "378/378 [==============================] - 0s 135us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 243/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7364 - val_loss: 4.3410\n",
      "Epoch 244/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7325 - val_loss: 4.3410\n",
      "Epoch 245/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 246/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7320 - val_loss: 4.3410\n",
      "Epoch 247/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 248/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 249/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7317 - val_loss: 4.3410\n",
      "Epoch 250/300\n",
      "378/378 [==============================] - 0s 136us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 251/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 252/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 253/300\n",
      "378/378 [==============================] - 0s 141us/step - loss: 4.7333 - val_loss: 4.3410\n",
      "Epoch 254/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 255/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 6.104 - 0s 123us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 256/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 257/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 258/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 259/300\n",
      "378/378 [==============================] - 0s 133us/step - loss: 4.7338 - val_loss: 4.3410\n",
      "Epoch 260/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 4.7337 - val_loss: 4.3410\n",
      "Epoch 261/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 262/300\n",
      "378/378 [==============================] - 0s 131us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 263/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 264/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 4.7322 - val_loss: 4.3410\n",
      "Epoch 265/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7326 - val_loss: 4.3410\n",
      "Epoch 266/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7461 - val_loss: 4.3410\n",
      "Epoch 267/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 4.7322 - val_loss: 4.3410\n",
      "Epoch 268/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 269/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 270/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7318 - val_loss: 4.3410\n",
      "Epoch 271/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 272/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 4.7319 - val_loss: 4.3410\n",
      "Epoch 273/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7331 - val_loss: 4.3410\n",
      "Epoch 274/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 4.7338 - val_loss: 4.3410\n",
      "Epoch 275/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 276/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 277/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 4.7328 - val_loss: 4.3410\n",
      "Epoch 278/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 279/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 280/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 281/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 282/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 283/300\n",
      "378/378 [==============================] - 0s 142us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 284/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 285/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7379 - val_loss: 4.3410\n",
      "Epoch 286/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 287/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 288/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 289/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 290/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 291/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 292/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 4.7324 - val_loss: 4.3410\n",
      "Epoch 293/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 294/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 295/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 4.7327 - val_loss: 4.3410\n",
      "Epoch 296/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 297/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 298/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 4.7315 - val_loss: 4.3410\n",
      "Epoch 299/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7340 - val_loss: 4.3410\n",
      "Epoch 300/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 4.7322 - val_loss: 4.3410\n",
      "oven\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 85.4946 - val_loss: 24.5830\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 23.2607 - val_loss: 14.3820\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 16.4812 - val_loss: 13.7477\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 15.5363 - val_loss: 13.6055\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 15.1210 - val_loss: 13.5485\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.8384 - val_loss: 13.5376\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 14.7746 - val_loss: 13.5344\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.6760 - val_loss: 13.5317\n",
      "Epoch 9/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 134us/step - loss: 14.6388 - val_loss: 13.5292\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.5970 - val_loss: 13.5281\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.5895 - val_loss: 13.5295\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 14.5554 - val_loss: 13.5289\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.5966 - val_loss: 13.5239\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.5681 - val_loss: 13.5176\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.5617 - val_loss: 13.5187\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 14.5268 - val_loss: 13.5165\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.5224 - val_loss: 13.5165\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.5293 - val_loss: 13.5165\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4986 - val_loss: 13.5171\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 14.5334 - val_loss: 13.5167\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 14.5318 - val_loss: 13.5176\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 14.4993 - val_loss: 13.5195\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.4883 - val_loss: 13.5187\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.5015 - val_loss: 13.5181\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.5031 - val_loss: 13.5165\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4649 - val_loss: 13.5165\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.5126 - val_loss: 13.5165\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4765 - val_loss: 13.5165\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.5286 - val_loss: 13.5165\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4686 - val_loss: 13.5165\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4754 - val_loss: 13.5165\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4791 - val_loss: 13.5165\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4849 - val_loss: 13.5165\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4909 - val_loss: 13.5165\n",
      "Epoch 35/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.4872 - val_loss: 13.5165\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4868 - val_loss: 13.5165\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.4879 - val_loss: 13.5165\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 136us/step - loss: 14.5085 - val_loss: 13.5165\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 147us/step - loss: 14.4830 - val_loss: 13.5165\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4634 - val_loss: 13.5165\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4808 - val_loss: 13.5165\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4824 - val_loss: 13.5165\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.4680 - val_loss: 13.5165\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.5001 - val_loss: 13.5165\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 8.339 - 0s 129us/step - loss: 14.4864 - val_loss: 13.5165\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 8.701 - 0s 133us/step - loss: 14.4744 - val_loss: 13.5165\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 14.4934 - val_loss: 13.5165\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4646 - val_loss: 13.5165\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4704 - val_loss: 13.5165\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 14.4709 - val_loss: 13.5165\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4679 - val_loss: 13.5165\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 14.4801 - val_loss: 13.5165\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4552 - val_loss: 13.5165\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 14.4637 - val_loss: 13.5165\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.4611 - val_loss: 13.5165\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4682 - val_loss: 13.5165\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4561 - val_loss: 13.5165\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 14.4487 - val_loss: 13.5165\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4685 - val_loss: 13.5165\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.4620 - val_loss: 13.5165\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4613 - val_loss: 13.5165\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4651 - val_loss: 13.5171\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.4619 - val_loss: 13.5205\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4586 - val_loss: 13.5199\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4815 - val_loss: 13.5173\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4579 - val_loss: 13.5165\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4718 - val_loss: 13.5165\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4608 - val_loss: 13.5165\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4773 - val_loss: 13.5165\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4585 - val_loss: 13.5165\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4619 - val_loss: 13.5165\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.4703 - val_loss: 13.5165\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4532 - val_loss: 13.5165\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.4636 - val_loss: 13.5165\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4611 - val_loss: 13.5165\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 14.4746 - val_loss: 13.5165\n",
      "Epoch 77/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 14.4516 - val_loss: 13.5165\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 165us/step - loss: 14.4638 - val_loss: 13.5165\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 14.4767 - val_loss: 13.5165\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 154us/step - loss: 14.4590 - val_loss: 13.5165\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4584 - val_loss: 13.5165\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.4599 - val_loss: 13.5165\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.4577 - val_loss: 13.5165\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 149us/step - loss: 14.4513 - val_loss: 13.5165\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 14.4619 - val_loss: 13.5165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4617 - val_loss: 13.5180\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4544 - val_loss: 13.5179\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 14.4448 - val_loss: 13.5183\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4630 - val_loss: 13.5165\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4542 - val_loss: 13.5165\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4567 - val_loss: 13.5181\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 14.4473 - val_loss: 13.5075\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 14.4660 - val_loss: 13.5007\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4361 - val_loss: 13.5165\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 14.4503 - val_loss: 13.5090\n",
      "Epoch 96/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.4592 - val_loss: 13.4720\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 14.4329 - val_loss: 13.4639\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 14.4308 - val_loss: 13.4675\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.3995 - val_loss: 13.4251\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 141us/step - loss: 14.4174 - val_loss: 13.3731\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 14.3545 - val_loss: 13.3941\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 14.3508 - val_loss: 13.3359\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.3103 - val_loss: 13.3279\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.2802 - val_loss: 13.3228\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 14.2609 - val_loss: 13.2721\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.2139 - val_loss: 13.2903\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.1983 - val_loss: 13.2771\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 14.0828 - val_loss: 13.1795\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 14.0993 - val_loss: 13.0846\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 14.0076 - val_loss: 13.1016\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 14.0304 - val_loss: 13.1182\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 13.9632 - val_loss: 13.0712\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.9006 - val_loss: 13.0824\n",
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.9263 - val_loss: 13.0582\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.7625 - val_loss: 13.1548\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.8256 - val_loss: 13.0120\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.7829 - val_loss: 13.0370\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.8275 - val_loss: 12.9281\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 13.8204 - val_loss: 12.9648\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.7946 - val_loss: 12.9704\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.7651 - val_loss: 12.9854\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.7233 - val_loss: 12.9734\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.6040 - val_loss: 12.9451\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 139us/step - loss: 13.6852 - val_loss: 12.8959\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.6510 - val_loss: 12.8554\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 211us/step - loss: 13.5999 - val_loss: 12.8074\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.5885 - val_loss: 12.8861\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.4836 - val_loss: 12.8323\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 13.4353 - val_loss: 12.7927\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.6071 - val_loss: 12.7419\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 13.5183 - val_loss: 12.6590\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.4677 - val_loss: 12.6050\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.4859 - val_loss: 12.6885\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 124us/step - loss: 13.5135 - val_loss: 12.6759\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.3991 - val_loss: 12.7643\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.4379 - val_loss: 12.7539\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.4065 - val_loss: 12.7252\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.3669 - val_loss: 12.7693\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.3049 - val_loss: 12.7138\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 13.3928 - val_loss: 12.7738\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.4343 - val_loss: 12.6631\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.2826 - val_loss: 12.6483\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.3228 - val_loss: 12.5174\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.2871 - val_loss: 12.3914\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.2982 - val_loss: 12.5352\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 13.2078 - val_loss: 12.2640\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.1396 - val_loss: 12.2060\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.2084 - val_loss: 12.2880\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1740 - val_loss: 12.5510\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.1966 - val_loss: 12.2309\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.1939 - val_loss: 12.3188\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 13.1223 - val_loss: 12.3875\n",
      "Epoch 153/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.3229 - val_loss: 12.6054\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.1322 - val_loss: 12.4197\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.0788 - val_loss: 12.3705\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1723 - val_loss: 12.4310\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.0013 - val_loss: 12.2281\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.0248 - val_loss: 12.4465\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 13.2175 - val_loss: 12.5020\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.0156 - val_loss: 12.6371\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1961 - val_loss: 12.2693\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 13.1523 - val_loss: 12.6401\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.1433 - val_loss: 12.6641\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 13.0366 - val_loss: 12.6070\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.1367 - val_loss: 12.6366\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.1739 - val_loss: 12.5923\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 13.0676 - val_loss: 12.5381\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.0624 - val_loss: 12.5364\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.0203 - val_loss: 12.5293\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.9440 - val_loss: 12.6299\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.1113 - val_loss: 12.6174\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 13.0804 - val_loss: 12.5971\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.9573 - val_loss: 12.5656\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 13.1154 - val_loss: 12.5035\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.1325 - val_loss: 12.6030\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 145us/step - loss: 13.1203 - val_loss: 12.5818\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 13.0066 - val_loss: 12.3141\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.9833 - val_loss: 12.4126\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 13.0624 - val_loss: 12.4718\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 140us/step - loss: 13.0250 - val_loss: 12.5041\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 144us/step - loss: 12.9819 - val_loss: 12.3958\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 151us/step - loss: 12.9655 - val_loss: 12.6193\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 146us/step - loss: 12.9404 - val_loss: 12.5503\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 12.9704 - val_loss: 12.6115\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 13.0110 - val_loss: 12.5220\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 20.68 - 0s 129us/step - loss: 12.9959 - val_loss: 12.2808\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 12.9206 - val_loss: 12.1808\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 13.0088 - val_loss: 12.2347\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 13.0576 - val_loss: 12.1633\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.9613 - val_loss: 12.1796\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9055 - val_loss: 12.2568\n",
      "Epoch 192/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 12.9760 - val_loss: 12.3352\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 12.9435 - val_loss: 12.2705\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 105us/step - loss: 12.9510 - val_loss: 12.1830\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.9667 - val_loss: 12.2900\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9616 - val_loss: 12.3646\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 12.9087 - val_loss: 12.4651\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9797 - val_loss: 12.4514\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 13.0087 - val_loss: 12.4321\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 12.9954 - val_loss: 12.2824\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.8830 - val_loss: 12.4137\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 12.9623 - val_loss: 12.5426\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.8850 - val_loss: 12.6690\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.8357 - val_loss: 12.6236\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9581 - val_loss: 12.4853\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.9478 - val_loss: 12.2952\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 12.9523 - val_loss: 12.4060\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 12.8466 - val_loss: 12.3185\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.9324 - val_loss: 12.3965\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 138us/step - loss: 12.8792 - val_loss: 12.6624\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 127us/step - loss: 12.9159 - val_loss: 12.4546\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 118us/step - loss: 12.9178 - val_loss: 12.4683\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.8867 - val_loss: 12.4595\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8817 - val_loss: 12.3445\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.8515 - val_loss: 12.4234\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.9086 - val_loss: 12.3110\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8702 - val_loss: 12.3821\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 121us/step - loss: 12.9218 - val_loss: 12.2241\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7628 - val_loss: 12.2175\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.9640 - val_loss: 12.3532\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8357 - val_loss: 12.4899\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 126us/step - loss: 12.8780 - val_loss: 12.3274\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 12.7763 - val_loss: 12.2431\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8570 - val_loss: 12.4958\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 125us/step - loss: 12.7729 - val_loss: 12.3801\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 134us/step - loss: 12.7953 - val_loss: 12.5323\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 12.7385 - val_loss: 12.4820\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.8622 - val_loss: 12.6100\n",
      "Epoch 229/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.7570 - val_loss: 12.4748\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 131us/step - loss: 12.8744 - val_loss: 12.4058\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 12.7637 - val_loss: 12.5549\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7855 - val_loss: 12.4922\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 135us/step - loss: 12.7410 - val_loss: 12.4675\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.8480 - val_loss: 12.3852\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.7787 - val_loss: 12.2245\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.7565 - val_loss: 12.2052\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7496 - val_loss: 12.2602\n",
      "Epoch 238/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 131us/step - loss: 12.7644 - val_loss: 12.3515\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 133us/step - loss: 12.8034 - val_loss: 12.4435\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 119us/step - loss: 12.6870 - val_loss: 12.4177\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 128us/step - loss: 12.7014 - val_loss: 12.5220\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 132us/step - loss: 12.6575 - val_loss: 12.5753\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 122us/step - loss: 12.7996 - val_loss: 12.5444\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 10.83 - 0s 128us/step - loss: 12.7116 - val_loss: 12.3564\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 137us/step - loss: 12.6985 - val_loss: 12.5121\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.6630 - val_loss: 12.5820\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 129us/step - loss: 12.6963 - val_loss: 12.7727\n",
      "Epoch 248/250\n",
      "378/378 [==============================] - 0s 120us/step - loss: 12.7422 - val_loss: 12.8242\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 123us/step - loss: 12.7765 - val_loss: 12.7122\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 130us/step - loss: 12.7599 - val_loss: 12.5652\n"
     ]
    }
   ],
   "source": [
    "#pred_appliance = {}\n",
    "sequence_length=24\n",
    "num_iterations_dictionary = {'hvac':400,'fridge':500,'mw':250,'dw':250,'oven':250, 'wm':300}\n",
    "for appliance in APPLIANCES_ORDER[2:]:\n",
    "\n",
    "\n",
    "    print(appliance)\n",
    "    print(\"*\"*20)\n",
    "    np.random.seed(0)\n",
    "    from keras.layers.merge import Subtract, Minimum\n",
    "    model = Sequential()\n",
    "    filters=20\n",
    "    kernel_size=2\n",
    "    model.add(InputLayer(input_shape=(sequence_length,1)))\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ,name='C1'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Conv1D(filters=20,\n",
    "                     kernel_size=5,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Conv1D(filters=25,\n",
    "                     kernel_size=3,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Conv1D(filters=30,\n",
    "                     kernel_size=2,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1 ))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(sequence_length, activation='relu'))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "    model.compile('adam','mean_absolute_error')\n",
    "    model.fit(train_agg.reshape(-1, 24, 1), train_appliance[appliance], epochs=num_iterations_dictionary[appliance], validation_split=0.1)\n",
    "    pred_appliance[appliance] = model.predict(test_agg.reshape(-1,24,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('per-appliance.pdf','wb') as f:\n",
    "    f.write(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='pdf'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
