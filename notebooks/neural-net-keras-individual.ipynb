{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from common import APPLIANCES_ORDER\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = np.load('../1H-input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subset_dataset(tensor):\n",
    "    t_subset = tensor[:, :, 180:194, :]\n",
    "    all_indices = np.array(list(range(320)))\n",
    "    for i in range(1, 7):\n",
    "        valid_homes = pd.DataFrame(t_subset[:, i, :].reshape(320, 14*24)).dropna().index\n",
    "        all_indices = np.intersect1d(all_indices, valid_homes)\n",
    "    t_subset = t_subset[all_indices, :, :, :].reshape(52, 7, 14*24)\n",
    "    \n",
    "    # Create artificial aggregate\n",
    "    t_subset[:, 0, :] = 0.0\n",
    "    for i in range(1, 7):\n",
    "        t_subset[:, 0, :] = t_subset[:, 0, :] + t_subset[:, i, :]\n",
    "    # t_subset is of shape (#home, appliance, days*hours)\n",
    "    return t_subset, all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all, valid_homes = create_subset_dataset(tensor)\n",
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_objective(y_pred, y_true):\n",
    "    with tf.name_scope(None):\n",
    "        return tf.losses.absolute_difference(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/nipun/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "import keras\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "n_movies = 3\n",
    "n_users=3\n",
    "n_latent_factors=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg = t_all[:30, 0, :].reshape(30*14, 24)\n",
    "train_appliance = {}\n",
    "test_appliance = {}\n",
    "for appliance_num, appliance in enumerate(APPLIANCES_ORDER[1:]):\n",
    "    train_appliance[appliance] = t_all[:30, appliance_num+1, :].reshape(30*14, 24)\n",
    "    test_appliance[appliance] = t_all[30:, appliance_num+1, :].reshape(22*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_hvac = t_all[30:, 1, :].reshape(22*14, 24)\n",
    "test_fridge = t_all[30:, 2, :].reshape(22*14, 24)\n",
    "\n",
    "test_mw = t_all[30:, 3, :].reshape(22*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "test_agg = t_all[30:, 0, :].reshape(22*14, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 24)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_hvac_fridge = np.hstack([train_hvac, train_fridge])\n",
    "test_hvac_fridge = np.hstack([test_hvac, test_fridge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvac\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/300\n",
      "378/378 [==============================] - 0s 1ms/step - loss: 661.6838 - val_loss: 666.3338\n",
      "Epoch 2/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 591.9552 - val_loss: 590.7587\n",
      "Epoch 3/300\n",
      "378/378 [==============================] - 0s 51us/step - loss: 520.2719 - val_loss: 519.7037\n",
      "Epoch 4/300\n",
      "378/378 [==============================] - 0s 51us/step - loss: 454.8880 - val_loss: 450.5059\n",
      "Epoch 5/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 389.6605 - val_loss: 381.3647\n",
      "Epoch 6/300\n",
      "378/378 [==============================] - 0s 51us/step - loss: 340.8408 - val_loss: 325.7024\n",
      "Epoch 7/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 311.5913 - val_loss: 279.7422\n",
      "Epoch 8/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 274.5177 - val_loss: 252.5243\n",
      "Epoch 9/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 253.1699 - val_loss: 236.5009\n",
      "Epoch 10/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 232.3254 - val_loss: 210.2190\n",
      "Epoch 11/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 207.1601 - val_loss: 188.7835\n",
      "Epoch 12/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 187.0578 - val_loss: 175.4448\n",
      "Epoch 13/300\n",
      "378/378 [==============================] - 0s 49us/step - loss: 168.8666 - val_loss: 153.2943\n",
      "Epoch 14/300\n",
      "378/378 [==============================] - 0s 48us/step - loss: 154.0500 - val_loss: 143.2210\n",
      "Epoch 15/300\n",
      "378/378 [==============================] - 0s 47us/step - loss: 149.7838 - val_loss: 140.8269\n",
      "Epoch 16/300\n",
      "378/378 [==============================] - 0s 49us/step - loss: 144.6259 - val_loss: 140.9123\n",
      "Epoch 17/300\n",
      "378/378 [==============================] - 0s 46us/step - loss: 140.8270 - val_loss: 140.7998\n",
      "Epoch 18/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 137.7452 - val_loss: 140.7386\n",
      "Epoch 19/300\n",
      "378/378 [==============================] - 0s 49us/step - loss: 136.3308 - val_loss: 140.8164\n",
      "Epoch 20/300\n",
      "378/378 [==============================] - 0s 49us/step - loss: 135.6806 - val_loss: 140.8029\n",
      "Epoch 21/300\n",
      "378/378 [==============================] - 0s 49us/step - loss: 132.6984 - val_loss: 140.8296\n",
      "Epoch 22/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 132.9278 - val_loss: 140.8399\n",
      "Epoch 23/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 131.2327 - val_loss: 140.7992\n",
      "Epoch 24/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 133.2514 - val_loss: 140.7231\n",
      "Epoch 25/300\n",
      "378/378 [==============================] - 0s 66us/step - loss: 130.9194 - val_loss: 140.6565\n",
      "Epoch 26/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 132.8930 - val_loss: 140.6072\n",
      "Epoch 27/300\n",
      "378/378 [==============================] - 0s 59us/step - loss: 132.2502 - val_loss: 140.5252\n",
      "Epoch 28/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 131.3921 - val_loss: 140.4510\n",
      "Epoch 29/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 132.5399 - val_loss: 140.3843\n",
      "Epoch 30/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 131.8676 - val_loss: 140.3483\n",
      "Epoch 31/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 130.4077 - val_loss: 140.3679\n",
      "Epoch 32/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 128.8600 - val_loss: 140.3384\n",
      "Epoch 33/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 129.8561 - val_loss: 140.3007\n",
      "Epoch 34/300\n",
      "378/378 [==============================] - 0s 49us/step - loss: 130.3328 - val_loss: 140.2832\n",
      "Epoch 35/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 129.4058 - val_loss: 140.2677\n",
      "Epoch 36/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 129.7686 - val_loss: 140.2344\n",
      "Epoch 37/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 129.9283 - val_loss: 140.1951\n",
      "Epoch 38/300\n",
      "378/378 [==============================] - 0s 47us/step - loss: 130.4127 - val_loss: 140.1553\n",
      "Epoch 39/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 130.7554 - val_loss: 140.0806\n",
      "Epoch 40/300\n",
      "378/378 [==============================] - 0s 49us/step - loss: 128.8196 - val_loss: 140.0237\n",
      "Epoch 41/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 129.5811 - val_loss: 139.9884\n",
      "Epoch 42/300\n",
      "378/378 [==============================] - 0s 46us/step - loss: 130.1186 - val_loss: 139.9583\n",
      "Epoch 43/300\n",
      "378/378 [==============================] - 0s 45us/step - loss: 129.1605 - val_loss: 139.9353\n",
      "Epoch 44/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 130.1548 - val_loss: 139.8980\n",
      "Epoch 45/300\n",
      "378/378 [==============================] - 0s 48us/step - loss: 129.2811 - val_loss: 139.8567\n",
      "Epoch 46/300\n",
      "378/378 [==============================] - 0s 48us/step - loss: 127.4658 - val_loss: 139.8432\n",
      "Epoch 47/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 128.1523 - val_loss: 139.8436\n",
      "Epoch 48/300\n",
      "378/378 [==============================] - 0s 49us/step - loss: 129.4098 - val_loss: 139.8265\n",
      "Epoch 49/300\n",
      "378/378 [==============================] - 0s 48us/step - loss: 128.7498 - val_loss: 139.8231\n",
      "Epoch 50/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 128.5832 - val_loss: 139.8303\n",
      "Epoch 51/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 128.9945 - val_loss: 139.8297\n",
      "Epoch 52/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 128.0357 - val_loss: 139.8175\n",
      "Epoch 53/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 128.6308 - val_loss: 139.8136\n",
      "Epoch 54/300\n",
      "378/378 [==============================] - 0s 46us/step - loss: 127.6388 - val_loss: 139.8051\n",
      "Epoch 55/300\n",
      "378/378 [==============================] - 0s 51us/step - loss: 128.5852 - val_loss: 139.7966\n",
      "Epoch 56/300\n",
      "378/378 [==============================] - 0s 49us/step - loss: 128.1530 - val_loss: 139.7799\n",
      "Epoch 57/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 128.1829 - val_loss: 139.7685\n",
      "Epoch 58/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 129.1592 - val_loss: 139.7616\n",
      "Epoch 59/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 128.4734 - val_loss: 139.7570\n",
      "Epoch 60/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 128.7630 - val_loss: 139.7386\n",
      "Epoch 61/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 128.2538 - val_loss: 139.7154\n",
      "Epoch 62/300\n",
      "378/378 [==============================] - 0s 58us/step - loss: 128.9239 - val_loss: 139.6755\n",
      "Epoch 63/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 129.1183 - val_loss: 139.6510\n",
      "Epoch 64/300\n",
      "378/378 [==============================] - 0s 68us/step - loss: 127.5278 - val_loss: 139.6411\n",
      "Epoch 65/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 128.2425 - val_loss: 139.6339\n",
      "Epoch 66/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 128.1037 - val_loss: 139.6214\n",
      "Epoch 67/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 128.6005 - val_loss: 139.6240\n",
      "Epoch 68/300\n",
      "378/378 [==============================] - 0s 53us/step - loss: 128.0964 - val_loss: 139.6214\n",
      "Epoch 69/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 127.7643 - val_loss: 139.6276\n",
      "Epoch 70/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 127.8480 - val_loss: 139.6171\n",
      "Epoch 71/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 128.3643 - val_loss: 139.6091\n",
      "Epoch 72/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 127.9342 - val_loss: 139.5944\n",
      "Epoch 73/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 128.0625 - val_loss: 139.5816\n",
      "Epoch 74/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 128.5663 - val_loss: 139.5677\n",
      "Epoch 75/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 128.5791 - val_loss: 139.5594\n",
      "Epoch 76/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 127.5213 - val_loss: 139.5536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 127.6365 - val_loss: 139.5523\n",
      "Epoch 78/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 127.8330 - val_loss: 139.5443\n",
      "Epoch 79/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 129.608 - 0s 58us/step - loss: 128.1786 - val_loss: 139.5379\n",
      "Epoch 80/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 128.0439 - val_loss: 139.5374\n",
      "Epoch 81/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 127.8161 - val_loss: 139.5313\n",
      "Epoch 82/300\n",
      "378/378 [==============================] - 0s 46us/step - loss: 127.6848 - val_loss: 139.5253\n",
      "Epoch 83/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 127.7055 - val_loss: 139.5151\n",
      "Epoch 84/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 127.8220 - val_loss: 139.4930\n",
      "Epoch 85/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 127.5482 - val_loss: 139.4731\n",
      "Epoch 86/300\n",
      "378/378 [==============================] - 0s 51us/step - loss: 127.9319 - val_loss: 139.4631\n",
      "Epoch 87/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 127.4570 - val_loss: 139.4517\n",
      "Epoch 88/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 127.6264 - val_loss: 139.4422\n",
      "Epoch 89/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 128.1408 - val_loss: 139.4115\n",
      "Epoch 90/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 127.8888 - val_loss: 139.3842\n",
      "Epoch 91/300\n",
      "378/378 [==============================] - 0s 51us/step - loss: 128.1916 - val_loss: 139.3669\n",
      "Epoch 92/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 126.043 - 0s 49us/step - loss: 127.5925 - val_loss: 139.3628\n",
      "Epoch 93/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 127.7130 - val_loss: 139.3738\n",
      "Epoch 94/300\n",
      "378/378 [==============================] - 0s 51us/step - loss: 127.6288 - val_loss: 139.3762\n",
      "Epoch 95/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 128.2678 - val_loss: 139.3621\n",
      "Epoch 96/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 127.9262 - val_loss: 139.3534\n",
      "Epoch 97/300\n",
      "378/378 [==============================] - 0s 51us/step - loss: 127.6810 - val_loss: 139.3428\n",
      "Epoch 98/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 127.4808 - val_loss: 139.3260\n",
      "Epoch 99/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 127.6759 - val_loss: 139.3097\n",
      "Epoch 100/300\n",
      "378/378 [==============================] - 0s 52us/step - loss: 127.3561 - val_loss: 139.3139\n",
      "Epoch 101/300\n",
      "378/378 [==============================] - 0s 51us/step - loss: 128.0823 - val_loss: 139.3232\n",
      "Epoch 102/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 127.6143 - val_loss: 139.3412\n",
      "Epoch 103/300\n",
      "378/378 [==============================] - 0s 46us/step - loss: 127.8013 - val_loss: 139.3657\n",
      "Epoch 104/300\n",
      "378/378 [==============================] - 0s 49us/step - loss: 127.4023 - val_loss: 139.3657\n",
      "Epoch 105/300\n",
      "378/378 [==============================] - 0s 47us/step - loss: 127.8639 - val_loss: 139.3657\n",
      "Epoch 106/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 127.5771 - val_loss: 139.3657\n",
      "Epoch 107/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 127.6990 - val_loss: 139.3657\n",
      "Epoch 108/300\n",
      "378/378 [==============================] - 0s 78us/step - loss: 127.9906 - val_loss: 139.3657\n",
      "Epoch 109/300\n",
      "378/378 [==============================] - 0s 50us/step - loss: 127.7648 - val_loss: 139.3657\n",
      "Epoch 110/300\n",
      "378/378 [==============================] - 0s 55us/step - loss: 127.6065 - val_loss: 139.3657\n",
      "Epoch 111/300\n",
      "378/378 [==============================] - 0s 88us/step - loss: 127.3161 - val_loss: 139.3657\n",
      "Epoch 112/300\n",
      "378/378 [==============================] - 0s 85us/step - loss: 127.4637 - val_loss: 139.3657\n",
      "Epoch 113/300\n",
      "378/378 [==============================] - 0s 70us/step - loss: 127.5148 - val_loss: 139.3657\n",
      "Epoch 114/300\n",
      "378/378 [==============================] - 0s 77us/step - loss: 128.2400 - val_loss: 139.3657\n",
      "Epoch 115/300\n",
      "378/378 [==============================] - 0s 71us/step - loss: 127.6829 - val_loss: 139.3657\n",
      "Epoch 116/300\n",
      "378/378 [==============================] - 0s 73us/step - loss: 127.9204 - val_loss: 139.3657\n",
      "Epoch 117/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 127.5591 - val_loss: 139.3657\n",
      "Epoch 118/300\n",
      "378/378 [==============================] - 0s 85us/step - loss: 127.8480 - val_loss: 139.3657\n",
      "Epoch 119/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 116.620 - 0s 60us/step - loss: 127.4917 - val_loss: 139.3657\n",
      "Epoch 120/300\n",
      "378/378 [==============================] - 0s 63us/step - loss: 127.5772 - val_loss: 139.3657\n",
      "Epoch 121/300\n",
      "378/378 [==============================] - 0s 74us/step - loss: 127.9660 - val_loss: 139.3657\n",
      "Epoch 122/300\n",
      "378/378 [==============================] - 0s 71us/step - loss: 127.8665 - val_loss: 139.3657\n",
      "Epoch 123/300\n",
      "378/378 [==============================] - 0s 64us/step - loss: 127.5150 - val_loss: 139.3657\n",
      "Epoch 124/300\n",
      "378/378 [==============================] - 0s 77us/step - loss: 127.5936 - val_loss: 139.3657\n",
      "Epoch 125/300\n",
      "378/378 [==============================] - 0s 74us/step - loss: 128.1108 - val_loss: 139.3657\n",
      "Epoch 126/300\n",
      "378/378 [==============================] - 0s 75us/step - loss: 127.7965 - val_loss: 139.3657\n",
      "Epoch 127/300\n",
      "378/378 [==============================] - 0s 85us/step - loss: 127.5257 - val_loss: 139.3657\n",
      "Epoch 128/300\n",
      "378/378 [==============================] - 0s 81us/step - loss: 127.5335 - val_loss: 139.3657\n",
      "Epoch 129/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 127.4638 - val_loss: 139.3657\n",
      "Epoch 130/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 127.8345 - val_loss: 139.3657\n",
      "Epoch 131/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 127.3438 - val_loss: 139.3657\n",
      "Epoch 132/300\n",
      "378/378 [==============================] - 0s 63us/step - loss: 127.8509 - val_loss: 139.3657\n",
      "Epoch 133/300\n",
      "378/378 [==============================] - 0s 60us/step - loss: 127.4404 - val_loss: 139.3657\n",
      "Epoch 134/300\n",
      "378/378 [==============================] - 0s 76us/step - loss: 127.9176 - val_loss: 139.3657\n",
      "Epoch 135/300\n",
      "378/378 [==============================] - 0s 65us/step - loss: 127.6411 - val_loss: 139.3657\n",
      "Epoch 136/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 127.4574 - val_loss: 139.3657\n",
      "Epoch 137/300\n",
      "378/378 [==============================] - 0s 57us/step - loss: 127.7479 - val_loss: 139.3657\n",
      "Epoch 138/300\n",
      "378/378 [==============================] - 0s 54us/step - loss: 127.7012 - val_loss: 139.3657\n",
      "Epoch 139/300\n",
      "378/378 [==============================] - 0s 56us/step - loss: 128.1346 - val_loss: 139.3657\n",
      "Epoch 140/300\n",
      "378/378 [==============================] - 0s 143us/step - loss: 127.2635 - val_loss: 139.3657\n",
      "Epoch 141/300\n",
      "378/378 [==============================] - 0s 102us/step - loss: 127.7890 - val_loss: 139.3657\n",
      "Epoch 142/300\n",
      " 32/378 [=>............................] - ETA: 0s - loss: 109.4108"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-212e8d604e26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_agg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_appliance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mappliance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mpred_appliance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mappliance\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_agg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_appliance = {}\n",
    "for appliance in APPLIANCES_ORDER[1:]:\n",
    "    print(appliance)\n",
    "    print(\"*\"*20)\n",
    "    np.random.seed(0)\n",
    "    from keras.layers.merge import Subtract, Minimum\n",
    "    from keras import regularizers\n",
    "    agg_input = keras.layers.Input(shape=[24],name='Aggregate')\n",
    "    appliance_dense_1 = keras.layers.Dense(units=20,name='Appliance-layer-1',activation='relu')(agg_input)\n",
    "    dropout = keras.layers.Dropout(rate=0.3,name='Droput-Appliance')(appliance_dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    out = keras.layers.Dense(units=24,name='Appliance-output',activation='relu')(dropout)\n",
    "    out = Minimum(name='Clip-to-agg')([out, agg_input])\n",
    "\n",
    "\n",
    "    model = keras.Model(agg_input, out)\n",
    "    model.compile('adam','mean_absolute_error')\n",
    "    model.fit(train_agg, train_appliance[appliance], epochs=300, validation_split=0.1)\n",
    "    pred_appliance[appliance] = model.predict(test_agg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('per-appliance.pdf','wb') as f:\n",
    "    f.write(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='pdf'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Aggregate (InputLayer)          (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Appliance-layer-1 (Dense)       (None, 20)           500         Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Droput-Appliance (Dropout)      (None, 20)           0           Appliance-layer-1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Appliance-output (Dense)        (None, 24)           504         Droput-Appliance[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "minimum_2 (Minimum)             (None, 24)           0           Appliance-output[0][0]           \n",
      "                                                                 Aggregate[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,004\n",
      "Trainable params: 1,004\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = {}\n",
    "for appliance in APPLIANCES_ORDER[1:]:\n",
    "    mae[appliance] = mean_absolute_error(test_appliance[appliance], pred_appliance[appliance])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dw         14.515641\n",
       "fridge     42.986721\n",
       "hvac      135.127259\n",
       "mw          6.303220\n",
       "oven       19.784436\n",
       "wm          5.618949\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 24)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.4446940174\n",
      "996.766596489\n"
     ]
    }
   ],
   "source": [
    "pred_hvac = model.predict(test_agg)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(pred_hvac, test_fridge))\n",
    "print(mean_absolute_error(pred_hvac, test_agg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.907349e-06\n",
       "1    -3.877686e+00\n",
       "2    -9.150000e+00\n",
       "3     0.000000e+00\n",
       "4     0.000000e+00\n",
       "5    -9.633333e+00\n",
       "6     0.000000e+00\n",
       "7     2.861023e-06\n",
       "8    -8.683333e+00\n",
       "9     9.536743e-07\n",
       "10    0.000000e+00\n",
       "11    9.536743e-07\n",
       "12   -9.583333e+00\n",
       "13   -9.516666e+00\n",
       "14   -4.711666e+01\n",
       "15    3.099442e-06\n",
       "16   -4.685000e+01\n",
       "17    9.536743e-07\n",
       "18   -7.310000e+01\n",
       "19   -7.350000e+01\n",
       "20   -4.180000e+01\n",
       "21    0.000000e+00\n",
       "22   -9.616667e+00\n",
       "23   -9.533334e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.DataFrame(pred_hvac) - pd.DataFrame(test_agg)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c39139b38>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEIoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c391557b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(test_mw[1, :]).plot(label='GT')\n",
    "#pd.Series(test_agg[1, :]).plot(label='GT')\n",
    "\n",
    "\n",
    "pd.Series(model.predict(test_agg[1:2])[0, :24]).plot(label='Pred')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
