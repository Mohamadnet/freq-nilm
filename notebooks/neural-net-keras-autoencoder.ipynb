{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from common import APPLIANCES_ORDER\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = np.load('../1H-input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subset_dataset(tensor):\n",
    "    t_subset = tensor[:, :, 180:194, :]\n",
    "    all_indices = np.array(list(range(320)))\n",
    "    for i in range(1, 7):\n",
    "        valid_homes = pd.DataFrame(t_subset[:, i, :].reshape(320, 14*24)).dropna().index\n",
    "        all_indices = np.intersect1d(all_indices, valid_homes)\n",
    "    t_subset = t_subset[all_indices, :, :, :].reshape(52, 7, 14*24)\n",
    "    \n",
    "    # Create artificial aggregate\n",
    "    t_subset[:, 0, :] = 0.0\n",
    "    for i in range(1, 7):\n",
    "        t_subset[:, 0, :] = t_subset[:, 0, :] + t_subset[:, i, :]\n",
    "    # t_subset is of shape (#home, appliance, days*hours)\n",
    "    return t_subset, all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all, valid_homes = create_subset_dataset(tensor)\n",
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_objective(y_pred, y_true):\n",
    "    with tf.name_scope(None):\n",
    "        return tf.losses.absolute_difference(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/nipun/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "import keras\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "n_movies = 3\n",
    "n_users=3\n",
    "n_latent_factors=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg = t_all[:30, 0, :].reshape(30*14, 24)\n",
    "train_hvac = t_all[:30, 1, :].reshape(30*14, 24)\n",
    "train_fridge = t_all[:30, 2, :].reshape(30*14, 24)\n",
    "train_mw = t_all[:30, 3, :].reshape(30*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_hvac = t_all[30:, 1, :].reshape(22*14, 24)\n",
    "test_fridge = t_all[30:, 2, :].reshape(22*14, 24)\n",
    "\n",
    "test_mw = t_all[30:, 3, :].reshape(22*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "test_agg = t_all[30:, 0, :].reshape(22*14, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 24)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_hvac_fridge = np.hstack([train_hvac, train_fridge])\n",
    "test_hvac_fridge = np.hstack([test_hvac, test_fridge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"377pt\" viewBox=\"0.00 0.00 415.85 377.00\" width=\"416pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 373)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-373 411.846,-373 411.846,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 121250422232 -->\n",
       "<g class=\"node\" id=\"node1\"><title>121250422232</title>\n",
       "<polygon fill=\"none\" points=\"92.1768,-324.5 92.1768,-368.5 368.381,-368.5 368.381,-324.5 92.1768,-324.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.12\" y=\"-342.3\">Aggregate: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"236.063,-324.5 236.063,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.898\" y=\"-353.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"236.063,-346.5 291.732,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.898\" y=\"-331.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"291.732,-324.5 291.732,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.057\" y=\"-353.3\">(None, 24)</text>\n",
       "<polyline fill=\"none\" points=\"291.732,-346.5 368.381,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.057\" y=\"-331.3\">(None, 24)</text>\n",
       "</g>\n",
       "<!-- 121250422288 -->\n",
       "<g class=\"node\" id=\"node2\"><title>121250422288</title>\n",
       "<polygon fill=\"none\" points=\"8.98486,-243.5 8.98486,-287.5 281.573,-287.5 281.573,-243.5 8.98486,-243.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"79.1201\" y=\"-261.3\">HVAC-layer-1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"149.255,-243.5 149.255,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.09\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"149.255,-265.5 204.924,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.09\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"204.924,-243.5 204.924,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"243.249\" y=\"-272.3\">(None, 24)</text>\n",
       "<polyline fill=\"none\" points=\"204.924,-265.5 281.573,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"243.249\" y=\"-250.3\">(None, 10)</text>\n",
       "</g>\n",
       "<!-- 121250422232&#45;&gt;121250422288 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>121250422232-&gt;121250422288</title>\n",
       "<path d=\"M207.5,-324.329C197.687,-315.209 186.07,-304.412 175.548,-294.632\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"177.828,-291.973 168.12,-287.729 173.062,-297.101 177.828,-291.973\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 121250422456 -->\n",
       "<g class=\"node\" id=\"node5\"><title>121250422456</title>\n",
       "<polygon fill=\"none\" points=\"50.7114,-0.5 50.7114,-44.5 407.846,-44.5 407.846,-0.5 50.7114,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"124.634\" y=\"-18.3\">minimum_9: Minimum</text>\n",
       "<polyline fill=\"none\" points=\"198.556,-0.5 198.556,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"226.391\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"198.556,-22.5 254.225,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"226.391\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"254.225,-0.5 254.225,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"331.036\" y=\"-29.3\">[(None, 24), (None, 24)]</text>\n",
       "<polyline fill=\"none\" points=\"254.225,-22.5 407.846,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.549\" y=\"-7.3\">(None, 24)</text>\n",
       "</g>\n",
       "<!-- 121250422232&#45;&gt;121250422456 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>121250422232-&gt;121250422456</title>\n",
       "<path d=\"M260.856,-324.321C272.223,-314.671 283.899,-302.216 290.279,-288 313.214,-236.895 294.661,-217.999 293.279,-162 292.39,-125.967 304.239,-113.793 289.279,-81 284.226,-69.9247 275.986,-59.9038 267.209,-51.4454\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"269.533,-48.8284 259.768,-44.719 264.839,-54.0213 269.533,-48.8284\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 121250422624 -->\n",
       "<g class=\"node\" id=\"node3\"><title>121250422624</title>\n",
       "<polygon fill=\"none\" points=\"0,-162.5 0,-206.5 284.558,-206.5 284.558,-162.5 0,-162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76.1201\" y=\"-180.3\">Droput-HVAC: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"152.24,-162.5 152.24,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180.075\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"152.24,-184.5 207.909,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180.075\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"207.909,-162.5 207.909,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.233\" y=\"-191.3\">(None, 10)</text>\n",
       "<polyline fill=\"none\" points=\"207.909,-184.5 284.558,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.233\" y=\"-169.3\">(None, 10)</text>\n",
       "</g>\n",
       "<!-- 121250422288&#45;&gt;121250422624 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>121250422288-&gt;121250422624</title>\n",
       "<path d=\"M144.475,-243.329C144.165,-235.183 143.805,-225.699 143.467,-216.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"146.962,-216.589 143.085,-206.729 139.967,-216.855 146.962,-216.589\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 121250422064 -->\n",
       "<g class=\"node\" id=\"node4\"><title>121250422064</title>\n",
       "<polygon fill=\"none\" points=\"11.7793,-81.5 11.7793,-125.5 280.778,-125.5 280.778,-81.5 11.7793,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.1201\" y=\"-99.3\">HVAC-output: Dense</text>\n",
       "<polyline fill=\"none\" points=\"148.461,-81.5 148.461,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176.295\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"148.461,-103.5 204.13,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176.295\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"204.13,-81.5 204.13,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.454\" y=\"-110.3\">(None, 10)</text>\n",
       "<polyline fill=\"none\" points=\"204.13,-103.5 280.778,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.454\" y=\"-88.3\">(None, 24)</text>\n",
       "</g>\n",
       "<!-- 121250422624&#45;&gt;121250422064 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>121250422624-&gt;121250422064</title>\n",
       "<path d=\"M143.351,-162.329C143.763,-154.183 144.243,-144.699 144.694,-135.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"148.194,-135.893 145.204,-125.729 141.203,-135.539 148.194,-135.893\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 121250422064&#45;&gt;121250422456 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>121250422064-&gt;121250422456</title>\n",
       "<path d=\"M168.521,-81.3294C178.104,-72.2085 189.447,-61.4122 199.722,-51.6322\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"202.144,-54.1586 206.975,-44.729 197.318,-49.0882 202.144,-54.1586\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "from keras.layers.merge import Subtract, Minimum\n",
    "agg_input = keras.layers.Input(shape=[24],name='Aggregate')\n",
    "hvac_dense_1 = keras.layers.Dense(units=10,name='HVAC-layer-1',activation='relu')(agg_input)\n",
    "dropout = keras.layers.Dropout(rate=0.3,name='Droput-HVAC')(hvac_dense_1)\n",
    "\n",
    "\n",
    "out = keras.layers.Dense(units=24,name='HVAC-output',activation='relu')(dropout)\n",
    "out = Minimum()([out, agg_input])\n",
    "\n",
    "\n",
    "model = keras.Model(agg_input, out)\n",
    "\n",
    "\n",
    "SVG(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Aggregate (InputLayer)          (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "HVAC-layer-1 (Dense)            (None, 10)           250         Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Droput-HVAC (Dropout)           (None, 10)           0           HVAC-layer-1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "HVAC-output (Dense)             (None, 24)           264         Droput-HVAC[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "minimum_9 (Minimum)             (None, 24)           0           HVAC-output[0][0]                \n",
      "                                                                 Aggregate[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 514\n",
      "Trainable params: 514\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 336 samples, validate on 84 samples\n",
      "Epoch 1/300\n",
      "336/336 [==============================] - 1s 3ms/step - loss: 692.6790 - val_loss: 702.7856\n",
      "Epoch 2/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 653.0743 - val_loss: 664.2887\n",
      "Epoch 3/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 617.9533 - val_loss: 624.9523\n",
      "Epoch 4/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 572.8267 - val_loss: 582.4222\n",
      "Epoch 5/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 530.8641 - val_loss: 532.7503\n",
      "Epoch 6/300\n",
      "336/336 [==============================] - 0s 58us/step - loss: 503.9723 - val_loss: 475.7102\n",
      "Epoch 7/300\n",
      "336/336 [==============================] - 0s 56us/step - loss: 455.5215 - val_loss: 411.7104\n",
      "Epoch 8/300\n",
      "336/336 [==============================] - 0s 56us/step - loss: 416.7683 - val_loss: 353.6269\n",
      "Epoch 9/300\n",
      "336/336 [==============================] - 0s 56us/step - loss: 371.1581 - val_loss: 315.4803\n",
      "Epoch 10/300\n",
      "336/336 [==============================] - 0s 60us/step - loss: 339.0267 - val_loss: 288.3747\n",
      "Epoch 11/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 327.8502 - val_loss: 265.6668\n",
      "Epoch 12/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 293.5484 - val_loss: 241.7563\n",
      "Epoch 13/300\n",
      "336/336 [==============================] - 0s 58us/step - loss: 289.8612 - val_loss: 215.1163\n",
      "Epoch 14/300\n",
      "336/336 [==============================] - 0s 59us/step - loss: 261.7061 - val_loss: 202.5070\n",
      "Epoch 15/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 257.5845 - val_loss: 196.0480\n",
      "Epoch 16/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 250.0681 - val_loss: 192.5681\n",
      "Epoch 17/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 236.7762 - val_loss: 190.9095\n",
      "Epoch 18/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 227.8701 - val_loss: 189.9575\n",
      "Epoch 19/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 221.5607 - val_loss: 189.6031\n",
      "Epoch 20/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 218.6312 - val_loss: 189.1489\n",
      "Epoch 21/300\n",
      "336/336 [==============================] - 0s 64us/step - loss: 214.5537 - val_loss: 188.8623\n",
      "Epoch 22/300\n",
      "336/336 [==============================] - 0s 60us/step - loss: 213.6219 - val_loss: 188.6074\n",
      "Epoch 23/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 211.3863 - val_loss: 188.1147\n",
      "Epoch 24/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 212.0084 - val_loss: 187.8976\n",
      "Epoch 25/300\n",
      "336/336 [==============================] - 0s 72us/step - loss: 202.0613 - val_loss: 186.3481\n",
      "Epoch 26/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 209.5050 - val_loss: 183.1684\n",
      "Epoch 27/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 200.9121 - val_loss: 180.5610\n",
      "Epoch 28/300\n",
      "336/336 [==============================] - 0s 58us/step - loss: 195.4936 - val_loss: 178.2179\n",
      "Epoch 29/300\n",
      "336/336 [==============================] - 0s 58us/step - loss: 195.3655 - val_loss: 176.5757\n",
      "Epoch 30/300\n",
      "336/336 [==============================] - 0s 58us/step - loss: 192.1816 - val_loss: 176.0077\n",
      "Epoch 31/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 188.4637 - val_loss: 175.9597\n",
      "Epoch 32/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 189.5612 - val_loss: 175.9254\n",
      "Epoch 33/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 185.8556 - val_loss: 175.8387\n",
      "Epoch 34/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 188.5775 - val_loss: 175.6668\n",
      "Epoch 35/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 189.3086 - val_loss: 175.6706\n",
      "Epoch 36/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 186.6730 - val_loss: 175.5875\n",
      "Epoch 37/300\n",
      "336/336 [==============================] - 0s 56us/step - loss: 187.1960 - val_loss: 175.5340\n",
      "Epoch 38/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 188.0336 - val_loss: 175.5094\n",
      "Epoch 39/300\n",
      "336/336 [==============================] - 0s 60us/step - loss: 181.7569 - val_loss: 175.4679\n",
      "Epoch 40/300\n",
      "336/336 [==============================] - 0s 56us/step - loss: 182.7203 - val_loss: 175.4368\n",
      "Epoch 41/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 180.7321 - val_loss: 175.4287\n",
      "Epoch 42/300\n",
      "336/336 [==============================] - 0s 58us/step - loss: 185.5607 - val_loss: 175.4078\n",
      "Epoch 43/300\n",
      "336/336 [==============================] - 0s 58us/step - loss: 180.8858 - val_loss: 175.3837\n",
      "Epoch 44/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 181.0289 - val_loss: 175.3618\n",
      "Epoch 45/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 181.4405 - val_loss: 175.3800\n",
      "Epoch 46/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 182.4863 - val_loss: 175.4047\n",
      "Epoch 47/300\n",
      "336/336 [==============================] - 0s 56us/step - loss: 176.6953 - val_loss: 175.4309\n",
      "Epoch 48/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 181.1354 - val_loss: 175.4314\n",
      "Epoch 49/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 184.5894 - val_loss: 175.4314\n",
      "Epoch 50/300\n",
      "336/336 [==============================] - 0s 56us/step - loss: 178.1411 - val_loss: 175.4314\n",
      "Epoch 51/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 177.5961 - val_loss: 175.4314\n",
      "Epoch 52/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 177.3643 - val_loss: 175.4314\n",
      "Epoch 53/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 177.7334 - val_loss: 175.4314\n",
      "Epoch 54/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 177.7982 - val_loss: 175.4314\n",
      "Epoch 55/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 180.9265 - val_loss: 175.4314\n",
      "Epoch 56/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 176.9166 - val_loss: 175.4314\n",
      "Epoch 57/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 176.8938 - val_loss: 175.4314\n",
      "Epoch 58/300\n",
      "336/336 [==============================] - 0s 64us/step - loss: 176.9771 - val_loss: 175.4314\n",
      "Epoch 59/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 172.9778 - val_loss: 175.4314\n",
      "Epoch 60/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 178.0100 - val_loss: 175.4314\n",
      "Epoch 61/300\n",
      "336/336 [==============================] - 0s 79us/step - loss: 173.2464 - val_loss: 175.4314\n",
      "Epoch 62/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 173.4995 - val_loss: 175.4314\n",
      "Epoch 63/300\n",
      "336/336 [==============================] - 0s 73us/step - loss: 173.1214 - val_loss: 175.4314\n",
      "Epoch 64/300\n",
      "336/336 [==============================] - 0s 92us/step - loss: 169.5637 - val_loss: 171.5798\n",
      "Epoch 65/300\n",
      "336/336 [==============================] - 0s 80us/step - loss: 151.8825 - val_loss: 135.8109\n",
      "Epoch 66/300\n",
      "336/336 [==============================] - 0s 89us/step - loss: 139.7118 - val_loss: 133.7658\n",
      "Epoch 67/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 135.2442 - val_loss: 133.7658\n",
      "Epoch 68/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 132.2976 - val_loss: 133.7658\n",
      "Epoch 69/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 132.7552 - val_loss: 133.7658\n",
      "Epoch 70/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 132.9624 - val_loss: 133.7658\n",
      "Epoch 71/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 131.6701 - val_loss: 133.7658\n",
      "Epoch 72/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 130.7086 - val_loss: 133.7658\n",
      "Epoch 73/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 133.7049 - val_loss: 133.7658\n",
      "Epoch 74/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 129.2363 - val_loss: 133.7658\n",
      "Epoch 75/300\n",
      "336/336 [==============================] - 0s 53us/step - loss: 128.6995 - val_loss: 133.7658\n",
      "Epoch 76/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 132.9263 - val_loss: 133.7658\n",
      "Epoch 77/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 131.6657 - val_loss: 133.7658\n",
      "Epoch 78/300\n",
      "336/336 [==============================] - 0s 59us/step - loss: 130.7667 - val_loss: 133.7658\n",
      "Epoch 79/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 129.9167 - val_loss: 133.7658\n",
      "Epoch 80/300\n",
      "336/336 [==============================] - 0s 53us/step - loss: 130.7108 - val_loss: 133.7658\n",
      "Epoch 81/300\n",
      "336/336 [==============================] - 0s 58us/step - loss: 129.3181 - val_loss: 133.7658\n",
      "Epoch 82/300\n",
      "336/336 [==============================] - 0s 54us/step - loss: 128.9547 - val_loss: 133.7658\n",
      "Epoch 83/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 131.9954 - val_loss: 133.7658\n",
      "Epoch 84/300\n",
      "336/336 [==============================] - 0s 52us/step - loss: 130.8886 - val_loss: 133.7658\n",
      "Epoch 85/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 129.7843 - val_loss: 133.7658\n",
      "Epoch 86/300\n",
      "336/336 [==============================] - 0s 54us/step - loss: 130.0027 - val_loss: 133.7658\n",
      "Epoch 87/300\n",
      "336/336 [==============================] - 0s 52us/step - loss: 128.4356 - val_loss: 133.7658\n",
      "Epoch 88/300\n",
      "336/336 [==============================] - 0s 52us/step - loss: 129.9437 - val_loss: 133.7658\n",
      "Epoch 89/300\n",
      "336/336 [==============================] - 0s 56us/step - loss: 129.9515 - val_loss: 133.7658\n",
      "Epoch 90/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 129.7333 - val_loss: 133.7658\n",
      "Epoch 91/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 130.0661 - val_loss: 133.7658\n",
      "Epoch 92/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 130.3049 - val_loss: 133.7658\n",
      "Epoch 93/300\n",
      "336/336 [==============================] - 0s 59us/step - loss: 129.9771 - val_loss: 133.7658\n",
      "Epoch 94/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 127.9847 - val_loss: 133.7658\n",
      "Epoch 95/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 129.5259 - val_loss: 133.7658\n",
      "Epoch 96/300\n",
      "336/336 [==============================] - 0s 56us/step - loss: 128.5186 - val_loss: 133.7658\n",
      "Epoch 97/300\n",
      "336/336 [==============================] - 0s 59us/step - loss: 129.8236 - val_loss: 133.7658\n",
      "Epoch 98/300\n",
      "336/336 [==============================] - 0s 59us/step - loss: 129.3404 - val_loss: 133.7658\n",
      "Epoch 99/300\n",
      "336/336 [==============================] - 0s 54us/step - loss: 127.7027 - val_loss: 133.7658\n",
      "Epoch 100/300\n",
      "336/336 [==============================] - 0s 54us/step - loss: 128.8893 - val_loss: 133.7658\n",
      "Epoch 101/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 128.5517 - val_loss: 133.7658\n",
      "Epoch 102/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 129.4396 - val_loss: 133.7658\n",
      "Epoch 103/300\n",
      "336/336 [==============================] - 0s 53us/step - loss: 128.6435 - val_loss: 133.7658\n",
      "Epoch 104/300\n",
      "336/336 [==============================] - 0s 54us/step - loss: 128.8074 - val_loss: 133.7658\n",
      "Epoch 105/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 128.4578 - val_loss: 133.7658\n",
      "Epoch 106/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 130.1595 - val_loss: 133.7658\n",
      "Epoch 107/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 127.8871 - val_loss: 133.7658\n",
      "Epoch 108/300\n",
      "336/336 [==============================] - 0s 64us/step - loss: 129.1790 - val_loss: 133.7658\n",
      "Epoch 109/300\n",
      "336/336 [==============================] - 0s 76us/step - loss: 129.8513 - val_loss: 133.7658\n",
      "Epoch 110/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 128.4147 - val_loss: 133.7658\n",
      "Epoch 111/300\n",
      "336/336 [==============================] - 0s 78us/step - loss: 127.9701 - val_loss: 133.7658\n",
      "Epoch 112/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 128.0621 - val_loss: 133.7658\n",
      "Epoch 113/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 131.5403 - val_loss: 133.7658\n",
      "Epoch 114/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 127.7314 - val_loss: 133.7658\n",
      "Epoch 115/300\n",
      "336/336 [==============================] - 0s 59us/step - loss: 127.4862 - val_loss: 133.7658\n",
      "Epoch 116/300\n",
      "336/336 [==============================] - 0s 72us/step - loss: 129.4078 - val_loss: 133.7658\n",
      "Epoch 117/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 128.0165 - val_loss: 133.7658\n",
      "Epoch 118/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 128.9569 - val_loss: 133.7658\n",
      "Epoch 119/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 128.7156 - val_loss: 133.7658\n",
      "Epoch 120/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 129.4570 - val_loss: 133.7658\n",
      "Epoch 121/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 127.7896 - val_loss: 133.7658\n",
      "Epoch 122/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 128.9238 - val_loss: 133.7658\n",
      "Epoch 123/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 129.5153 - val_loss: 133.7658\n",
      "Epoch 124/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 129.3165 - val_loss: 133.7658\n",
      "Epoch 125/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 130.1235 - val_loss: 133.7658\n",
      "Epoch 126/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 128.9951 - val_loss: 133.7658\n",
      "Epoch 127/300\n",
      "336/336 [==============================] - 0s 78us/step - loss: 127.9323 - val_loss: 133.7658\n",
      "Epoch 128/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 129.2945 - val_loss: 133.7658\n",
      "Epoch 129/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 128.2094 - val_loss: 133.7658\n",
      "Epoch 130/300\n",
      "336/336 [==============================] - ETA: 0s - loss: 123.634 - 0s 68us/step - loss: 128.7053 - val_loss: 133.7658\n",
      "Epoch 131/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 129.1798 - val_loss: 133.7658\n",
      "Epoch 132/300\n",
      "336/336 [==============================] - ETA: 0s - loss: 134.595 - 0s 64us/step - loss: 127.4713 - val_loss: 133.7658\n",
      "Epoch 133/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 128.2646 - val_loss: 133.7658\n",
      "Epoch 134/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 131.2487 - val_loss: 133.7658\n",
      "Epoch 135/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 129.5097 - val_loss: 133.7658\n",
      "Epoch 136/300\n",
      "336/336 [==============================] - 0s 76us/step - loss: 128.2577 - val_loss: 133.7658\n",
      "Epoch 137/300\n",
      "336/336 [==============================] - 0s 74us/step - loss: 128.1542 - val_loss: 133.7658\n",
      "Epoch 138/300\n",
      "336/336 [==============================] - 0s 74us/step - loss: 130.2284 - val_loss: 133.7658\n",
      "Epoch 139/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 128.6886 - val_loss: 133.7658\n",
      "Epoch 140/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 128.0238 - val_loss: 133.7658\n",
      "Epoch 141/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 127.9227 - val_loss: 133.7658\n",
      "Epoch 142/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 129.1601 - val_loss: 133.7658\n",
      "Epoch 143/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 129.1107 - val_loss: 133.7658\n",
      "Epoch 144/300\n",
      "336/336 [==============================] - 0s 75us/step - loss: 127.6508 - val_loss: 133.7658\n",
      "Epoch 145/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 128.4891 - val_loss: 133.7658\n",
      "Epoch 146/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 129.0460 - val_loss: 133.7658\n",
      "Epoch 147/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 127.8333 - val_loss: 133.7658\n",
      "Epoch 148/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 128.6062 - val_loss: 133.7658\n",
      "Epoch 149/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 127.9381 - val_loss: 133.7658\n",
      "Epoch 150/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 127.4177 - val_loss: 133.7658\n",
      "Epoch 151/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 128.9241 - val_loss: 133.7658\n",
      "Epoch 152/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 0s 66us/step - loss: 128.5321 - val_loss: 133.7658\n",
      "Epoch 153/300\n",
      "336/336 [==============================] - 0s 79us/step - loss: 129.4547 - val_loss: 133.7658\n",
      "Epoch 154/300\n",
      "336/336 [==============================] - 0s 59us/step - loss: 128.0349 - val_loss: 133.7658\n",
      "Epoch 155/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 128.1979 - val_loss: 133.7658\n",
      "Epoch 156/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 128.0771 - val_loss: 133.7658\n",
      "Epoch 157/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 127.8169 - val_loss: 133.7658\n",
      "Epoch 158/300\n",
      "336/336 [==============================] - 0s 74us/step - loss: 127.6376 - val_loss: 133.7658\n",
      "Epoch 159/300\n",
      "336/336 [==============================] - 0s 60us/step - loss: 127.8972 - val_loss: 133.7658\n",
      "Epoch 160/300\n",
      "336/336 [==============================] - 0s 58us/step - loss: 128.6477 - val_loss: 133.7658\n",
      "Epoch 161/300\n",
      "336/336 [==============================] - 0s 72us/step - loss: 128.7535 - val_loss: 133.7658\n",
      "Epoch 162/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 128.7552 - val_loss: 133.7658\n",
      "Epoch 163/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 127.4268 - val_loss: 133.7658\n",
      "Epoch 164/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 130.0970 - val_loss: 133.7658\n",
      "Epoch 165/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 127.6508 - val_loss: 133.7658\n",
      "Epoch 166/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 127.6201 - val_loss: 133.7658\n",
      "Epoch 167/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 127.5048 - val_loss: 133.7658\n",
      "Epoch 168/300\n",
      "336/336 [==============================] - ETA: 0s - loss: 130.577 - 0s 60us/step - loss: 127.7025 - val_loss: 133.7658\n",
      "Epoch 169/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 127.5237 - val_loss: 133.7658\n",
      "Epoch 170/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 127.5506 - val_loss: 133.7658\n",
      "Epoch 171/300\n",
      "336/336 [==============================] - 0s 64us/step - loss: 127.6280 - val_loss: 133.7658\n",
      "Epoch 172/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 127.9742 - val_loss: 133.7658\n",
      "Epoch 173/300\n",
      "336/336 [==============================] - 0s 57us/step - loss: 127.4134 - val_loss: 133.7658\n",
      "Epoch 174/300\n",
      "336/336 [==============================] - 0s 72us/step - loss: 127.4355 - val_loss: 133.7658\n",
      "Epoch 175/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 127.8922 - val_loss: 133.7658\n",
      "Epoch 176/300\n",
      "336/336 [==============================] - 0s 72us/step - loss: 127.5068 - val_loss: 133.7658\n",
      "Epoch 177/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 127.5475 - val_loss: 133.7658\n",
      "Epoch 178/300\n",
      "336/336 [==============================] - 0s 73us/step - loss: 127.7605 - val_loss: 133.7658\n",
      "Epoch 179/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 128.4511 - val_loss: 133.7658\n",
      "Epoch 180/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 127.6004 - val_loss: 133.7658\n",
      "Epoch 181/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 127.7362 - val_loss: 133.7658\n",
      "Epoch 182/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.4724 - val_loss: 133.7658\n",
      "Epoch 183/300\n",
      "336/336 [==============================] - ETA: 0s - loss: 132.003 - 0s 62us/step - loss: 127.3083 - val_loss: 133.7658\n",
      "Epoch 184/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 127.3534 - val_loss: 133.7658\n",
      "Epoch 185/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 127.5559 - val_loss: 133.7658\n",
      "Epoch 186/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 127.4255 - val_loss: 133.7658\n",
      "Epoch 187/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 129.3986 - val_loss: 133.7658\n",
      "Epoch 188/300\n",
      "336/336 [==============================] - 0s 72us/step - loss: 127.3720 - val_loss: 133.7658\n",
      "Epoch 189/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 127.9828 - val_loss: 133.7658\n",
      "Epoch 190/300\n",
      "336/336 [==============================] - 0s 77us/step - loss: 129.3226 - val_loss: 133.7658\n",
      "Epoch 191/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 127.6427 - val_loss: 133.7658\n",
      "Epoch 192/300\n",
      "336/336 [==============================] - 0s 73us/step - loss: 127.7818 - val_loss: 133.7658\n",
      "Epoch 193/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.3934 - val_loss: 133.7658\n",
      "Epoch 194/300\n",
      "336/336 [==============================] - 0s 72us/step - loss: 127.9176 - val_loss: 133.7658\n",
      "Epoch 195/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 128.1219 - val_loss: 133.7658\n",
      "Epoch 196/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.7551 - val_loss: 133.7658\n",
      "Epoch 197/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 127.7169 - val_loss: 133.7658\n",
      "Epoch 198/300\n",
      "336/336 [==============================] - 0s 73us/step - loss: 127.4814 - val_loss: 133.7658\n",
      "Epoch 199/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 128.8728 - val_loss: 133.7658\n",
      "Epoch 200/300\n",
      "336/336 [==============================] - 0s 74us/step - loss: 127.9932 - val_loss: 133.7658\n",
      "Epoch 201/300\n",
      "336/336 [==============================] - ETA: 0s - loss: 119.724 - 0s 74us/step - loss: 127.8194 - val_loss: 133.7658\n",
      "Epoch 202/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 127.6030 - val_loss: 133.7658\n",
      "Epoch 203/300\n",
      "336/336 [==============================] - 0s 76us/step - loss: 128.0707 - val_loss: 133.7658\n",
      "Epoch 204/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.4469 - val_loss: 133.7658\n",
      "Epoch 205/300\n",
      "336/336 [==============================] - 0s 77us/step - loss: 127.3878 - val_loss: 133.7658\n",
      "Epoch 206/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.4252 - val_loss: 133.7658\n",
      "Epoch 207/300\n",
      "336/336 [==============================] - 0s 75us/step - loss: 128.1630 - val_loss: 133.7658\n",
      "Epoch 208/300\n",
      "336/336 [==============================] - 0s 64us/step - loss: 128.9359 - val_loss: 133.7658\n",
      "Epoch 209/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.5200 - val_loss: 133.7658\n",
      "Epoch 210/300\n",
      "336/336 [==============================] - 0s 53us/step - loss: 127.9289 - val_loss: 133.7658\n",
      "Epoch 211/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 128.0905 - val_loss: 133.7658\n",
      "Epoch 212/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 127.9751 - val_loss: 133.7658\n",
      "Epoch 213/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 128.1921 - val_loss: 133.7658\n",
      "Epoch 214/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 127.8278 - val_loss: 133.7658\n",
      "Epoch 215/300\n",
      "336/336 [==============================] - 0s 86us/step - loss: 127.4425 - val_loss: 133.7658\n",
      "Epoch 216/300\n",
      "336/336 [==============================] - 0s 81us/step - loss: 127.3766 - val_loss: 133.7658\n",
      "Epoch 217/300\n",
      "336/336 [==============================] - 0s 78us/step - loss: 127.9149 - val_loss: 133.7658\n",
      "Epoch 218/300\n",
      "336/336 [==============================] - 0s 79us/step - loss: 128.9381 - val_loss: 133.7658\n",
      "Epoch 219/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 127.8307 - val_loss: 133.7658\n",
      "Epoch 220/300\n",
      "336/336 [==============================] - 0s 72us/step - loss: 127.4580 - val_loss: 133.7658\n",
      "Epoch 221/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 127.3681 - val_loss: 133.7658\n",
      "Epoch 222/300\n",
      "336/336 [==============================] - 0s 75us/step - loss: 127.6549 - val_loss: 133.7658\n",
      "Epoch 223/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 127.5009 - val_loss: 133.7658\n",
      "Epoch 224/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 127.7096 - val_loss: 133.7658\n",
      "Epoch 225/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 127.6722 - val_loss: 133.7658\n",
      "Epoch 226/300\n",
      "336/336 [==============================] - 0s 60us/step - loss: 127.3552 - val_loss: 133.7658\n",
      "Epoch 227/300\n",
      "336/336 [==============================] - 0s 60us/step - loss: 127.9594 - val_loss: 133.7658\n",
      "Epoch 228/300\n",
      "336/336 [==============================] - 0s 60us/step - loss: 127.6185 - val_loss: 133.7658\n",
      "Epoch 229/300\n",
      "336/336 [==============================] - 0s 60us/step - loss: 127.5970 - val_loss: 133.7658\n",
      "Epoch 230/300\n",
      "336/336 [==============================] - 0s 60us/step - loss: 127.6845 - val_loss: 133.7658\n",
      "Epoch 231/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 127.5203 - val_loss: 133.7658\n",
      "Epoch 232/300\n",
      "336/336 [==============================] - ETA: 0s - loss: 123.536 - 0s 60us/step - loss: 127.6013 - val_loss: 133.7658\n",
      "Epoch 233/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.4434 - val_loss: 133.7658\n",
      "Epoch 234/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 128.4986 - val_loss: 133.7658\n",
      "Epoch 235/300\n",
      "336/336 [==============================] - 0s 64us/step - loss: 127.8313 - val_loss: 133.7658\n",
      "Epoch 236/300\n",
      "336/336 [==============================] - 0s 75us/step - loss: 128.0290 - val_loss: 133.7658\n",
      "Epoch 237/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 128.0486 - val_loss: 133.7658\n",
      "Epoch 238/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 127.6078 - val_loss: 133.7658\n",
      "Epoch 239/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 127.5091 - val_loss: 133.7658\n",
      "Epoch 240/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 127.5010 - val_loss: 133.7658\n",
      "Epoch 241/300\n",
      "336/336 [==============================] - 0s 72us/step - loss: 127.4466 - val_loss: 133.7658\n",
      "Epoch 242/300\n",
      "336/336 [==============================] - 0s 58us/step - loss: 128.0611 - val_loss: 133.7658\n",
      "Epoch 243/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 127.4462 - val_loss: 133.7658\n",
      "Epoch 244/300\n",
      "336/336 [==============================] - 0s 59us/step - loss: 127.2001 - val_loss: 133.7658\n",
      "Epoch 245/300\n",
      "336/336 [==============================] - 0s 56us/step - loss: 127.4153 - val_loss: 133.7658\n",
      "Epoch 246/300\n",
      "336/336 [==============================] - 0s 55us/step - loss: 127.3631 - val_loss: 133.7658\n",
      "Epoch 247/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 127.4924 - val_loss: 133.7658\n",
      "Epoch 248/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 127.4335 - val_loss: 133.7658\n",
      "Epoch 249/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 127.3688 - val_loss: 133.7658\n",
      "Epoch 250/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 127.6383 - val_loss: 133.7658\n",
      "Epoch 251/300\n",
      "336/336 [==============================] - 0s 75us/step - loss: 127.5823 - val_loss: 133.7658\n",
      "Epoch 252/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 127.7930 - val_loss: 133.7658\n",
      "Epoch 253/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 128.3223 - val_loss: 133.7658\n",
      "Epoch 254/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.6905 - val_loss: 133.7658\n",
      "Epoch 255/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 127.4822 - val_loss: 133.7658\n",
      "Epoch 256/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 127.4984 - val_loss: 133.7658\n",
      "Epoch 257/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 129.5880 - val_loss: 133.7658\n",
      "Epoch 258/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 127.4316 - val_loss: 133.7658\n",
      "Epoch 259/300\n",
      "336/336 [==============================] - 0s 79us/step - loss: 127.4198 - val_loss: 133.7658\n",
      "Epoch 260/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 127.4271 - val_loss: 133.7658\n",
      "Epoch 261/300\n",
      "336/336 [==============================] - 0s 69us/step - loss: 127.5349 - val_loss: 133.7658\n",
      "Epoch 262/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 127.9403 - val_loss: 133.7658\n",
      "Epoch 263/300\n",
      "336/336 [==============================] - 0s 73us/step - loss: 127.4576 - val_loss: 133.7658\n",
      "Epoch 264/300\n",
      "336/336 [==============================] - 0s 60us/step - loss: 127.4524 - val_loss: 133.7658\n",
      "Epoch 265/300\n",
      "336/336 [==============================] - 0s 77us/step - loss: 127.6966 - val_loss: 133.7658\n",
      "Epoch 266/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 127.4196 - val_loss: 133.7658\n",
      "Epoch 267/300\n",
      "336/336 [==============================] - 0s 81us/step - loss: 127.4172 - val_loss: 133.7658\n",
      "Epoch 268/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 127.3341 - val_loss: 133.7658\n",
      "Epoch 269/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 127.5549 - val_loss: 133.7658\n",
      "Epoch 270/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.4591 - val_loss: 133.7658\n",
      "Epoch 271/300\n",
      "336/336 [==============================] - 0s 64us/step - loss: 127.4397 - val_loss: 133.7658\n",
      "Epoch 272/300\n",
      "336/336 [==============================] - 0s 59us/step - loss: 127.6069 - val_loss: 133.7658\n",
      "Epoch 273/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 127.3210 - val_loss: 133.7658\n",
      "Epoch 274/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 126.8927 - val_loss: 133.7658\n",
      "Epoch 275/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 127.3697 - val_loss: 133.7658\n",
      "Epoch 276/300\n",
      "336/336 [==============================] - 0s 74us/step - loss: 127.6538 - val_loss: 133.7658\n",
      "Epoch 277/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 127.3884 - val_loss: 133.7658\n",
      "Epoch 278/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 127.5091 - val_loss: 133.7658\n",
      "Epoch 279/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.8392 - val_loss: 133.7658\n",
      "Epoch 280/300\n",
      "336/336 [==============================] - 0s 74us/step - loss: 128.1825 - val_loss: 133.7658\n",
      "Epoch 281/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.4474 - val_loss: 133.7658\n",
      "Epoch 282/300\n",
      "336/336 [==============================] - 0s 77us/step - loss: 127.3741 - val_loss: 133.7658\n",
      "Epoch 283/300\n",
      "336/336 [==============================] - 0s 61us/step - loss: 127.2466 - val_loss: 133.7658\n",
      "Epoch 284/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 127.6420 - val_loss: 133.7658\n",
      "Epoch 285/300\n",
      "336/336 [==============================] - 0s 62us/step - loss: 127.4862 - val_loss: 133.7658\n",
      "Epoch 286/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 127.4956 - val_loss: 133.7658\n",
      "Epoch 287/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.2613 - val_loss: 133.7658\n",
      "Epoch 288/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 127.3240 - val_loss: 133.7658\n",
      "Epoch 289/300\n",
      "336/336 [==============================] - 0s 67us/step - loss: 127.3993 - val_loss: 133.7658\n",
      "Epoch 290/300\n",
      "336/336 [==============================] - 0s 65us/step - loss: 127.5211 - val_loss: 133.7658\n",
      "Epoch 291/300\n",
      "336/336 [==============================] - 0s 72us/step - loss: 127.3809 - val_loss: 133.7658\n",
      "Epoch 292/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 127.5509 - val_loss: 133.7658\n",
      "Epoch 293/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 127.5355 - val_loss: 133.7658\n",
      "Epoch 294/300\n",
      "336/336 [==============================] - 0s 88us/step - loss: 127.5802 - val_loss: 133.7658\n",
      "Epoch 295/300\n",
      "336/336 [==============================] - 0s 63us/step - loss: 127.3778 - val_loss: 133.7658\n",
      "Epoch 296/300\n",
      "336/336 [==============================] - 0s 78us/step - loss: 127.8628 - val_loss: 133.7658\n",
      "Epoch 297/300\n",
      "336/336 [==============================] - 0s 71us/step - loss: 127.4913 - val_loss: 133.7658\n",
      "Epoch 298/300\n",
      "336/336 [==============================] - 0s 70us/step - loss: 127.5329 - val_loss: 133.7658\n",
      "Epoch 299/300\n",
      "336/336 [==============================] - 0s 66us/step - loss: 127.5818 - val_loss: 133.7658\n",
      "Epoch 300/300\n",
      "336/336 [==============================] - 0s 68us/step - loss: 127.2345 - val_loss: 133.7658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3b964fd0>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','mean_absolute_error')\n",
    "model.fit(train_agg, train_hvac, epochs=300, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  284.06668091,   260.91665649,   280.2166748 ,   287.58334351,\n",
       "         378.4833374 ,  1008.7166748 ,    23.43333244,   429.66665649,\n",
       "         460.81668091,   585.70001221,   349.2333374 ,   770.61663818,\n",
       "         563.9666748 ,   694.04998779,  1034.51672363,  1055.44995117,\n",
       "         657.29998779,  1054.2833252 ,   453.5       ,   492.45001221,\n",
       "         521.66668701,   243.7666626 ,   235.9833374 ,     0.        ])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hvac[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 24)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.12725879\n",
      "2.32021058244e-05\n"
     ]
    }
   ],
   "source": [
    "pred_hvac = model.predict(test_agg)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(pred_hvac, test_hvac))\n",
    "print(mean_absolute_error(pred_hvac, test_agg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.000061\n",
       "1     0.000060\n",
       "2     0.000058\n",
       "3     0.000058\n",
       "4     0.000079\n",
       "5     0.000116\n",
       "6     0.000062\n",
       "7     0.000059\n",
       "8     0.000082\n",
       "9     0.000054\n",
       "10    0.000099\n",
       "11    0.000114\n",
       "12    0.000116\n",
       "13    0.000116\n",
       "14    0.000242\n",
       "15    0.000233\n",
       "16    0.000221\n",
       "17    0.000226\n",
       "18    0.000233\n",
       "19    0.000233\n",
       "20    0.000221\n",
       "21    0.000099\n",
       "22    0.000114\n",
       "23    0.000115\n",
       "dtype: float64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.DataFrame(pred_hvac) - pd.DataFrame(test_agg)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c372f0c88>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c372f0b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(test_hvac[1, :]).plot(label='GT')\n",
    "#pd.Series(test_agg[1, :]).plot(label='GT')\n",
    "\n",
    "\n",
    "pd.Series(model.predict(test_agg[1:2])[0, :24]).plot(label='Pred')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
