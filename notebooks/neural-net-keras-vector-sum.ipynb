{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from common import APPLIANCES_ORDER\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = np.load('../1H-input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subset_dataset(tensor):\n",
    "    t_subset = tensor[:, :, 180:194, :]\n",
    "    all_indices = np.array(list(range(320)))\n",
    "    for i in range(1, 7):\n",
    "        valid_homes = pd.DataFrame(t_subset[:, i, :].reshape(320, 14*24)).dropna().index\n",
    "        all_indices = np.intersect1d(all_indices, valid_homes)\n",
    "    t_subset = t_subset[all_indices, :, :, :].reshape(52, 7, 14*24)\n",
    "    \n",
    "    # Create artificial aggregate\n",
    "    t_subset[:, 0, :] = 0.0\n",
    "    for i in range(1, 7):\n",
    "        t_subset[:, 0, :] = t_subset[:, 0, :] + t_subset[:, i, :]\n",
    "    # t_subset is of shape (#home, appliance, days*hours)\n",
    "    return t_subset, all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all, valid_homes = create_subset_dataset(tensor)\n",
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_objective(y_pred, y_true):\n",
    "    with tf.name_scope(None):\n",
    "        return tf.losses.absolute_difference(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/nipun/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "import keras\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "n_movies = 3\n",
    "n_users=3\n",
    "n_latent_factors=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg = t_all[:30, 0, :].reshape(30*14, 24)\n",
    "train_appliances = t_all[:30, 1:, :].reshape(30*14, 6*24)\n",
    "train_hvac = t_all[:30, 1, :].reshape(30*14, 24)\n",
    "train_fridge = t_all[:30, 2, :].reshape(30*14, 24)\n",
    "train_mw = t_all[:30, 3, :].reshape(30*14, 24)\n",
    "train_dw = t_all[:30, 4, :].reshape(30*14, 24)\n",
    "train_wm = t_all[:30, 5, :].reshape(30*14, 24)\n",
    "train_oven = t_all[:30, 6, :].reshape(30*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_hvac = t_all[30:, 1, :].reshape(22*14, 24)\n",
    "test_fridge = t_all[30:, 2, :].reshape(22*14, 24)\n",
    "test_mw = t_all[30:, 3, :].reshape(22*14, 24)\n",
    "test_dw = t_all[30:, 4, :].reshape(22*14, 24)\n",
    "test_wm = t_all[30:, 5, :].reshape(22*14, 24)\n",
    "test_oven = t_all[30:, 6, :].reshape(22*14, 24)\n",
    "test_appliance = t_all[30:, 1:, :].reshape(22*14, 6*24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_agg = t_all[30:, 0, :].reshape(22*14, 24)\n",
    "test_appliances = t_all[30:, 1:, :].reshape(22*14, 6*24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 24)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_hvac_fridge = np.hstack([train_hvac, train_fridge])\n",
    "test_hvac_fridge = np.hstack([test_hvac, test_fridge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<tf.Tensor 'Adding_3/add_4:0' shape=(?, 24) dtype=float32>,\n",
       "       <tf.Tensor 'Clipped-hvac_3/Minimum:0' shape=(?, 24) dtype=float32>,\n",
       "       <tf.Tensor 'Clipped-fridge_3/Minimum:0' shape=(?, 24) dtype=float32>,\n",
       "       <tf.Tensor 'Clipped-mw_3/Minimum:0' shape=(?, 24) dtype=float32>,\n",
       "       <tf.Tensor 'Clipped-dw_3/Minimum:0' shape=(?, 24) dtype=float32>,\n",
       "       <tf.Tensor 'Clipped-wm_3/Minimum:0' shape=(?, 24) dtype=float32>,\n",
       "       <tf.Tensor 'Clipped-oven_3/Minimum:0' shape=(?, 24) dtype=float32>], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([layers['Sum'], np.array([layers['{}-clipped-output'.format(appliance)] for appliance in APPLIANCES_ORDER[1:]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvac\n",
      "fridge\n",
      "mw\n",
      "dw\n",
      "wm\n",
      "oven\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nipun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/nipun/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.merge import Add, Minimum, Concatenate\n",
    "layers = {}\n",
    "np.random.seed(0)\n",
    "layers['Aggregate'] = keras.layers.Input(shape=[24],name='Aggregate')\n",
    "for appliance_num, appliance in enumerate(APPLIANCES_ORDER[1:]):\n",
    "    \n",
    "    layers['{}-dense'.format(appliance)] = keras.layers.Dense(units=20,name='{}-layer-1'.format(appliance), activation='relu')(layers['Aggregate'])\n",
    "   \n",
    "    print(appliance)\n",
    "    layers['{}-dropout'.format(appliance)] = keras.layers.Dropout(rate=0.1,name='Droput-{}'.format(appliance))(layers['{}-dense'.format(appliance)])\n",
    "    layers['{}-output'.format(appliance)]= keras.layers.Dense(units=24,name='{}-output'.format(appliance), activation='relu')(layers['{}-dropout'.format(appliance)])\n",
    "    layers['{}-clipped-output'.format(appliance)] = Minimum(name='Clipped-{}'.format(appliance))([layers['{}-output'.format(appliance)], layers['Aggregate']])\n",
    "    \n",
    "    # Subtracting out appliance from prev\n",
    "layers['Sum']= Add(name='Adding')([layers['{}-clipped-output'.format(appliance)] for appliance in APPLIANCES_ORDER[1:]])\n",
    "\n",
    "\n",
    "out = np.hstack([layers['Sum'], np.array([layers['{}-clipped-output'.format(appliance)] for appliance in APPLIANCES_ORDER[1:]])])\n",
    "out = keras.layers.merge(out.tolist(), mode='concat',name='Concat')\n",
    "model = keras.Model(layers['Aggregate'], out)\n",
    "\n",
    "with open('sum-to-aggregate.pdf','wb') as f:\n",
    "    f.write(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='pdf'))\n",
    "\n",
    "#SVG(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Aggregate (InputLayer)          (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hvac-layer-1 (Dense)            (None, 20)           500         Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "fridge-layer-1 (Dense)          (None, 20)           500         Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mw-layer-1 (Dense)              (None, 20)           500         Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dw-layer-1 (Dense)              (None, 20)           500         Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "wm-layer-1 (Dense)              (None, 20)           500         Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "oven-layer-1 (Dense)            (None, 20)           500         Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Droput-hvac (Dropout)           (None, 20)           0           hvac-layer-1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Droput-fridge (Dropout)         (None, 20)           0           fridge-layer-1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Droput-mw (Dropout)             (None, 20)           0           mw-layer-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Droput-dw (Dropout)             (None, 20)           0           dw-layer-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Droput-wm (Dropout)             (None, 20)           0           wm-layer-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Droput-oven (Dropout)           (None, 20)           0           oven-layer-1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hvac-output (Dense)             (None, 24)           504         Droput-hvac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "fridge-output (Dense)           (None, 24)           504         Droput-fridge[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mw-output (Dense)               (None, 24)           504         Droput-mw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dw-output (Dense)               (None, 24)           504         Droput-dw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "wm-output (Dense)               (None, 24)           504         Droput-wm[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "oven-output (Dense)             (None, 24)           504         Droput-oven[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-hvac (Minimum)          (None, 24)           0           hvac-output[0][0]                \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-fridge (Minimum)        (None, 24)           0           fridge-output[0][0]              \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-mw (Minimum)            (None, 24)           0           mw-output[0][0]                  \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-dw (Minimum)            (None, 24)           0           dw-output[0][0]                  \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-wm (Minimum)            (None, 24)           0           wm-output[0][0]                  \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-oven (Minimum)          (None, 24)           0           oven-output[0][0]                \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Adding (Add)                    (None, 24)           0           Clipped-hvac[0][0]               \n",
      "                                                                 Clipped-fridge[0][0]             \n",
      "                                                                 Clipped-mw[0][0]                 \n",
      "                                                                 Clipped-dw[0][0]                 \n",
      "                                                                 Clipped-wm[0][0]                 \n",
      "                                                                 Clipped-oven[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 6,024\n",
      "Trainable params: 6,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/300\n",
      "378/378 [==============================] - 1s 3ms/step - loss: 36.4242 - val_loss: 39.8188\n",
      "Epoch 2/300\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.4225 - val_loss: 39.8188\n",
      "Epoch 3/300\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.4349 - val_loss: 39.8188\n",
      "Epoch 4/300\n",
      "378/378 [==============================] - 0s 93us/step - loss: 36.4371 - val_loss: 39.8188\n",
      "Epoch 5/300\n",
      "378/378 [==============================] - 0s 95us/step - loss: 36.4756 - val_loss: 39.8259\n",
      "Epoch 6/300\n",
      "378/378 [==============================] - 0s 96us/step - loss: 36.4519 - val_loss: 39.8350\n",
      "Epoch 7/300\n",
      "378/378 [==============================] - 0s 99us/step - loss: 36.4365 - val_loss: 39.8382\n",
      "Epoch 8/300\n",
      "378/378 [==============================] - 0s 97us/step - loss: 36.4252 - val_loss: 39.8188\n",
      "Epoch 9/300\n",
      "378/378 [==============================] - 0s 98us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 10/300\n",
      "378/378 [==============================] - 0s 99us/step - loss: 36.4286 - val_loss: 39.8188\n",
      "Epoch 11/300\n",
      "378/378 [==============================] - 0s 97us/step - loss: 36.4289 - val_loss: 39.8188\n",
      "Epoch 12/300\n",
      "378/378 [==============================] - 0s 102us/step - loss: 36.4290 - val_loss: 39.8188\n",
      "Epoch 13/300\n",
      "378/378 [==============================] - 0s 102us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 14/300\n",
      "378/378 [==============================] - 0s 105us/step - loss: 36.4282 - val_loss: 39.8188\n",
      "Epoch 15/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 16/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4211 - val_loss: 39.8188\n",
      "Epoch 17/300\n",
      "378/378 [==============================] - 0s 115us/step - loss: 36.4219 - val_loss: 39.8188\n",
      "Epoch 18/300\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4308 - val_loss: 39.8188\n",
      "Epoch 19/300\n",
      "378/378 [==============================] - 0s 100us/step - loss: 36.4226 - val_loss: 39.8188\n",
      "Epoch 20/300\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.4214 - val_loss: 39.8188\n",
      "Epoch 21/300\n",
      "378/378 [==============================] - 0s 105us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 22/300\n",
      "378/378 [==============================] - 0s 101us/step - loss: 36.4392 - val_loss: 39.8188\n",
      "Epoch 23/300\n",
      "378/378 [==============================] - 0s 98us/step - loss: 36.4486 - val_loss: 39.8188\n",
      "Epoch 24/300\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.5021 - val_loss: 39.8188\n",
      "Epoch 25/300\n",
      "378/378 [==============================] - 0s 100us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 26/300\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.4265 - val_loss: 39.8188\n",
      "Epoch 27/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 32.88 - 0s 97us/step - loss: 36.4268 - val_loss: 39.8188\n",
      "Epoch 28/300\n",
      "378/378 [==============================] - 0s 115us/step - loss: 36.4339 - val_loss: 39.8188\n",
      "Epoch 29/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4314 - val_loss: 39.8188\n",
      "Epoch 30/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.4220 - val_loss: 39.8188\n",
      "Epoch 31/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.4314 - val_loss: 39.8188\n",
      "Epoch 32/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 36.4344 - val_loss: 39.8188\n",
      "Epoch 33/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 36.4294 - val_loss: 39.8188\n",
      "Epoch 34/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4348 - val_loss: 39.8188\n",
      "Epoch 35/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4372 - val_loss: 39.8188\n",
      "Epoch 36/300\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.4260 - val_loss: 39.8188\n",
      "Epoch 37/300\n",
      "378/378 [==============================] - 0s 102us/step - loss: 36.4224 - val_loss: 39.8188\n",
      "Epoch 38/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4516 - val_loss: 39.8188\n",
      "Epoch 39/300\n",
      "378/378 [==============================] - 0s 100us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 40/300\n",
      "378/378 [==============================] - 0s 96us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 41/300\n",
      "378/378 [==============================] - 0s 105us/step - loss: 36.4242 - val_loss: 39.8188\n",
      "Epoch 42/300\n",
      "378/378 [==============================] - 0s 98us/step - loss: 36.4235 - val_loss: 39.8188\n",
      "Epoch 43/300\n",
      "378/378 [==============================] - 0s 98us/step - loss: 36.4225 - val_loss: 39.8188\n",
      "Epoch 44/300\n",
      "378/378 [==============================] - 0s 105us/step - loss: 36.4232 - val_loss: 39.8188\n",
      "Epoch 45/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4258 - val_loss: 39.8188\n",
      "Epoch 46/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4508 - val_loss: 39.8188\n",
      "Epoch 47/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4227 - val_loss: 39.8188\n",
      "Epoch 48/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 36.4227 - val_loss: 39.8188\n",
      "Epoch 49/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.4351 - val_loss: 39.8188\n",
      "Epoch 50/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 51/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4316 - val_loss: 39.8188\n",
      "Epoch 52/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 36.4248 - val_loss: 39.8188\n",
      "Epoch 53/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 54/300\n",
      "378/378 [==============================] - 0s 98us/step - loss: 36.4230 - val_loss: 39.8188\n",
      "Epoch 55/300\n",
      "378/378 [==============================] - 0s 96us/step - loss: 36.4250 - val_loss: 39.8188\n",
      "Epoch 56/300\n",
      "378/378 [==============================] - 0s 92us/step - loss: 36.4220 - val_loss: 39.8188\n",
      "Epoch 57/300\n",
      "378/378 [==============================] - 0s 99us/step - loss: 36.4328 - val_loss: 39.8188\n",
      "Epoch 58/300\n",
      "378/378 [==============================] - 0s 96us/step - loss: 36.4279 - val_loss: 39.8188\n",
      "Epoch 59/300\n",
      "378/378 [==============================] - 0s 101us/step - loss: 36.4231 - val_loss: 39.8188\n",
      "Epoch 60/300\n",
      "378/378 [==============================] - 0s 93us/step - loss: 36.4249 - val_loss: 39.8188\n",
      "Epoch 61/300\n",
      "378/378 [==============================] - 0s 95us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 62/300\n",
      "378/378 [==============================] - 0s 96us/step - loss: 36.4217 - val_loss: 39.8188\n",
      "Epoch 63/300\n",
      "378/378 [==============================] - 0s 96us/step - loss: 36.4475 - val_loss: 39.8188\n",
      "Epoch 64/300\n",
      "378/378 [==============================] - 0s 99us/step - loss: 36.4285 - val_loss: 39.8188\n",
      "Epoch 65/300\n",
      "378/378 [==============================] - 0s 99us/step - loss: 36.4216 - val_loss: 39.8188\n",
      "Epoch 66/300\n",
      "378/378 [==============================] - 0s 98us/step - loss: 36.4239 - val_loss: 39.8188\n",
      "Epoch 67/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 68/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 69/300\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.4257 - val_loss: 39.8188\n",
      "Epoch 70/300\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 71/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4213 - val_loss: 39.8188\n",
      "Epoch 72/300\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.4259 - val_loss: 39.8188\n",
      "Epoch 73/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4329 - val_loss: 39.8188\n",
      "Epoch 74/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.4221 - val_loss: 39.8188\n",
      "Epoch 75/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.4218 - val_loss: 39.8188\n",
      "Epoch 76/300\n",
      "378/378 [==============================] - 0s 128us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 77/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.4278 - val_loss: 39.8188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300\n",
      "378/378 [==============================] - 0s 130us/step - loss: 36.4214 - val_loss: 39.8188\n",
      "Epoch 79/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4224 - val_loss: 39.8188\n",
      "Epoch 80/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4217 - val_loss: 39.8188\n",
      "Epoch 81/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.4222 - val_loss: 39.8188\n",
      "Epoch 82/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 83/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 84/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 85/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4295 - val_loss: 39.8188\n",
      "Epoch 86/300\n",
      "378/378 [==============================] - 0s 110us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 87/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.4295 - val_loss: 39.8188\n",
      "Epoch 88/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 89/300\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 90/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 91/300\n",
      "378/378 [==============================] - 0s 102us/step - loss: 36.4210 - val_loss: 39.8188\n",
      "Epoch 92/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4211 - val_loss: 39.8188\n",
      "Epoch 93/300\n",
      "378/378 [==============================] - 0s 94us/step - loss: 36.4242 - val_loss: 39.8188\n",
      "Epoch 94/300\n",
      "378/378 [==============================] - 0s 100us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 95/300\n",
      "378/378 [==============================] - 0s 105us/step - loss: 36.4273 - val_loss: 39.8188\n",
      "Epoch 96/300\n",
      "378/378 [==============================] - 0s 99us/step - loss: 36.4286 - val_loss: 39.8188\n",
      "Epoch 97/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.4425 - val_loss: 39.8188\n",
      "Epoch 98/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.4239 - val_loss: 39.8188\n",
      "Epoch 99/300\n",
      "378/378 [==============================] - 0s 109us/step - loss: 36.4221 - val_loss: 39.8188\n",
      "Epoch 100/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 101/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 102/300\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.4232 - val_loss: 39.8188\n",
      "Epoch 103/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 104/300\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4280 - val_loss: 39.8188\n",
      "Epoch 105/300\n",
      "378/378 [==============================] - 0s 110us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 106/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 36.4247 - val_loss: 39.8188\n",
      "Epoch 107/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 108/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.4292 - val_loss: 39.8188\n",
      "Epoch 109/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4243 - val_loss: 39.8188\n",
      "Epoch 110/300\n",
      "378/378 [==============================] - 0s 109us/step - loss: 36.4316 - val_loss: 39.8188\n",
      "Epoch 111/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 112/300\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.4215 - val_loss: 39.8188\n",
      "Epoch 113/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4218 - val_loss: 39.8188\n",
      "Epoch 114/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.4305 - val_loss: 39.8188\n",
      "Epoch 115/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.4229 - val_loss: 39.8188\n",
      "Epoch 116/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 117/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 36.4218 - val_loss: 39.8188\n",
      "Epoch 118/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 119/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 120/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 121/300\n",
      "378/378 [==============================] - 0s 100us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 122/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4418 - val_loss: 39.8188\n",
      "Epoch 123/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.4227 - val_loss: 39.8188\n",
      "Epoch 124/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 36.4309 - val_loss: 39.8188\n",
      "Epoch 125/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.4234 - val_loss: 39.8188\n",
      "Epoch 126/300\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.4416 - val_loss: 39.8188\n",
      "Epoch 127/300\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.4234 - val_loss: 39.8188\n",
      "Epoch 128/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4231 - val_loss: 39.8188\n",
      "Epoch 129/300\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 130/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4216 - val_loss: 39.8188\n",
      "Epoch 131/300\n",
      "378/378 [==============================] - 0s 102us/step - loss: 36.4251 - val_loss: 39.8188\n",
      "Epoch 132/300\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4229 - val_loss: 39.8188\n",
      "Epoch 133/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4215 - val_loss: 39.8188\n",
      "Epoch 134/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 36.4224 - val_loss: 39.8188\n",
      "Epoch 135/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4219 - val_loss: 39.8188\n",
      "Epoch 136/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 137/300\n",
      "378/378 [==============================] - 0s 109us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 138/300\n",
      "378/378 [==============================] - 0s 109us/step - loss: 36.4276 - val_loss: 39.8188\n",
      "Epoch 139/300\n",
      "378/378 [==============================] - 0s 115us/step - loss: 36.4227 - val_loss: 39.8188\n",
      "Epoch 140/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4226 - val_loss: 39.8188\n",
      "Epoch 141/300\n",
      "378/378 [==============================] - 0s 105us/step - loss: 36.4246 - val_loss: 39.8188\n",
      "Epoch 142/300\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4434 - val_loss: 39.8188\n",
      "Epoch 143/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.4294 - val_loss: 39.8188\n",
      "Epoch 144/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 145/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 146/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.4498 - val_loss: 39.8188\n",
      "Epoch 147/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.4218 - val_loss: 39.8188\n",
      "Epoch 148/300\n",
      "378/378 [==============================] - 0s 105us/step - loss: 36.4239 - val_loss: 39.8188\n",
      "Epoch 149/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.4233 - val_loss: 39.8188\n",
      "Epoch 150/300\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 151/300\n",
      "378/378 [==============================] - 0s 105us/step - loss: 36.4356 - val_loss: 39.8188\n",
      "Epoch 152/300\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 153/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 154/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.4209 - val_loss: 39.8188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 36.4242 - val_loss: 39.8188\n",
      "Epoch 156/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.4229 - val_loss: 39.8188\n",
      "Epoch 157/300\n",
      "378/378 [==============================] - 0s 109us/step - loss: 36.4212 - val_loss: 39.8188\n",
      "Epoch 158/300\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4250 - val_loss: 39.8188\n",
      "Epoch 159/300\n",
      "378/378 [==============================] - 0s 110us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 160/300\n",
      "378/378 [==============================] - 0s 110us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 161/300\n",
      "378/378 [==============================] - 0s 99us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 162/300\n",
      "378/378 [==============================] - 0s 115us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 163/300\n",
      "378/378 [==============================] - 0s 115us/step - loss: 36.4222 - val_loss: 39.8188\n",
      "Epoch 164/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 36.4256 - val_loss: 39.8188\n",
      "Epoch 165/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 166/300\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 167/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 168/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.4213 - val_loss: 39.8188\n",
      "Epoch 169/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 170/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 171/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 172/300\n",
      "378/378 [==============================] - 0s 149us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 173/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 174/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 175/300\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.4336 - val_loss: 39.8188\n",
      "Epoch 176/300\n",
      "378/378 [==============================] - 0s 100us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 177/300\n",
      "378/378 [==============================] - 0s 101us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 178/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 179/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 180/300\n",
      "378/378 [==============================] - 0s 115us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 181/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 182/300\n",
      "378/378 [==============================] - 0s 110us/step - loss: 36.4215 - val_loss: 39.8188\n",
      "Epoch 183/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4230 - val_loss: 39.8188\n",
      "Epoch 184/300\n",
      "378/378 [==============================] - 0s 105us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 185/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4213 - val_loss: 39.8188\n",
      "Epoch 186/300\n",
      "378/378 [==============================] - 0s 101us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 187/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 188/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 189/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 190/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.4267 - val_loss: 39.8188\n",
      "Epoch 191/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 36.4213 - val_loss: 39.8188\n",
      "Epoch 192/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 193/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 194/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.4218 - val_loss: 39.8188\n",
      "Epoch 195/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 196/300\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 197/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.4341 - val_loss: 39.8188\n",
      "Epoch 198/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4223 - val_loss: 39.8188\n",
      "Epoch 199/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 200/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4362 - val_loss: 39.8188\n",
      "Epoch 201/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 36.4212 - val_loss: 39.8188\n",
      "Epoch 202/300\n",
      "378/378 [==============================] - 0s 137us/step - loss: 36.4212 - val_loss: 39.8188\n",
      "Epoch 203/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 34.39 - 0s 131us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 204/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 36.4221 - val_loss: 39.8188\n",
      "Epoch 205/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.4250 - val_loss: 39.8188\n",
      "Epoch 206/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 207/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 208/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4234 - val_loss: 39.8188\n",
      "Epoch 209/300\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 210/300\n",
      "378/378 [==============================] - 0s 102us/step - loss: 36.4232 - val_loss: 39.8188\n",
      "Epoch 211/300\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4245 - val_loss: 39.8188\n",
      "Epoch 212/300\n",
      "378/378 [==============================] - 0s 102us/step - loss: 36.4216 - val_loss: 39.8188\n",
      "Epoch 213/300\n",
      "378/378 [==============================] - 0s 99us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 214/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 215/300\n",
      "378/378 [==============================] - 0s 96us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 216/300\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 217/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 218/300\n",
      "378/378 [==============================] - 0s 115us/step - loss: 36.4222 - val_loss: 39.8188\n",
      "Epoch 219/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.4221 - val_loss: 39.8188\n",
      "Epoch 220/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 36.4210 - val_loss: 39.8188\n",
      "Epoch 221/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.4212 - val_loss: 39.8188\n",
      "Epoch 222/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 223/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4263 - val_loss: 39.8188\n",
      "Epoch 224/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 225/300\n",
      "378/378 [==============================] - 0s 127us/step - loss: 36.4211 - val_loss: 39.8188\n",
      "Epoch 226/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.4211 - val_loss: 39.8188\n",
      "Epoch 227/300\n",
      "378/378 [==============================] - 0s 125us/step - loss: 36.4211 - val_loss: 39.8188\n",
      "Epoch 228/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.4215 - val_loss: 39.8188\n",
      "Epoch 229/300\n",
      "378/378 [==============================] - 0s 115us/step - loss: 36.4226 - val_loss: 39.8188\n",
      "Epoch 230/300\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 231/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 117us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 232/300\n",
      "378/378 [==============================] - 0s 122us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 233/300\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.4235 - val_loss: 39.8188\n",
      "Epoch 234/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4217 - val_loss: 39.8188\n",
      "Epoch 235/300\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.4217 - val_loss: 39.8188\n",
      "Epoch 236/300\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.4214 - val_loss: 39.8188\n",
      "Epoch 237/300\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 238/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4210 - val_loss: 39.8188\n",
      "Epoch 239/300\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 240/300\n",
      "378/378 [==============================] - 0s 99us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 241/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4318 - val_loss: 39.8188\n",
      "Epoch 242/300\n",
      "378/378 [==============================] - 0s 124us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 243/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 244/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 245/300\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.4223 - val_loss: 39.8188\n",
      "Epoch 246/300\n",
      "378/378 [==============================] - 0s 110us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 247/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4212 - val_loss: 39.8188\n",
      "Epoch 248/300\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.4215 - val_loss: 39.8188\n",
      "Epoch 249/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 250/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 251/300\n",
      "378/378 [==============================] - 0s 110us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 252/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 253/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 254/300\n",
      "378/378 [==============================] - 0s 98us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 255/300\n",
      "378/378 [==============================] - 0s 105us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 256/300\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.4215 - val_loss: 39.8188\n",
      "Epoch 257/300\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 258/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4231 - val_loss: 39.8188\n",
      "Epoch 259/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 260/300\n",
      "378/378 [==============================] - 0s 109us/step - loss: 36.4304 - val_loss: 39.8188\n",
      "Epoch 261/300\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 262/300\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 263/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 264/300\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 265/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.4226 - val_loss: 39.8188\n",
      "Epoch 266/300\n",
      "378/378 [==============================] - 0s 126us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 267/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 268/300\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 269/300\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.4216 - val_loss: 39.8188\n",
      "Epoch 270/300\n",
      "378/378 [==============================] - 0s 110us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 271/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4232 - val_loss: 39.8188\n",
      "Epoch 272/300\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.4240 - val_loss: 39.8188\n",
      "Epoch 273/300\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 274/300\n",
      "378/378 [==============================] - 0s 109us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 275/300\n",
      "378/378 [==============================] - 0s 101us/step - loss: 36.4210 - val_loss: 39.8188\n",
      "Epoch 276/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 277/300\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 278/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 279/300\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 280/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 281/300\n",
      "378/378 [==============================] - 0s 109us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 282/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 283/300\n",
      "378/378 [==============================] - 0s 116us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 284/300\n",
      "378/378 [==============================] - 0s 109us/step - loss: 36.4340 - val_loss: 39.8188\n",
      "Epoch 285/300\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4221 - val_loss: 39.8188\n",
      "Epoch 286/300\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 287/300\n",
      "378/378 [==============================] - 0s 109us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 288/300\n",
      "378/378 [==============================] - 0s 99us/step - loss: 36.4218 - val_loss: 39.8188\n",
      "Epoch 289/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 290/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 291/300\n",
      "378/378 [==============================] - 0s 101us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 292/300\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 293/300\n",
      "378/378 [==============================] - 0s 103us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 294/300\n",
      "378/378 [==============================] - 0s 96us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 295/300\n",
      "378/378 [==============================] - 0s 100us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 296/300\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 297/300\n",
      "378/378 [==============================] - ETA: 0s - loss: 32.68 - 0s 96us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 298/300\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.4270 - val_loss: 39.8188\n",
      "Epoch 299/300\n",
      "378/378 [==============================] - 0s 132us/step - loss: 36.4209 - val_loss: 39.8188\n",
      "Epoch 300/300\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.4209 - val_loss: 39.8188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a24ef0198>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','mean_absolute_error')\n",
    "model.fit(train_agg, np.hstack([train_agg, train_hvac, train_fridge, train_mw, train_dw, train_wm, train_oven]), epochs=300, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.12725879\n",
      "88.9532061557\n",
      "6.3002141955\n",
      "14.4991161736\n",
      "5.61752117864\n",
      "19.7572015104\n"
     ]
    }
   ],
   "source": [
    "pred_hvac = model.predict(test_agg)[:, 24:48]\n",
    "pred_fridge = model.predict(test_agg)[:, 48:72]\n",
    "pred_mw = model.predict(test_agg)[:, 72:96]\n",
    "pred_dw = model.predict(test_agg)[:, 96:120]\n",
    "pred_wm = model.predict(test_agg)[:, 120:144]\n",
    "pred_oven = model.predict(test_agg)[:, 144:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(pred_hvac, test_hvac))\n",
    "print(mean_absolute_error(pred_fridge, test_fridge))\n",
    "print(mean_absolute_error(pred_mw, test_mw))\n",
    "print(mean_absolute_error(pred_dw, test_dw))\n",
    "print(mean_absolute_error(pred_wm, test_wm))\n",
    "print(mean_absolute_error(pred_oven, test_oven))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.907349e-06\n",
       "1    -3.877686e+00\n",
       "2    -9.150000e+00\n",
       "3     0.000000e+00\n",
       "4     0.000000e+00\n",
       "5    -9.633333e+00\n",
       "6     0.000000e+00\n",
       "7     2.861023e-06\n",
       "8    -8.683333e+00\n",
       "9     9.536743e-07\n",
       "10    0.000000e+00\n",
       "11    9.536743e-07\n",
       "12   -9.583333e+00\n",
       "13   -9.516666e+00\n",
       "14   -4.711666e+01\n",
       "15    3.099442e-06\n",
       "16   -4.685000e+01\n",
       "17    9.536743e-07\n",
       "18   -7.310000e+01\n",
       "19   -7.350000e+01\n",
       "20   -4.180000e+01\n",
       "21    0.000000e+00\n",
       "22   -9.616667e+00\n",
       "23   -9.533334e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.DataFrame(pred_hvac) - pd.DataFrame(test_agg)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c39139b38>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYnFWV+PHvqa33pLqTJoSkOwvExBBCd4gYZDHiQlwQ\n9BEER4URRZRh9PfMjNuMI78ZcXRUGDfwF38i/J5R0BFEUFQ2AUVUQhKyrxA6nT3pqixd3V3b+f1R\n9VYqSXe6u+p9qyrV5/M8/VTVrbfeuql0Tt2ce99zRVUxxhhTvXzl7oAxxhhvWaA3xpgqZ4HeGGOq\nnAV6Y4ypchbojTGmylmgN8aYKmeB3hhjqpwFemOMqXLDBnoRaROR34vIOhFZKyKfyra3iMjjIrI5\ne9uc95rPi8gWEdkoIpd5+QcwxhhzcjLclbEiMhmYrKrLRaQJeBG4Erge6FHVr4rI54BmVf2siMwF\n7gPOB84AngBeo6qpod5j4sSJOn36dDf+PMYYM2a8+OKL+1W1dbjjAsMdoKq7gF3Z+4dFZD0wBbgC\nWJw97F7gaeCz2fb7VXUAeEVEtpAJ+s8P9R7Tp09n2bJlw3XFGGNMHhF5dSTHjSpHLyLTgU7gL8Ck\n7JcAwG5gUvb+FGB73su6s23GGGPKYMSBXkQagQeAT6vqofznNJP/GVV1NBG5UUSWiciyffv2jeal\nxhhjRmFEgV5EgmSC/I9V9cFs855s/t7J4+/Ntu8A2vJePjXbdgxVXaqqC1V1YWvrsCkmY4wxBRo2\nRy8iAvwQWK+qt+c99TBwHfDV7O0v89p/IiK3k5mMnQX81c1OG2OMI5FI0N3dTX9/f7m74pna2lqm\nTp1KMBgs6PXDBnrgQuBDwGoRWZlt+wKZAP8zEbkBeBW4GkBV14rIz4B1QBK4+WQrbowxphjd3d00\nNTUxffp0MuPS6qKqHDhwgO7ubmbMmFHQOUay6uaPwFCf3puHeM1twG0F9cgYY0ahv7+/aoM8gIgw\nYcIEipnLtCtjjTGnvGoN8o5i/3wW6I0xnnly/R52RPvK3Y0xzwK9McYTqson/ns5P/rjK+Xuiuf2\n7NnDBz7wAWbOnMl5553HBRdcwE9/+lM6Ojro6OigsbGR2bNn09HRwYc//OGS928kk7HGGDNqh/qT\nxFNpenrj5e6Kp1SVK6+8kuuuu46f/OQnALz66qs8/PDDrFyZWb+yePFivvGNb7Bw4cKy9NFG9MYY\nT0RjmQAfiVV3oH/qqacIhULcdNNNubZp06Zxyy23lLFXx7IRvTHGE85IvieWKNl7/u9H1rJu56Hh\nDxyFuWeM40uXnz3k82vXrmXBggWuvqfbbERvjPFENBvgo1U+oj/ezTffzLnnnsvrXve6cnclx0b0\nxhhPOCmbSAlz9CcbeXvl7LPP5oEHHsg9/t73vsf+/fvLlo8fjI3ojTGeiGRH9If6kyRT6TL3xjuX\nXnop/f393HXXXbm2WCxWxh6dyAK9McYT+SP5aF/p8vSlJiI89NBDPPPMM8yYMYPzzz+f6667jq99\n7Wvl7lqOpW6MMZ7IX20TjcWZ2FhTxt54a/Lkydx///1DPv/000+XrjODsBG9McYT0bzVNpESrrwx\nJ7JAb4zxRCQWpzaYCTGlnJA1J7JAb4zxRE9vnBkTG4Hqv2iq0lmgN8Z4IhpLMLO1AbDUTblZoDfG\neCISizM1XEfI77MRfZlZoDfGuK4vnmIgmSZcHyJcHyTaayP6cho20IvI3SKyV0TW5LX9VERWZn+2\nOVsMish0EenLe+77XnbeGFOZerIj+Ob6IM31odzjauX3++no6GDevHlcddVVRV0w9fTTT/Oud73L\nxd6NbER/D7Akv0FV36+qHaraATwAPJj39FbnOVW9CWPMmOOssmluCNHcEKz6ejd1dXWsXLmSNWvW\nEAqF+P73jx3jqirpdPmuDh420Kvqs0DPYM9JZn+rq4H7XO6XMeYU5qyhb64P0VwfGlOTsRdffDFb\ntmxh27ZtzJ49mw9/+MPMmzeP7du389hjj3HBBRewYMECrrrqKo4cOQLAb3/7W+bMmcOCBQt48MEH\nh3mH0Sv2ytiLgT2qujmvbUY2lXMQ+BdV/UOR72GMOcVE8lI34fpQ6Ub0v/kc7F7t7jlPPwfe/tUR\nHZpMJvnNb37DkiWZJMjmzZu59957WbRoEfv37+fLX/4yTzzxBA0NDXzta1/j9ttv5zOf+Qwf+9jH\neOqppzjrrLN4//vf727/KT7QX8uxo/ldQLuqHhCR84CHRORsVT2hQLSI3AjcCNDe3l5kN4wxlcQJ\n9OH6EM31QSKxBKpatZt49/X10dHRAWRG9DfccAM7d+5k2rRpLFq0CIA///nPrFu3jgsvvBCAeDzO\nBRdcwIYNG5gxYwazZs0C4IMf/CBLly51tX8FB3oRCQDvBc5z2lR1ABjI3n9RRLYCrwGWHf96VV0K\nLAVYuHChFtoPY0zliWRX2YTrg7Q0hEillUP9ScbXBb194xGOvN3m5OiP19DQkLuvqrz1rW/lvvuO\nzXQP9jq3FbO88i3ABlXtdhpEpFVE/Nn7M4FZwMvFddEYc6qJxOI01QYI+n2E60PA2NuA5HiLFi3i\nueeeY8uWLQD09vayadMm5syZw7Zt29i6dSvACV8EbhjJ8sr7gOeB2SLSLSI3ZJ+6hhMnYS8BVmVz\n9D8HblLVQSdyjTHVKxqL05wN8M31mVH8WJqQHUxrayv33HMP1157LfPnz8+lbWpra1m6dCnvfOc7\nWbBgAaeddprr7z1s6kZVrx2i/fpB2h4gs9zSGDOG9cQSuQDvjOirubCZs3om3/Tp01mzZs0xbZde\neikvvPDCCccuWbKEDRs2eNY/uzLWGOO6aCxOc0MmwLdkb60MQvlYoDfGuC5iqZuKYoHeGOO6aG+C\ncDbAj6sN4hNvJ2NVq3vhXrF/Pgv0xhhXxZNpDg8kcyN6n08YXxekx6McfW1tLQcOHKjaYK+qHDhw\ngNra2oLPYXvGGmNcFe07elWso7k+dMzWgm6aOnUq3d3d7Nu3z5PzV4La2lqmTp1a8Ost0BtjXJWr\nc5OdhHXuezUZGwwGmTFjhifnrhaWujHGuCpXubI+L9BnyyCY8rBAb4xx1dE6N0dTN+H6UFWvo690\nFuiNMa6K5JUodmRG9Bboy8UCvTHGVUdLFB+box9IpumLp8rVrTHNAr0xxlXRWILaoI+6kD/X5gR9\nG9WXhwV6Y4yrenrjx4zm4ehSS6/W0puTs0BvjHFVNBbPFTJzHC1VbCtvysECvTHGVZFYgpaGYzcY\nscJm5WWB3hjjqsigI/pM4B/rm4+UiwV6Y4yrIr3xY8ofAITrMoG/p9dSN+Vggd4Y45p0WjnYlzhh\nMjYU8NFYE7DUTZmMZCvBu0Vkr4isyWu7VUR2iMjK7M878p77vIhsEZGNInKZVx03xlSeQ/0J0soJ\ngR6guSFoqZsyGcmI/h5gySDtd6hqR/bnUQARmUtmL9mzs6+509ks3BhT/XJXxR43GQuZ4G/1bspj\n2ECvqs8CI93g+wrgflUdUNVXgC3A+UX0zxhzCnHWyR8/Geu0WeqmPIrJ0d8iIquyqZ3mbNsUYHve\nMd3ZthOIyI0iskxEllVzHWljxpLoIOUPHFbvpnwKDfR3ATOBDmAX8M3RnkBVl6rqQlVd2NraWmA3\njDGVxEnNtAwa6ENEbdVNWRQU6FV1j6qmVDUN/ICj6ZkdQFveoVOzbcaYMcAZ0YeHyNEfHkiSSKVL\n3a0xr6BALyKT8x6+B3BW5DwMXCMiNSIyA5gF/LW4LhpjThU9vXECPqGp5sTN65wJWkvflN6wWwmK\nyH3AYmCiiHQDXwIWi0gHoMA24OMAqrpWRH4GrAOSwM2qanVJjRkjIrEE4fogInLCc/n1bk5rKnyj\nazN6wwZ6Vb12kOYfnuT424DbiumUMebUFI2dWLnS4eTtbaep0rMrY40xromcJNA79W5sLX3pWaA3\nxrgm0ps4Zq/YfM1WwbJsLNAbY1xzshF9c71NxpaLBXpjjCtUlWgskRu5H68u6Kcm4LPNR8rAAr0x\nxhWxeIp4Kn1CiWKHiGTq3dhkbMlZoDfGuMKpczNU6gYyE7KWuik9C/TGGFc4KZmhJmPBKliWiwV6\nY4wrnJF6yxA5euc5G9GXngV6Y4wrnAA+WIliR7g+aJOxZWCB3hjjikguR3/y1E00Fied1lJ1y2CB\n3hjjEif3Pr5u6EAfrg+S1syWg6Z0LNAbY1wRjcUZXxck4B86rLTkro61QF9KFuiNMa6IxBInTdvA\n0aWXNiFbWhbojTGuiMTiJ52IhbzCZnbRVElZoDfGuCJT52akI3pL3ZSSBXpjjCsivUPXuXE4z0ct\ndVNSwwZ6EblbRPaKyJq8tq+LyAYRWSUivxCRcLZ9uoj0icjK7M/3vey8MaZynGzTEce42gB+n1iO\nvsRGMqK/B1hyXNvjwDxVnQ9sAj6f99xWVe3I/tzkTjeNMZVsIJmiN54aNnUjIoTrgvT0WuqmlIYN\n9Kr6LNBzXNtjqprMPvwzMNWDvhljThFH69ycfESfOSZoqZsScyNH/xHgN3mPZ2TTNs+IyMUunN8Y\nU+FGUufGYfVuSm/YzcFPRkT+GUgCP8427QLaVfWAiJwHPCQiZ6vqoUFeeyNwI0B7e3sx3TDGlFmk\nd/jKlY5wfYjtPTGvu2TyFDyiF5HrgXcBf6OqCqCqA6p6IHv/RWAr8JrBXq+qS1V1oaoubG1tLbQb\nxpgK4IzQh5uMzRwTzNWuN6VRUKAXkSXAZ4B3q2osr71VRPzZ+zOBWcDLbnTUGFO5RhfoQ0RjCbLj\nQ1MCI1leeR/wPDBbRLpF5Abgu0AT8PhxyygvAVaJyErg58BNqtoz6ImNMVVjJJuOOJobQsRTaWLx\nlNfdMlnD5uhV9dpBmn84xLEPAA8U2yljzKkl0hunPuSnNugf9lhnCWYkFqehpqhpQjNCdmWsMaZo\nPSO4WMrhLMG0DUhKxwK9MaZo0VhiRGkbOJrHtwnZ0rFAb4wpWiQWH9EaeoCWhqOpG1MaFuiNMUXL\njOgtdVOpLNAbY4rW0zt8iWJHuM5G9KVmgd4YU5RUWjnUP/IRfcDvo6k2YJuPlJAFemNMUQ72JVCF\nlhGO6MGpd2Opm1KxQG+MKUruqtgRTsZCJk9vqZvSsUBvjCmKk4IZaeoGMhdN2WRs6VigN8YUxUnB\njHQyNnNsyNbRl5AFemNMUUZT0MyRKWxmgb5ULNAbY4oSLSBH31wfpDeeIp5Me9Utk8cCvTGmKD29\nCYJ+oSE0fEEzR7jBuWjKRvWlYIHeGFOUaCxOuD6EiIz4NU4+v8cCfUlYoDfGFCUSi9Myivw8kDve\n2YLQeMsCvTGmKJFRVK50HK13YyP6UrBAb4wpSqR35LXoHc25CpY2oi+FkWwleLeI7BWRNXltLSLy\nuIhszt425z33eRHZIiIbReQyrzpujKkMkVgiF7hHyvlisKtjS2MkI/p7gCXHtX0OeFJVZwFPZh8j\nInOBa4Czs6+509ks3BhTfVSV6Ch2l3LUBv3UBf1W2KxEhg30qvoscPwG31cA92bv3wtcmdd+v6oO\nqOorwBbgfJf6aoypMEcGkiTTOupAD5mVN5a6KY1Cc/STVHVX9v5uYFL2/hRge95x3dm2E4jIjSKy\nTESW7du3r8BuGGPKyalXM9rJ2Mxr7OrYUil6MlZVFdACXrdUVReq6sLW1tZiu2GMKQOnXk1BI/qG\noK2jL5FCA/0eEZkMkL3dm23fAbTlHTc122aMqUKFlCh2ZOrdWOqmFAoN9A8D12XvXwf8Mq/9GhGp\nEZEZwCzgr8V10RhTqaIFVK50NFtN+pIJDHeAiNwHLAYmikg38CXgq8DPROQG4FXgagBVXSsiPwPW\nAUngZlVNedR3Y0yZFVK50tFcH+RgX4JUWvH7Rl4+wYzesIFeVa8d4qk3D3H8bcBtxXTKGHNqiPTG\nEYFxdYVNxqpmtiJsKSD1Y0bOrow1xhQsEksQrgsWNCJ3grulb7xngd4YU7BIARdLOZwlmbbE0nsW\n6I0xBYsWUNDM0WwVLEvGAr0xpmA9BRQ0czivs7X03rNAb4wpWDQWL2gNPRytYGmpG+9ZoDfGFCwS\nSxS0hh6gsSZAwCdW76YELNAbYwrSn0jRl0jlNhEZLRGxejclYoHeGFOQYi6WcjTXB3P1cox3LNAb\nYwrirJZpGeWmI/maG0KWuikBC/TGmII4KZdCUzeQGdFb6sZ7FuiNMQWJ5AqaFRPobURfChbojTEF\n6cnl6AtP3YTrQ0R642S2tTBesUBvjClItLf41E1LQ5BkWjkykHSrW2YQFuiNMQWJxBI01gQIBQoP\nI86XhG1A4i0L9MaYgkRj8YLr3Dhy9W5sQtZTFuiNMQXpKaJypcPJ79taem8Nu/HIUERkNvDTvKaZ\nwL8CYeBjwL5s+xdU9dGCe2iMqUiRWKLgOjcO5/WWuvFWwYFeVTcCHQAi4iezCfgvgL8F7lDVb7jS\nQ2NMRYrG4kyfUF/UOSx1UxpupW7eDGxV1VddOp8xpsJFiihR7BhfF0QEW0vvMbcC/TXAfXmPbxGR\nVSJyt4g0u/QexpgKkUylOdSfLHoy1u8TxtUGiViO3lNFB3oRCQHvBv4n23QXmXx9B7AL+OYQr7tR\nRJaJyLJ9+/YNdogxpkJF+5w6N8Vv6t3SELLUjcfcGNG/HViuqnsAVHWPqqZUNQ38ADh/sBep6lJV\nXaiqC1tbW13ohjGmVNyoc+MI1wdtMtZjbgT6a8lL24jI5Lzn3gOsceE9jDEV5Gidm+JSN5lz2Ije\nawWvugEQkQbgrcDH85r/U0Q6AAW2HfecMaYKOOvei52MhcyIfsOuQ0WfxwytqECvqr3AhOPaPlRU\nj4wxFc9J3RS7jh6gxSpYes6ujD0FDCRT9FrRJ1NBXE3dNIToS6ToT6SKPpcZnAX6U8CtD6/jqu8/\nX+5uGJMTicUJBXzUBf1Fn8tZomkTst6xQH8KeG7LftbtOmT1QEzFyFwsFUREij6Xk+e332/vWKCv\ncPuPDNDVEwNgRVekzL0xJiMSS7gyEQtHA71tKegdC/QVbkVXdND7xpRT1IXKlY7m7ObiNiHrHQv0\nFW5FV4SAT5h1WiMrttuI3lSGTOXK4idiwQqblYIF+gq3vCvC3DPGccGZE1jZFSWVtr01TflFeuOu\nXBULRydjrd6NdyzQV7BkKs2q7oN0toXpbA/TG0+xee/hcnfLjHGqSrQvQYtLgb4m4Kch5LfUjYcs\n0FewTXuOEIun6GxvprMtUwTU8vSm3A71J0mltejKlfnC9SGbjPWQBfoK5uTkO9vDTJtQT0tDyFbe\nmLLLXRXr0ogeMhOylqP3jgX6Crb81SgTGkK0t9QjInS2hVluI3pTZrk6Ny5NxkLmS6PHUjeesUBf\nwVZsj9DZHs5dlNLZHmbL3iMc7LN/EKZ8ornyBy6O6C114ykL9BUqGovz8r5eOtuPbtDl3H9pu43q\nTflEvEjd1NsuU16yQF+hVmaDeWdbONc2f+p4RGxC1pRXxIMRfbg+xKH+JMlU2rVzmqMs0Feo5V1R\nfALz8wJ9U22Q2ZOaWG4TsqaMIr1xfAJNtUVVOT+GUwUzamlJT1igr1AruiK8ZlITjTXH/mPqbA+z\ncnuUtF04Zcokki1/4PMVX9DM4dS1tzy9NyzQV6B0Wlm5PXpMft7R2dbMwb4ErxzoLUPPjMlMxrq5\nhh7yyyDYiN4LRQV6EdkmIqtFZKWILMu2tYjI4yKyOXt7YrQyJ/Xy/iMc7k/S2R4+4TmnzfL0plwi\nLhY0c+QCvU3IesKNEf2bVLVDVRdmH38OeFJVZwFPZh+bUVj+aiaILxhkRH9mayNNtQHL05uy6XGx\nzo0jV+/GUjee8CJ1cwVwb/b+vcCVHrxHVVuxPcK42gAzJzac8JzPJ3S0hW1Eb8omGkvQ4uLFUgAt\nDZa68VKxgV6BJ0TkRRG5Mds2SVV3Ze/vBiYV+R5jzoquKB3tzUNOdnW2N7Nx9yHbR9aUhRepm/qQ\nn5DfZyN6jxQb6C9S1Q7g7cDNInJJ/pOqqmS+DE4gIjeKyDIRWbZv374iu1E9jgwk2bjn8DHr54/X\n2R4mrbCq+2AJe2YM9MVTDCTTrqduRIRwfZBor43ovVBUoFfVHdnbvcAvgPOBPSIyGSB7u3eI1y5V\n1YWqurC1tbWYblSVl7ZHUYUF04aew3a+BCxPb0qtJ3dVrLupm8w5Q7nzG3cVHOhFpEFEmpz7wNuA\nNcDDwHXZw64DfllsJ8cSpzplx9ShR/Th+hAzWxssT29KLpIraObuiD5zzqCto/dIMZe2TQJ+kS24\nFQB+oqq/FZEXgJ+JyA3Aq8DVxXdz7FjRFeXM1gbGDzNi6mxr5plNe1HVXNEzY7zmRUEzR3N9iM17\nj7h+XlNEoFfVl4FzB2k/ALy5mE6NVarKiu1RLp1z2rDHdraHeWB5N92RPtpa6kvQO2PyC5q5n7qx\nzUe8Y1fGVpBXD8To6Y0Pun7+eM6FU5anN6XkBHq3J2MhW8EyliCzhsO4yQJ9BcnfUWo4syc1UR/y\nW57elFQkuyrG7RIIkFlLn0orh/pt2bDbLNBXkBVdURpCfl4zqWnYYwN+H/OnjretBU1JRWJxmmoD\nBP3uhw7nfwmWvnGfBfoKsqIryrltYfwjrArY2d7M2p2H6E+kPO6ZMRlRDy6WcjTnyiDYWnq3WaCv\nEH3xFOt3HRpR2sbR2RYmmVbW7LALp0xp9MQSnkzEwtERvRU2c58F+gqxesdBkmmls23kxT6dMsaW\npzelEo3FPVlDD/n1bizQu80CfYXIXSg1ihF9a1MNbS11uUlcY7zmRZ0bh6VuvGOBvkKs6IoybUI9\nExtrRvW6zrZmG9Gbkon2ur/piGNcbRCf2GSsFyzQVwBVZXlX5KSFzIbS2R5m18F+dh3s86BnxhwV\nT6Y5PJD0bETv8wnj64L0WI7edRboK8DOg/3sPTww6NaBw1lgeXpTItE+7+rcOJobQrkyC8Y9Fugr\ngJOfH82KG8drJ48jFPDZenrjuaN1brxJ3WTOHbLJWA9YoK8AK7qi1AR8vHbyuFG/NhTwcc6U8Tai\nL1A6nUmb2WX3w8tVrvQodZM5d9AmYz1ggb4CLO+KMH/q+IKvNuxsC7Nqx0HiybTLPat+P/7Lq7z3\nzj/xu7V7yt2Vine0zo13I/pwfcjW0XvAAn2ZDSRTrN1xqKD8vGPBtGbiyTTrdx1ysWfVL5FK8/1n\nXgbg209utlH9MJyRdouHOfqWBkvdeMECfZmt23mIeCpd0Iobh5Pbtzz96Dzy0k52RPt41/zJrNt1\niCfXD7oZmsk6WqLYu0Afrg8ykEzTF7eyHm6yQF9mTm79ZFsHDmfy+DpOH1fLiu2Wpx+pdFq58+mt\nzDm9iTve30FbSx3fecpG9ScTjSWoDfqoDfo9ew/nS8RG9e6yQF9my7sinDG+lknjaos6T2d72GrT\nj8Lj6/ewZe8RPrH4TIJ+HzcvPouXug/yzCbbqH4oPb3eXRXrcFb02Fp6dxWzZ2ybiPxeRNaJyFoR\n+VS2/VYR2SEiK7M/73Cvu9VnRVe0qPy8Y0F7M9t7+th3eMCFXkF/IsW2/b2unKvSqGZG8+0t9bzz\nnMkAvHfBVKaE6/iW5eqH5GXlSkdzrlSxrbxxUzEj+iTwD6o6F1gE3Cwic7PP3aGqHdmfR4vuZZXa\ne6ifHdG+gtbPH885x0oX0jeqys0/Xs5b73iGLVW4h+fzWw/w0vYoN14yk0B2pVMo4OMTi89kRVeU\n57YcKHMPK1MklqC5wbsVN3D0YixL3bir4ECvqrtUdXn2/mFgPTDFrY6NBU5O3Y0R/bwp4wn4xJUJ\n2d+u2c2TG/aSTCu3Pry26ka4dz69ldamGt533tRj2q9aOJXTx9Xy7ac2l6lnlS0Si3uyhWA+Z+mm\n1btxlys5ehGZDnQCf8k23SIiq0TkbhEZNIqJyI0iskxElu3bNzbzosu7IgT9wtlnjP5CqePVBv3M\nPWNc0Xn6w/0Jbn1kLXMnj+OL75zLH7fs59HVu4vuX6VY1R3lj1v2c8NFM06YVKwJ+LnpjTP56ys9\n/PllG9UfL9Ib9/SqWIBwXeaLpKfXUjduKjrQi0gj8ADwaVU9BNwFzAQ6gF3ANwd7naouVdWFqrqw\ntbW12G6cklZ0RZl7xnjXVjEsaG9mVfdBkqnCL5z65mOb2Ht4gK+89xyue8N0zj5jHP/+q3X0DlTH\nPp53/n4r42oD/M3r2wd9/prz22ltquHbT9qoPl86rRzsS9Di8Yg+FPDRVBOw1I3Ligr0IhIkE+R/\nrKoPAqjqHlVNqWoa+AFwfvHdrD7JVJpV3dGi1s8fr7M9TCyeYtOewvLqL22Pcu/z2/jQoml0ZLc0\n/Lcr5rH7UH9VBL4tew/zu3W7ue4N02mqHXxkWhv08/FLZvKnrQdYtq2nxD2sXIf6E6QVz1M3AOGG\noKVuXFbMqhsBfgisV9Xb89on5x32HmBN4d2rXht2H6Y/kS5q/fzxnN2pCtmIJJlK84VfrKa1sYZ/\nvGx2rv28ac1cvXAqP/zjK2zec9i1vpbDXU+/TE3Ax/VvmH7S4/7m9dOY2Bji209tKU3HTgHOVbFe\nT8aCU9jMUjduKmZEfyHwIeDS45ZS/qeIrBaRVcCbgP/lRkerTa5ipYsj+raWOiY0hFj+6uhX3tz7\n/Kus3XmIL11+NuOOG+1+dskcGmoC/OsvT92J2R3RPn65cgfXvK6dCcNs7lIX8vPRi2fy7KZ9drVx\nlrOuvSQjeqtg6bpiVt38UVVFVefnL6VU1Q+p6jnZ9ner6i43O1wtVnRFmdhYw9TmOtfOKSJ0todH\nPaLfGe3jm49tZPHsVt5xzuknPD+hsYZ/umw2z798gEdWnZp/nT94NlPT5mOXzBzR8R9aNI3m+iDf\nsVE9cHQVjNc5+sx7BC3Qu8yujC2TFdujdLaHyWTA3NPZ3szL+3pHleO89eG1pFX59yvmDdmfa89v\n55wp4/nyr9Zx5BSbmD1wZID7X+jiys4pTAmP7Iu1oSbARy+eyVMb9rK6+6DHPax8udRNiUb0UVt1\n4yoL9GUi4NRrAAANgklEQVQQ6Y3zyv7e3O5QbhrthVOPrd3NY+v28Kk3v4a2lvohj/P7hH+/ch77\njgzwrSc2udLXUvnRc9sYSKa56Y1njup1H75gGuNqA3zH1tXnBg7hEuXoDw8kSRSxeswcywJ9GTip\nFTeuiD3e/KlhfALLR7ARSe9AklsfXsvsSU189OIZwx7f0Rbmmte1cfdz29i4+9SYmD3cn+De57dx\n2dzTOeu0xlG9tqk2yA0XzeSxdXtYt3Nsl4Du6Y0T8AlNNQHP38uZ8LX0jXss0JfBiq4oPoH5U8e7\nfu7GmgCvmdQ0oknEOx7fxM6D/XzlvfNGvOnJP102h6baAP/6yzWnxMTsj//SxeH+JJ980+hG847r\nL5xOU02A7/5+bI/qI7EE4fqQ66nGwVi9G/dZoC+DFV1R5pw+jvqQN6OjBdOaWbk9Sjo9dCBes+Mg\ndz/3Ch94fTvnTWsZ8blbGkJ85rI5/OWVHh5+aacb3fVMfyLFD//4ChfPmsj8qYX972l8XZDrL5zO\no6t3s+kUX15ajExBM+/TNpBXqtgqWLrGAn2JpdLKyu1RFkxzP23j6GwLc7g/ycv7B79wKpVW/vkX\nq2lpCPHZy+aM+vzvf10b504dz5d/vZ7D/ZU76vr5i93sOzzAJxYXNpp3fOTCGTSE/Hx3DK/AiZSg\ncqXDqXdja+ndY4G+xLbsPcKRgWTu4iYvOEXShlpP/99/fpWXug/yxXfNZXwBozTnitn9Rwb4rycq\nM6WRTKX5P89upaMtzAUzJxR1ruaGEB9+w3QeWbWzKqt5jkSkN+HpXrH5rIKl+yzQl1juQikPJmId\nMyc2MK42MOh6+j2H+vn67zZy8ayJvPvcMwp+j3Pbwlx7fjv3/GkbG3ZX3kTlr1btYntPH59cfKYr\neeWPXjSD2oCfO38/Nkf1kVjc071i87XYLlOus0BfYiu6ooTrg8yY2ODZe/h8Qmd7c26bwnz/9sg6\n4qn0SdfMj9Q/vW0242oDfPGhypqYTaeVu57eyqzTGnnLaye5cs4JjTV8cFE7D63cUbUbsgxFVYlm\nJ2NLoS7kpybgs8lYF1mgL7EV2yN0trl/odTxOtvDbNxz+JiLm36/YS+/Xr2Lv7/0LKa78EXT3BDi\nc2+fwwvbIvxixY6iz+eWpzbsZeOew3xi8Zn4fO59zh+7ZCZBv487nx5bo/pYPEU8lS7ZZCxk693Y\nZKxrLNCX0KH+BJv3HnFlo5HhdLY3o5qpSAkQiyf5l4fWcNZpjdx4SXGTk/muOq+NjrYwX3l0PQf7\nyj8Cy2wTuIUp4TouLyI1NZjTmmr5wOvbeXD5Drb3xFw993D2Hu7nnude4X13/Ym33fEMtz++iS17\nS7MKyKlzU6rJWMhMyFrqxj0W6Evope1RVL3Nzzs6sssJnTmBbz25mR3RPm67ch6hgHt/7T6f8OUr\n53GgN84dj5f/itm/vNLD8q4oH3/jzBFfGzAaN70x87+EO5/e6vq5jxfpjXPfX7v4wA/+zKKvPMmt\nj2TKT7Q0hPjOU5t5y+3PsuS/nuV7v99C1wHvvniiucqVpQv0LQ1WwdJN3l/mZnJWdEURyUxkem18\nfZCzTmtkRVeUDbsP8cM/vMLVC6fy+iJXoAxm3pTxfPD10/h/z2/j6oVtzHVhx6xC3fn0ViY2hrh6\nYZsn5580rpb3L2zj/he6uOXSszhjhLVzRupwf4LH1+3hkZd28ofN+0mmlRkTG/i7S2dx+fzJzJrU\nBGT2G3509S4eWbWLr/9uI1//3UbObQtz+fzJvGv+GZw+vta1Pjkj61KnbtZX4CT/qcoCfQkt74ow\n67TGE8oAe6WzLcyTG/byhQdXM64uyOff/lrP3usf3zabX6/exb/+cg0/+/gFrubGR2rNjoM8u2kf\nn1ky27VduwZz0+Izuf+FLr7/zFb+7Yp5RZ+vL57iqQ17eeSlnTy1cS/xZJop4TpuuHgGl88/g7PP\nGHfCnM5p42q5/sIZXH/hDLojMX69ahePrNrJl3+9ntseXc/rprdw+bln8PZ5pzNxmLLMw3ECfakm\nYzPvFbTJWBdZoC8RVWVFV5QlZ59YBtgrne3N/M+L3fT0xvnmVed6+l/v8fVBPvf2OXzm56t4cMWO\nEzbeLoU7n95CU02ADy6a5un7TAnX8b7z2rj/r9v55OKzCho9DyRT/GHTfh5ZtZPH1+0hFk/R2lTD\nB85v5/Jzz6CzLTziL8upzfV8/I1n8vE3nsnL+47wq1W7ePilnXzxoTXc+vBa3nDmBC4/9wwuO/t0\nxteNfpAR6S3PiD4ai5NOa1kGDdWmIgL9up2HmH/r78rdDU8pcLg/WZL8vMN5rwtmTuC9C6Z4/n7v\nWzCV+//axX88up6LzprIpHE1JamNArB13xF+s2Y3n3jjmSX5H9MnF5/J/yzbzvU/+iunjasllU6T\nTCnJdOYn/3EqrSTTaVJ5z/cOJBlIpgnXB7miYwqXnzuZ18+YgL/IoDaztZG/f/Msbrn0LDbuOcwj\nL+3kkZd28Zmfr+ILD66mPnT0fzr5fzf5f01y3PN98RQiFPQlUajmhhBphdfd9gQigk8yffSJ4Mv2\ny+cDwXlOEMn03SdCwO+jJuAjFMjc1gT81ASd+9nHgfxjMs8HfD7iyRT9yTQDiTT9yRT9iRQDyXTm\nNpFmIJmiP5E+pr0/mSKVUhRQBUWzt5nHHPM477gSLUv2LNCLyBLgW4Af+L+q+tWhjg3XB3nvgtKP\nAEutNujnHfMnD3+gS+ac3sS/vPO1vOOcySUJuL7sFbPv/u4fWfQfTxLy+5jQGKK1qYaJjTVMzLt/\ntC1zf1xtYNA+JlJpegeSHBlI0juQyt5mHjv3eweSPLt5PyG/j49cNHwVTje0tdTz6bfM4rdrd3Oo\nL0HAJ/h9Qm02WDiPg34ffp/kHgf8medqAj4unDWRi86a6MmksYgw5/RxzDl9HP/4ttms6j7I4+v2\nDLqXQH6w0WPaj96fNamRgAf9HMo7zjmd7T0xEqk06WxAVIV0Nkims1Hz6ONjj0mmlYFkmoFEisP9\nSQ4k4wwkM4F5IJkmnkznHp8s1gb9Qm3uS8JPbd5tbdBPU22A2qCf2qAfv08QyH7hZL94BODol9Ax\nz0HR/y5Xj/A48eIbRUT8wCbgrUA38AJwraquG+z4hQsX6rJly1zvhymP5V0RVnRF2Xd4gP1HBo65\nPdAbJzVIsbWQ38fExhDj6oLE4qlcMB9Ijqwmecjv49NvncUnF5/l9h/HVDFVJZFS4qnMl0IipYQC\nvlxAL/Z/WF4TkRdVdeFwx3k1oj8f2KKqL2c7cz9wBTBooDfVZUF785CbqqTTSrQvceKXQPb+4f4k\n9SE/DTUBmmoCNGR/Gmv8NNYEaajx05hrC+Tuu7lk1IwdIkIoIIQCPhpLUGu/XLz6k00Btuc97gZe\nP+TR+zfDj97pUVdMJfEBLdmf2UMd1Jf9Mca4omzDIBG5UUSWiciyRMKWURljjFe8GtHvAPKvWJma\nbctR1aXAUsjk6PnbX3vUFWOMqVIfGdkcglcj+heAWSIyQ0RCwDXAwx69lzHGmJPwZESvqkkR+Tvg\nd2SWV96tqmu9eC9jjDEn59k0s6o+Cjzq1fmNMcaMjK1JM8aYKmeB3hhjqpwFemOMqXIW6I0xpspZ\noDfGmCrnSVGzUXdC5DCwsdz9qHATgf3l7kSFs89oePYZndyp9vlMU9XW4Q6qlCo+G0dSgW0sE5Fl\n9hmdnH1Gw7PP6OSq9fOx1I0xxlQ5C/TGGFPlKiXQLy13B04B9hkNzz6j4dlndHJV+flUxGSsMcYY\n71TKiN4YY4xHyh7oRWSJiGwUkS0i8rly96cSicg2EVktIitFxDbXBUTkbhHZKyJr8tpaRORxEdmc\nvR18P8MxYIjP51YR2ZH9PVopIu8oZx/LTUTaROT3IrJORNaKyKey7VX3e1TWQJ/dRPx7wNuBucC1\nIjK3nH2qYG9S1Y5qXPpVoHuAJce1fQ54UlVnAU9mH49V93Di5wNwR/b3qCNbYXYsSwL/oKpzgUXA\nzdn4U3W/R+Ue0ec2EVfVOOBsIm7MSanqs0DPcc1XAPdm798LXFnSTlWQIT4fk0dVd6nq8uz9w8B6\nMvtdV93vUbkD/WCbiE8pU18qmQJPiMiLInJjuTtTwSap6q7s/d3ApHJ2pkLdIiKrsqmdUz4l4RYR\nmQ50An+hCn+Pyh3ozchcpKodZFJcN4vIJeXuUKXTzHIyW1J2rLuAmUAHsAv4Znm7UxlEpBF4APi0\nqh7Kf65afo/KHeiH3UTcgKruyN7uBX5BJuVlTrRHRCYDZG/3lrk/FUVV96hqSlXTwA+w3yNEJEgm\nyP9YVR/MNlfd71G5A71tIj4MEWkQkSbnPvA2YM3JXzVmPQxcl71/HfDLMval4jjBK+s9jPHfIxER\n4IfAelW9Pe+pqvs9KvsFU9klXv/F0U3EbytrhyqMiMwkM4qHTBG6n9hnBCJyH7CYTLXBPcCXgIeA\nnwHtwKvA1ao6Jickh/h8FpNJ2yiwDfh4Xi56zBGRi4A/AKuBdLb5C2Ty9FX1e1T2QG+MMcZb5U7d\nGGOM8ZgFemOMqXIW6I0xpspZoDfGmCpngd4YY6qcBXpjjKlyFuiNMabKWaA3xpgq9/8B4ATz1R1a\nEIoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c391557b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(test_mw[1, :]).plot(label='GT')\n",
    "#pd.Series(test_agg[1, :]).plot(label='GT')\n",
    "\n",
    "\n",
    "pd.Series(model.predict(test_agg[1:2])[0, :24]).plot(label='Pred')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
