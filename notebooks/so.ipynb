{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "time_points = np.linspace(0, 8*np.pi, 8000)\n",
    "\n",
    "seq_length = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(time_points) - seq_length, 1):\n",
    "    seq_in = np.sin(time_points)[i:i + seq_length]\n",
    "    seq_out = np.cos(time_points)[i]\n",
    "    dataX.append([seq_in])\n",
    "    dataY.append(seq_out)\n",
    "\n",
    "X = np.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "y = np.reshape(dataY, (len(dataY), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 0.5159 - val_loss: 0.4761\n",
      "Epoch 2/20\n",
      " - 2s - loss: 0.5139 - val_loss: 0.4912\n",
      "Epoch 3/20\n",
      " - 2s - loss: 0.5144 - val_loss: 0.4785\n",
      "Epoch 4/20\n",
      " - 2s - loss: 0.5136 - val_loss: 0.4817\n",
      "Epoch 5/20\n",
      " - 2s - loss: 0.5130 - val_loss: 0.4890\n",
      "Epoch 6/20\n",
      " - 2s - loss: 0.5131 - val_loss: 0.4948\n",
      "Epoch 7/20\n",
      " - 2s - loss: 0.5132 - val_loss: 0.4889\n",
      "Epoch 8/20\n",
      " - 2s - loss: 0.5131 - val_loss: 0.4827\n",
      "Epoch 9/20\n",
      " - 2s - loss: 0.5127 - val_loss: 0.4927\n",
      "Epoch 10/20\n",
      " - 2s - loss: 0.5123 - val_loss: 0.4860\n",
      "Epoch 11/20\n",
      " - 2s - loss: 0.5121 - val_loss: 0.4827\n",
      "Epoch 12/20\n",
      " - 2s - loss: 0.5113 - val_loss: 0.5025\n",
      "Epoch 13/20\n",
      " - 2s - loss: 0.5119 - val_loss: 0.4875\n",
      "Epoch 14/20\n",
      " - 2s - loss: 0.5112 - val_loss: 0.4811\n",
      "Epoch 15/20\n",
      " - 2s - loss: 0.5119 - val_loss: 0.4819\n",
      "Epoch 16/20\n",
      " - 2s - loss: 0.5108 - val_loss: 0.4826\n",
      "Epoch 17/20\n",
      " - 2s - loss: 0.5104 - val_loss: 0.4910\n",
      "Epoch 18/20\n",
      " - 2s - loss: 0.5108 - val_loss: 0.4851\n",
      "Epoch 19/20\n",
      " - 2s - loss: 0.5101 - val_loss: 0.4885\n",
      "Epoch 20/20\n",
      " - 2s - loss: 0.5097 - val_loss: 0.4934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x181e649518>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(16, input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "\n",
    "model.add(Dense(y.shape[1], activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 0.5157 - val_loss: 0.4697\n",
      "Epoch 2/20\n",
      " - 2s - loss: 0.5150 - val_loss: 0.4773\n",
      "Epoch 3/20\n",
      " - 2s - loss: 0.5142 - val_loss: 0.4794\n",
      "Epoch 4/20\n",
      " - 2s - loss: 0.5139 - val_loss: 0.4839\n",
      "Epoch 5/20\n",
      " - 2s - loss: 0.5133 - val_loss: 0.4912\n",
      "Epoch 6/20\n",
      " - 2s - loss: 0.5136 - val_loss: 0.4824\n",
      "Epoch 7/20\n",
      " - 2s - loss: 0.5132 - val_loss: 0.4840\n",
      "Epoch 8/20\n",
      " - 2s - loss: 0.5137 - val_loss: 0.4782\n",
      "Epoch 9/20\n",
      " - 2s - loss: 0.5127 - val_loss: 0.4970\n",
      "Epoch 10/20\n",
      " - 2s - loss: 0.5129 - val_loss: 0.4827\n",
      "Epoch 11/20\n",
      " - 2s - loss: 0.5128 - val_loss: 0.4900\n",
      "Epoch 12/20\n",
      " - 2s - loss: 0.5128 - val_loss: 0.4895\n",
      "Epoch 13/20\n",
      " - 2s - loss: 0.5123 - val_loss: 0.4868\n",
      "Epoch 14/20\n",
      " - 2s - loss: 0.5121 - val_loss: 0.4977\n",
      "Epoch 15/20\n",
      " - 2s - loss: 0.5120 - val_loss: 0.4879\n",
      "Epoch 16/20\n",
      " - 2s - loss: 0.5118 - val_loss: 0.4890\n",
      "Epoch 17/20\n",
      " - 2s - loss: 0.5115 - val_loss: 0.4840\n",
      "Epoch 18/20\n",
      " - 2s - loss: 0.5109 - val_loss: 0.4851\n",
      "Epoch 19/20\n",
      " - 2s - loss: 0.5108 - val_loss: 0.4857\n",
      "Epoch 20/20\n",
      " - 2s - loss: 0.5102 - val_loss: 0.4907\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/20\n",
      " - 2s - loss: 0.5162 - val_loss: 0.4752\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.5145 - val_loss: 0.4823\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.5148 - val_loss: 0.4790\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.5142 - val_loss: 0.4863\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.5147 - val_loss: 0.4839\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.5135 - val_loss: 0.4861\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.5143 - val_loss: 0.4866\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.5135 - val_loss: 0.4847\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.5135 - val_loss: 0.4880\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.5132 - val_loss: 0.4812\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.5132 - val_loss: 0.4795\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.5129 - val_loss: 0.4841\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.5132 - val_loss: 0.4860\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.5129 - val_loss: 0.4851\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.5126 - val_loss: 0.4885\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.5128 - val_loss: 0.4809\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.5124 - val_loss: 0.4863\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.5119 - val_loss: 0.4879\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.5118 - val_loss: 0.4930\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.5119 - val_loss: 0.4824\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 0.5254 - val_loss: 0.4781\n",
      "Epoch 2/20\n",
      " - 0s - loss: 0.5146 - val_loss: 0.4832\n",
      "Epoch 3/20\n",
      " - 0s - loss: 0.5145 - val_loss: 0.4759\n",
      "Epoch 4/20\n",
      " - 0s - loss: 0.5144 - val_loss: 0.4765\n",
      "Epoch 5/20\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4790\n",
      "Epoch 6/20\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4830\n",
      "Epoch 7/20\n",
      " - 0s - loss: 0.5138 - val_loss: 0.4841\n",
      "Epoch 8/20\n",
      " - 0s - loss: 0.5137 - val_loss: 0.4792\n",
      "Epoch 9/20\n",
      " - 0s - loss: 0.5134 - val_loss: 0.4905\n",
      "Epoch 10/20\n",
      " - 0s - loss: 0.5137 - val_loss: 0.4852\n",
      "Epoch 11/20\n",
      " - 0s - loss: 0.5136 - val_loss: 0.4852\n",
      "Epoch 12/20\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4844\n",
      "Epoch 13/20\n",
      " - 0s - loss: 0.5134 - val_loss: 0.4860\n",
      "Epoch 14/20\n",
      " - 0s - loss: 0.5134 - val_loss: 0.4883\n",
      "Epoch 15/20\n",
      " - 0s - loss: 0.5132 - val_loss: 0.4852\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.5136 - val_loss: 0.4833\n",
      "Epoch 17/20\n",
      " - 0s - loss: 0.5133 - val_loss: 0.4804\n",
      "Epoch 18/20\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4903\n",
      "Epoch 19/20\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4841\n",
      "Epoch 20/20\n",
      " - 0s - loss: 0.5133 - val_loss: 0.4907\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 0.5167 - val_loss: 0.4694\n",
      "Epoch 2/20\n",
      " - 0s - loss: 0.5154 - val_loss: 0.4730\n",
      "Epoch 3/20\n",
      " - 0s - loss: 0.5148 - val_loss: 0.4735\n",
      "Epoch 4/20\n",
      " - 0s - loss: 0.5142 - val_loss: 0.4750\n",
      "Epoch 5/20\n",
      " - 0s - loss: 0.5140 - val_loss: 0.4793\n",
      "Epoch 6/20\n",
      " - 0s - loss: 0.5137 - val_loss: 0.4780\n",
      "Epoch 7/20\n",
      " - 0s - loss: 0.5138 - val_loss: 0.4800\n",
      "Epoch 8/20\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4817\n",
      "Epoch 9/20\n",
      " - 0s - loss: 0.5130 - val_loss: 0.4812\n",
      "Epoch 10/20\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4871\n",
      "Epoch 11/20\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4846\n",
      "Epoch 12/20\n",
      " - 0s - loss: 0.5126 - val_loss: 0.4825\n",
      "Epoch 13/20\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4871\n",
      "Epoch 14/20\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4847\n",
      "Epoch 15/20\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4890\n",
      "Epoch 16/20\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4848\n",
      "Epoch 17/20\n",
      " - 0s - loss: 0.5126 - val_loss: 0.4845\n",
      "Epoch 18/20\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4871\n",
      "Epoch 19/20\n",
      " - 0s - loss: 0.5123 - val_loss: 0.4877\n",
      "Epoch 20/20\n",
      " - 0s - loss: 0.5119 - val_loss: 0.4858\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 0.5168 - val_loss: 0.4720\n",
      "Epoch 2/20\n",
      " - 0s - loss: 0.5155 - val_loss: 0.4711\n",
      "Epoch 3/20\n",
      " - 0s - loss: 0.5153 - val_loss: 0.4709\n",
      "Epoch 4/20\n",
      " - 0s - loss: 0.5148 - val_loss: 0.4716\n",
      "Epoch 5/20\n",
      " - 0s - loss: 0.5149 - val_loss: 0.4737\n",
      "Epoch 6/20\n",
      " - 0s - loss: 0.5145 - val_loss: 0.4736\n",
      "Epoch 7/20\n",
      " - 0s - loss: 0.5142 - val_loss: 0.4744\n",
      "Epoch 8/20\n",
      " - 0s - loss: 0.5140 - val_loss: 0.4770\n",
      "Epoch 9/20\n",
      " - 0s - loss: 0.5138 - val_loss: 0.4763\n",
      "Epoch 10/20\n",
      " - 0s - loss: 0.5138 - val_loss: 0.4779\n",
      "Epoch 11/20\n",
      " - 0s - loss: 0.5136 - val_loss: 0.4791\n",
      "Epoch 12/20\n",
      " - 0s - loss: 0.5136 - val_loss: 0.4784\n",
      "Epoch 13/20\n",
      " - 0s - loss: 0.5137 - val_loss: 0.4784\n",
      "Epoch 14/20\n",
      " - 0s - loss: 0.5138 - val_loss: 0.4803\n",
      "Epoch 15/20\n",
      " - 0s - loss: 0.5133 - val_loss: 0.4814\n",
      "Epoch 16/20\n",
      " - 0s - loss: 0.5132 - val_loss: 0.4791\n",
      "Epoch 17/20\n",
      " - 0s - loss: 0.5132 - val_loss: 0.4815\n",
      "Epoch 18/20\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4804\n",
      "Epoch 19/20\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4818\n",
      "Epoch 20/20\n",
      " - 0s - loss: 0.5132 - val_loss: 0.4828\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/50\n",
      " - 3s - loss: 0.5163 - val_loss: 0.4843\n",
      "Epoch 2/50\n",
      " - 2s - loss: 0.5151 - val_loss: 0.4723\n",
      "Epoch 3/50\n",
      " - 2s - loss: 0.5145 - val_loss: 0.4766\n",
      "Epoch 4/50\n",
      " - 2s - loss: 0.5146 - val_loss: 0.4823\n",
      "Epoch 5/50\n",
      " - 2s - loss: 0.5142 - val_loss: 0.4796\n",
      "Epoch 6/50\n",
      " - 2s - loss: 0.5139 - val_loss: 0.4775\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.5140 - val_loss: 0.4800\n",
      "Epoch 8/50\n",
      " - 2s - loss: 0.5135 - val_loss: 0.4830\n",
      "Epoch 9/50\n",
      " - 2s - loss: 0.5132 - val_loss: 0.4776\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.5135 - val_loss: 0.4837\n",
      "Epoch 11/50\n",
      " - 2s - loss: 0.5127 - val_loss: 0.4840\n",
      "Epoch 12/50\n",
      " - 2s - loss: 0.5128 - val_loss: 0.4839\n",
      "Epoch 13/50\n",
      " - 2s - loss: 0.5131 - val_loss: 0.4807\n",
      "Epoch 14/50\n",
      " - 2s - loss: 0.5128 - val_loss: 0.4882\n",
      "Epoch 15/50\n",
      " - 2s - loss: 0.5128 - val_loss: 0.4828\n",
      "Epoch 16/50\n",
      " - 2s - loss: 0.5122 - val_loss: 0.4830\n",
      "Epoch 17/50\n",
      " - 2s - loss: 0.5123 - val_loss: 0.4917\n",
      "Epoch 18/50\n",
      " - 2s - loss: 0.5117 - val_loss: 0.4948\n",
      "Epoch 19/50\n",
      " - 2s - loss: 0.5118 - val_loss: 0.4957\n",
      "Epoch 20/50\n",
      " - 2s - loss: 0.5111 - val_loss: 0.4843\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.5117 - val_loss: 0.4915\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.5113 - val_loss: 0.4910\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.5108 - val_loss: 0.4936\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.5107 - val_loss: 0.4925\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.5100 - val_loss: 0.4954\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.5097 - val_loss: 0.4940\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.5083 - val_loss: 0.4933\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.5082 - val_loss: 0.5031\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.5083 - val_loss: 0.4944\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.5079 - val_loss: 0.4911\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.5073 - val_loss: 0.5023\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.5077 - val_loss: 0.4944\n",
      "Epoch 33/50\n",
      " - 3s - loss: 0.5071 - val_loss: 0.4840\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.5063 - val_loss: 0.4992\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.5056 - val_loss: 0.4867\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.5057 - val_loss: 0.4873\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.5048 - val_loss: 0.4870\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.5045 - val_loss: 0.4883\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.5045 - val_loss: 0.4943\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.5029 - val_loss: 0.4871\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.5033 - val_loss: 0.4688\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.5025 - val_loss: 0.4758\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.5010 - val_loss: 0.4765\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.5003 - val_loss: 0.4944\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.4996 - val_loss: 0.4842\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.4986 - val_loss: 0.4993\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.4977 - val_loss: 0.4822\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.4973 - val_loss: 0.4658\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.4958 - val_loss: 0.4964\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.4922 - val_loss: 0.5123\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 0.5156 - val_loss: 0.4723\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.5147 - val_loss: 0.4734\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 0.5142 - val_loss: 0.4839\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.5133 - val_loss: 0.4822\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.5137 - val_loss: 0.4843\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.5134 - val_loss: 0.4813\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.5138 - val_loss: 0.4815\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.5132 - val_loss: 0.4889\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.5129 - val_loss: 0.4813\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.5130 - val_loss: 0.4851\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.5126 - val_loss: 0.4882\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.5124 - val_loss: 0.4896\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.5125 - val_loss: 0.4888\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.5123 - val_loss: 0.4848\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.5123 - val_loss: 0.4831\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.5123 - val_loss: 0.4853\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.5116 - val_loss: 0.4829\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.5118 - val_loss: 0.4882\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.5117 - val_loss: 0.4912\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.5117 - val_loss: 0.4870\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.5112 - val_loss: 0.4904\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.5112 - val_loss: 0.4875\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.5107 - val_loss: 0.4824\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.5107 - val_loss: 0.4898\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.5102 - val_loss: 0.4859\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.5097 - val_loss: 0.4897\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.5088 - val_loss: 0.4944\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.5091 - val_loss: 0.4902\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.5083 - val_loss: 0.4821\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.5078 - val_loss: 0.4932\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.5081 - val_loss: 0.4922\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.5066 - val_loss: 0.4937\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.5062 - val_loss: 0.4959\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.5057 - val_loss: 0.4931\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.5054 - val_loss: 0.4950\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.5054 - val_loss: 0.4974\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.5050 - val_loss: 0.4903\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.5046 - val_loss: 0.4903\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.5046 - val_loss: 0.4879\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.5036 - val_loss: 0.4853\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.5026 - val_loss: 0.5100\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.5031 - val_loss: 0.4923\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.5019 - val_loss: 0.5022\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.5026 - val_loss: 0.5005\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.5012 - val_loss: 0.4847\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.5003 - val_loss: 0.4844\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.4987 - val_loss: 0.4992\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.4999 - val_loss: 0.4959\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.4982 - val_loss: 0.4866\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.4979 - val_loss: 0.4970\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 0.5187 - val_loss: 0.4724\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.5150 - val_loss: 0.4755\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.5153 - val_loss: 0.4798\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.5150 - val_loss: 0.4785\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.5147 - val_loss: 0.4825\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.5145 - val_loss: 0.4786\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.5141 - val_loss: 0.4849\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4803\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.5138 - val_loss: 0.4861\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.5140 - val_loss: 0.4837\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.5137 - val_loss: 0.4863\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.5137 - val_loss: 0.4836\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.5136 - val_loss: 0.4852\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.5137 - val_loss: 0.4826\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.5136 - val_loss: 0.4844\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.5133 - val_loss: 0.4821\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.5134 - val_loss: 0.4907\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4829\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.5136 - val_loss: 0.4873\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.5132 - val_loss: 0.4841\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4878\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4865\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.5130 - val_loss: 0.4870\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.5129 - val_loss: 0.4840\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4878\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.5128 - val_loss: 0.4862\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4857\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4861\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4899\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.5123 - val_loss: 0.4907\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.5123 - val_loss: 0.4901\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4866\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.5121 - val_loss: 0.4903\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4897\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.5122 - val_loss: 0.4860\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4866\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.5120 - val_loss: 0.4900\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.5119 - val_loss: 0.4855\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.5119 - val_loss: 0.4893\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.5118 - val_loss: 0.4862\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.5111 - val_loss: 0.4904\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.5114 - val_loss: 0.4875\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.5110 - val_loss: 0.4879\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.5111 - val_loss: 0.4876\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.5108 - val_loss: 0.4923\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.5111 - val_loss: 0.4898\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.5105 - val_loss: 0.4894\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.5102 - val_loss: 0.4903\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.5107 - val_loss: 0.5013\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.5103 - val_loss: 0.4926\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 0.5175 - val_loss: 0.4744\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.5157 - val_loss: 0.4734\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.5147 - val_loss: 0.4751\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.5145 - val_loss: 0.4785\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4743\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.5148 - val_loss: 0.4761\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4776\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.5140 - val_loss: 0.4764\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.5140 - val_loss: 0.4785\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.5134 - val_loss: 0.4819\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.5134 - val_loss: 0.4788\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.5133 - val_loss: 0.4819\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.5132 - val_loss: 0.4842\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.5130 - val_loss: 0.4807\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4813\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.5130 - val_loss: 0.4812\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4839\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4836\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.5126 - val_loss: 0.4808\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4840\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4887\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.5123 - val_loss: 0.4837\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.5122 - val_loss: 0.4856\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.5121 - val_loss: 0.4867\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.5118 - val_loss: 0.4906\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4892\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.5117 - val_loss: 0.4889\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.5114 - val_loss: 0.4873\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.5117 - val_loss: 0.4888\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.5116 - val_loss: 0.4854\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.5116 - val_loss: 0.4873\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.5111 - val_loss: 0.4876\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.5113 - val_loss: 0.4899\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.5110 - val_loss: 0.4948\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.5108 - val_loss: 0.4864\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.5106 - val_loss: 0.4879\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.5109 - val_loss: 0.4873\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.5110 - val_loss: 0.4900\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.5107 - val_loss: 0.4892\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.5113 - val_loss: 0.4906\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.5101 - val_loss: 0.4891\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.5107 - val_loss: 0.4887\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.5109 - val_loss: 0.4866\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.5100 - val_loss: 0.4905\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.5097 - val_loss: 0.4895\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.5103 - val_loss: 0.4904\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.5099 - val_loss: 0.4936\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.5102 - val_loss: 0.4927\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.5100 - val_loss: 0.4951\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.5106 - val_loss: 0.4950\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 0.5216 - val_loss: 0.4655\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.5161 - val_loss: 0.4728\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.5156 - val_loss: 0.4722\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.5153 - val_loss: 0.4743\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.5149 - val_loss: 0.4731\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.5147 - val_loss: 0.4731\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.5146 - val_loss: 0.4738\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.5144 - val_loss: 0.4760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      " - 0s - loss: 0.5142 - val_loss: 0.4773\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.5140 - val_loss: 0.4776\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4762\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.5138 - val_loss: 0.4787\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4809\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.5135 - val_loss: 0.4810\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.5135 - val_loss: 0.4800\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.5135 - val_loss: 0.4804\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.5136 - val_loss: 0.4834\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.5135 - val_loss: 0.4828\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.5134 - val_loss: 0.4824\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.5134 - val_loss: 0.4819\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.5132 - val_loss: 0.4834\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.5130 - val_loss: 0.4833\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4841\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.5130 - val_loss: 0.4835\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4860\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4860\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.5128 - val_loss: 0.4833\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4854\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4861\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4835\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.5128 - val_loss: 0.4876\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4858\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.5126 - val_loss: 0.4869\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4861\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.5128 - val_loss: 0.4838\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.5130 - val_loss: 0.4856\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.5123 - val_loss: 0.4880\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.5122 - val_loss: 0.4861\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4887\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4860\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.5122 - val_loss: 0.4863\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.5122 - val_loss: 0.4860\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.5120 - val_loss: 0.4871\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.5120 - val_loss: 0.4878\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.5121 - val_loss: 0.4903\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4888\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.5119 - val_loss: 0.4881\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.5118 - val_loss: 0.4870\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.5118 - val_loss: 0.4882\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.5118 - val_loss: 0.4869\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.5166 - val_loss: 0.4659\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.5150 - val_loss: 0.4760\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.5140 - val_loss: 0.4743\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.5146 - val_loss: 0.4826\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.5137 - val_loss: 0.4883\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.5142 - val_loss: 0.4855\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.5128 - val_loss: 0.4844\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.5133 - val_loss: 0.4903\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.5133 - val_loss: 0.4854\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.5133 - val_loss: 0.4870\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.5129 - val_loss: 0.4873\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.5126 - val_loss: 0.4872\n",
      "Epoch 13/100\n",
      " - 2s - loss: 0.5126 - val_loss: 0.4840\n",
      "Epoch 14/100\n",
      " - 2s - loss: 0.5124 - val_loss: 0.4831\n",
      "Epoch 15/100\n",
      " - 2s - loss: 0.5125 - val_loss: 0.4832\n",
      "Epoch 16/100\n",
      " - 2s - loss: 0.5120 - val_loss: 0.4897\n",
      "Epoch 17/100\n",
      " - 2s - loss: 0.5119 - val_loss: 0.4914\n",
      "Epoch 18/100\n",
      " - 2s - loss: 0.5115 - val_loss: 0.4832\n",
      "Epoch 19/100\n",
      " - 2s - loss: 0.5112 - val_loss: 0.4913\n",
      "Epoch 20/100\n",
      " - 2s - loss: 0.5109 - val_loss: 0.4884\n",
      "Epoch 21/100\n",
      " - 2s - loss: 0.5108 - val_loss: 0.4806\n",
      "Epoch 22/100\n",
      " - 2s - loss: 0.5107 - val_loss: 0.4875\n",
      "Epoch 23/100\n",
      " - 2s - loss: 0.5097 - val_loss: 0.4937\n",
      "Epoch 24/100\n",
      " - 2s - loss: 0.5098 - val_loss: 0.4884\n",
      "Epoch 25/100\n",
      " - 2s - loss: 0.5088 - val_loss: 0.4911\n",
      "Epoch 26/100\n",
      " - 2s - loss: 0.5088 - val_loss: 0.4864\n",
      "Epoch 27/100\n",
      " - 2s - loss: 0.5086 - val_loss: 0.4907\n",
      "Epoch 28/100\n",
      " - 2s - loss: 0.5075 - val_loss: 0.4817\n",
      "Epoch 29/100\n",
      " - 2s - loss: 0.5081 - val_loss: 0.4907\n",
      "Epoch 30/100\n",
      " - 2s - loss: 0.5077 - val_loss: 0.4846\n",
      "Epoch 31/100\n",
      " - 2s - loss: 0.5065 - val_loss: 0.4816\n",
      "Epoch 32/100\n",
      " - 2s - loss: 0.5069 - val_loss: 0.5027\n",
      "Epoch 33/100\n",
      " - 2s - loss: 0.5070 - val_loss: 0.4975\n",
      "Epoch 34/100\n",
      " - 2s - loss: 0.5060 - val_loss: 0.4998\n",
      "Epoch 35/100\n",
      " - 2s - loss: 0.5062 - val_loss: 0.4928\n",
      "Epoch 36/100\n",
      " - 2s - loss: 0.5055 - val_loss: 0.4872\n",
      "Epoch 37/100\n",
      " - 2s - loss: 0.5045 - val_loss: 0.4948\n",
      "Epoch 38/100\n",
      " - 2s - loss: 0.5041 - val_loss: 0.4940\n",
      "Epoch 39/100\n",
      " - 2s - loss: 0.5034 - val_loss: 0.5105\n",
      "Epoch 40/100\n",
      " - 2s - loss: 0.5038 - val_loss: 0.4842\n",
      "Epoch 41/100\n",
      " - 2s - loss: 0.5029 - val_loss: 0.5012\n",
      "Epoch 42/100\n",
      " - 2s - loss: 0.5020 - val_loss: 0.4836\n",
      "Epoch 43/100\n",
      " - 2s - loss: 0.5012 - val_loss: 0.4828\n",
      "Epoch 44/100\n",
      " - 2s - loss: 0.5022 - val_loss: 0.4960\n",
      "Epoch 45/100\n",
      " - 2s - loss: 0.5016 - val_loss: 0.4966\n",
      "Epoch 46/100\n",
      " - 2s - loss: 0.5000 - val_loss: 0.4777\n",
      "Epoch 47/100\n",
      " - 2s - loss: 0.4995 - val_loss: 0.4840\n",
      "Epoch 48/100\n",
      " - 2s - loss: 0.4999 - val_loss: 0.4949\n",
      "Epoch 49/100\n",
      " - 2s - loss: 0.4985 - val_loss: 0.4780\n",
      "Epoch 50/100\n",
      " - 2s - loss: 0.4985 - val_loss: 0.4698\n",
      "Epoch 51/100\n",
      " - 2s - loss: 0.4976 - val_loss: 0.5125\n",
      "Epoch 52/100\n",
      " - 2s - loss: 0.4966 - val_loss: 0.4905\n",
      "Epoch 53/100\n",
      " - 2s - loss: 0.4950 - val_loss: 0.4908\n",
      "Epoch 54/100\n",
      " - 2s - loss: 0.4942 - val_loss: 0.5149\n",
      "Epoch 55/100\n",
      " - 2s - loss: 0.4920 - val_loss: 0.4537\n",
      "Epoch 56/100\n",
      " - 2s - loss: 0.4920 - val_loss: 0.4709\n",
      "Epoch 57/100\n",
      " - 2s - loss: 0.4899 - val_loss: 0.4614\n",
      "Epoch 58/100\n",
      " - 2s - loss: 0.4870 - val_loss: 0.4942\n",
      "Epoch 59/100\n",
      " - 2s - loss: 0.4868 - val_loss: 0.4743\n",
      "Epoch 60/100\n",
      " - 2s - loss: 0.4816 - val_loss: 0.4469\n",
      "Epoch 61/100\n",
      " - 2s - loss: 0.4836 - val_loss: 0.4597\n",
      "Epoch 62/100\n",
      " - 2s - loss: 0.4813 - val_loss: 0.4539\n",
      "Epoch 63/100\n",
      " - 2s - loss: 0.4791 - val_loss: 0.4593\n",
      "Epoch 64/100\n",
      " - 2s - loss: 0.4737 - val_loss: 0.4615\n",
      "Epoch 65/100\n",
      " - 2s - loss: 0.4703 - val_loss: 0.4246\n",
      "Epoch 66/100\n",
      " - 2s - loss: 0.4640 - val_loss: 0.4144\n",
      "Epoch 67/100\n",
      " - 2s - loss: 0.4622 - val_loss: 0.4191\n",
      "Epoch 68/100\n",
      " - 2s - loss: 0.4588 - val_loss: 0.4905\n",
      "Epoch 69/100\n",
      " - 2s - loss: 0.4522 - val_loss: 0.4994\n",
      "Epoch 70/100\n",
      " - 2s - loss: 0.4465 - val_loss: 0.4205\n",
      "Epoch 71/100\n",
      " - 3s - loss: 0.4402 - val_loss: 0.4176\n",
      "Epoch 72/100\n",
      " - 3s - loss: 0.4386 - val_loss: 0.3850\n",
      "Epoch 73/100\n",
      " - 2s - loss: 0.4359 - val_loss: 0.3854\n",
      "Epoch 74/100\n",
      " - 2s - loss: 0.4273 - val_loss: 0.4385\n",
      "Epoch 75/100\n",
      " - 2s - loss: 0.4201 - val_loss: 0.3878\n",
      "Epoch 76/100\n",
      " - 2s - loss: 0.4136 - val_loss: 0.4266\n",
      "Epoch 77/100\n",
      " - 2s - loss: 0.4032 - val_loss: 0.4455\n",
      "Epoch 78/100\n",
      " - 2s - loss: 0.3964 - val_loss: 0.3858\n",
      "Epoch 79/100\n",
      " - 2s - loss: 0.3845 - val_loss: 0.3791\n",
      "Epoch 80/100\n",
      " - 2s - loss: 0.3767 - val_loss: 0.3175\n",
      "Epoch 81/100\n",
      " - 2s - loss: 0.3681 - val_loss: 0.3652\n",
      "Epoch 82/100\n",
      " - 2s - loss: 0.3629 - val_loss: 0.3040\n",
      "Epoch 83/100\n",
      " - 2s - loss: 0.3486 - val_loss: 0.3505\n",
      "Epoch 84/100\n",
      " - 2s - loss: 0.3378 - val_loss: 0.3217\n",
      "Epoch 85/100\n",
      " - 2s - loss: 0.3325 - val_loss: 0.3392\n",
      "Epoch 86/100\n",
      " - 2s - loss: 0.3162 - val_loss: 0.2621\n",
      "Epoch 87/100\n",
      " - 2s - loss: 0.3109 - val_loss: 0.2752\n",
      "Epoch 88/100\n",
      " - 2s - loss: 0.2963 - val_loss: 0.2451\n",
      "Epoch 89/100\n",
      " - 2s - loss: 0.2832 - val_loss: 0.4216\n",
      "Epoch 90/100\n",
      " - 2s - loss: 0.2672 - val_loss: 0.3717\n",
      "Epoch 91/100\n",
      " - 2s - loss: 0.2636 - val_loss: 0.3115\n",
      "Epoch 92/100\n",
      " - 2s - loss: 0.2495 - val_loss: 0.2189\n",
      "Epoch 93/100\n",
      " - 2s - loss: 0.2473 - val_loss: 0.2586\n",
      "Epoch 94/100\n",
      " - 2s - loss: 0.2232 - val_loss: 0.2397\n",
      "Epoch 95/100\n",
      " - 2s - loss: 0.2106 - val_loss: 0.1895\n",
      "Epoch 96/100\n",
      " - 2s - loss: 0.2088 - val_loss: 0.4026\n",
      "Epoch 97/100\n",
      " - 2s - loss: 0.1981 - val_loss: 0.1774\n",
      "Epoch 98/100\n",
      " - 2s - loss: 0.1874 - val_loss: 0.1821\n",
      "Epoch 99/100\n",
      " - 2s - loss: 0.1692 - val_loss: 0.2706\n",
      "Epoch 100/100\n",
      " - 2s - loss: 0.1681 - val_loss: 0.1531\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.5162 - val_loss: 0.4731\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.5153 - val_loss: 0.4731\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.5140 - val_loss: 0.4793\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.5140 - val_loss: 0.4808\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.5141 - val_loss: 0.4832\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.5138 - val_loss: 0.4800\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.5135 - val_loss: 0.4836\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.5132 - val_loss: 0.4802\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.5132 - val_loss: 0.4904\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.5131 - val_loss: 0.4764\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.5131 - val_loss: 0.4909\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.5130 - val_loss: 0.4909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      " - 1s - loss: 0.5127 - val_loss: 0.4858\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.5130 - val_loss: 0.4864\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.5129 - val_loss: 0.4875\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.5126 - val_loss: 0.4807\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.5128 - val_loss: 0.4848\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.5123 - val_loss: 0.4880\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.5124 - val_loss: 0.4850\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.5125 - val_loss: 0.4808\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.5125 - val_loss: 0.4887\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.5121 - val_loss: 0.4866\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.5122 - val_loss: 0.4858\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.5120 - val_loss: 0.4849\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.5117 - val_loss: 0.4983\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.5120 - val_loss: 0.4942\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.5112 - val_loss: 0.4831\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.5119 - val_loss: 0.4902\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.5107 - val_loss: 0.4836\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.5114 - val_loss: 0.4886\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.5110 - val_loss: 0.4854\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.5108 - val_loss: 0.4871\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.5102 - val_loss: 0.4862\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.5101 - val_loss: 0.4869\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.5095 - val_loss: 0.4862\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.5082 - val_loss: 0.4832\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.5089 - val_loss: 0.4932\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.5079 - val_loss: 0.4880\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.5075 - val_loss: 0.4853\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.5064 - val_loss: 0.4807\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.5068 - val_loss: 0.4983\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.5059 - val_loss: 0.4895\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.5052 - val_loss: 0.4871\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.5047 - val_loss: 0.4978\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.5039 - val_loss: 0.4859\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.5053 - val_loss: 0.4879\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.5032 - val_loss: 0.4833\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.5027 - val_loss: 0.4858\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.5011 - val_loss: 0.5038\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.5005 - val_loss: 0.4885\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.5010 - val_loss: 0.4846\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.5012 - val_loss: 0.4868\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.5004 - val_loss: 0.4959\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.4994 - val_loss: 0.4821\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.4985 - val_loss: 0.4860\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.4986 - val_loss: 0.4879\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.4986 - val_loss: 0.4820\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.4974 - val_loss: 0.4823\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.4974 - val_loss: 0.4904\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.4979 - val_loss: 0.4870\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.4959 - val_loss: 0.4776\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.4955 - val_loss: 0.4906\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.4941 - val_loss: 0.4641\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.4935 - val_loss: 0.4801\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.4937 - val_loss: 0.4975\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.4928 - val_loss: 0.5046\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.4907 - val_loss: 0.4718\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.4902 - val_loss: 0.4867\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.4910 - val_loss: 0.4990\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.4910 - val_loss: 0.4694\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.4877 - val_loss: 0.4907\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.4867 - val_loss: 0.4570\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.4856 - val_loss: 0.4715\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.4874 - val_loss: 0.4650\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.4831 - val_loss: 0.4461\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.4826 - val_loss: 0.4952\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.4817 - val_loss: 0.4693\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.4806 - val_loss: 0.4685\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.4825 - val_loss: 0.5076\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.4768 - val_loss: 0.4651\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.4743 - val_loss: 0.4572\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.4749 - val_loss: 0.4518\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.4729 - val_loss: 0.4500\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.4703 - val_loss: 0.5046\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.4682 - val_loss: 0.4766\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.4703 - val_loss: 0.4564\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.4658 - val_loss: 0.4555\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.4616 - val_loss: 0.4651\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.4602 - val_loss: 0.4038\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.4614 - val_loss: 0.4717\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.4545 - val_loss: 0.4562\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.4518 - val_loss: 0.4848\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.4481 - val_loss: 0.4408\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.4491 - val_loss: 0.4281\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.4446 - val_loss: 0.4098\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.4402 - val_loss: 0.4252\n",
      "Epoch 97/100\n",
      " - 1s - loss: 0.4383 - val_loss: 0.4021\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.4354 - val_loss: 0.4443\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.4344 - val_loss: 0.4399\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.4289 - val_loss: 0.4940\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.5167 - val_loss: 0.4724\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.5151 - val_loss: 0.4743\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.5151 - val_loss: 0.4759\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.5146 - val_loss: 0.4802\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4793\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4790\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4806\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5138 - val_loss: 0.4855\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.5135 - val_loss: 0.4845\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5135 - val_loss: 0.4882\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.5133 - val_loss: 0.4833\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4869\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5132 - val_loss: 0.4888\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4865\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4862\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.5130 - val_loss: 0.4859\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4832\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.5126 - val_loss: 0.4844\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4877\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.5126 - val_loss: 0.4848\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.5126 - val_loss: 0.4860\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4846\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4843\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.5121 - val_loss: 0.4867\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4892\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4862\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.5122 - val_loss: 0.4880\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.5120 - val_loss: 0.4874\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.5121 - val_loss: 0.4851\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.5114 - val_loss: 0.4945\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.5122 - val_loss: 0.4854\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.5119 - val_loss: 0.4838\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.5112 - val_loss: 0.4815\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.5118 - val_loss: 0.4881\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.5113 - val_loss: 0.4902\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.5116 - val_loss: 0.4914\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.5117 - val_loss: 0.4857\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.5112 - val_loss: 0.4871\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.5109 - val_loss: 0.4859\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.5109 - val_loss: 0.4893\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.5107 - val_loss: 0.4880\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.5110 - val_loss: 0.4905\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.5105 - val_loss: 0.4912\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.5106 - val_loss: 0.4910\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.5100 - val_loss: 0.4866\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.5097 - val_loss: 0.4910\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.5096 - val_loss: 0.4857\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.5090 - val_loss: 0.4948\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.5092 - val_loss: 0.4917\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.5088 - val_loss: 0.4949\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.5089 - val_loss: 0.4909\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.5084 - val_loss: 0.4947\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.5080 - val_loss: 0.4897\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.5089 - val_loss: 0.4977\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.5076 - val_loss: 0.4950\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.5076 - val_loss: 0.4881\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.5071 - val_loss: 0.4947\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.5083 - val_loss: 0.4882\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.5074 - val_loss: 0.4908\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.5070 - val_loss: 0.4890\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.5069 - val_loss: 0.4902\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.5064 - val_loss: 0.4940\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.5058 - val_loss: 0.4972\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.5067 - val_loss: 0.4968\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.5067 - val_loss: 0.4938\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.5054 - val_loss: 0.4902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      " - 0s - loss: 0.5054 - val_loss: 0.4866\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.5059 - val_loss: 0.4888\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.5068 - val_loss: 0.4931\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.5053 - val_loss: 0.4943\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.5052 - val_loss: 0.4933\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.5046 - val_loss: 0.4875\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.5057 - val_loss: 0.5055\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.5042 - val_loss: 0.4984\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.5049 - val_loss: 0.4892\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.5036 - val_loss: 0.4883\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.5036 - val_loss: 0.5055\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.5039 - val_loss: 0.4938\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.5031 - val_loss: 0.4938\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.5027 - val_loss: 0.4999\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.5024 - val_loss: 0.4798\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.5030 - val_loss: 0.4919\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.5026 - val_loss: 0.4922\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.5012 - val_loss: 0.5001\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.5015 - val_loss: 0.4931\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.5009 - val_loss: 0.4953\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.5019 - val_loss: 0.4900\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.4997 - val_loss: 0.4918\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.5000 - val_loss: 0.4908\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.4997 - val_loss: 0.4743\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.5005 - val_loss: 0.4896\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.5002 - val_loss: 0.4904\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.4977 - val_loss: 0.4780\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.4988 - val_loss: 0.4924\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.4968 - val_loss: 0.4749\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.4975 - val_loss: 0.4934\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.4986 - val_loss: 0.5002\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.4964 - val_loss: 0.4909\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.4961 - val_loss: 0.4914\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.4950 - val_loss: 0.4820\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.5361 - val_loss: 0.4759\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.5153 - val_loss: 0.4715\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.5147 - val_loss: 0.4708\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.5140 - val_loss: 0.4742\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.5136 - val_loss: 0.4809\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.5133 - val_loss: 0.4751\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4786\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5132 - val_loss: 0.4827\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4827\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4786\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.5129 - val_loss: 0.4827\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4794\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4828\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.5128 - val_loss: 0.4814\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.5128 - val_loss: 0.4846\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.5126 - val_loss: 0.4861\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.5126 - val_loss: 0.4846\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.5123 - val_loss: 0.4831\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.5122 - val_loss: 0.4830\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.5121 - val_loss: 0.4843\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.5122 - val_loss: 0.4829\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.5121 - val_loss: 0.4854\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.5119 - val_loss: 0.4806\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.5118 - val_loss: 0.4880\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.5121 - val_loss: 0.4869\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.5117 - val_loss: 0.4850\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.5117 - val_loss: 0.4900\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.5125 - val_loss: 0.4864\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.5116 - val_loss: 0.4844\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.5117 - val_loss: 0.4868\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.5115 - val_loss: 0.4857\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.5115 - val_loss: 0.4835\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.5113 - val_loss: 0.4863\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.5116 - val_loss: 0.4842\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.5115 - val_loss: 0.4821\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.5112 - val_loss: 0.4828\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.5119 - val_loss: 0.4816\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.5109 - val_loss: 0.4829\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.5110 - val_loss: 0.4845\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.5108 - val_loss: 0.4874\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.5106 - val_loss: 0.4867\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.5102 - val_loss: 0.4923\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.5103 - val_loss: 0.4850\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.5100 - val_loss: 0.4868\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.5102 - val_loss: 0.4883\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.5098 - val_loss: 0.4858\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.5095 - val_loss: 0.4856\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.5095 - val_loss: 0.4836\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.5094 - val_loss: 0.4830\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.5090 - val_loss: 0.4846\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.5095 - val_loss: 0.4857\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.5091 - val_loss: 0.4948\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.5084 - val_loss: 0.4844\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.5081 - val_loss: 0.4897\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.5078 - val_loss: 0.4875\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.5078 - val_loss: 0.4896\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.5072 - val_loss: 0.4902\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.5072 - val_loss: 0.4847\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.5063 - val_loss: 0.4853\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.5074 - val_loss: 0.4879\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.5070 - val_loss: 0.4884\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.5055 - val_loss: 0.4891\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.5059 - val_loss: 0.4938\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.5055 - val_loss: 0.4937\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.5059 - val_loss: 0.4856\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.5055 - val_loss: 0.4968\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.5059 - val_loss: 0.5055\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.5061 - val_loss: 0.4944\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.5048 - val_loss: 0.4925\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.5036 - val_loss: 0.4849\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.5032 - val_loss: 0.4883\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.5044 - val_loss: 0.4983\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.5037 - val_loss: 0.4894\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.5019 - val_loss: 0.4853\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.5015 - val_loss: 0.4914\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.5027 - val_loss: 0.4847\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.5021 - val_loss: 0.4895\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.5022 - val_loss: 0.4892\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.5009 - val_loss: 0.4881\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.5005 - val_loss: 0.4961\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.5006 - val_loss: 0.4917\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.5012 - val_loss: 0.4948\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.5002 - val_loss: 0.4859\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.5002 - val_loss: 0.4884\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.4988 - val_loss: 0.4844\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.5045 - val_loss: 0.4848\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.5018 - val_loss: 0.4974\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.4987 - val_loss: 0.4813\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.4985 - val_loss: 0.4899\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.4978 - val_loss: 0.4942\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.4977 - val_loss: 0.5015\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.4975 - val_loss: 0.4786\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.4994 - val_loss: 0.4925\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.4968 - val_loss: 0.4841\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.4982 - val_loss: 0.5002\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.4961 - val_loss: 0.4834\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.4949 - val_loss: 0.4801\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.4960 - val_loss: 0.4905\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.4954 - val_loss: 0.4757\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.4963 - val_loss: 0.4791\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.5203 - val_loss: 0.4710\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.5157 - val_loss: 0.4701\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.5153 - val_loss: 0.4735\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.5149 - val_loss: 0.4717\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.5144 - val_loss: 0.4755\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.5142 - val_loss: 0.4768\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.5138 - val_loss: 0.4769\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5137 - val_loss: 0.4794\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.5139 - val_loss: 0.4791\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5133 - val_loss: 0.4821\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.5133 - val_loss: 0.4809\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5136 - val_loss: 0.4803\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5132 - val_loss: 0.4816\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.5133 - val_loss: 0.4816\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.5132 - val_loss: 0.4854\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4826\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.5131 - val_loss: 0.4842\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4850\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4850\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.5128 - val_loss: 0.4832\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4863\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.5126 - val_loss: 0.4857\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4865\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.5127 - val_loss: 0.4832\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.5126 - val_loss: 0.4877\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.5122 - val_loss: 0.4857\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4853\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.5121 - val_loss: 0.4876\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.5124 - val_loss: 0.4870\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.5119 - val_loss: 0.4853\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.5120 - val_loss: 0.4885\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.5120 - val_loss: 0.4853\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.5120 - val_loss: 0.4841\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.5116 - val_loss: 0.4876\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.5119 - val_loss: 0.4878\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.5115 - val_loss: 0.4861\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.5117 - val_loss: 0.4857\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.5113 - val_loss: 0.4912\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.5121 - val_loss: 0.4852\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.5113 - val_loss: 0.4870\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.5115 - val_loss: 0.4878\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.5111 - val_loss: 0.4893\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.5114 - val_loss: 0.4916\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.5115 - val_loss: 0.4884\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.5107 - val_loss: 0.4874\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.5113 - val_loss: 0.4869\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.5108 - val_loss: 0.4892\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.5109 - val_loss: 0.4859\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.5105 - val_loss: 0.4908\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.5104 - val_loss: 0.4876\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.5106 - val_loss: 0.4879\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.5108 - val_loss: 0.4889\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.5110 - val_loss: 0.4967\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.5110 - val_loss: 0.4890\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.5099 - val_loss: 0.4875\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.5100 - val_loss: 0.4898\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.5099 - val_loss: 0.4909\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.5097 - val_loss: 0.4913\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.5094 - val_loss: 0.4933\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.5094 - val_loss: 0.4889\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.5091 - val_loss: 0.4871\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.5094 - val_loss: 0.4879\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.5092 - val_loss: 0.4861\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.5093 - val_loss: 0.4891\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.5088 - val_loss: 0.4889\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.5084 - val_loss: 0.4857\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.5092 - val_loss: 0.4891\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.5085 - val_loss: 0.4899\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.5083 - val_loss: 0.4922\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.5081 - val_loss: 0.4909\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.5082 - val_loss: 0.4876\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.5076 - val_loss: 0.4859\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.5090 - val_loss: 0.4860\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.5075 - val_loss: 0.4884\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.5071 - val_loss: 0.4886\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.5075 - val_loss: 0.4865\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.5075 - val_loss: 0.4912\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.5068 - val_loss: 0.4930\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.5065 - val_loss: 0.4918\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.5063 - val_loss: 0.4907\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.5062 - val_loss: 0.4867\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.5061 - val_loss: 0.4880\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.5059 - val_loss: 0.4882\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.5058 - val_loss: 0.4975\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.5057 - val_loss: 0.4915\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.5053 - val_loss: 0.4985\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.5054 - val_loss: 0.4905\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.5051 - val_loss: 0.4896\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.5050 - val_loss: 0.4916\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.5054 - val_loss: 0.4902\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.5047 - val_loss: 0.4871\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.5058 - val_loss: 0.4974\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.5050 - val_loss: 0.4904\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.5051 - val_loss: 0.4939\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.5047 - val_loss: 0.4833\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.5039 - val_loss: 0.4926\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.5029 - val_loss: 0.5031\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.5043 - val_loss: 0.4892\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.5037 - val_loss: 0.4900\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.5059 - val_loss: 0.5009\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/200\n",
      " - 4s - loss: 0.5163 - val_loss: 0.4769\n",
      "Epoch 2/200\n",
      " - 2s - loss: 0.5146 - val_loss: 0.4857\n",
      "Epoch 3/200\n",
      " - 2s - loss: 0.5138 - val_loss: 0.4802\n",
      "Epoch 4/200\n",
      " - 2s - loss: 0.5136 - val_loss: 0.4845\n",
      "Epoch 5/200\n",
      " - 2s - loss: 0.5134 - val_loss: 0.4726\n",
      "Epoch 6/200\n",
      " - 2s - loss: 0.5136 - val_loss: 0.4760\n",
      "Epoch 7/200\n",
      " - 2s - loss: 0.5129 - val_loss: 0.4942\n",
      "Epoch 8/200\n",
      " - 2s - loss: 0.5131 - val_loss: 0.4844\n",
      "Epoch 9/200\n",
      " - 2s - loss: 0.5122 - val_loss: 0.4811\n",
      "Epoch 10/200\n",
      " - 2s - loss: 0.5126 - val_loss: 0.4808\n",
      "Epoch 11/200\n",
      " - 2s - loss: 0.5125 - val_loss: 0.4922\n",
      "Epoch 12/200\n",
      " - 2s - loss: 0.5125 - val_loss: 0.4897\n",
      "Epoch 13/200\n",
      " - 2s - loss: 0.5117 - val_loss: 0.4794\n",
      "Epoch 14/200\n",
      " - 2s - loss: 0.5119 - val_loss: 0.4854\n",
      "Epoch 15/200\n",
      " - 2s - loss: 0.5117 - val_loss: 0.4883\n",
      "Epoch 16/200\n",
      " - 2s - loss: 0.5113 - val_loss: 0.4863\n",
      "Epoch 17/200\n",
      " - 2s - loss: 0.5113 - val_loss: 0.4889\n",
      "Epoch 18/200\n",
      " - 2s - loss: 0.5106 - val_loss: 0.4865\n",
      "Epoch 19/200\n",
      " - 2s - loss: 0.5099 - val_loss: 0.4834\n",
      "Epoch 20/200\n",
      " - 2s - loss: 0.5097 - val_loss: 0.4839\n",
      "Epoch 21/200\n",
      " - 2s - loss: 0.5090 - val_loss: 0.4874\n",
      "Epoch 22/200\n",
      " - 2s - loss: 0.5087 - val_loss: 0.4874\n",
      "Epoch 23/200\n",
      " - 2s - loss: 0.5082 - val_loss: 0.4853\n",
      "Epoch 24/200\n",
      " - 2s - loss: 0.5075 - val_loss: 0.5000\n",
      "Epoch 25/200\n",
      " - 2s - loss: 0.5060 - val_loss: 0.4788\n",
      "Epoch 26/200\n",
      " - 2s - loss: 0.5069 - val_loss: 0.4958\n",
      "Epoch 27/200\n",
      " - 2s - loss: 0.5057 - val_loss: 0.4960\n",
      "Epoch 28/200\n",
      " - 2s - loss: 0.5062 - val_loss: 0.4945\n",
      "Epoch 29/200\n",
      " - 2s - loss: 0.5057 - val_loss: 0.4895\n",
      "Epoch 30/200\n",
      " - 2s - loss: 0.5038 - val_loss: 0.4984\n",
      "Epoch 31/200\n",
      " - 2s - loss: 0.5039 - val_loss: 0.4938\n",
      "Epoch 32/200\n",
      " - 2s - loss: 0.5048 - val_loss: 0.4946\n",
      "Epoch 33/200\n",
      " - 2s - loss: 0.5034 - val_loss: 0.4851\n",
      "Epoch 34/200\n",
      " - 2s - loss: 0.5032 - val_loss: 0.5034\n",
      "Epoch 35/200\n",
      " - 2s - loss: 0.5020 - val_loss: 0.4906\n",
      "Epoch 36/200\n",
      " - 2s - loss: 0.5022 - val_loss: 0.4891\n",
      "Epoch 37/200\n",
      " - 2s - loss: 0.5012 - val_loss: 0.4801\n",
      "Epoch 38/200\n",
      " - 2s - loss: 0.5008 - val_loss: 0.4939\n",
      "Epoch 39/200\n",
      " - 2s - loss: 0.5001 - val_loss: 0.4918\n",
      "Epoch 40/200\n",
      " - 2s - loss: 0.4992 - val_loss: 0.4923\n",
      "Epoch 41/200\n",
      " - 2s - loss: 0.4985 - val_loss: 0.4772\n",
      "Epoch 42/200\n",
      " - 2s - loss: 0.4979 - val_loss: 0.5013\n",
      "Epoch 43/200\n",
      " - 2s - loss: 0.4971 - val_loss: 0.4898\n",
      "Epoch 44/200\n",
      " - 2s - loss: 0.4963 - val_loss: 0.4851\n",
      "Epoch 45/200\n",
      " - 2s - loss: 0.4944 - val_loss: 0.4795\n",
      "Epoch 46/200\n",
      " - 2s - loss: 0.4927 - val_loss: 0.4714\n",
      "Epoch 47/200\n",
      " - 2s - loss: 0.4934 - val_loss: 0.4880\n",
      "Epoch 48/200\n",
      " - 2s - loss: 0.4913 - val_loss: 0.4764\n",
      "Epoch 49/200\n",
      " - 2s - loss: 0.4915 - val_loss: 0.4816\n",
      "Epoch 50/200\n",
      " - 2s - loss: 0.4881 - val_loss: 0.4638\n",
      "Epoch 51/200\n",
      " - 2s - loss: 0.4886 - val_loss: 0.4801\n",
      "Epoch 52/200\n",
      " - 2s - loss: 0.4850 - val_loss: 0.4824\n",
      "Epoch 53/200\n",
      " - 2s - loss: 0.4857 - val_loss: 0.4596\n",
      "Epoch 54/200\n",
      " - 2s - loss: 0.4839 - val_loss: 0.4764\n",
      "Epoch 55/200\n",
      " - 2s - loss: 0.4852 - val_loss: 0.4825\n",
      "Epoch 56/200\n",
      " - 2s - loss: 0.4809 - val_loss: 0.4737\n",
      "Epoch 57/200\n",
      " - 2s - loss: 0.4773 - val_loss: 0.4632\n",
      "Epoch 58/200\n",
      " - 2s - loss: 0.4760 - val_loss: 0.4603\n",
      "Epoch 59/200\n",
      " - 2s - loss: 0.4731 - val_loss: 0.4633\n",
      "Epoch 60/200\n",
      " - 2s - loss: 0.4724 - val_loss: 0.4928\n",
      "Epoch 61/200\n",
      " - 2s - loss: 0.4713 - val_loss: 0.4578\n",
      "Epoch 62/200\n",
      " - 2s - loss: 0.4697 - val_loss: 0.4909\n",
      "Epoch 63/200\n",
      " - 2s - loss: 0.4673 - val_loss: 0.4678\n",
      "Epoch 64/200\n",
      " - 2s - loss: 0.4604 - val_loss: 0.4351\n",
      "Epoch 65/200\n",
      " - 2s - loss: 0.4588 - val_loss: 0.4763\n",
      "Epoch 66/200\n",
      " - 2s - loss: 0.4594 - val_loss: 0.4605\n",
      "Epoch 67/200\n",
      " - 2s - loss: 0.4502 - val_loss: 0.4458\n",
      "Epoch 68/200\n",
      " - 2s - loss: 0.4457 - val_loss: 0.4340\n",
      "Epoch 69/200\n",
      " - 2s - loss: 0.4462 - val_loss: 0.4083\n",
      "Epoch 70/200\n",
      " - 2s - loss: 0.4422 - val_loss: 0.4261\n",
      "Epoch 71/200\n",
      " - 2s - loss: 0.4403 - val_loss: 0.4637\n",
      "Epoch 72/200\n",
      " - 2s - loss: 0.4315 - val_loss: 0.4347\n",
      "Epoch 73/200\n",
      " - 2s - loss: 0.4255 - val_loss: 0.3989\n",
      "Epoch 74/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4200 - val_loss: 0.4081\n",
      "Epoch 75/200\n",
      " - 2s - loss: 0.4126 - val_loss: 0.3972\n",
      "Epoch 76/200\n",
      " - 2s - loss: 0.4115 - val_loss: 0.3903\n",
      "Epoch 77/200\n",
      " - 2s - loss: 0.3993 - val_loss: 0.3807\n",
      "Epoch 78/200\n",
      " - 2s - loss: 0.3925 - val_loss: 0.3590\n",
      "Epoch 79/200\n",
      " - 2s - loss: 0.3806 - val_loss: 0.3695\n",
      "Epoch 80/200\n",
      " - 2s - loss: 0.3809 - val_loss: 0.3369\n",
      "Epoch 81/200\n",
      " - 2s - loss: 0.3712 - val_loss: 0.3375\n",
      "Epoch 82/200\n",
      " - 2s - loss: 0.3707 - val_loss: 0.3424\n",
      "Epoch 83/200\n",
      " - 2s - loss: 0.3503 - val_loss: 0.4166\n",
      "Epoch 84/200\n",
      " - 2s - loss: 0.3472 - val_loss: 0.3886\n",
      "Epoch 85/200\n",
      " - 2s - loss: 0.3363 - val_loss: 0.3342\n",
      "Epoch 86/200\n",
      " - 3s - loss: 0.3281 - val_loss: 0.2889\n",
      "Epoch 87/200\n",
      " - 2s - loss: 0.3161 - val_loss: 0.3257\n",
      "Epoch 88/200\n",
      " - 2s - loss: 0.3070 - val_loss: 0.3162\n",
      "Epoch 89/200\n",
      " - 2s - loss: 0.2991 - val_loss: 0.3845\n",
      "Epoch 90/200\n",
      " - 2s - loss: 0.2873 - val_loss: 0.2683\n",
      "Epoch 91/200\n",
      " - 2s - loss: 0.2762 - val_loss: 0.2727\n",
      "Epoch 92/200\n",
      " - 2s - loss: 0.2752 - val_loss: 0.3149\n",
      "Epoch 93/200\n",
      " - 3s - loss: 0.2619 - val_loss: 0.2126\n",
      "Epoch 94/200\n",
      " - 2s - loss: 0.2397 - val_loss: 0.2458\n",
      "Epoch 95/200\n",
      " - 2s - loss: 0.2266 - val_loss: 0.2310\n",
      "Epoch 96/200\n",
      " - 2s - loss: 0.2270 - val_loss: 0.2960\n",
      "Epoch 97/200\n",
      " - 2s - loss: 0.2064 - val_loss: 0.1876\n",
      "Epoch 98/200\n",
      " - 2s - loss: 0.2046 - val_loss: 0.1783\n",
      "Epoch 99/200\n",
      " - 2s - loss: 0.2042 - val_loss: 0.1872\n",
      "Epoch 100/200\n",
      " - 2s - loss: 0.2011 - val_loss: 0.1784\n",
      "Epoch 101/200\n",
      " - 2s - loss: 0.1839 - val_loss: 0.1890\n",
      "Epoch 102/200\n",
      " - 2s - loss: 0.1807 - val_loss: 0.2179\n",
      "Epoch 103/200\n",
      " - 2s - loss: 0.1645 - val_loss: 0.1469\n",
      "Epoch 104/200\n",
      " - 2s - loss: 0.1504 - val_loss: 0.1371\n",
      "Epoch 105/200\n",
      " - 2s - loss: 0.1495 - val_loss: 0.1936\n",
      "Epoch 106/200\n",
      " - 2s - loss: 0.1429 - val_loss: 0.1782\n",
      "Epoch 107/200\n",
      " - 2s - loss: 0.1446 - val_loss: 0.1129\n",
      "Epoch 108/200\n",
      " - 2s - loss: 0.1338 - val_loss: 0.1164\n",
      "Epoch 109/200\n",
      " - 2s - loss: 0.1214 - val_loss: 0.1131\n",
      "Epoch 110/200\n",
      " - 2s - loss: 0.1163 - val_loss: 0.0976\n",
      "Epoch 111/200\n",
      " - 2s - loss: 0.1124 - val_loss: 0.1934\n",
      "Epoch 112/200\n",
      " - 2s - loss: 0.1088 - val_loss: 0.0880\n",
      "Epoch 113/200\n",
      " - 2s - loss: 0.1123 - val_loss: 0.1142\n",
      "Epoch 114/200\n",
      " - 2s - loss: 0.1035 - val_loss: 0.0823\n",
      "Epoch 115/200\n",
      " - 2s - loss: 0.1015 - val_loss: 0.1085\n",
      "Epoch 116/200\n",
      " - 2s - loss: 0.1042 - val_loss: 0.2572\n",
      "Epoch 117/200\n",
      " - 2s - loss: 0.1064 - val_loss: 0.0916\n",
      "Epoch 118/200\n",
      " - 2s - loss: 0.1055 - val_loss: 0.0916\n",
      "Epoch 119/200\n",
      " - 2s - loss: 0.0910 - val_loss: 0.0777\n",
      "Epoch 120/200\n",
      " - 2s - loss: 0.0877 - val_loss: 0.1128\n",
      "Epoch 121/200\n",
      " - 2s - loss: 0.0869 - val_loss: 0.0642\n",
      "Epoch 122/200\n",
      " - 2s - loss: 0.0833 - val_loss: 0.1343\n",
      "Epoch 123/200\n",
      " - 2s - loss: 0.1028 - val_loss: 0.1351\n",
      "Epoch 124/200\n",
      " - 2s - loss: 0.0734 - val_loss: 0.0576\n",
      "Epoch 125/200\n",
      " - 2s - loss: 0.0740 - val_loss: 0.0925\n",
      "Epoch 126/200\n",
      " - 2s - loss: 0.0766 - val_loss: 0.0599\n",
      "Epoch 127/200\n",
      " - 2s - loss: 0.0796 - val_loss: 0.0582\n",
      "Epoch 128/200\n",
      " - 2s - loss: 0.0965 - val_loss: 0.0657\n",
      "Epoch 129/200\n",
      " - 2s - loss: 0.0621 - val_loss: 0.0482\n",
      "Epoch 130/200\n",
      " - 2s - loss: 0.0593 - val_loss: 0.0519\n",
      "Epoch 131/200\n",
      " - 2s - loss: 0.0695 - val_loss: 0.0579\n",
      "Epoch 132/200\n",
      " - 2s - loss: 0.0704 - val_loss: 0.0771\n",
      "Epoch 133/200\n",
      " - 2s - loss: 0.0695 - val_loss: 0.0516\n",
      "Epoch 134/200\n",
      " - 2s - loss: 0.0542 - val_loss: 0.1182\n",
      "Epoch 135/200\n",
      " - 2s - loss: 0.0615 - val_loss: 0.1420\n",
      "Epoch 136/200\n",
      " - 2s - loss: 0.0683 - val_loss: 0.0518\n",
      "Epoch 137/200\n",
      " - 2s - loss: 0.0535 - val_loss: 0.0374\n",
      "Epoch 138/200\n",
      " - 2s - loss: 0.0632 - val_loss: 0.0502\n",
      "Epoch 139/200\n",
      " - 2s - loss: 0.0483 - val_loss: 0.0731\n",
      "Epoch 140/200\n",
      " - 2s - loss: 0.0544 - val_loss: 0.0400\n",
      "Epoch 141/200\n",
      " - 2s - loss: 0.0633 - val_loss: 0.0572\n",
      "Epoch 142/200\n",
      " - 2s - loss: 0.0527 - val_loss: 0.1561\n",
      "Epoch 143/200\n",
      " - 2s - loss: 0.0497 - val_loss: 0.0305\n",
      "Epoch 144/200\n",
      " - 2s - loss: 0.0475 - val_loss: 0.0748\n",
      "Epoch 145/200\n",
      " - 2s - loss: 0.0508 - val_loss: 0.0261\n",
      "Epoch 146/200\n",
      " - 2s - loss: 0.0468 - val_loss: 0.0652\n",
      "Epoch 147/200\n",
      " - 2s - loss: 0.0711 - val_loss: 0.0777\n",
      "Epoch 148/200\n",
      " - 2s - loss: 0.0445 - val_loss: 0.0593\n",
      "Epoch 149/200\n",
      " - 2s - loss: 0.0444 - val_loss: 0.0369\n",
      "Epoch 150/200\n",
      " - 2s - loss: 0.0464 - val_loss: 0.0390\n",
      "Epoch 151/200\n",
      " - 2s - loss: 0.0540 - val_loss: 0.0452\n",
      "Epoch 152/200\n",
      " - 2s - loss: 0.0432 - val_loss: 0.0268\n",
      "Epoch 153/200\n",
      " - 2s - loss: 0.0545 - val_loss: 0.0508\n",
      "Epoch 154/200\n",
      " - 2s - loss: 0.0518 - val_loss: 0.0340\n",
      "Epoch 155/200\n",
      " - 2s - loss: 0.0388 - val_loss: 0.0323\n",
      "Epoch 156/200\n",
      " - 2s - loss: 0.0515 - val_loss: 0.0271\n",
      "Epoch 157/200\n",
      " - 2s - loss: 0.0397 - val_loss: 0.0211\n",
      "Epoch 158/200\n",
      " - 2s - loss: 0.0399 - val_loss: 0.0230\n",
      "Epoch 159/200\n",
      " - 2s - loss: 0.0575 - val_loss: 0.0492\n",
      "Epoch 160/200\n",
      " - 2s - loss: 0.0402 - val_loss: 0.0230\n",
      "Epoch 161/200\n",
      " - 2s - loss: 0.0397 - val_loss: 0.0231\n",
      "Epoch 162/200\n",
      " - 2s - loss: 0.0300 - val_loss: 0.0271\n",
      "Epoch 163/200\n",
      " - 2s - loss: 0.0401 - val_loss: 0.0272\n",
      "Epoch 164/200\n",
      " - 2s - loss: 0.0388 - val_loss: 0.0194\n",
      "Epoch 165/200\n",
      " - 2s - loss: 0.0438 - val_loss: 0.0185\n",
      "Epoch 166/200\n",
      " - 2s - loss: 0.0362 - val_loss: 0.0319\n",
      "Epoch 167/200\n",
      " - 2s - loss: 0.0428 - val_loss: 0.0206\n",
      "Epoch 168/200\n",
      " - 2s - loss: 0.0458 - val_loss: 0.0301\n",
      "Epoch 169/200\n",
      " - 2s - loss: 0.0319 - val_loss: 0.0497\n",
      "Epoch 170/200\n",
      " - 2s - loss: 0.0387 - val_loss: 0.0244\n",
      "Epoch 171/200\n",
      " - 2s - loss: 0.0319 - val_loss: 0.0215\n",
      "Epoch 172/200\n",
      " - 2s - loss: 0.0491 - val_loss: 0.0330\n",
      "Epoch 173/200\n",
      " - 2s - loss: 0.0405 - val_loss: 0.0256\n",
      "Epoch 174/200\n",
      " - 2s - loss: 0.0333 - val_loss: 0.0234\n",
      "Epoch 175/200\n",
      " - 2s - loss: 0.0476 - val_loss: 0.0183\n",
      "Epoch 176/200\n",
      " - 2s - loss: 0.0396 - val_loss: 0.0272\n",
      "Epoch 177/200\n",
      " - 2s - loss: 0.0301 - val_loss: 0.0338\n",
      "Epoch 178/200\n",
      " - 2s - loss: 0.0402 - val_loss: 0.0243\n",
      "Epoch 179/200\n",
      " - 2s - loss: 0.0345 - val_loss: 0.0736\n",
      "Epoch 180/200\n",
      " - 2s - loss: 0.0350 - val_loss: 0.0798\n",
      "Epoch 181/200\n",
      " - 2s - loss: 0.0312 - val_loss: 0.0263\n",
      "Epoch 182/200\n",
      " - 2s - loss: 0.0398 - val_loss: 0.0202\n",
      "Epoch 183/200\n",
      " - 2s - loss: 0.0423 - val_loss: 0.0352\n",
      "Epoch 184/200\n",
      " - 2s - loss: 0.0239 - val_loss: 0.0143\n",
      "Epoch 185/200\n",
      " - 2s - loss: 0.0328 - val_loss: 0.0159\n",
      "Epoch 186/200\n",
      " - 2s - loss: 0.0244 - val_loss: 0.0814\n",
      "Epoch 187/200\n",
      " - 2s - loss: 0.0434 - val_loss: 0.0149\n",
      "Epoch 188/200\n",
      " - 2s - loss: 0.0272 - val_loss: 0.0175\n",
      "Epoch 189/200\n",
      " - 2s - loss: 0.0270 - val_loss: 0.0138\n",
      "Epoch 190/200\n",
      " - 2s - loss: 0.0270 - val_loss: 0.0236\n",
      "Epoch 191/200\n",
      " - 2s - loss: 0.0351 - val_loss: 0.0151\n",
      "Epoch 192/200\n",
      " - 2s - loss: 0.0240 - val_loss: 0.0358\n",
      "Epoch 193/200\n",
      " - 2s - loss: 0.0301 - val_loss: 0.0165\n",
      "Epoch 194/200\n",
      " - 2s - loss: 0.0252 - val_loss: 0.0365\n",
      "Epoch 195/200\n",
      " - 2s - loss: 0.0351 - val_loss: 0.0143\n",
      "Epoch 196/200\n",
      " - 2s - loss: 0.0312 - val_loss: 0.0198\n",
      "Epoch 197/200\n",
      " - 2s - loss: 0.0254 - val_loss: 0.0229\n",
      "Epoch 198/200\n",
      " - 2s - loss: 0.0275 - val_loss: 0.0429\n",
      "Epoch 199/200\n",
      " - 2s - loss: 0.0323 - val_loss: 0.0216\n",
      "Epoch 200/200\n",
      " - 2s - loss: 0.0245 - val_loss: 0.0274\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/200\n",
      " - 3s - loss: 0.5160 - val_loss: 0.4786\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-706dc895c23d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1105\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mdirect\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \"\"\"\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = {}\n",
    "for epochs in [20, 50, 100, 200]:\n",
    "    out[epochs] = {}\n",
    "    for batch_size in [10, 20, 50, 100, 200]:\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(16, input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "\n",
    "        model.add(Dense(y.shape[1], activation='linear'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.fit(X[:6000], y[:6000], epochs=epochs, batch_size=batch_size, verbose=2, validation_split=0.3)\n",
    "        out[epochs][batch_size] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1831e35748>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOEQrCY1Go9E4RCsJjUaj0TjkfwFNHOyUOIWzPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18331abb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.DataFrame(y)[:][0].plot(label='GT')\n",
    "pd.DataFrame(out[200][10].predict(X))[:][0].plot(label='Pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
