{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from dataloader import APPLIANCE_ORDER, get_train_test\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda_av = False\n",
    "if torch.cuda.is_available():\n",
    "    cuda_av = True\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, cell_type, hidden_size, num_layers, bidirectional):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        if cell_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(input_size=1, hidden_size=hidden_size,\n",
    "                              num_layers=num_layers, batch_first=True,\n",
    "                              bidirectional=bidirectional)\n",
    "        elif cell_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(input_size=1, hidden_size=hidden_size,\n",
    "                              num_layers=num_layers, batch_first=True,\n",
    "                              bidirectional=bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(input_size=1, hidden_size=hidden_size,\n",
    "                               num_layers=num_layers, batch_first=True,\n",
    "                               bidirectional=bidirectional)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size * self.num_directions, 1)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred, hidden = self.rnn(x, None)\n",
    "        pred = self.linear(pred).view(pred.data.shape[0], -1, 1)\n",
    "        pred = torch.min(pred, x)\n",
    "        return pred\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=7, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(20)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(20, 16, kernel_size=2, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv4 = nn.ConvTranspose2d(64, 16, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv5 = nn.ConvTranspose2d(16, 6, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(6)\n",
    "\n",
    "        self.conv6 = nn.ConvTranspose2d(6, 1, kernel_size=5, stride=1, padding=2) \n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        \n",
    "        e1 = self.conv1(input)\n",
    "        bn1 = self.bn1(self.act(e1))\n",
    "        e2 = self.bn2(self.conv2(bn1))        \n",
    "        e5 = self.bn5(self.conv5(e2))\n",
    "        e6 = self.conv6(e5)\n",
    "        return e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(train, valid, test):\n",
    "    out_train = [None for temp in range(len(ORDER))]\n",
    "    for a_num, appliance in enumerate(ORDER):\n",
    "        out_train[a_num] = Variable(\n",
    "            torch.Tensor(train[:, APPLIANCE_ORDER.index(appliance), :, :].reshape((train.shape[0], 1, -1, 24))))\n",
    "        if cuda_av:\n",
    "            out_train[a_num] = out_train[a_num].cuda()\n",
    "\n",
    "    out_valid = [None for temp in range(len(ORDER))]\n",
    "    for a_num, appliance in enumerate(ORDER):\n",
    "        out_valid[a_num] = Variable(\n",
    "            torch.Tensor(valid[:, APPLIANCE_ORDER.index(appliance), :, :].reshape((valid.shape[0], 1, -1, 24))))\n",
    "        if cuda_av:\n",
    "            out_valid[a_num] = out_valid[a_num].cuda()\n",
    "            \n",
    "    out_test = [None for temp in range(len(ORDER))]\n",
    "    for a_num, appliance in enumerate(ORDER):\n",
    "        out_test[a_num] = Variable(\n",
    "            torch.Tensor(test[:, APPLIANCE_ORDER.index(appliance), :, :].reshape((test.shape[0], 1, -1, 24))))\n",
    "        if cuda_av:\n",
    "            out_test[a_num] = out_test[a_num].cuda()\n",
    "\n",
    "    return out_train, out_valid, out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 3\n",
    "fold_num = 1\n",
    "num_folds = 5\n",
    "train, test = get_train_test(dataset, num_folds=num_folds, fold_num=fold_num)\n",
    "valid = train[int(0.8*len(train)):].copy()\n",
    "train = train[:int(0.8 * len(train))].copy()\n",
    "train_aggregate = train[:, 0, :, :].reshape(train.shape[0], 1, -1, 24)\n",
    "valid_aggregate = valid[:, 0, :, :].reshape(valid.shape[0], 1, -1, 24)\n",
    "test_aggregate = test[:, 0, :, :].reshape(test.shape[0], 1, -1, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ORDER = ['fridge', 'dr', 'hvac', 'dw', 'mw']\n",
    "out_train, out_valid, out_test = preprocess(train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Variable(torch.Tensor(train_aggregate), requires_grad=False)\n",
    "valid_inp = Variable(torch.Tensor(valid_aggregate), requires_grad=False)\n",
    "test_inp = Variable(torch.Tensor(test_aggregate), requires_grad=False)\n",
    "if cuda_av:\n",
    "    inp = inp.cuda()\n",
    "    valid_inp = valid_inp.cuda()\n",
    "    test_inp = test_inp.cuda()\n",
    "valid_out = torch.cat([out_valid[appliance_num] for appliance_num, appliance in enumerate(ORDER)])\n",
    "test_out = torch.cat([out_test[appliance_num] for appliance_num, appliance in enumerate(ORDER)])\n",
    "train_out = torch.cat([out_train[appliance_num] for appliance_num, appliance in enumerate(ORDER)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AppliancesRNNCNN(nn.Module):\n",
    "    def __init__(self, cell_type, hidden_size, num_layers, bidirectional, num_appliance):\n",
    "        super(AppliancesRNNCNN, self).__init__()\n",
    "        self.num_appliance = num_appliance\n",
    "        self.preds = {}\n",
    "        self.order = ORDER\n",
    "        for appliance in range(self.num_appliance):\n",
    "            if ORDER[appliance] in ['fridge']:\n",
    "                print(\"use RNN\")\n",
    "                if cuda_av:\n",
    "                    setattr(self, \"Appliance_\" + str(appliance), CustomRNN(cell_type, hidden_size,\n",
    "                                                                           num_layers, bidirectional).cuda())\n",
    "                else:\n",
    "                    setattr(self, \"Appliance_\" + str(appliance), CustomRNN(cell_type, hidden_size,\n",
    "                                                                           num_layers, bidirectional))\n",
    "            else:\n",
    "                print(\"use CNN\")\n",
    "                if cuda_av:\n",
    "                    setattr(self, \"Appliance_\" + str(appliance), CustomCNN().cuda())\n",
    "                else:\n",
    "                    setattr(self, \"Appliance_\" + str(appliance), CustomCNN())\n",
    "\n",
    "\n",
    "    def forward(self, *args):\n",
    "        agg_current = args[0]\n",
    "        flag = False\n",
    "        if np.random.random() > args[1]:\n",
    "            flag = True\n",
    "        else:\n",
    "            pass\n",
    "        for appliance in range(self.num_appliance):\n",
    "            agg_current = agg_current.contiguous()\n",
    "            if ORDER[appliance] in ['fridge']:\n",
    "                agg_current = agg_current.view(agg_current.shape[0], -1, 1)\n",
    "            else:\n",
    "                agg_current = agg_current.view(agg_current.shape[0], 1, -1, 24)\n",
    "            \n",
    "            self.preds[appliance] = getattr(self, \"Appliance_\" + str(appliance))(agg_current)\n",
    "            \n",
    "            agg_current = agg_current.view(agg_current.shape[0], 1, -1, 24)\n",
    "            self.preds[appliance] = self.preds[appliance].view(self.preds[appliance].shape[0], 1, -1, 24)\n",
    "            \n",
    "            \n",
    "            if flag:\n",
    "                agg_current = agg_current - self.preds[appliance]\n",
    "            else:\n",
    "                agg_current = agg_current - args[2 + appliance]\n",
    "\n",
    "        return torch.cat([self.preds[a] for a in range(self.num_appliance)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use RNN\n",
      "use CNN\n",
      "use CNN\n",
      "use CNN\n",
      "use CNN\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.L1Loss()\n",
    "model = AppliancesRNNCNN('LSTM', 20, 1, True, len(ORDER))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "p=0\n",
    "params = [inp, p]\n",
    "\n",
    "if cuda_av:\n",
    "    model = model.cuda()\n",
    "    loss_func = loss_func.cuda()\n",
    "    \n",
    "for a_num, appliance in enumerate(ORDER):\n",
    "    params.append(out_train[a_num])\n",
    "\n",
    "if cuda_av:\n",
    "    train_out = train_out.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 210.89698791503906 221.7128143310547\n",
      "1 210.66017150878906 221.4812469482422\n",
      "2 210.69065856933594 221.57400512695312\n",
      "3 209.8641357421875 220.7063751220703\n",
      "4 209.2834014892578 220.17694091796875\n",
      "5 208.50921630859375 219.4387969970703\n",
      "6 207.69821166992188 218.6646728515625\n",
      "7 206.85777282714844 217.8498077392578\n",
      "8 205.8075714111328 216.88803100585938\n",
      "9 204.57257080078125 215.64382934570312\n",
      "10 203.26336669921875 214.4214324951172\n",
      "11 201.80450439453125 213.13726806640625\n",
      "12 200.13754272460938 211.46995544433594\n",
      "13 198.32077026367188 209.8058319091797\n",
      "14 196.3573455810547 208.05860900878906\n",
      "15 194.19772338867188 205.9311065673828\n",
      "16 191.8430938720703 203.7175750732422\n",
      "17 189.3262939453125 201.48435974121094\n",
      "18 186.63369750976562 198.6735382080078\n",
      "19 183.72027587890625 196.0208282470703\n",
      "20 180.7397003173828 193.55601501464844\n",
      "21 177.4696502685547 189.91958618164062\n",
      "22 174.00746154785156 186.62120056152344\n",
      "23 170.31593322753906 183.6483154296875\n",
      "24 166.49398803710938 180.0713653564453\n",
      "25 162.56874084472656 175.7552032470703\n",
      "26 158.34695434570312 171.85142517089844\n",
      "27 154.10047912597656 168.5004425048828\n",
      "28 149.70814514160156 163.57579040527344\n",
      "29 145.2460479736328 158.96568298339844\n",
      "30 140.73736572265625 154.89315795898438\n",
      "31 136.2503204345703 150.2084503173828\n",
      "32 131.8922576904297 145.75180053710938\n",
      "33 127.63253021240234 140.76182556152344\n",
      "34 123.43770599365234 136.293701171875\n",
      "35 119.66263580322266 132.1427764892578\n",
      "36 116.10921478271484 128.27676391601562\n",
      "37 113.80931854248047 123.61775970458984\n",
      "38 109.83216857910156 118.86290740966797\n",
      "39 107.85934448242188 118.7762680053711\n",
      "40 106.10277557373047 114.07418060302734\n",
      "41 104.18810272216797 109.2889633178711\n",
      "42 102.08076477050781 107.2893295288086\n",
      "43 102.02935791015625 109.09688568115234\n",
      "44 100.20803833007812 104.0940170288086\n",
      "45 99.94857788085938 103.46761322021484\n",
      "46 99.58616638183594 104.51113891601562\n",
      "47 99.40580749511719 103.30892944335938\n",
      "48 98.90337371826172 101.37052154541016\n",
      "49 99.22848510742188 101.01531982421875\n",
      "50 99.01492309570312 101.46524047851562\n",
      "51 98.8896484375 101.88829803466797\n",
      "52 98.8281478881836 101.51787567138672\n",
      "53 98.36727142333984 101.20095825195312\n",
      "54 98.12741088867188 101.5981216430664\n",
      "55 97.82439422607422 101.74311065673828\n",
      "56 97.40070343017578 101.02779388427734\n",
      "57 97.13668823242188 100.59815216064453\n",
      "58 96.69074249267578 100.5844955444336\n",
      "59 96.32938385009766 100.84140014648438\n",
      "60 95.66120910644531 99.72520446777344\n",
      "61 95.2732162475586 98.98988342285156\n",
      "62 94.98269653320312 98.9163818359375\n",
      "63 94.51011657714844 98.86798095703125\n",
      "64 94.20439147949219 99.25846862792969\n",
      "65 93.9872055053711 98.84878540039062\n",
      "66 93.70686340332031 98.65181732177734\n",
      "67 93.53500366210938 99.08139038085938\n",
      "68 93.36885070800781 98.51789093017578\n",
      "69 93.20645141601562 98.6310806274414\n",
      "70 93.07958221435547 99.49005889892578\n",
      "71 92.96265411376953 98.02467346191406\n",
      "72 92.81097412109375 98.46080017089844\n",
      "73 92.67189025878906 98.73616027832031\n",
      "74 92.6434326171875 98.04344940185547\n",
      "75 92.49988555908203 99.19477844238281\n",
      "76 92.37055206298828 97.3634262084961\n",
      "77 92.26892852783203 98.25238037109375\n",
      "78 92.14983367919922 97.96834564208984\n",
      "79 92.02751159667969 98.34260559082031\n",
      "80 91.83003234863281 97.24762725830078\n",
      "81 91.76417541503906 98.21636962890625\n",
      "82 91.69036865234375 96.17984771728516\n",
      "83 91.6222152709961 98.55858612060547\n",
      "84 91.49211120605469 96.2401123046875\n",
      "85 91.31796264648438 97.0355453491211\n",
      "86 91.37332153320312 97.31812286376953\n",
      "87 91.42150115966797 96.28366088867188\n",
      "88 91.00196838378906 97.10517883300781\n",
      "89 90.79485321044922 96.60748291015625\n",
      "90 90.8076171875 95.76708984375\n",
      "91 90.76225280761719 97.14289093017578\n",
      "92 90.56166076660156 95.46998596191406\n",
      "93 90.42510986328125 95.55760955810547\n",
      "94 90.46698760986328 97.06375122070312\n",
      "95 90.66551208496094 94.2786636352539\n",
      "96 90.6864013671875 98.13154602050781\n",
      "97 90.86048126220703 94.69815826416016\n",
      "98 90.52961730957031 96.3505630493164\n",
      "99 90.49842071533203 97.01592254638672\n",
      "100 90.21336364746094 94.66249084472656\n",
      "101 90.052978515625 96.10039520263672\n",
      "102 90.37635040283203 96.54925537109375\n",
      "103 89.96829986572266 94.36186218261719\n",
      "104 89.83003997802734 95.73957061767578\n",
      "105 90.1220703125 96.5381088256836\n",
      "106 89.7320327758789 94.66622924804688\n",
      "107 89.73928833007812 95.60887908935547\n",
      "108 89.94717407226562 95.70984649658203\n",
      "109 89.4485092163086 95.16365051269531\n",
      "110 89.50700378417969 94.8406753540039\n",
      "111 89.54088592529297 95.40909576416016\n",
      "112 89.3388671875 96.3722152709961\n",
      "113 89.57147216796875 93.47859191894531\n",
      "114 89.24729919433594 96.31233978271484\n",
      "115 89.21723937988281 94.77264404296875\n",
      "116 89.12151336669922 94.34058380126953\n",
      "117 88.8906021118164 94.90441131591797\n",
      "118 88.98357391357422 95.1942367553711\n",
      "119 88.90492248535156 95.00594329833984\n",
      "120 88.89485168457031 93.56338500976562\n",
      "121 89.19857788085938 97.23009490966797\n",
      "122 89.64122772216797 92.57939147949219\n",
      "123 89.37611389160156 96.87261962890625\n",
      "124 89.0750961303711 94.42184448242188\n",
      "125 88.58097076416016 93.96601104736328\n",
      "126 89.0105972290039 95.67613983154297\n",
      "127 89.00538635253906 93.86872100830078\n",
      "128 88.4229507446289 94.40895080566406\n",
      "129 88.82841491699219 95.22396087646484\n",
      "130 88.62195587158203 93.31752014160156\n",
      "131 88.17913818359375 94.51744079589844\n",
      "132 88.51158142089844 94.86203002929688\n",
      "133 88.16822814941406 93.20785522460938\n",
      "134 88.33938598632812 95.13475799560547\n",
      "135 88.14386749267578 93.62772369384766\n",
      "136 87.94046020507812 93.96934509277344\n",
      "137 88.05071258544922 95.08808135986328\n",
      "138 87.87211608886719 92.91075897216797\n",
      "139 87.79393005371094 94.46715545654297\n",
      "140 87.8212661743164 93.9274673461914\n",
      "141 87.51908874511719 93.5263900756836\n",
      "142 87.71509552001953 93.90180969238281\n",
      "143 87.74347686767578 93.18441009521484\n",
      "144 87.69029235839844 95.59156799316406\n",
      "145 87.70321655273438 92.55204010009766\n",
      "146 87.2868881225586 93.91744995117188\n",
      "147 87.3309555053711 93.8838882446289\n",
      "148 87.21044158935547 92.7489242553711\n",
      "149 87.24859619140625 94.00559997558594\n",
      "150 87.3016357421875 92.49502563476562\n",
      "151 87.326904296875 95.06212615966797\n",
      "152 87.2269287109375 91.76641845703125\n",
      "153 86.97571563720703 93.90357971191406\n",
      "154 86.79113006591797 93.07353210449219\n",
      "155 86.79398345947266 92.46742248535156\n",
      "156 86.73091888427734 93.53624725341797\n",
      "157 86.86145782470703 91.97595977783203\n",
      "158 86.75152587890625 94.04566192626953\n",
      "159 86.61040496826172 91.77814483642578\n",
      "160 86.48055267333984 93.31100463867188\n",
      "161 86.33126068115234 92.23856353759766\n",
      "162 86.28361511230469 92.22492218017578\n",
      "163 86.24603271484375 93.09944915771484\n",
      "164 86.22959899902344 91.52276611328125\n",
      "165 86.29328155517578 93.63204956054688\n",
      "166 86.6282958984375 90.87964630126953\n",
      "167 86.57023620605469 94.51473999023438\n",
      "168 86.17901611328125 90.54073333740234\n",
      "169 85.9016342163086 92.00675964355469\n",
      "170 85.90777587890625 92.75830841064453\n",
      "171 86.11377716064453 90.38008117675781\n",
      "172 85.90455627441406 93.19309997558594\n",
      "173 85.62468719482422 91.04217529296875\n",
      "174 85.56346130371094 91.21446990966797\n",
      "175 85.5908203125 92.5927963256836\n",
      "176 85.67105102539062 90.34180450439453\n",
      "177 85.74775695800781 93.38786315917969\n",
      "178 85.58856964111328 90.0511474609375\n",
      "179 85.32213592529297 91.8250503540039\n",
      "180 85.24079132080078 91.30958557128906\n",
      "181 85.30078887939453 89.80316925048828\n",
      "182 85.37554168701172 92.7124252319336\n",
      "183 85.09261322021484 90.06109619140625\n",
      "184 84.96322631835938 90.9583969116211\n",
      "185 84.939697265625 91.52843475341797\n",
      "186 85.0425796508789 89.7699966430664\n",
      "187 85.09496307373047 92.34778594970703\n",
      "188 84.901611328125 89.47224426269531\n",
      "189 84.716552734375 91.1720962524414\n",
      "190 84.59140014648438 90.19215393066406\n",
      "191 84.6346206665039 89.53572845458984\n",
      "192 84.92235565185547 91.8044662475586\n",
      "193 85.08055877685547 89.8777847290039\n",
      "194 84.88526916503906 91.13737487792969\n",
      "195 84.6987533569336 91.1743392944336\n",
      "196 84.7218017578125 88.90862274169922\n",
      "197 84.95023345947266 92.41458129882812\n",
      "198 84.54183959960938 88.78854370117188\n",
      "199 84.22174835205078 90.12842559814453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-5f45da5dd0e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-139feb339bf7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0magg_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magg_current\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_current\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mappliance\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Appliance_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappliance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_current\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0magg_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magg_current\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_current\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-52d29ccbf9d0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36m_do_forward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mflat_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward_extended\u001b[0;34m(self, input, weight, hx)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_descs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescriptor_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_descs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_descs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhx_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/__init__.py\u001b[0m in \u001b[0;36mdescriptor\u001b[0;34m(tensor, N)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mN\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mdescriptor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDescriptorArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mdescriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mcheck_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnnCreateTensorDescriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k in range(500):\n",
    "    pred = model(*params)\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_func(pred, train_out)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if cuda_av:\n",
    "        test_inp = test_inp.cuda()\n",
    "    test_params = [test_inp, -2]\n",
    "    for i in range(len(ORDER)):\n",
    "        test_params.append(None)\n",
    "    test_pr = model(*test_params)\n",
    "    test_loss = loss_func(test_pr, test_out)\n",
    "    \n",
    "    print(k, loss.data[0], test_loss.data[0])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = torch.split(test_pr, test_aggregate.shape[0])\n",
    "test_fold = [None for x in range(len(ORDER))]\n",
    "if cuda_av:\n",
    "    for appliance_num, appliance in enumerate(ORDER):\n",
    "        test_fold[appliance_num] = test_pred[appliance_num].cpu().data.numpy().reshape(-1, 24)\n",
    "else:\n",
    "    for appliance_num, appliance in enumerate(ORDER):\n",
    "        test_fold[appliance_num] = test_pred[appliance_num].data.numpy().reshape(-1, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_gt_fold = [None for x in range(len(ORDER))]\n",
    "for appliance_num, appliance in enumerate(ORDER):\n",
    "    test_gt_fold[appliance_num] = test[:, APPLIANCE_ORDER.index(appliance), :, :].reshape(\n",
    "        test_aggregate.shape[0],\n",
    "        -1, 1).reshape(-1, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_error = {}\n",
    "\n",
    "for appliance_num, appliance in enumerate(ORDER):\n",
    "    test_error[appliance] = mean_absolute_error(test_fold[appliance_num], test_gt_fold[appliance_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dr': 51.731544589242937,\n",
       " 'dw': 11.026257301709313,\n",
       " 'fridge': 27.915676667744833,\n",
       " 'hvac': 300.75305767115384,\n",
       " 'mw': 7.3359302610540444}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dr': 51.895019602707926,\n",
       " 'dw': 11.02489129080851,\n",
       " 'fridge': 28.159192659996876,\n",
       " 'hvac': 285.86724458947918,\n",
       " 'mw': 7.4520018024466692}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
