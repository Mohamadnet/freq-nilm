{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from common import APPLIANCES_ORDER\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = np.load('../1H-input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subset_dataset(tensor):\n",
    "    t_subset = tensor[:, :, 180:194, :]\n",
    "    all_indices = np.array(list(range(320)))\n",
    "    for i in range(1, 7):\n",
    "        valid_homes = pd.DataFrame(t_subset[:, i, :].reshape(320, 14*24)).dropna().index\n",
    "        all_indices = np.intersect1d(all_indices, valid_homes)\n",
    "    t_subset = t_subset[all_indices, :, :, :].reshape(52, 7, 14*24)\n",
    "    \n",
    "    # Create artificial aggregate\n",
    "    t_subset[:, 0, :] = 0.0\n",
    "    for i in range(1, 7):\n",
    "        t_subset[:, 0, :] = t_subset[:, 0, :] + t_subset[:, i, :]\n",
    "    # t_subset is of shape (#home, appliance, days*hours)\n",
    "    return t_subset, all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all, valid_homes = create_subset_dataset(tensor)\n",
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_objective(y_pred, y_true):\n",
    "    with tf.name_scope(None):\n",
    "        return tf.losses.absolute_difference(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "import keras\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "n_movies = 3\n",
    "n_users=3\n",
    "n_latent_factors=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg = t_all[:30, 0, :].reshape(30*14, 24)\n",
    "train_appliance = t_all[:30, 1:, :].reshape(30*14, 6*24)\n",
    "\n",
    "train_hvac = t_all[:30, 1, :].reshape(30*14, 24)\n",
    "train_fridge = t_all[:30, 2, :].reshape(30*14, 24)\n",
    "train_mw = t_all[:30, 3, :].reshape(30*14, 24)\n",
    "train_dw = t_all[:30, 4, :].reshape(30*14, 24)\n",
    "train_wm = t_all[:30, 5, :].reshape(30*14, 24)\n",
    "train_oven = t_all[:30, 6, :].reshape(30*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_agg_new = train_hvac + train_fridge\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_hvac = t_all[30:, 1, :].reshape(22*14, 24)\n",
    "test_fridge = t_all[30:, 2, :].reshape(22*14, 24)\n",
    "test_mw = t_all[30:, 3, :].reshape(22*14, 24)\n",
    "test_dw = t_all[30:, 4, :].reshape(22*14, 24)\n",
    "test_wm = t_all[30:, 5, :].reshape(22*14, 24)\n",
    "test_oven = t_all[30:, 6, :].reshape(22*14, 24)\n",
    "test_appliance = t_all[30:, 1:, :].reshape(22*14, 6*24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_agg = t_all[30:, 0, :].reshape(22*14, 24)\n",
    "test_agg_new = test_hvac + test_fridge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hvac_fridge = np.hstack([train_hvac, train_fridge])\n",
    "train_all_appliances = np.hstack([train_hvac, train_fridge, train_mw, train_dw, train_wm, train_oven])\n",
    "test_all_appliances = np.hstack([test_hvac, test_fridge, test_mw, test_dw, test_wm, test_oven])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((420, 144), (308, 144))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_appliances.shape, test_all_appliances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(train_all_appliances, train_appliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvac      894.701044\n",
       "fridge     89.265252\n",
       "oven       16.646041\n",
       "dw         13.984549\n",
       "mw          6.417846\n",
       "wm          5.083840\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENERGY_MEAN = {APPLIANCES_ORDER[i]:np.mean(t_all[:, i, :]) for i in list(range(1, 7))}\n",
    "ENERGY_MEAN = pd.Series(ENERGY_MEAN)\n",
    "ENERGY_MEAN.sort_values(inplace=True, ascending=False)\n",
    "ENERGY_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "from keras.layers.merge import Subtract, Minimum, Multiply, Maximum\n",
    "import keras.backend as K\n",
    "\n",
    "layers = {}\n",
    "appliance=\"hvac\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvac Aggregate\n",
      "fridge Subtracting-hvac\n",
      "oven Subtracting-fridge\n",
      "dw Subtracting-oven\n",
      "mw Subtracting-dw\n",
      "wm Subtracting-mw\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "layers['Aggregate'] = keras.layers.Input(shape=[24],name='Aggregate')\n",
    "for appliance_num, appliance in enumerate(ENERGY_MEAN.index[:]):\n",
    "    if appliance_num==0:\n",
    "        prev = 'Aggregate'\n",
    "        layers['{}-dense'.format(appliance)] = keras.layers.Dense(units=20,name='{}-layer-1'.format(appliance), activation='relu')(layers['Aggregate'])\n",
    "    else:\n",
    "        prev = 'Subtracting-{}'.format(ENERGY_MEAN.index[appliance_num-1])\n",
    "        layers['{}-dense'.format(appliance)] = keras.layers.Dense(units=20,name='{}-layer-1'.format(appliance), activation='relu')(layers[prev])\n",
    "    print(appliance, prev)\n",
    "    layers['{}-dropout'.format(appliance)] = keras.layers.Dropout(rate=0.1,name='Droput-{}'.format(appliance))(layers['{}-dense'.format(appliance)])\n",
    "    layers['{}-output'.format(appliance)]= keras.layers.Dense(units=24,name='{}-output'.format(appliance), activation='relu')(layers['{}-dropout'.format(appliance)])\n",
    "    layers['{}-clipped-output'.format(appliance)] = Minimum(name='Clipped-{}'.format(appliance))([layers['{}-output'.format(appliance)], layers['Aggregate']])\n",
    "    \n",
    "    # Subtracting out appliance from prev\n",
    "    layers['Subtracting-{}'.format(appliance)] = Subtract(name='Subtracting-{}'.format(appliance))([layers[prev], layers['{}-clipped-output'.format(appliance)]])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "concat = keras.layers.merge([layers['{}-clipped-output'.format(appliance)] for appliance in APPLIANCES_ORDER[1:]], mode='concat',name='Concat')\n",
    "model = keras.Model(layers['Aggregate'], concat )\n",
    "\n",
    "with open('tree.pdf','wb') as f:\n",
    "    f.write(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='pdf'))\n",
    "\n",
    "\n",
    "\n",
    "#SVG(model_to_dot(model,  show_shapes=False, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Aggregate (InputLayer)          (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hvac-layer-1 (Dense)            (None, 20)           500         Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Droput-hvac (Dropout)           (None, 20)           0           hvac-layer-1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hvac-output (Dense)             (None, 24)           504         Droput-hvac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-hvac (Minimum)          (None, 24)           0           hvac-output[0][0]                \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Subtracting-hvac (Subtract)     (None, 24)           0           Aggregate[0][0]                  \n",
      "                                                                 Clipped-hvac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fridge-layer-1 (Dense)          (None, 20)           500         Subtracting-hvac[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Droput-fridge (Dropout)         (None, 20)           0           fridge-layer-1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fridge-output (Dense)           (None, 24)           504         Droput-fridge[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-fridge (Minimum)        (None, 24)           0           fridge-output[0][0]              \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Subtracting-fridge (Subtract)   (None, 24)           0           Subtracting-hvac[0][0]           \n",
      "                                                                 Clipped-fridge[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "oven-layer-1 (Dense)            (None, 20)           500         Subtracting-fridge[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Droput-oven (Dropout)           (None, 20)           0           oven-layer-1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "oven-output (Dense)             (None, 24)           504         Droput-oven[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-oven (Minimum)          (None, 24)           0           oven-output[0][0]                \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Subtracting-oven (Subtract)     (None, 24)           0           Subtracting-fridge[0][0]         \n",
      "                                                                 Clipped-oven[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dw-layer-1 (Dense)              (None, 20)           500         Subtracting-oven[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Droput-dw (Dropout)             (None, 20)           0           dw-layer-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dw-output (Dense)               (None, 24)           504         Droput-dw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-dw (Minimum)            (None, 24)           0           dw-output[0][0]                  \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Subtracting-dw (Subtract)       (None, 24)           0           Subtracting-oven[0][0]           \n",
      "                                                                 Clipped-dw[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mw-layer-1 (Dense)              (None, 20)           500         Subtracting-dw[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Droput-mw (Dropout)             (None, 20)           0           mw-layer-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mw-output (Dense)               (None, 24)           504         Droput-mw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-mw (Minimum)            (None, 24)           0           mw-output[0][0]                  \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Subtracting-mw (Subtract)       (None, 24)           0           Subtracting-dw[0][0]             \n",
      "                                                                 Clipped-mw[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "wm-layer-1 (Dense)              (None, 20)           500         Subtracting-mw[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Droput-wm (Dropout)             (None, 20)           0           wm-layer-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "wm-output (Dense)               (None, 24)           504         Droput-wm[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Clipped-wm (Minimum)            (None, 24)           0           wm-output[0][0]                  \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Concat (Merge)                  (None, 144)          0           Clipped-hvac[0][0]               \n",
      "                                                                 Clipped-fridge[0][0]             \n",
      "                                                                 Clipped-mw[0][0]                 \n",
      "                                                                 Clipped-dw[0][0]                 \n",
      "                                                                 Clipped-wm[0][0]                 \n",
      "                                                                 Clipped-oven[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 6,024\n",
      "Trainable params: 6,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/2000\n",
      "378/378 [==============================] - 3s 9ms/step - loss: 259.3444 - val_loss: 239.8792\n",
      "Epoch 2/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 214.6353 - val_loss: 200.5159\n",
      "Epoch 3/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 176.3073 - val_loss: 166.2251\n",
      "Epoch 4/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 143.7865 - val_loss: 136.7294\n",
      "Epoch 5/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 116.8717 - val_loss: 113.7115\n",
      "Epoch 6/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 97.8363 - val_loss: 97.2235\n",
      "Epoch 7/2000\n",
      "378/378 [==============================] - 0s 160us/step - loss: 83.8354 - val_loss: 84.9449\n",
      "Epoch 8/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 74.3178 - val_loss: 75.7906\n",
      "Epoch 9/2000\n",
      "378/378 [==============================] - 0s 205us/step - loss: 67.8812 - val_loss: 69.6405\n",
      "Epoch 10/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 65.3370 - val_loss: 65.3649\n",
      "Epoch 11/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 60.7147 - val_loss: 62.6248\n",
      "Epoch 12/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 59.3550 - val_loss: 60.5547\n",
      "Epoch 13/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 57.8376 - val_loss: 59.1263\n",
      "Epoch 14/2000\n",
      "378/378 [==============================] - 0s 147us/step - loss: 56.6986 - val_loss: 58.1983\n",
      "Epoch 15/2000\n",
      "378/378 [==============================] - 0s 162us/step - loss: 55.3698 - val_loss: 56.6916\n",
      "Epoch 16/2000\n",
      "378/378 [==============================] - 0s 170us/step - loss: 53.1369 - val_loss: 53.0482\n",
      "Epoch 17/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 49.7385 - val_loss: 50.1999\n",
      "Epoch 18/2000\n",
      "378/378 [==============================] - 0s 169us/step - loss: 45.2473 - val_loss: 46.5242\n",
      "Epoch 19/2000\n",
      "378/378 [==============================] - 0s 173us/step - loss: 43.6068 - val_loss: 46.6616\n",
      "Epoch 20/2000\n",
      "378/378 [==============================] - 0s 155us/step - loss: 42.7731 - val_loss: 46.7761\n",
      "Epoch 21/2000\n",
      "378/378 [==============================] - 0s 143us/step - loss: 43.0318 - val_loss: 46.8086\n",
      "Epoch 22/2000\n",
      "378/378 [==============================] - 0s 172us/step - loss: 42.6522 - val_loss: 46.8048\n",
      "Epoch 23/2000\n",
      "378/378 [==============================] - 0s 190us/step - loss: 42.5658 - val_loss: 46.7898\n",
      "Epoch 24/2000\n",
      "378/378 [==============================] - 0s 170us/step - loss: 42.7163 - val_loss: 46.8218\n",
      "Epoch 25/2000\n",
      "378/378 [==============================] - 0s 200us/step - loss: 42.6462 - val_loss: 46.7687\n",
      "Epoch 26/2000\n",
      "378/378 [==============================] - 0s 157us/step - loss: 42.5375 - val_loss: 46.7615\n",
      "Epoch 27/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 42.7107 - val_loss: 46.7687\n",
      "Epoch 28/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 42.5923 - val_loss: 46.8060\n",
      "Epoch 29/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 42.5012 - val_loss: 46.8435\n",
      "Epoch 30/2000\n",
      "378/378 [==============================] - 0s 153us/step - loss: 42.5279 - val_loss: 46.8372\n",
      "Epoch 31/2000\n",
      "378/378 [==============================] - 0s 186us/step - loss: 42.3642 - val_loss: 46.8334\n",
      "Epoch 32/2000\n",
      "378/378 [==============================] - 0s 158us/step - loss: 42.3204 - val_loss: 46.8244\n",
      "Epoch 33/2000\n",
      "378/378 [==============================] - 0s 161us/step - loss: 42.4785 - val_loss: 46.8133\n",
      "Epoch 34/2000\n",
      "378/378 [==============================] - 0s 191us/step - loss: 42.2400 - val_loss: 46.8026\n",
      "Epoch 35/2000\n",
      "378/378 [==============================] - 0s 179us/step - loss: 42.2756 - val_loss: 46.7927\n",
      "Epoch 36/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 42.3885 - val_loss: 46.7836\n",
      "Epoch 37/2000\n",
      "378/378 [==============================] - 0s 156us/step - loss: 42.3010 - val_loss: 46.7736\n",
      "Epoch 38/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 42.1805 - val_loss: 46.7300\n",
      "Epoch 39/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 42.0694 - val_loss: 46.5962\n",
      "Epoch 40/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 42.3254 - val_loss: 46.4879\n",
      "Epoch 41/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 42.0608 - val_loss: 46.4222\n",
      "Epoch 42/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 42.2064 - val_loss: 46.4376\n",
      "Epoch 43/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 42.3661 - val_loss: 46.3637\n",
      "Epoch 44/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 42.1898 - val_loss: 46.3925\n",
      "Epoch 45/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 42.1408 - val_loss: 46.3140\n",
      "Epoch 46/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 42.0106 - val_loss: 46.2411\n",
      "Epoch 47/2000\n",
      "378/378 [==============================] - 0s 320us/step - loss: 42.0645 - val_loss: 46.1307\n",
      "Epoch 48/2000\n",
      "378/378 [==============================] - 0s 160us/step - loss: 41.7701 - val_loss: 46.0507\n",
      "Epoch 49/2000\n",
      "378/378 [==============================] - 0s 151us/step - loss: 41.8195 - val_loss: 45.9303\n",
      "Epoch 50/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 42.0822 - val_loss: 45.8784\n",
      "Epoch 51/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 41.7231 - val_loss: 45.8308\n",
      "Epoch 52/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 41.7066 - val_loss: 45.7666\n",
      "Epoch 53/2000\n",
      "378/378 [==============================] - 0s 156us/step - loss: 41.4635 - val_loss: 45.7316\n",
      "Epoch 54/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 41.1221 - val_loss: 44.9616\n",
      "Epoch 55/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 41.1080 - val_loss: 44.9213\n",
      "Epoch 56/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 41.1211 - val_loss: 44.7627\n",
      "Epoch 57/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 40.4764 - val_loss: 44.6894\n",
      "Epoch 58/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 40.3701 - val_loss: 44.5295\n",
      "Epoch 59/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 40.3554 - val_loss: 43.6007\n",
      "Epoch 60/2000\n",
      "378/378 [==============================] - 0s 155us/step - loss: 39.8552 - val_loss: 43.3838\n",
      "Epoch 61/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 39.4339 - val_loss: 43.1962\n",
      "Epoch 62/2000\n",
      "378/378 [==============================] - 0s 156us/step - loss: 39.1725 - val_loss: 43.0991\n",
      "Epoch 63/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 39.1382 - val_loss: 42.9434\n",
      "Epoch 64/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 39.2488 - val_loss: 42.8066\n",
      "Epoch 65/2000\n",
      "378/378 [==============================] - 0s 154us/step - loss: 39.1566 - val_loss: 42.7255\n",
      "Epoch 66/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 39.0089 - val_loss: 42.4656\n",
      "Epoch 67/2000\n",
      "378/378 [==============================] - 0s 248us/step - loss: 38.7359 - val_loss: 42.2401\n",
      "Epoch 68/2000\n",
      "378/378 [==============================] - ETA: 0s - loss: 38.18 - 0s 126us/step - loss: 38.8066 - val_loss: 42.1379\n",
      "Epoch 69/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 38.3292 - val_loss: 42.1197\n",
      "Epoch 70/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 38.2573 - val_loss: 42.0518\n",
      "Epoch 71/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 38.0066 - val_loss: 42.1508\n",
      "Epoch 72/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 38.0257 - val_loss: 42.2352\n",
      "Epoch 73/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 38.2904 - val_loss: 42.3502\n",
      "Epoch 74/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 38.0262 - val_loss: 42.3610\n",
      "Epoch 75/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 38.1582 - val_loss: 42.2671\n",
      "Epoch 76/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.9651 - val_loss: 42.2169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 38.1731 - val_loss: 42.2522\n",
      "Epoch 78/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 37.9182 - val_loss: 42.3405\n",
      "Epoch 79/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 37.9703 - val_loss: 42.4616\n",
      "Epoch 80/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 38.0840 - val_loss: 42.4402\n",
      "Epoch 81/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 37.9368 - val_loss: 42.4578\n",
      "Epoch 82/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 37.9914 - val_loss: 42.4847\n",
      "Epoch 83/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 37.8447 - val_loss: 42.4869\n",
      "Epoch 84/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 37.7324 - val_loss: 42.4614\n",
      "Epoch 85/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 37.7931 - val_loss: 42.4440\n",
      "Epoch 86/2000\n",
      "378/378 [==============================] - ETA: 0s - loss: 36.56 - 0s 122us/step - loss: 38.1417 - val_loss: 42.4912\n",
      "Epoch 87/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 37.7989 - val_loss: 42.4500\n",
      "Epoch 88/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 37.6781 - val_loss: 42.2467\n",
      "Epoch 89/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 37.8030 - val_loss: 42.2555\n",
      "Epoch 90/2000\n",
      "378/378 [==============================] - 0s 168us/step - loss: 37.9352 - val_loss: 42.0345\n",
      "Epoch 91/2000\n",
      "378/378 [==============================] - 0s 165us/step - loss: 38.0282 - val_loss: 41.8323\n",
      "Epoch 92/2000\n",
      "378/378 [==============================] - 0s 149us/step - loss: 37.9882 - val_loss: 41.8404\n",
      "Epoch 93/2000\n",
      "378/378 [==============================] - 0s 162us/step - loss: 37.9215 - val_loss: 41.5787\n",
      "Epoch 94/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 37.6772 - val_loss: 41.7217\n",
      "Epoch 95/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 38.0679 - val_loss: 41.5751\n",
      "Epoch 96/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 37.6770 - val_loss: 41.5645\n",
      "Epoch 97/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 37.7335 - val_loss: 41.9263\n",
      "Epoch 98/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 37.7189 - val_loss: 41.9288\n",
      "Epoch 99/2000\n",
      "378/378 [==============================] - ETA: 0s - loss: 40.25 - 0s 127us/step - loss: 37.6665 - val_loss: 41.9270\n",
      "Epoch 100/2000\n",
      "378/378 [==============================] - 0s 169us/step - loss: 37.6212 - val_loss: 41.8479\n",
      "Epoch 101/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 37.4986 - val_loss: 41.9178\n",
      "Epoch 102/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 37.7241 - val_loss: 42.0364\n",
      "Epoch 103/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 37.5955 - val_loss: 42.2844\n",
      "Epoch 104/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 37.4858 - val_loss: 42.1549\n",
      "Epoch 105/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 37.5022 - val_loss: 42.0257\n",
      "Epoch 106/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 37.6302 - val_loss: 41.5698\n",
      "Epoch 107/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 37.6724 - val_loss: 41.2725\n",
      "Epoch 108/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 37.5518 - val_loss: 41.8487\n",
      "Epoch 109/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 37.5863 - val_loss: 41.9762\n",
      "Epoch 110/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 37.3362 - val_loss: 41.7699\n",
      "Epoch 111/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 37.6617 - val_loss: 41.6355\n",
      "Epoch 112/2000\n",
      "378/378 [==============================] - 0s 165us/step - loss: 37.6209 - val_loss: 41.4222\n",
      "Epoch 113/2000\n",
      "378/378 [==============================] - ETA: 0s - loss: 38.93 - 0s 119us/step - loss: 37.5170 - val_loss: 41.8378\n",
      "Epoch 114/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.4872 - val_loss: 42.0104\n",
      "Epoch 115/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 37.7227 - val_loss: 41.9355\n",
      "Epoch 116/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 37.6540 - val_loss: 41.8131\n",
      "Epoch 117/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 37.6100 - val_loss: 42.0456\n",
      "Epoch 118/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 37.7061 - val_loss: 42.1419\n",
      "Epoch 119/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 37.5829 - val_loss: 41.9779\n",
      "Epoch 120/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 37.3513 - val_loss: 42.0439\n",
      "Epoch 121/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 37.4296 - val_loss: 42.0362\n",
      "Epoch 122/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.5671 - val_loss: 42.0981\n",
      "Epoch 123/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 37.3123 - val_loss: 42.1873\n",
      "Epoch 124/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 37.7578 - val_loss: 42.1345\n",
      "Epoch 125/2000\n",
      "378/378 [==============================] - 0s 158us/step - loss: 37.4506 - val_loss: 42.0689\n",
      "Epoch 126/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 37.3706 - val_loss: 41.9716\n",
      "Epoch 127/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 37.4810 - val_loss: 41.9713\n",
      "Epoch 128/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.3080 - val_loss: 41.8808\n",
      "Epoch 129/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.4951 - val_loss: 41.7434\n",
      "Epoch 130/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.4477 - val_loss: 41.8281\n",
      "Epoch 131/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 37.1280 - val_loss: 41.6820\n",
      "Epoch 132/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 37.6713 - val_loss: 41.8753\n",
      "Epoch 133/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 37.1794 - val_loss: 41.9752\n",
      "Epoch 134/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 37.3020 - val_loss: 41.8579\n",
      "Epoch 135/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 37.3659 - val_loss: 41.6036\n",
      "Epoch 136/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 37.3607 - val_loss: 41.8832\n",
      "Epoch 137/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.0820 - val_loss: 41.9457\n",
      "Epoch 138/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 37.0764 - val_loss: 41.9788\n",
      "Epoch 139/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 37.0113 - val_loss: 41.7308\n",
      "Epoch 140/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 37.0607 - val_loss: 41.5889\n",
      "Epoch 141/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 37.2060 - val_loss: 41.7056\n",
      "Epoch 142/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 37.1120 - val_loss: 41.6951\n",
      "Epoch 143/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 37.1843 - val_loss: 41.1955\n",
      "Epoch 144/2000\n",
      "378/378 [==============================] - 0s 170us/step - loss: 37.0547 - val_loss: 40.6625\n",
      "Epoch 145/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 37.2011 - val_loss: 40.8436\n",
      "Epoch 146/2000\n",
      "378/378 [==============================] - 0s 149us/step - loss: 36.9512 - val_loss: 40.8686\n",
      "Epoch 147/2000\n",
      "378/378 [==============================] - 0s 227us/step - loss: 37.1148 - val_loss: 40.9669\n",
      "Epoch 148/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 36.8997 - val_loss: 40.7477\n",
      "Epoch 149/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 37.2101 - val_loss: 40.7096\n",
      "Epoch 150/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.9782 - val_loss: 40.9196\n",
      "Epoch 151/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 36.8676 - val_loss: 40.8755\n",
      "Epoch 152/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 128us/step - loss: 36.8044 - val_loss: 40.8155\n",
      "Epoch 153/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.9889 - val_loss: 40.9074\n",
      "Epoch 154/2000\n",
      "378/378 [==============================] - 0s 197us/step - loss: 36.9862 - val_loss: 40.8738\n",
      "Epoch 155/2000\n",
      "378/378 [==============================] - 0s 161us/step - loss: 36.9718 - val_loss: 40.7932\n",
      "Epoch 156/2000\n",
      "378/378 [==============================] - 0s 233us/step - loss: 36.9964 - val_loss: 40.6545\n",
      "Epoch 157/2000\n",
      "378/378 [==============================] - 0s 265us/step - loss: 36.8551 - val_loss: 40.7230\n",
      "Epoch 158/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 36.9066 - val_loss: 40.9438\n",
      "Epoch 159/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 36.8025 - val_loss: 40.9730\n",
      "Epoch 160/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 36.7767 - val_loss: 40.6323\n",
      "Epoch 161/2000\n",
      "378/378 [==============================] - 0s 208us/step - loss: 36.7296 - val_loss: 40.4252\n",
      "Epoch 162/2000\n",
      "378/378 [==============================] - 0s 167us/step - loss: 36.8474 - val_loss: 40.3888\n",
      "Epoch 163/2000\n",
      "378/378 [==============================] - 0s 143us/step - loss: 36.8017 - val_loss: 40.4866\n",
      "Epoch 164/2000\n",
      "378/378 [==============================] - 0s 161us/step - loss: 36.5950 - val_loss: 40.5805\n",
      "Epoch 165/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 36.5626 - val_loss: 40.9778\n",
      "Epoch 166/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.7224 - val_loss: 40.6193\n",
      "Epoch 167/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.4284 - val_loss: 40.2731\n",
      "Epoch 168/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 36.7785 - val_loss: 40.4037\n",
      "Epoch 169/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 36.6345 - val_loss: 40.6359\n",
      "Epoch 170/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 37.0983 - val_loss: 40.6579\n",
      "Epoch 171/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.6338 - val_loss: 40.4396\n",
      "Epoch 172/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.6186 - val_loss: 40.5206\n",
      "Epoch 173/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.5984 - val_loss: 40.4367\n",
      "Epoch 174/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 36.6895 - val_loss: 40.3831\n",
      "Epoch 175/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 36.6801 - val_loss: 40.5382\n",
      "Epoch 176/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.4251 - val_loss: 40.8043\n",
      "Epoch 177/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 36.6388 - val_loss: 40.6973\n",
      "Epoch 178/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 36.7512 - val_loss: 40.5105\n",
      "Epoch 179/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 36.6825 - val_loss: 40.3128\n",
      "Epoch 180/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.4346 - val_loss: 40.7167\n",
      "Epoch 181/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.7934 - val_loss: 41.1050\n",
      "Epoch 182/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 36.6033 - val_loss: 40.9557\n",
      "Epoch 183/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.3866 - val_loss: 41.0723\n",
      "Epoch 184/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 36.4126 - val_loss: 40.7031\n",
      "Epoch 185/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 36.4175 - val_loss: 40.4546\n",
      "Epoch 186/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 36.6576 - val_loss: 40.6207\n",
      "Epoch 187/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 36.4446 - val_loss: 40.6513\n",
      "Epoch 188/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 36.4922 - val_loss: 40.5628\n",
      "Epoch 189/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 36.5348 - val_loss: 40.1904\n",
      "Epoch 190/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 36.3632 - val_loss: 40.0944\n",
      "Epoch 191/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.3258 - val_loss: 40.0807\n",
      "Epoch 192/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.2996 - val_loss: 40.2962\n",
      "Epoch 193/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 36.3325 - val_loss: 40.5184\n",
      "Epoch 194/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 36.3913 - val_loss: 40.3998\n",
      "Epoch 195/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 36.2974 - val_loss: 40.5640\n",
      "Epoch 196/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.4574 - val_loss: 40.7252\n",
      "Epoch 197/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 36.3248 - val_loss: 40.6830\n",
      "Epoch 198/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 36.4927 - val_loss: 40.5467\n",
      "Epoch 199/2000\n",
      "378/378 [==============================] - 0s 104us/step - loss: 36.0589 - val_loss: 40.4848\n",
      "Epoch 200/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.3270 - val_loss: 40.2909\n",
      "Epoch 201/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.1611 - val_loss: 40.5051\n",
      "Epoch 202/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 36.1886 - val_loss: 40.7257\n",
      "Epoch 203/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 36.4251 - val_loss: 40.7911\n",
      "Epoch 204/2000\n",
      "378/378 [==============================] - 0s 106us/step - loss: 36.4375 - val_loss: 40.5815\n",
      "Epoch 205/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.2220 - val_loss: 40.4365\n",
      "Epoch 206/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 36.3705 - val_loss: 40.3336\n",
      "Epoch 207/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 36.2580 - val_loss: 40.6215\n",
      "Epoch 208/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 36.2211 - val_loss: 40.5898\n",
      "Epoch 209/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.1984 - val_loss: 40.6835\n",
      "Epoch 210/2000\n",
      "378/378 [==============================] - 0s 171us/step - loss: 36.3660 - val_loss: 40.5374\n",
      "Epoch 211/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 36.2416 - val_loss: 40.6232\n",
      "Epoch 212/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.4799 - val_loss: 40.7974\n",
      "Epoch 213/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 36.2331 - val_loss: 40.8724\n",
      "Epoch 214/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.3177 - val_loss: 40.7932\n",
      "Epoch 215/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 36.1720 - val_loss: 40.4391\n",
      "Epoch 216/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 36.1695 - val_loss: 40.3708\n",
      "Epoch 217/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 36.2559 - val_loss: 40.3641\n",
      "Epoch 218/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.3014 - val_loss: 40.6745\n",
      "Epoch 219/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 36.2570 - val_loss: 40.6435\n",
      "Epoch 220/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 36.0926 - val_loss: 40.5609\n",
      "Epoch 221/2000\n",
      "378/378 [==============================] - ETA: 0s - loss: 39.89 - 0s 161us/step - loss: 36.1105 - val_loss: 40.6054\n",
      "Epoch 222/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 36.2679 - val_loss: 40.8782\n",
      "Epoch 223/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 36.0833 - val_loss: 40.5285\n",
      "Epoch 224/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 36.2379 - val_loss: 40.0816\n",
      "Epoch 225/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 36.1277 - val_loss: 40.1904\n",
      "Epoch 226/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.6845 - val_loss: 40.2969\n",
      "Epoch 227/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 36.1001 - val_loss: 40.7570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 36.0196 - val_loss: 40.8327\n",
      "Epoch 229/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 36.0879 - val_loss: 40.9386\n",
      "Epoch 230/2000\n",
      "378/378 [==============================] - 0s 105us/step - loss: 35.8500 - val_loss: 40.7582\n",
      "Epoch 231/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 35.9579 - val_loss: 40.6504\n",
      "Epoch 232/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 35.9302 - val_loss: 40.8128\n",
      "Epoch 233/2000\n",
      "378/378 [==============================] - 0s 195us/step - loss: 35.9856 - val_loss: 40.7335\n",
      "Epoch 234/2000\n",
      "378/378 [==============================] - 0s 156us/step - loss: 35.9036 - val_loss: 40.6979\n",
      "Epoch 235/2000\n",
      "378/378 [==============================] - 0s 155us/step - loss: 36.1464 - val_loss: 40.9410\n",
      "Epoch 236/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 35.9408 - val_loss: 40.7096\n",
      "Epoch 237/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 36.0120 - val_loss: 40.7102\n",
      "Epoch 238/2000\n",
      "378/378 [==============================] - ETA: 0s - loss: 37.44 - 0s 118us/step - loss: 36.1364 - val_loss: 40.8487\n",
      "Epoch 239/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 35.9534 - val_loss: 40.8707\n",
      "Epoch 240/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 35.8699 - val_loss: 40.9409\n",
      "Epoch 241/2000\n",
      "378/378 [==============================] - 0s 164us/step - loss: 35.7226 - val_loss: 40.7650\n",
      "Epoch 242/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 35.8662 - val_loss: 40.6747\n",
      "Epoch 243/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 35.7047 - val_loss: 40.9134\n",
      "Epoch 244/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 35.9900 - val_loss: 41.0278\n",
      "Epoch 245/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 36.1174 - val_loss: 40.7204\n",
      "Epoch 246/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 35.7747 - val_loss: 40.6108\n",
      "Epoch 247/2000\n",
      "378/378 [==============================] - 0s 168us/step - loss: 35.7914 - val_loss: 40.4771\n",
      "Epoch 248/2000\n",
      "378/378 [==============================] - 0s 158us/step - loss: 35.7125 - val_loss: 40.5903\n",
      "Epoch 249/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 35.8116 - val_loss: 40.4415\n",
      "Epoch 250/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 35.8661 - val_loss: 40.2231\n",
      "Epoch 251/2000\n",
      "378/378 [==============================] - 0s 209us/step - loss: 35.7875 - val_loss: 40.2419\n",
      "Epoch 252/2000\n",
      "378/378 [==============================] - 0s 216us/step - loss: 35.8305 - val_loss: 40.3783\n",
      "Epoch 253/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 35.6694 - val_loss: 40.3716\n",
      "Epoch 254/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 35.8536 - val_loss: 40.5336\n",
      "Epoch 255/2000\n",
      "378/378 [==============================] - 0s 168us/step - loss: 35.8626 - val_loss: 40.6353\n",
      "Epoch 256/2000\n",
      "378/378 [==============================] - 0s 166us/step - loss: 35.9531 - val_loss: 40.5359\n",
      "Epoch 257/2000\n",
      "378/378 [==============================] - 0s 210us/step - loss: 35.5335 - val_loss: 40.4293\n",
      "Epoch 258/2000\n",
      "378/378 [==============================] - 0s 201us/step - loss: 35.8391 - val_loss: 40.4740\n",
      "Epoch 259/2000\n",
      "378/378 [==============================] - 0s 163us/step - loss: 35.8207 - val_loss: 40.5546\n",
      "Epoch 260/2000\n",
      "378/378 [==============================] - 0s 206us/step - loss: 35.6561 - val_loss: 40.4710\n",
      "Epoch 261/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 35.4692 - val_loss: 40.4800\n",
      "Epoch 262/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 35.5700 - val_loss: 40.6207\n",
      "Epoch 263/2000\n",
      "378/378 [==============================] - 0s 205us/step - loss: 35.7155 - val_loss: 40.5983\n",
      "Epoch 264/2000\n",
      "378/378 [==============================] - 0s 214us/step - loss: 35.5807 - val_loss: 40.6997\n",
      "Epoch 265/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 35.7540 - val_loss: 40.6593\n",
      "Epoch 266/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 35.4083 - val_loss: 40.6212\n",
      "Epoch 267/2000\n",
      "378/378 [==============================] - 0s 151us/step - loss: 35.7758 - val_loss: 40.4987\n",
      "Epoch 268/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 35.5800 - val_loss: 40.2986\n",
      "Epoch 269/2000\n",
      "378/378 [==============================] - 0s 151us/step - loss: 35.5538 - val_loss: 40.1217\n",
      "Epoch 270/2000\n",
      "378/378 [==============================] - 0s 160us/step - loss: 35.5627 - val_loss: 40.2777\n",
      "Epoch 271/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 35.6071 - val_loss: 40.2153\n",
      "Epoch 272/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 35.5647 - val_loss: 40.0676\n",
      "Epoch 273/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.8170 - val_loss: 40.0519\n",
      "Epoch 274/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 35.7430 - val_loss: 40.1654\n",
      "Epoch 275/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 35.2508 - val_loss: 40.4737\n",
      "Epoch 276/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 35.5542 - val_loss: 40.8749\n",
      "Epoch 277/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 35.3306 - val_loss: 40.7943\n",
      "Epoch 278/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.5750 - val_loss: 40.4734\n",
      "Epoch 279/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 35.4068 - val_loss: 40.4662\n",
      "Epoch 280/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 35.4708 - val_loss: 40.3022\n",
      "Epoch 281/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 35.4399 - val_loss: 40.3461\n",
      "Epoch 282/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 35.3060 - val_loss: 40.3743\n",
      "Epoch 283/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 35.2394 - val_loss: 40.4485\n",
      "Epoch 284/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 35.1795 - val_loss: 40.6030\n",
      "Epoch 285/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 35.0675 - val_loss: 40.4669\n",
      "Epoch 286/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 35.3058 - val_loss: 40.2675\n",
      "Epoch 287/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 35.3590 - val_loss: 40.2038\n",
      "Epoch 288/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.3586 - val_loss: 40.3467\n",
      "Epoch 289/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 35.2272 - val_loss: 40.4549\n",
      "Epoch 290/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 35.0016 - val_loss: 40.2897\n",
      "Epoch 291/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 35.3054 - val_loss: 40.4057\n",
      "Epoch 292/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 35.0612 - val_loss: 40.4910\n",
      "Epoch 293/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 35.0946 - val_loss: 40.5020\n",
      "Epoch 294/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 35.0740 - val_loss: 40.4421\n",
      "Epoch 295/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 35.1758 - val_loss: 40.5783\n",
      "Epoch 296/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 34.8612 - val_loss: 40.2243\n",
      "Epoch 297/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 34.9537 - val_loss: 40.0956\n",
      "Epoch 298/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 34.8450 - val_loss: 40.1681\n",
      "Epoch 299/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 35.0211 - val_loss: 40.1114\n",
      "Epoch 300/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 34.8201 - val_loss: 40.0133\n",
      "Epoch 301/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 34.6069 - val_loss: 40.0521\n",
      "Epoch 302/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 34.8895 - val_loss: 40.0609\n",
      "Epoch 303/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 111us/step - loss: 35.0562 - val_loss: 40.1125\n",
      "Epoch 304/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 34.8240 - val_loss: 40.1612\n",
      "Epoch 305/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 34.7929 - val_loss: 40.1978\n",
      "Epoch 306/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 34.9732 - val_loss: 40.1878\n",
      "Epoch 307/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 34.9981 - val_loss: 40.1244\n",
      "Epoch 308/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 34.8969 - val_loss: 40.3172\n",
      "Epoch 309/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 34.7166 - val_loss: 40.3820\n",
      "Epoch 310/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 34.6974 - val_loss: 40.0775\n",
      "Epoch 311/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 34.9032 - val_loss: 39.6798\n",
      "Epoch 312/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 34.4870 - val_loss: 39.5562\n",
      "Epoch 313/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 34.7435 - val_loss: 39.7305\n",
      "Epoch 314/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 34.6475 - val_loss: 39.7975\n",
      "Epoch 315/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 34.5811 - val_loss: 39.7965\n",
      "Epoch 316/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 34.5977 - val_loss: 39.4093\n",
      "Epoch 317/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 34.3868 - val_loss: 39.3951\n",
      "Epoch 318/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 34.7772 - val_loss: 39.3264\n",
      "Epoch 319/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 34.6835 - val_loss: 39.4940\n",
      "Epoch 320/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 34.7159 - val_loss: 39.7831\n",
      "Epoch 321/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.3870 - val_loss: 39.2320\n",
      "Epoch 322/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 34.5088 - val_loss: 39.4882\n",
      "Epoch 323/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 34.4565 - val_loss: 39.5419\n",
      "Epoch 324/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 34.6428 - val_loss: 39.3817\n",
      "Epoch 325/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 34.3235 - val_loss: 39.2223\n",
      "Epoch 326/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 33.9999 - val_loss: 39.3275\n",
      "Epoch 327/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 34.3554 - val_loss: 39.4165\n",
      "Epoch 328/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 34.3399 - val_loss: 39.4234\n",
      "Epoch 329/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 34.1230 - val_loss: 39.2088\n",
      "Epoch 330/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 34.1060 - val_loss: 39.1287\n",
      "Epoch 331/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 34.0553 - val_loss: 39.1484\n",
      "Epoch 332/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 34.0564 - val_loss: 39.0238\n",
      "Epoch 333/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 34.2869 - val_loss: 38.9594\n",
      "Epoch 334/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.9755 - val_loss: 38.9437\n",
      "Epoch 335/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 34.1196 - val_loss: 39.0234\n",
      "Epoch 336/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.8868 - val_loss: 38.8349\n",
      "Epoch 337/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 34.2556 - val_loss: 39.0061\n",
      "Epoch 338/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 34.1946 - val_loss: 38.7584\n",
      "Epoch 339/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 34.0274 - val_loss: 38.9276\n",
      "Epoch 340/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 34.1747 - val_loss: 38.8246\n",
      "Epoch 341/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 34.0707 - val_loss: 38.9271\n",
      "Epoch 342/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 34.0946 - val_loss: 38.6559\n",
      "Epoch 343/2000\n",
      "378/378 [==============================] - 0s 106us/step - loss: 34.2631 - val_loss: 39.1252\n",
      "Epoch 344/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 34.1499 - val_loss: 38.7062\n",
      "Epoch 345/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.8398 - val_loss: 38.8024\n",
      "Epoch 346/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 34.0793 - val_loss: 38.7740\n",
      "Epoch 347/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 34.1537 - val_loss: 38.6068\n",
      "Epoch 348/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 33.8279 - val_loss: 38.7620\n",
      "Epoch 349/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 33.6849 - val_loss: 38.5089\n",
      "Epoch 350/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 33.4886 - val_loss: 38.7070\n",
      "Epoch 351/2000\n",
      "378/378 [==============================] - 0s 103us/step - loss: 33.4482 - val_loss: 38.7140\n",
      "Epoch 352/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.7779 - val_loss: 38.6888\n",
      "Epoch 353/2000\n",
      "378/378 [==============================] - 0s 103us/step - loss: 33.5060 - val_loss: 38.6216\n",
      "Epoch 354/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.7666 - val_loss: 38.5991\n",
      "Epoch 355/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 33.7846 - val_loss: 38.5530\n",
      "Epoch 356/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.5699 - val_loss: 38.4918\n",
      "Epoch 357/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.7666 - val_loss: 38.2309\n",
      "Epoch 358/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 33.5976 - val_loss: 38.3567\n",
      "Epoch 359/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.5265 - val_loss: 38.7278\n",
      "Epoch 360/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 33.7716 - val_loss: 38.6874\n",
      "Epoch 361/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.4827 - val_loss: 38.4210\n",
      "Epoch 362/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 33.6376 - val_loss: 38.2799\n",
      "Epoch 363/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 33.5948 - val_loss: 38.2991\n",
      "Epoch 364/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 33.3427 - val_loss: 38.3047\n",
      "Epoch 365/2000\n",
      "378/378 [==============================] - 0s 157us/step - loss: 33.4931 - val_loss: 38.3873\n",
      "Epoch 366/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 33.2858 - val_loss: 38.3983\n",
      "Epoch 367/2000\n",
      "378/378 [==============================] - 0s 143us/step - loss: 33.6541 - val_loss: 38.5699\n",
      "Epoch 368/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 33.3468 - val_loss: 38.4183\n",
      "Epoch 369/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.2169 - val_loss: 38.1147\n",
      "Epoch 370/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 33.4087 - val_loss: 38.3158\n",
      "Epoch 371/2000\n",
      "378/378 [==============================] - ETA: 0s - loss: 31.17 - 0s 131us/step - loss: 33.6677 - val_loss: 38.4777\n",
      "Epoch 372/2000\n",
      "378/378 [==============================] - 0s 161us/step - loss: 33.6060 - val_loss: 38.2695\n",
      "Epoch 373/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 33.4029 - val_loss: 38.0451\n",
      "Epoch 374/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 33.6771 - val_loss: 38.2575\n",
      "Epoch 375/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 33.4329 - val_loss: 38.1975\n",
      "Epoch 376/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 33.3681 - val_loss: 38.2182\n",
      "Epoch 377/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 33.4210 - val_loss: 38.0460\n",
      "Epoch 378/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 33.5682 - val_loss: 38.0550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 33.0924 - val_loss: 37.9576\n",
      "Epoch 380/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 33.4654 - val_loss: 37.9941\n",
      "Epoch 381/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 33.2043 - val_loss: 37.9196\n",
      "Epoch 382/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 33.5040 - val_loss: 37.8843\n",
      "Epoch 383/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 33.2490 - val_loss: 38.0230\n",
      "Epoch 384/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 33.4466 - val_loss: 37.8726\n",
      "Epoch 385/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 33.1135 - val_loss: 38.0440\n",
      "Epoch 386/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 33.0610 - val_loss: 37.9431\n",
      "Epoch 387/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 33.1813 - val_loss: 37.9645\n",
      "Epoch 388/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 33.3494 - val_loss: 38.0148\n",
      "Epoch 389/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.1680 - val_loss: 38.0173\n",
      "Epoch 390/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 33.0261 - val_loss: 37.9164\n",
      "Epoch 391/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 33.2479 - val_loss: 37.9104\n",
      "Epoch 392/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.0983 - val_loss: 37.8411\n",
      "Epoch 393/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 33.0710 - val_loss: 38.1620\n",
      "Epoch 394/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.9162 - val_loss: 38.3391\n",
      "Epoch 395/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 33.0173 - val_loss: 38.0424\n",
      "Epoch 396/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 33.0144 - val_loss: 37.8858\n",
      "Epoch 397/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 33.0062 - val_loss: 37.8494\n",
      "Epoch 398/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 33.1886 - val_loss: 38.3825\n",
      "Epoch 399/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 32.9479 - val_loss: 38.0481\n",
      "Epoch 400/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.7731 - val_loss: 37.9072\n",
      "Epoch 401/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 33.2609 - val_loss: 37.7719\n",
      "Epoch 402/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.9764 - val_loss: 37.9432\n",
      "Epoch 403/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.9281 - val_loss: 37.8583\n",
      "Epoch 404/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 32.9156 - val_loss: 37.9211\n",
      "Epoch 405/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.8282 - val_loss: 37.8460\n",
      "Epoch 406/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.9874 - val_loss: 37.7509\n",
      "Epoch 407/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 32.8747 - val_loss: 37.6830\n",
      "Epoch 408/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 32.8711 - val_loss: 37.9272\n",
      "Epoch 409/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 32.8964 - val_loss: 38.0630\n",
      "Epoch 410/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 33.0087 - val_loss: 38.1507\n",
      "Epoch 411/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.7644 - val_loss: 38.0321\n",
      "Epoch 412/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 32.9102 - val_loss: 37.9712\n",
      "Epoch 413/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.8543 - val_loss: 37.9868\n",
      "Epoch 414/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 32.7625 - val_loss: 37.7058\n",
      "Epoch 415/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 33.1104 - val_loss: 37.7962\n",
      "Epoch 416/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.7559 - val_loss: 38.0970\n",
      "Epoch 417/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 32.8279 - val_loss: 38.1931\n",
      "Epoch 418/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 32.8127 - val_loss: 38.1153\n",
      "Epoch 419/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.8420 - val_loss: 37.9056\n",
      "Epoch 420/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.7963 - val_loss: 37.9881\n",
      "Epoch 421/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 32.6828 - val_loss: 37.9041\n",
      "Epoch 422/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 32.9900 - val_loss: 37.8426\n",
      "Epoch 423/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.8863 - val_loss: 38.1836\n",
      "Epoch 424/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.8786 - val_loss: 37.7629\n",
      "Epoch 425/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 32.9305 - val_loss: 37.8706\n",
      "Epoch 426/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 32.8681 - val_loss: 37.8321\n",
      "Epoch 427/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 32.8913 - val_loss: 38.0974\n",
      "Epoch 428/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.7061 - val_loss: 37.9781\n",
      "Epoch 429/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 32.7434 - val_loss: 38.0034\n",
      "Epoch 430/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 32.6101 - val_loss: 37.9293\n",
      "Epoch 431/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 32.6805 - val_loss: 37.7801\n",
      "Epoch 432/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.5753 - val_loss: 37.6439\n",
      "Epoch 433/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 32.5112 - val_loss: 37.7646\n",
      "Epoch 434/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 32.6241 - val_loss: 37.7976\n",
      "Epoch 435/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 32.5367 - val_loss: 37.8455\n",
      "Epoch 436/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 32.7118 - val_loss: 37.7399\n",
      "Epoch 437/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 32.5481 - val_loss: 37.8566\n",
      "Epoch 438/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 32.7492 - val_loss: 37.8789\n",
      "Epoch 439/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 32.5900 - val_loss: 37.8261\n",
      "Epoch 440/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 32.5016 - val_loss: 37.7450\n",
      "Epoch 441/2000\n",
      "378/378 [==============================] - 0s 143us/step - loss: 32.5129 - val_loss: 37.7473\n",
      "Epoch 442/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 32.4411 - val_loss: 37.7404\n",
      "Epoch 443/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.5947 - val_loss: 37.7795\n",
      "Epoch 444/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 32.5277 - val_loss: 37.9194\n",
      "Epoch 445/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 32.5298 - val_loss: 37.8955\n",
      "Epoch 446/2000\n",
      "378/378 [==============================] - 0s 103us/step - loss: 32.3716 - val_loss: 37.9518\n",
      "Epoch 447/2000\n",
      "378/378 [==============================] - 0s 103us/step - loss: 32.4125 - val_loss: 37.8647\n",
      "Epoch 448/2000\n",
      "378/378 [==============================] - 0s 105us/step - loss: 32.3214 - val_loss: 37.7699\n",
      "Epoch 449/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 32.5973 - val_loss: 37.8009\n",
      "Epoch 450/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 32.5090 - val_loss: 37.8132\n",
      "Epoch 451/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 32.3012 - val_loss: 37.6907\n",
      "Epoch 452/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.5175 - val_loss: 37.7819\n",
      "Epoch 453/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 32.4648 - val_loss: 37.6749\n",
      "Epoch 454/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 32.4388 - val_loss: 37.9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 455/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 32.3520 - val_loss: 37.8796\n",
      "Epoch 456/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 32.1631 - val_loss: 37.9454\n",
      "Epoch 457/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 32.3590 - val_loss: 38.0835\n",
      "Epoch 458/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 32.2162 - val_loss: 37.7275\n",
      "Epoch 459/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 32.2993 - val_loss: 37.9197\n",
      "Epoch 460/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 32.1705 - val_loss: 37.8438\n",
      "Epoch 461/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.1915 - val_loss: 37.7940\n",
      "Epoch 462/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 32.2231 - val_loss: 37.8691\n",
      "Epoch 463/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 32.4858 - val_loss: 37.9535\n",
      "Epoch 464/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 32.1614 - val_loss: 37.7692\n",
      "Epoch 465/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 32.3050 - val_loss: 37.7846\n",
      "Epoch 466/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 32.3199 - val_loss: 37.5750\n",
      "Epoch 467/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 32.3248 - val_loss: 37.7690\n",
      "Epoch 468/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.2141 - val_loss: 37.6250\n",
      "Epoch 469/2000\n",
      "378/378 [==============================] - 0s 106us/step - loss: 32.2827 - val_loss: 37.8391\n",
      "Epoch 470/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 31.9360 - val_loss: 37.7438\n",
      "Epoch 471/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.1885 - val_loss: 37.7566\n",
      "Epoch 472/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 32.0858 - val_loss: 37.7242\n",
      "Epoch 473/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 32.0645 - val_loss: 37.8342\n",
      "Epoch 474/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 32.2548 - val_loss: 37.8724\n",
      "Epoch 475/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.1519 - val_loss: 37.8396\n",
      "Epoch 476/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 32.3714 - val_loss: 37.8407\n",
      "Epoch 477/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 32.2600 - val_loss: 37.6117\n",
      "Epoch 478/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.9937 - val_loss: 37.7289\n",
      "Epoch 479/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 32.0754 - val_loss: 37.5160\n",
      "Epoch 480/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.9848 - val_loss: 37.6384\n",
      "Epoch 481/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 32.0524 - val_loss: 37.3802\n",
      "Epoch 482/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.8692 - val_loss: 37.4442\n",
      "Epoch 483/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 32.0488 - val_loss: 37.3831\n",
      "Epoch 484/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.9855 - val_loss: 37.6162\n",
      "Epoch 485/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 32.1605 - val_loss: 37.6849\n",
      "Epoch 486/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.9345 - val_loss: 37.6522\n",
      "Epoch 487/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.9906 - val_loss: 37.7853\n",
      "Epoch 488/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 32.0203 - val_loss: 37.5057\n",
      "Epoch 489/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 32.1566 - val_loss: 37.5102\n",
      "Epoch 490/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.8073 - val_loss: 37.5349\n",
      "Epoch 491/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 32.0807 - val_loss: 37.3063\n",
      "Epoch 492/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.8332 - val_loss: 37.3801\n",
      "Epoch 493/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.7514 - val_loss: 37.4519\n",
      "Epoch 494/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.9906 - val_loss: 37.5511\n",
      "Epoch 495/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.8962 - val_loss: 37.4718\n",
      "Epoch 496/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.8918 - val_loss: 37.3499\n",
      "Epoch 497/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.8557 - val_loss: 37.6453\n",
      "Epoch 498/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.7663 - val_loss: 37.6810\n",
      "Epoch 499/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 32.0368 - val_loss: 37.5109\n",
      "Epoch 500/2000\n",
      "378/378 [==============================] - 0s 104us/step - loss: 31.9986 - val_loss: 37.4048\n",
      "Epoch 501/2000\n",
      "378/378 [==============================] - 0s 105us/step - loss: 31.6002 - val_loss: 37.4145\n",
      "Epoch 502/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.8464 - val_loss: 37.3790\n",
      "Epoch 503/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.7610 - val_loss: 37.4582\n",
      "Epoch 504/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 31.6262 - val_loss: 37.4798\n",
      "Epoch 505/2000\n",
      "378/378 [==============================] - 0s 166us/step - loss: 32.0169 - val_loss: 37.3067\n",
      "Epoch 506/2000\n",
      "378/378 [==============================] - 0s 149us/step - loss: 31.7910 - val_loss: 37.3317\n",
      "Epoch 507/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 31.6419 - val_loss: 37.4037\n",
      "Epoch 508/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.7161 - val_loss: 37.3662\n",
      "Epoch 509/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.7843 - val_loss: 37.3124\n",
      "Epoch 510/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.8457 - val_loss: 37.2207\n",
      "Epoch 511/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.6882 - val_loss: 36.9916\n",
      "Epoch 512/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.6609 - val_loss: 37.3102\n",
      "Epoch 513/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.7167 - val_loss: 37.1203\n",
      "Epoch 514/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.6202 - val_loss: 37.4112\n",
      "Epoch 515/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.7552 - val_loss: 37.3198\n",
      "Epoch 516/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.8927 - val_loss: 37.2006\n",
      "Epoch 517/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.6655 - val_loss: 37.1406\n",
      "Epoch 518/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.7191 - val_loss: 37.1789\n",
      "Epoch 519/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.5951 - val_loss: 37.2959\n",
      "Epoch 520/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.6912 - val_loss: 37.1739\n",
      "Epoch 521/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.7255 - val_loss: 37.1738\n",
      "Epoch 522/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.7364 - val_loss: 37.4025\n",
      "Epoch 523/2000\n",
      "378/378 [==============================] - 0s 105us/step - loss: 31.5016 - val_loss: 37.3687\n",
      "Epoch 524/2000\n",
      "378/378 [==============================] - 0s 105us/step - loss: 31.5562 - val_loss: 37.2971\n",
      "Epoch 525/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 31.5758 - val_loss: 37.2934\n",
      "Epoch 526/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.5704 - val_loss: 37.1446\n",
      "Epoch 527/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.4757 - val_loss: 37.0558\n",
      "Epoch 528/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 31.3360 - val_loss: 37.0488\n",
      "Epoch 529/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.5062 - val_loss: 37.3091\n",
      "Epoch 530/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.3725 - val_loss: 37.3657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 531/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.4023 - val_loss: 37.4241\n",
      "Epoch 532/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.4472 - val_loss: 37.2714\n",
      "Epoch 533/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.4146 - val_loss: 37.0864\n",
      "Epoch 534/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.4727 - val_loss: 37.0891\n",
      "Epoch 535/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.5300 - val_loss: 36.8964\n",
      "Epoch 536/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.3550 - val_loss: 36.8546\n",
      "Epoch 537/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.5025 - val_loss: 36.8953\n",
      "Epoch 538/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.4148 - val_loss: 37.0014\n",
      "Epoch 539/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.5047 - val_loss: 36.9795\n",
      "Epoch 540/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.4105 - val_loss: 36.7491\n",
      "Epoch 541/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.2603 - val_loss: 36.9458\n",
      "Epoch 542/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.2975 - val_loss: 36.8727\n",
      "Epoch 543/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.4282 - val_loss: 36.8623\n",
      "Epoch 544/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.3282 - val_loss: 36.9713\n",
      "Epoch 545/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.3298 - val_loss: 37.0170\n",
      "Epoch 546/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.4815 - val_loss: 36.7264\n",
      "Epoch 547/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.3607 - val_loss: 36.9732\n",
      "Epoch 548/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.4985 - val_loss: 36.6843\n",
      "Epoch 549/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.4689 - val_loss: 36.9549\n",
      "Epoch 550/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 31.3545 - val_loss: 36.8843\n",
      "Epoch 551/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 31.2283 - val_loss: 37.0270\n",
      "Epoch 552/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 31.2851 - val_loss: 36.9629\n",
      "Epoch 553/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 31.1418 - val_loss: 36.8209\n",
      "Epoch 554/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.4132 - val_loss: 36.8337\n",
      "Epoch 555/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.2510 - val_loss: 36.6990\n",
      "Epoch 556/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.2344 - val_loss: 36.6463\n",
      "Epoch 557/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.4699 - val_loss: 36.7596\n",
      "Epoch 558/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.2752 - val_loss: 36.7611\n",
      "Epoch 559/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.3908 - val_loss: 36.7497\n",
      "Epoch 560/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.2734 - val_loss: 36.8373\n",
      "Epoch 561/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.2470 - val_loss: 36.9631\n",
      "Epoch 562/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.3681 - val_loss: 36.8867\n",
      "Epoch 563/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.3369 - val_loss: 36.8266\n",
      "Epoch 564/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.5065 - val_loss: 36.7332\n",
      "Epoch 565/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.3382 - val_loss: 36.4553\n",
      "Epoch 566/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.2890 - val_loss: 36.4629\n",
      "Epoch 567/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 31.4250 - val_loss: 36.4256\n",
      "Epoch 568/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.2735 - val_loss: 36.5911\n",
      "Epoch 569/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.1359 - val_loss: 36.8253\n",
      "Epoch 570/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.2528 - val_loss: 36.7704\n",
      "Epoch 571/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1569 - val_loss: 36.7023\n",
      "Epoch 572/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.4306 - val_loss: 36.7383\n",
      "Epoch 573/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.3145 - val_loss: 36.5696\n",
      "Epoch 574/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.1926 - val_loss: 36.5648\n",
      "Epoch 575/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.2555 - val_loss: 36.5056\n",
      "Epoch 576/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.2307 - val_loss: 36.4385\n",
      "Epoch 577/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.2718 - val_loss: 36.4982\n",
      "Epoch 578/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.2191 - val_loss: 36.4807\n",
      "Epoch 579/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.2799 - val_loss: 36.6603\n",
      "Epoch 580/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.1868 - val_loss: 36.6261\n",
      "Epoch 581/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.3146 - val_loss: 36.6223\n",
      "Epoch 582/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 31.1920 - val_loss: 36.6300\n",
      "Epoch 583/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1159 - val_loss: 36.6493\n",
      "Epoch 584/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.2820 - val_loss: 36.7424\n",
      "Epoch 585/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1725 - val_loss: 36.7739\n",
      "Epoch 586/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.3012 - val_loss: 36.7201\n",
      "Epoch 587/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.1007 - val_loss: 36.4525\n",
      "Epoch 588/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.2097 - val_loss: 36.4497\n",
      "Epoch 589/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.2516 - val_loss: 36.4401\n",
      "Epoch 590/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1089 - val_loss: 36.4913\n",
      "Epoch 591/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.2818 - val_loss: 36.5285\n",
      "Epoch 592/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.0962 - val_loss: 36.4990\n",
      "Epoch 593/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.2184 - val_loss: 36.4972\n",
      "Epoch 594/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1856 - val_loss: 36.5159\n",
      "Epoch 595/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1070 - val_loss: 36.4657\n",
      "Epoch 596/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.3760 - val_loss: 36.5178\n",
      "Epoch 597/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 31.2709 - val_loss: 36.4665\n",
      "Epoch 598/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.2786 - val_loss: 36.5345\n",
      "Epoch 599/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1462 - val_loss: 36.3208\n",
      "Epoch 600/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.1275 - val_loss: 36.3817\n",
      "Epoch 601/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.2985 - val_loss: 36.5405\n",
      "Epoch 602/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1169 - val_loss: 36.4278\n",
      "Epoch 603/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.0598 - val_loss: 36.3550\n",
      "Epoch 604/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.1079 - val_loss: 36.3639\n",
      "Epoch 605/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.3659 - val_loss: 36.4791\n",
      "Epoch 606/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1150 - val_loss: 36.3190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 607/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.0744 - val_loss: 36.4777\n",
      "Epoch 608/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1262 - val_loss: 36.5323\n",
      "Epoch 609/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.1881 - val_loss: 36.4327\n",
      "Epoch 610/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.1049 - val_loss: 36.3336\n",
      "Epoch 611/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.0743 - val_loss: 36.3091\n",
      "Epoch 612/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.3050 - val_loss: 36.4656\n",
      "Epoch 613/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1772 - val_loss: 36.4273\n",
      "Epoch 614/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.2357 - val_loss: 36.3166\n",
      "Epoch 615/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.1954 - val_loss: 36.3560\n",
      "Epoch 616/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.1944 - val_loss: 36.4276\n",
      "Epoch 617/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.2167 - val_loss: 36.5018\n",
      "Epoch 618/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1889 - val_loss: 36.3882\n",
      "Epoch 619/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1594 - val_loss: 36.3072\n",
      "Epoch 620/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.9941 - val_loss: 36.3672\n",
      "Epoch 621/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.0125 - val_loss: 36.6555\n",
      "Epoch 622/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.4827 - val_loss: 36.6546\n",
      "Epoch 623/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.0680 - val_loss: 36.3706\n",
      "Epoch 624/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.1460 - val_loss: 36.3337\n",
      "Epoch 625/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.0308 - val_loss: 36.5273\n",
      "Epoch 626/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.1906 - val_loss: 36.3348\n",
      "Epoch 627/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1622 - val_loss: 36.1725\n",
      "Epoch 628/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.0684 - val_loss: 36.3721\n",
      "Epoch 629/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.1171 - val_loss: 36.5119\n",
      "Epoch 630/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.9785 - val_loss: 36.3345\n",
      "Epoch 631/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.2257 - val_loss: 36.3975\n",
      "Epoch 632/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.9522 - val_loss: 36.4794\n",
      "Epoch 633/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.0507 - val_loss: 36.3818\n",
      "Epoch 634/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.0633 - val_loss: 36.3885\n",
      "Epoch 635/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.1734 - val_loss: 36.1681\n",
      "Epoch 636/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.1170 - val_loss: 36.2173\n",
      "Epoch 637/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.9895 - val_loss: 36.3469\n",
      "Epoch 638/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.0790 - val_loss: 36.4432\n",
      "Epoch 639/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.0054 - val_loss: 36.5935\n",
      "Epoch 640/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.8991 - val_loss: 36.4780\n",
      "Epoch 641/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.1526 - val_loss: 36.1821\n",
      "Epoch 642/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.0836 - val_loss: 36.0233\n",
      "Epoch 643/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.0704 - val_loss: 36.2926\n",
      "Epoch 644/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 31.0038 - val_loss: 36.2607\n",
      "Epoch 645/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.1761 - val_loss: 36.3407\n",
      "Epoch 646/2000\n",
      "378/378 [==============================] - 0s 106us/step - loss: 30.9681 - val_loss: 36.4835\n",
      "Epoch 647/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.0953 - val_loss: 36.2985\n",
      "Epoch 648/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.9816 - val_loss: 36.1387\n",
      "Epoch 649/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.9975 - val_loss: 36.1427\n",
      "Epoch 650/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.1807 - val_loss: 36.2924\n",
      "Epoch 651/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 31.0182 - val_loss: 36.4059\n",
      "Epoch 652/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.9734 - val_loss: 36.2106\n",
      "Epoch 653/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 31.1520 - val_loss: 36.3540\n",
      "Epoch 654/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.9924 - val_loss: 36.3088\n",
      "Epoch 655/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.0542 - val_loss: 36.3454\n",
      "Epoch 656/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.1181 - val_loss: 36.2239\n",
      "Epoch 657/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.1368 - val_loss: 36.3110\n",
      "Epoch 658/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 31.1017 - val_loss: 36.3659\n",
      "Epoch 659/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.0386 - val_loss: 36.2763\n",
      "Epoch 660/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.0118 - val_loss: 36.3301\n",
      "Epoch 661/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.9334 - val_loss: 36.2434\n",
      "Epoch 662/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.0387 - val_loss: 36.1082\n",
      "Epoch 663/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.0642 - val_loss: 36.2370\n",
      "Epoch 664/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 30.9776 - val_loss: 36.5148\n",
      "Epoch 665/2000\n",
      "378/378 [==============================] - 0s 190us/step - loss: 30.9341 - val_loss: 36.5356\n",
      "Epoch 666/2000\n",
      "378/378 [==============================] - 0s 174us/step - loss: 31.0480 - val_loss: 36.4639\n",
      "Epoch 667/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 30.9863 - val_loss: 36.2858\n",
      "Epoch 668/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.9352 - val_loss: 36.4530\n",
      "Epoch 669/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.0171 - val_loss: 36.5467\n",
      "Epoch 670/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.0070 - val_loss: 36.5533\n",
      "Epoch 671/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.9766 - val_loss: 36.4127\n",
      "Epoch 672/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8998 - val_loss: 36.3162\n",
      "Epoch 673/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.1024 - val_loss: 36.2327\n",
      "Epoch 674/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 31.0052 - val_loss: 36.2301\n",
      "Epoch 675/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.2149 - val_loss: 36.1488\n",
      "Epoch 676/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.0934 - val_loss: 36.3791\n",
      "Epoch 677/2000\n",
      "378/378 [==============================] - 0s 103us/step - loss: 31.0317 - val_loss: 36.3904\n",
      "Epoch 678/2000\n",
      "378/378 [==============================] - 0s 101us/step - loss: 30.8342 - val_loss: 36.4915\n",
      "Epoch 679/2000\n",
      "378/378 [==============================] - 0s 105us/step - loss: 31.0834 - val_loss: 36.4240\n",
      "Epoch 680/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.1207 - val_loss: 36.3048\n",
      "Epoch 681/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.9147 - val_loss: 36.2419\n",
      "Epoch 682/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.1105 - val_loss: 36.4410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 683/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 31.2200 - val_loss: 36.6077\n",
      "Epoch 684/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.2053 - val_loss: 36.5472\n",
      "Epoch 685/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.0086 - val_loss: 36.3885\n",
      "Epoch 686/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.9032 - val_loss: 36.2560\n",
      "Epoch 687/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.8135 - val_loss: 36.4512\n",
      "Epoch 688/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 31.2092 - val_loss: 36.4272\n",
      "Epoch 689/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.9211 - val_loss: 36.0686\n",
      "Epoch 690/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 31.0590 - val_loss: 36.2091\n",
      "Epoch 691/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.9953 - val_loss: 36.1175\n",
      "Epoch 692/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 30.9234 - val_loss: 36.2005\n",
      "Epoch 693/2000\n",
      "378/378 [==============================] - 0s 162us/step - loss: 30.9105 - val_loss: 36.2301\n",
      "Epoch 694/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 30.9022 - val_loss: 36.2918\n",
      "Epoch 695/2000\n",
      "378/378 [==============================] - 0s 156us/step - loss: 30.9819 - val_loss: 36.2081\n",
      "Epoch 696/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 30.9837 - val_loss: 36.1217\n",
      "Epoch 697/2000\n",
      "378/378 [==============================] - 0s 151us/step - loss: 30.8243 - val_loss: 36.0638\n",
      "Epoch 698/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 31.0222 - val_loss: 36.2567\n",
      "Epoch 699/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 30.9977 - val_loss: 36.1672\n",
      "Epoch 700/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 30.9388 - val_loss: 36.1952\n",
      "Epoch 701/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 31.0514 - val_loss: 36.1693\n",
      "Epoch 702/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 31.0560 - val_loss: 36.3318\n",
      "Epoch 703/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 31.0205 - val_loss: 36.3071\n",
      "Epoch 704/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.8613 - val_loss: 36.3364\n",
      "Epoch 705/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 30.9359 - val_loss: 36.4235\n",
      "Epoch 706/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 30.9445 - val_loss: 36.5585\n",
      "Epoch 707/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 30.8469 - val_loss: 36.3555\n",
      "Epoch 708/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 31.0947 - val_loss: 36.2536\n",
      "Epoch 709/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 31.0253 - val_loss: 36.2111\n",
      "Epoch 710/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 30.8836 - val_loss: 36.2814\n",
      "Epoch 711/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 30.9704 - val_loss: 36.2346\n",
      "Epoch 712/2000\n",
      "378/378 [==============================] - 0s 151us/step - loss: 30.9833 - val_loss: 36.1779\n",
      "Epoch 713/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 31.0955 - val_loss: 36.3288\n",
      "Epoch 714/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 30.8149 - val_loss: 36.3397\n",
      "Epoch 715/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 30.9540 - val_loss: 36.4022\n",
      "Epoch 716/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 31.1279 - val_loss: 36.1295\n",
      "Epoch 717/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.1189 - val_loss: 36.0989\n",
      "Epoch 718/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.9446 - val_loss: 36.3030\n",
      "Epoch 719/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.0090 - val_loss: 36.2076\n",
      "Epoch 720/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 31.0235 - val_loss: 36.0992\n",
      "Epoch 721/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.8430 - val_loss: 36.0837\n",
      "Epoch 722/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.8835 - val_loss: 36.1432\n",
      "Epoch 723/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.8994 - val_loss: 36.1846\n",
      "Epoch 724/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.7712 - val_loss: 36.3558\n",
      "Epoch 725/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 30.9390 - val_loss: 36.2757\n",
      "Epoch 726/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 30.9931 - val_loss: 36.3266\n",
      "Epoch 727/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 31.0470 - val_loss: 36.2311\n",
      "Epoch 728/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 30.8091 - val_loss: 36.1584\n",
      "Epoch 729/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 30.9224 - val_loss: 36.2199\n",
      "Epoch 730/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 30.7911 - val_loss: 36.1161\n",
      "Epoch 731/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 31.2012 - val_loss: 36.2041\n",
      "Epoch 732/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 31.0147 - val_loss: 36.4701\n",
      "Epoch 733/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 30.9944 - val_loss: 36.5124\n",
      "Epoch 734/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 31.0011 - val_loss: 35.8702\n",
      "Epoch 735/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.8398 - val_loss: 35.9582\n",
      "Epoch 736/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 31.0540 - val_loss: 36.1190\n",
      "Epoch 737/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.9091 - val_loss: 36.0258\n",
      "Epoch 738/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.9467 - val_loss: 36.1289\n",
      "Epoch 739/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.8786 - val_loss: 36.1456\n",
      "Epoch 740/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.7390 - val_loss: 36.0921\n",
      "Epoch 741/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.9020 - val_loss: 36.0297\n",
      "Epoch 742/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.7769 - val_loss: 36.1624\n",
      "Epoch 743/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 31.0209 - val_loss: 36.2821\n",
      "Epoch 744/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.8184 - val_loss: 35.9280\n",
      "Epoch 745/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8155 - val_loss: 35.9452\n",
      "Epoch 746/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.8641 - val_loss: 36.1470\n",
      "Epoch 747/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8162 - val_loss: 36.1513\n",
      "Epoch 748/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.9941 - val_loss: 36.1311\n",
      "Epoch 749/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.8894 - val_loss: 36.2892\n",
      "Epoch 750/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.9727 - val_loss: 36.3212\n",
      "Epoch 751/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 30.8325 - val_loss: 36.1122\n",
      "Epoch 752/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8768 - val_loss: 36.0491\n",
      "Epoch 753/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.9115 - val_loss: 36.1169\n",
      "Epoch 754/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.9652 - val_loss: 36.3194\n",
      "Epoch 755/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.7874 - val_loss: 36.2462\n",
      "Epoch 756/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.8088 - val_loss: 36.1460\n",
      "Epoch 757/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 30.8461 - val_loss: 36.2022\n",
      "Epoch 758/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.9424 - val_loss: 36.0855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 759/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 31.0069 - val_loss: 36.0984\n",
      "Epoch 760/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8844 - val_loss: 36.1415\n",
      "Epoch 761/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.8371 - val_loss: 36.0434\n",
      "Epoch 762/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 31.0055 - val_loss: 35.9936\n",
      "Epoch 763/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.7428 - val_loss: 36.1807\n",
      "Epoch 764/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.7038 - val_loss: 36.2827\n",
      "Epoch 765/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.6802 - val_loss: 36.1062\n",
      "Epoch 766/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.7323 - val_loss: 36.1028\n",
      "Epoch 767/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.9318 - val_loss: 36.1575\n",
      "Epoch 768/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.9862 - val_loss: 36.0597\n",
      "Epoch 769/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 30.7594 - val_loss: 35.9916\n",
      "Epoch 770/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 30.9256 - val_loss: 35.9949\n",
      "Epoch 771/2000\n",
      "378/378 [==============================] - 0s 154us/step - loss: 30.7005 - val_loss: 36.3400\n",
      "Epoch 772/2000\n",
      "378/378 [==============================] - 0s 161us/step - loss: 30.8826 - val_loss: 36.1515\n",
      "Epoch 773/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 30.8577 - val_loss: 35.9821\n",
      "Epoch 774/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 30.8519 - val_loss: 35.9044\n",
      "Epoch 775/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.7166 - val_loss: 36.2492\n",
      "Epoch 776/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.7567 - val_loss: 36.1456\n",
      "Epoch 777/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8290 - val_loss: 36.0677\n",
      "Epoch 778/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.7130 - val_loss: 36.2338\n",
      "Epoch 779/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.7763 - val_loss: 36.4844\n",
      "Epoch 780/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.7520 - val_loss: 36.2980\n",
      "Epoch 781/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.7789 - val_loss: 36.1473\n",
      "Epoch 782/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.9549 - val_loss: 36.2556\n",
      "Epoch 783/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.7633 - val_loss: 36.4886\n",
      "Epoch 784/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.8848 - val_loss: 36.2345\n",
      "Epoch 785/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.8283 - val_loss: 36.3495\n",
      "Epoch 786/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.7703 - val_loss: 36.6544\n",
      "Epoch 787/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.7576 - val_loss: 36.4341\n",
      "Epoch 788/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.9714 - val_loss: 35.9552\n",
      "Epoch 789/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.7733 - val_loss: 35.8109\n",
      "Epoch 790/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.8577 - val_loss: 35.8902\n",
      "Epoch 791/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.8726 - val_loss: 36.1528\n",
      "Epoch 792/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.8106 - val_loss: 36.0826\n",
      "Epoch 793/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.7827 - val_loss: 35.9077\n",
      "Epoch 794/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.8973 - val_loss: 36.0087\n",
      "Epoch 795/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.8813 - val_loss: 35.9472\n",
      "Epoch 796/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 31.0317 - val_loss: 35.8029\n",
      "Epoch 797/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.8227 - val_loss: 36.0278\n",
      "Epoch 798/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.8687 - val_loss: 36.0096\n",
      "Epoch 799/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.9779 - val_loss: 35.7939\n",
      "Epoch 800/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8075 - val_loss: 35.8321\n",
      "Epoch 801/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.8223 - val_loss: 36.4881\n",
      "Epoch 802/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.9607 - val_loss: 35.9705\n",
      "Epoch 803/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.8029 - val_loss: 36.0722\n",
      "Epoch 804/2000\n",
      "378/378 [==============================] - 0s 105us/step - loss: 30.8513 - val_loss: 36.0754\n",
      "Epoch 805/2000\n",
      "378/378 [==============================] - 0s 106us/step - loss: 30.8958 - val_loss: 36.3131\n",
      "Epoch 806/2000\n",
      "378/378 [==============================] - 0s 100us/step - loss: 30.8312 - val_loss: 36.4941\n",
      "Epoch 807/2000\n",
      "378/378 [==============================] - 0s 101us/step - loss: 30.8137 - val_loss: 36.3558\n",
      "Epoch 808/2000\n",
      "378/378 [==============================] - 0s 100us/step - loss: 30.8503 - val_loss: 36.3661\n",
      "Epoch 809/2000\n",
      "378/378 [==============================] - 0s 103us/step - loss: 30.7468 - val_loss: 36.3043\n",
      "Epoch 810/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.9385 - val_loss: 36.2481\n",
      "Epoch 811/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.7776 - val_loss: 36.1208\n",
      "Epoch 812/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.6482 - val_loss: 36.3688\n",
      "Epoch 813/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.7266 - val_loss: 36.2520\n",
      "Epoch 814/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.7073 - val_loss: 36.3430\n",
      "Epoch 815/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.8378 - val_loss: 36.2981\n",
      "Epoch 816/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.6035 - val_loss: 36.1743\n",
      "Epoch 817/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.7233 - val_loss: 36.2581\n",
      "Epoch 818/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.5394 - val_loss: 36.3208\n",
      "Epoch 819/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6511 - val_loss: 36.4471\n",
      "Epoch 820/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.7870 - val_loss: 36.3585\n",
      "Epoch 821/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.8641 - val_loss: 36.2584\n",
      "Epoch 822/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.6456 - val_loss: 36.1744\n",
      "Epoch 823/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.7921 - val_loss: 36.2398\n",
      "Epoch 824/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.8494 - val_loss: 36.1359\n",
      "Epoch 825/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.9325 - val_loss: 36.1725\n",
      "Epoch 826/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.7962 - val_loss: 36.2533\n",
      "Epoch 827/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.7193 - val_loss: 36.0835\n",
      "Epoch 828/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.8535 - val_loss: 36.1688\n",
      "Epoch 829/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.7571 - val_loss: 36.6457\n",
      "Epoch 830/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.8960 - val_loss: 36.2724\n",
      "Epoch 831/2000\n",
      "378/378 [==============================] - 0s 105us/step - loss: 30.9331 - val_loss: 36.5215\n",
      "Epoch 832/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.7140 - val_loss: 36.1090\n",
      "Epoch 833/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.6790 - val_loss: 36.1798\n",
      "Epoch 834/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.8294 - val_loss: 36.0879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 835/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.8629 - val_loss: 36.0308\n",
      "Epoch 836/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.8009 - val_loss: 36.1754\n",
      "Epoch 837/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.7165 - val_loss: 36.2331\n",
      "Epoch 838/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.8227 - val_loss: 36.0632\n",
      "Epoch 839/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.8308 - val_loss: 35.9781\n",
      "Epoch 840/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 30.7484 - val_loss: 36.0861\n",
      "Epoch 841/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.8463 - val_loss: 36.0278\n",
      "Epoch 842/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.5488 - val_loss: 36.3683\n",
      "Epoch 843/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6390 - val_loss: 36.2708\n",
      "Epoch 844/2000\n",
      "378/378 [==============================] - 0s 106us/step - loss: 30.7182 - val_loss: 36.2202\n",
      "Epoch 845/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 30.5364 - val_loss: 36.3092\n",
      "Epoch 846/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 30.7046 - val_loss: 36.1331\n",
      "Epoch 847/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.6167 - val_loss: 36.2544\n",
      "Epoch 848/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.8214 - val_loss: 36.2834\n",
      "Epoch 849/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 30.8607 - val_loss: 36.3578\n",
      "Epoch 850/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.7392 - val_loss: 36.2682\n",
      "Epoch 851/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.6192 - val_loss: 35.9989\n",
      "Epoch 852/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.6408 - val_loss: 36.3178\n",
      "Epoch 853/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.7691 - val_loss: 36.2528\n",
      "Epoch 854/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.5726 - val_loss: 36.1523\n",
      "Epoch 855/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6983 - val_loss: 36.0034\n",
      "Epoch 856/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6103 - val_loss: 36.0942\n",
      "Epoch 857/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.9467 - val_loss: 35.9712\n",
      "Epoch 858/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.5761 - val_loss: 35.9669\n",
      "Epoch 859/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 30.4883 - val_loss: 36.0101\n",
      "Epoch 860/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.6699 - val_loss: 36.0425\n",
      "Epoch 861/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.5896 - val_loss: 36.0075\n",
      "Epoch 862/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.5999 - val_loss: 36.0525\n",
      "Epoch 863/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.6735 - val_loss: 36.0665\n",
      "Epoch 864/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.5990 - val_loss: 36.0830\n",
      "Epoch 865/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.7666 - val_loss: 35.9820\n",
      "Epoch 866/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.7129 - val_loss: 36.1327\n",
      "Epoch 867/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.5736 - val_loss: 35.9775\n",
      "Epoch 868/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6821 - val_loss: 36.0566\n",
      "Epoch 869/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.7084 - val_loss: 35.9282\n",
      "Epoch 870/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6003 - val_loss: 35.8887\n",
      "Epoch 871/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 30.4470 - val_loss: 35.9834\n",
      "Epoch 872/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.5878 - val_loss: 36.1845\n",
      "Epoch 873/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 30.7173 - val_loss: 36.1304\n",
      "Epoch 874/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 30.6780 - val_loss: 35.9151\n",
      "Epoch 875/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 30.6817 - val_loss: 36.3084\n",
      "Epoch 876/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 30.6246 - val_loss: 36.4460\n",
      "Epoch 877/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.5897 - val_loss: 36.1711\n",
      "Epoch 878/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.4906 - val_loss: 35.9608\n",
      "Epoch 879/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.5784 - val_loss: 36.1793\n",
      "Epoch 880/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.4364 - val_loss: 36.1512\n",
      "Epoch 881/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 30.6065 - val_loss: 35.9028\n",
      "Epoch 882/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 30.6770 - val_loss: 36.0778\n",
      "Epoch 883/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6921 - val_loss: 35.8392\n",
      "Epoch 884/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.7559 - val_loss: 35.9871\n",
      "Epoch 885/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.5932 - val_loss: 36.1345\n",
      "Epoch 886/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.4889 - val_loss: 36.3219\n",
      "Epoch 887/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.5922 - val_loss: 36.3342\n",
      "Epoch 888/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.5041 - val_loss: 36.1133\n",
      "Epoch 889/2000\n",
      "378/378 [==============================] - 0s 100us/step - loss: 30.5508 - val_loss: 36.0958\n",
      "Epoch 890/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 30.6039 - val_loss: 36.0191\n",
      "Epoch 891/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.6196 - val_loss: 36.1135\n",
      "Epoch 892/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.6520 - val_loss: 35.9608\n",
      "Epoch 893/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.7904 - val_loss: 35.9567\n",
      "Epoch 894/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.5735 - val_loss: 36.2133\n",
      "Epoch 895/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 30.5978 - val_loss: 36.1217\n",
      "Epoch 896/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.6145 - val_loss: 36.0104\n",
      "Epoch 897/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.5702 - val_loss: 36.0382\n",
      "Epoch 898/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.5606 - val_loss: 36.1549\n",
      "Epoch 899/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.7035 - val_loss: 36.1052\n",
      "Epoch 900/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.5455 - val_loss: 36.0295\n",
      "Epoch 901/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.5718 - val_loss: 36.0965\n",
      "Epoch 902/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.7164 - val_loss: 35.9509\n",
      "Epoch 903/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.6063 - val_loss: 35.9825\n",
      "Epoch 904/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 30.7109 - val_loss: 36.0315\n",
      "Epoch 905/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 30.8170 - val_loss: 35.9113\n",
      "Epoch 906/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 30.5212 - val_loss: 35.7224\n",
      "Epoch 907/2000\n",
      "378/378 [==============================] - 0s 153us/step - loss: 30.5492 - val_loss: 35.8986\n",
      "Epoch 908/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 30.4068 - val_loss: 35.9920\n",
      "Epoch 909/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 30.3268 - val_loss: 36.0348\n",
      "Epoch 910/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 30.4050 - val_loss: 36.0498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 911/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.4247 - val_loss: 36.0255\n",
      "Epoch 912/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.5562 - val_loss: 35.8376\n",
      "Epoch 913/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 30.6664 - val_loss: 35.8999\n",
      "Epoch 914/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.3639 - val_loss: 36.1728\n",
      "Epoch 915/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 30.5672 - val_loss: 36.1318\n",
      "Epoch 916/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.5286 - val_loss: 35.8576\n",
      "Epoch 917/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.5091 - val_loss: 35.7203\n",
      "Epoch 918/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.5677 - val_loss: 35.8254\n",
      "Epoch 919/2000\n",
      "378/378 [==============================] - 0s 157us/step - loss: 30.5649 - val_loss: 35.9296\n",
      "Epoch 920/2000\n",
      "378/378 [==============================] - 0s 143us/step - loss: 30.4093 - val_loss: 35.6680\n",
      "Epoch 921/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 30.5588 - val_loss: 35.8301\n",
      "Epoch 922/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 30.6319 - val_loss: 35.9247\n",
      "Epoch 923/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 30.4312 - val_loss: 35.8465\n",
      "Epoch 924/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.4840 - val_loss: 35.7701\n",
      "Epoch 925/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.7595 - val_loss: 35.7954\n",
      "Epoch 926/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 30.4399 - val_loss: 36.1286\n",
      "Epoch 927/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 30.7581 - val_loss: 36.1667\n",
      "Epoch 928/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.7669 - val_loss: 35.8017\n",
      "Epoch 929/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.3753 - val_loss: 35.9584\n",
      "Epoch 930/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.4540 - val_loss: 35.9625\n",
      "Epoch 931/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 30.4413 - val_loss: 35.9789\n",
      "Epoch 932/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 30.5830 - val_loss: 35.8279\n",
      "Epoch 933/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 30.5533 - val_loss: 36.1358\n",
      "Epoch 934/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 30.5271 - val_loss: 35.9022\n",
      "Epoch 935/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.5305 - val_loss: 36.0408\n",
      "Epoch 936/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 30.6697 - val_loss: 36.1319\n",
      "Epoch 937/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 30.5108 - val_loss: 35.9861\n",
      "Epoch 938/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.4943 - val_loss: 35.8338\n",
      "Epoch 939/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.4384 - val_loss: 35.8472\n",
      "Epoch 940/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.3402 - val_loss: 35.9566\n",
      "Epoch 941/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 30.4685 - val_loss: 35.9951\n",
      "Epoch 942/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 30.4025 - val_loss: 35.8893\n",
      "Epoch 943/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 30.5239 - val_loss: 35.9410\n",
      "Epoch 944/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.5085 - val_loss: 35.7763\n",
      "Epoch 945/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.4403 - val_loss: 35.7349\n",
      "Epoch 946/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.4360 - val_loss: 35.9915\n",
      "Epoch 947/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.5539 - val_loss: 36.0144\n",
      "Epoch 948/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.5407 - val_loss: 36.0044\n",
      "Epoch 949/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4262 - val_loss: 36.0085\n",
      "Epoch 950/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.6958 - val_loss: 35.8807\n",
      "Epoch 951/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.5169 - val_loss: 35.7707\n",
      "Epoch 952/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.6423 - val_loss: 36.0222\n",
      "Epoch 953/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.6742 - val_loss: 35.9389\n",
      "Epoch 954/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.5427 - val_loss: 35.7244\n",
      "Epoch 955/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.7462 - val_loss: 36.0924\n",
      "Epoch 956/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.4055 - val_loss: 35.6913\n",
      "Epoch 957/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.6922 - val_loss: 36.0595\n",
      "Epoch 958/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.4495 - val_loss: 36.0803\n",
      "Epoch 959/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.5001 - val_loss: 35.9970\n",
      "Epoch 960/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.6333 - val_loss: 35.8547\n",
      "Epoch 961/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2703 - val_loss: 36.0069\n",
      "Epoch 962/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4609 - val_loss: 36.0428\n",
      "Epoch 963/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3551 - val_loss: 35.7946\n",
      "Epoch 964/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4677 - val_loss: 35.8670\n",
      "Epoch 965/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.3833 - val_loss: 35.7817\n",
      "Epoch 966/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.5040 - val_loss: 35.8953\n",
      "Epoch 967/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.7119 - val_loss: 35.8472\n",
      "Epoch 968/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3747 - val_loss: 36.0154\n",
      "Epoch 969/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.6764 - val_loss: 35.9296\n",
      "Epoch 970/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3529 - val_loss: 35.9770\n",
      "Epoch 971/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.4182 - val_loss: 35.9869\n",
      "Epoch 972/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.4273 - val_loss: 35.9057\n",
      "Epoch 973/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.3482 - val_loss: 35.7917\n",
      "Epoch 974/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3748 - val_loss: 35.8401\n",
      "Epoch 975/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.3620 - val_loss: 35.8341\n",
      "Epoch 976/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.4823 - val_loss: 35.7229\n",
      "Epoch 977/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.2564 - val_loss: 35.9330\n",
      "Epoch 978/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3323 - val_loss: 35.7100\n",
      "Epoch 979/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3882 - val_loss: 35.6670\n",
      "Epoch 980/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3533 - val_loss: 35.7140\n",
      "Epoch 981/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.3452 - val_loss: 35.9088\n",
      "Epoch 982/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3647 - val_loss: 35.8844\n",
      "Epoch 983/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.7069 - val_loss: 35.7972\n",
      "Epoch 984/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2051 - val_loss: 35.8403\n",
      "Epoch 985/2000\n",
      "378/378 [==============================] - 0s 103us/step - loss: 30.3325 - val_loss: 35.7367\n",
      "Epoch 986/2000\n",
      "378/378 [==============================] - 0s 102us/step - loss: 30.4936 - val_loss: 35.7683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 987/2000\n",
      "378/378 [==============================] - 0s 101us/step - loss: 30.4254 - val_loss: 35.7988\n",
      "Epoch 988/2000\n",
      "378/378 [==============================] - 0s 104us/step - loss: 30.3659 - val_loss: 35.9558\n",
      "Epoch 989/2000\n",
      "378/378 [==============================] - 0s 100us/step - loss: 30.4095 - val_loss: 36.0691\n",
      "Epoch 990/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.3382 - val_loss: 36.1907\n",
      "Epoch 991/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.3346 - val_loss: 35.9299\n",
      "Epoch 992/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2734 - val_loss: 36.1326\n",
      "Epoch 993/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.3160 - val_loss: 36.0107\n",
      "Epoch 994/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2873 - val_loss: 36.0743\n",
      "Epoch 995/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.3526 - val_loss: 35.9887\n",
      "Epoch 996/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 30.3438 - val_loss: 35.9714\n",
      "Epoch 997/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.2852 - val_loss: 36.2347\n",
      "Epoch 998/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 30.3938 - val_loss: 36.0808\n",
      "Epoch 999/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.7040 - val_loss: 35.9054\n",
      "Epoch 1000/2000\n",
      "378/378 [==============================] - ETA: 0s - loss: 31.33 - 0s 112us/step - loss: 30.5652 - val_loss: 35.9202\n",
      "Epoch 1001/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.5774 - val_loss: 36.1069\n",
      "Epoch 1002/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.5031 - val_loss: 35.9088\n",
      "Epoch 1003/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4435 - val_loss: 35.9297\n",
      "Epoch 1004/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.4263 - val_loss: 35.9888\n",
      "Epoch 1005/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4204 - val_loss: 36.0309\n",
      "Epoch 1006/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.3864 - val_loss: 36.1339\n",
      "Epoch 1007/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.4472 - val_loss: 35.9009\n",
      "Epoch 1008/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.4490 - val_loss: 36.0172\n",
      "Epoch 1009/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.4280 - val_loss: 36.0748\n",
      "Epoch 1010/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.3243 - val_loss: 36.2235\n",
      "Epoch 1011/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.3210 - val_loss: 36.1826\n",
      "Epoch 1012/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.4426 - val_loss: 36.0691\n",
      "Epoch 1013/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.5910 - val_loss: 36.0568\n",
      "Epoch 1014/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.5120 - val_loss: 36.0394\n",
      "Epoch 1015/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.5731 - val_loss: 36.1112\n",
      "Epoch 1016/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.4517 - val_loss: 36.1146\n",
      "Epoch 1017/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.3633 - val_loss: 36.2165\n",
      "Epoch 1018/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.5227 - val_loss: 36.0612\n",
      "Epoch 1019/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.5281 - val_loss: 36.2142\n",
      "Epoch 1020/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 30.3026 - val_loss: 36.1507\n",
      "Epoch 1021/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.5524 - val_loss: 36.4740\n",
      "Epoch 1022/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4299 - val_loss: 36.1699\n",
      "Epoch 1023/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.3891 - val_loss: 36.2517\n",
      "Epoch 1024/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.3731 - val_loss: 36.2370\n",
      "Epoch 1025/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.4844 - val_loss: 35.9806\n",
      "Epoch 1026/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.3886 - val_loss: 36.2666\n",
      "Epoch 1027/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4197 - val_loss: 36.0491\n",
      "Epoch 1028/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.2821 - val_loss: 35.9322\n",
      "Epoch 1029/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.4477 - val_loss: 36.0738\n",
      "Epoch 1030/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.4712 - val_loss: 36.2422\n",
      "Epoch 1031/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.4070 - val_loss: 36.0601\n",
      "Epoch 1032/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.3710 - val_loss: 36.2344\n",
      "Epoch 1033/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.2516 - val_loss: 36.0501\n",
      "Epoch 1034/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2887 - val_loss: 35.9900\n",
      "Epoch 1035/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3624 - val_loss: 36.0578\n",
      "Epoch 1036/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.3528 - val_loss: 36.1336\n",
      "Epoch 1037/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.3631 - val_loss: 36.0178\n",
      "Epoch 1038/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.5119 - val_loss: 36.1246\n",
      "Epoch 1039/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.3468 - val_loss: 35.9586\n",
      "Epoch 1040/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 30.3571 - val_loss: 36.1577\n",
      "Epoch 1041/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.4269 - val_loss: 35.8675\n",
      "Epoch 1042/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.4884 - val_loss: 36.0202\n",
      "Epoch 1043/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.2980 - val_loss: 35.8622\n",
      "Epoch 1044/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.5383 - val_loss: 35.8188\n",
      "Epoch 1045/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.5329 - val_loss: 35.7959\n",
      "Epoch 1046/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.2536 - val_loss: 35.8976\n",
      "Epoch 1047/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.4326 - val_loss: 36.3011\n",
      "Epoch 1048/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2150 - val_loss: 36.1225\n",
      "Epoch 1049/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.4528 - val_loss: 36.1499\n",
      "Epoch 1050/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.4139 - val_loss: 35.8576\n",
      "Epoch 1051/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.3863 - val_loss: 36.0392\n",
      "Epoch 1052/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2390 - val_loss: 36.0182\n",
      "Epoch 1053/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2975 - val_loss: 35.9707\n",
      "Epoch 1054/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.4273 - val_loss: 35.8979\n",
      "Epoch 1055/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.3513 - val_loss: 35.6716\n",
      "Epoch 1056/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.3061 - val_loss: 35.8495\n",
      "Epoch 1057/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.1836 - val_loss: 35.9793\n",
      "Epoch 1058/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.3431 - val_loss: 35.7211\n",
      "Epoch 1059/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.3862 - val_loss: 35.6418\n",
      "Epoch 1060/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2774 - val_loss: 35.7610\n",
      "Epoch 1061/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.4161 - val_loss: 35.8568\n",
      "Epoch 1062/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 113us/step - loss: 30.3846 - val_loss: 35.8625\n",
      "Epoch 1063/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.3139 - val_loss: 36.0404\n",
      "Epoch 1064/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 30.4506 - val_loss: 35.8590\n",
      "Epoch 1065/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 30.4192 - val_loss: 35.9306\n",
      "Epoch 1066/2000\n",
      "378/378 [==============================] - 0s 166us/step - loss: 30.6471 - val_loss: 35.8281\n",
      "Epoch 1067/2000\n",
      "378/378 [==============================] - 0s 154us/step - loss: 30.7329 - val_loss: 35.6259\n",
      "Epoch 1068/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.3736 - val_loss: 35.8639\n",
      "Epoch 1069/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.2858 - val_loss: 35.9289\n",
      "Epoch 1070/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 30.4640 - val_loss: 35.7302\n",
      "Epoch 1071/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 30.3018 - val_loss: 35.9673\n",
      "Epoch 1072/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.3142 - val_loss: 35.6552\n",
      "Epoch 1073/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 30.4567 - val_loss: 35.9532\n",
      "Epoch 1074/2000\n",
      "378/378 [==============================] - 0s 151us/step - loss: 30.4098 - val_loss: 35.6265\n",
      "Epoch 1075/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.2956 - val_loss: 35.7973\n",
      "Epoch 1076/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 30.4095 - val_loss: 35.9381\n",
      "Epoch 1077/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.3084 - val_loss: 35.9152\n",
      "Epoch 1078/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 30.3683 - val_loss: 36.0147\n",
      "Epoch 1079/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.3407 - val_loss: 36.2169\n",
      "Epoch 1080/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.3418 - val_loss: 35.7813\n",
      "Epoch 1081/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 30.3811 - val_loss: 35.7907\n",
      "Epoch 1082/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 30.3146 - val_loss: 36.1176\n",
      "Epoch 1083/2000\n",
      "378/378 [==============================] - 0s 143us/step - loss: 30.5484 - val_loss: 35.8993\n",
      "Epoch 1084/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 30.4519 - val_loss: 35.8178\n",
      "Epoch 1085/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.3357 - val_loss: 35.9838\n",
      "Epoch 1086/2000\n",
      "378/378 [==============================] - 0s 147us/step - loss: 30.3352 - val_loss: 35.8862\n",
      "Epoch 1087/2000\n",
      "378/378 [==============================] - 0s 169us/step - loss: 30.4405 - val_loss: 35.8473\n",
      "Epoch 1088/2000\n",
      "378/378 [==============================] - 0s 168us/step - loss: 30.3132 - val_loss: 35.8220\n",
      "Epoch 1089/2000\n",
      "378/378 [==============================] - 0s 157us/step - loss: 30.2247 - val_loss: 35.8486\n",
      "Epoch 1090/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.1740 - val_loss: 36.0266\n",
      "Epoch 1091/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.4493 - val_loss: 35.9234\n",
      "Epoch 1092/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.3188 - val_loss: 35.8882\n",
      "Epoch 1093/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.3056 - val_loss: 36.0380\n",
      "Epoch 1094/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.3821 - val_loss: 35.8586\n",
      "Epoch 1095/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 30.5173 - val_loss: 36.0024\n",
      "Epoch 1096/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.3705 - val_loss: 35.9762\n",
      "Epoch 1097/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.2034 - val_loss: 36.0447\n",
      "Epoch 1098/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.3335 - val_loss: 36.0994\n",
      "Epoch 1099/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.4602 - val_loss: 36.1126\n",
      "Epoch 1100/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3015 - val_loss: 36.0823\n",
      "Epoch 1101/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2811 - val_loss: 36.1054\n",
      "Epoch 1102/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.1721 - val_loss: 36.1423\n",
      "Epoch 1103/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.2462 - val_loss: 36.0248\n",
      "Epoch 1104/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.2661 - val_loss: 35.9439\n",
      "Epoch 1105/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.3783 - val_loss: 36.0975\n",
      "Epoch 1106/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.3913 - val_loss: 35.8679\n",
      "Epoch 1107/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.2825 - val_loss: 36.1244\n",
      "Epoch 1108/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2599 - val_loss: 36.0737\n",
      "Epoch 1109/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2003 - val_loss: 36.1741\n",
      "Epoch 1110/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.1835 - val_loss: 36.0745\n",
      "Epoch 1111/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.4043 - val_loss: 35.8859\n",
      "Epoch 1112/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.3231 - val_loss: 36.0852\n",
      "Epoch 1113/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.4331 - val_loss: 35.9312\n",
      "Epoch 1114/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2147 - val_loss: 35.8928\n",
      "Epoch 1115/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.2632 - val_loss: 36.2982\n",
      "Epoch 1116/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.4655 - val_loss: 35.9612\n",
      "Epoch 1117/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.1584 - val_loss: 35.6204\n",
      "Epoch 1118/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.0770 - val_loss: 35.8976\n",
      "Epoch 1119/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.1644 - val_loss: 35.9101\n",
      "Epoch 1120/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.0742 - val_loss: 35.8720\n",
      "Epoch 1121/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.3613 - val_loss: 35.9463\n",
      "Epoch 1122/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.2560 - val_loss: 35.7783\n",
      "Epoch 1123/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.4556 - val_loss: 35.7932\n",
      "Epoch 1124/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2934 - val_loss: 35.6596\n",
      "Epoch 1125/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.4047 - val_loss: 35.8161\n",
      "Epoch 1126/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.3283 - val_loss: 35.7507\n",
      "Epoch 1127/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.2313 - val_loss: 35.7981\n",
      "Epoch 1128/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.1399 - val_loss: 35.9377\n",
      "Epoch 1129/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.2030 - val_loss: 35.7695\n",
      "Epoch 1130/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.1841 - val_loss: 35.7786\n",
      "Epoch 1131/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.1938 - val_loss: 35.9371\n",
      "Epoch 1132/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.1865 - val_loss: 35.8138\n",
      "Epoch 1133/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.2057 - val_loss: 35.8383\n",
      "Epoch 1134/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2608 - val_loss: 35.9358\n",
      "Epoch 1135/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.1325 - val_loss: 36.1207\n",
      "Epoch 1136/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.4006 - val_loss: 35.7377\n",
      "Epoch 1137/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 136us/step - loss: 30.3650 - val_loss: 35.9465\n",
      "Epoch 1138/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 30.4044 - val_loss: 35.8026\n",
      "Epoch 1139/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 30.2428 - val_loss: 35.8037\n",
      "Epoch 1140/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 30.2078 - val_loss: 35.9516\n",
      "Epoch 1141/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.2717 - val_loss: 35.9255\n",
      "Epoch 1142/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2154 - val_loss: 36.0188\n",
      "Epoch 1143/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2426 - val_loss: 35.6360\n",
      "Epoch 1144/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.2099 - val_loss: 35.9393\n",
      "Epoch 1145/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 30.2692 - val_loss: 35.9427\n",
      "Epoch 1146/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2597 - val_loss: 35.8420\n",
      "Epoch 1147/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2991 - val_loss: 35.8212\n",
      "Epoch 1148/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.3295 - val_loss: 35.8827\n",
      "Epoch 1149/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.2353 - val_loss: 35.8798\n",
      "Epoch 1150/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.2926 - val_loss: 36.0090\n",
      "Epoch 1151/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.2932 - val_loss: 35.7728\n",
      "Epoch 1152/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.3707 - val_loss: 35.8579\n",
      "Epoch 1153/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.3018 - val_loss: 36.0238\n",
      "Epoch 1154/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 30.1775 - val_loss: 35.8349\n",
      "Epoch 1155/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.4238 - val_loss: 35.7208\n",
      "Epoch 1156/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.1948 - val_loss: 36.1205\n",
      "Epoch 1157/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.3263 - val_loss: 35.8120\n",
      "Epoch 1158/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.3817 - val_loss: 35.8510\n",
      "Epoch 1159/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 30.2114 - val_loss: 35.9261\n",
      "Epoch 1160/2000\n",
      "378/378 [==============================] - 0s 151us/step - loss: 30.3735 - val_loss: 35.8640\n",
      "Epoch 1161/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 30.3662 - val_loss: 36.0063\n",
      "Epoch 1162/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.2437 - val_loss: 35.7863\n",
      "Epoch 1163/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.3495 - val_loss: 35.8716\n",
      "Epoch 1164/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.5421 - val_loss: 36.1669\n",
      "Epoch 1165/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.0683 - val_loss: 35.9797\n",
      "Epoch 1166/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 30.2389 - val_loss: 35.8574\n",
      "Epoch 1167/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.2339 - val_loss: 35.8210\n",
      "Epoch 1168/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.2293 - val_loss: 35.9403\n",
      "Epoch 1169/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.1952 - val_loss: 36.0150\n",
      "Epoch 1170/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.2032 - val_loss: 36.2620\n",
      "Epoch 1171/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.1498 - val_loss: 36.1084\n",
      "Epoch 1172/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 30.2937 - val_loss: 36.1250\n",
      "Epoch 1173/2000\n",
      "378/378 [==============================] - 0s 167us/step - loss: 30.2200 - val_loss: 36.0897\n",
      "Epoch 1174/2000\n",
      "378/378 [==============================] - 0s 166us/step - loss: 30.4100 - val_loss: 36.2010\n",
      "Epoch 1175/2000\n",
      "378/378 [==============================] - 0s 160us/step - loss: 30.1442 - val_loss: 36.2014\n",
      "Epoch 1176/2000\n",
      "378/378 [==============================] - 0s 179us/step - loss: 30.2321 - val_loss: 36.0242\n",
      "Epoch 1177/2000\n",
      "378/378 [==============================] - 0s 184us/step - loss: 30.1199 - val_loss: 36.1939\n",
      "Epoch 1178/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.2096 - val_loss: 36.1199\n",
      "Epoch 1179/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.2324 - val_loss: 35.8515\n",
      "Epoch 1180/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 30.6085 - val_loss: 36.0451\n",
      "Epoch 1181/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.0589 - val_loss: 36.2266\n",
      "Epoch 1182/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.3201 - val_loss: 36.1689\n",
      "Epoch 1183/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.0968 - val_loss: 36.0123\n",
      "Epoch 1184/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.0934 - val_loss: 36.0011\n",
      "Epoch 1185/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.2887 - val_loss: 36.2687\n",
      "Epoch 1186/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.2460 - val_loss: 36.2705\n",
      "Epoch 1187/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 30.0656 - val_loss: 36.1271\n",
      "Epoch 1188/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 30.4017 - val_loss: 35.8874\n",
      "Epoch 1189/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.1993 - val_loss: 36.1472\n",
      "Epoch 1190/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.1137 - val_loss: 35.9854\n",
      "Epoch 1191/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.1385 - val_loss: 36.0481\n",
      "Epoch 1192/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 30.2834 - val_loss: 35.9563\n",
      "Epoch 1193/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 29.9952 - val_loss: 36.1043\n",
      "Epoch 1194/2000\n",
      "378/378 [==============================] - 0s 155us/step - loss: 30.1458 - val_loss: 36.3329\n",
      "Epoch 1195/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 30.1948 - val_loss: 36.1154\n",
      "Epoch 1196/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.1506 - val_loss: 35.9693\n",
      "Epoch 1197/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.2798 - val_loss: 35.8644\n",
      "Epoch 1198/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.1143 - val_loss: 36.1902\n",
      "Epoch 1199/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.0123 - val_loss: 35.8051\n",
      "Epoch 1200/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.2335 - val_loss: 35.8076\n",
      "Epoch 1201/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.0418 - val_loss: 35.7762\n",
      "Epoch 1202/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.1813 - val_loss: 35.8045\n",
      "Epoch 1203/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.1498 - val_loss: 35.9541\n",
      "Epoch 1204/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.9889 - val_loss: 35.7513\n",
      "Epoch 1205/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.2499 - val_loss: 35.7638\n",
      "Epoch 1206/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.2854 - val_loss: 35.7397\n",
      "Epoch 1207/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.0687 - val_loss: 35.6737\n",
      "Epoch 1208/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 30.0221 - val_loss: 35.9953\n",
      "Epoch 1209/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 30.2041 - val_loss: 35.8074\n",
      "Epoch 1210/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.1338 - val_loss: 35.7584\n",
      "Epoch 1211/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.0771 - val_loss: 35.7316\n",
      "Epoch 1212/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 132us/step - loss: 30.1126 - val_loss: 35.8438\n",
      "Epoch 1213/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 30.1211 - val_loss: 35.9697\n",
      "Epoch 1214/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 30.0618 - val_loss: 35.9011\n",
      "Epoch 1215/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 30.1722 - val_loss: 36.0557\n",
      "Epoch 1216/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.0995 - val_loss: 35.8990\n",
      "Epoch 1217/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 30.1926 - val_loss: 35.7713\n",
      "Epoch 1218/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.1079 - val_loss: 36.2485\n",
      "Epoch 1219/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.0952 - val_loss: 36.0627\n",
      "Epoch 1220/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.2123 - val_loss: 35.7982\n",
      "Epoch 1221/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.9803 - val_loss: 35.9560\n",
      "Epoch 1222/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.4781 - val_loss: 35.9058\n",
      "Epoch 1223/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 30.3267 - val_loss: 35.8124\n",
      "Epoch 1224/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 30.0712 - val_loss: 35.8435\n",
      "Epoch 1225/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 29.9668 - val_loss: 35.8072\n",
      "Epoch 1226/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 30.2426 - val_loss: 35.6733\n",
      "Epoch 1227/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 30.0901 - val_loss: 35.9561\n",
      "Epoch 1228/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 30.1135 - val_loss: 35.7592\n",
      "Epoch 1229/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 30.2962 - val_loss: 35.6885\n",
      "Epoch 1230/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 30.0121 - val_loss: 36.0250\n",
      "Epoch 1231/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 30.1297 - val_loss: 35.5854\n",
      "Epoch 1232/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 30.0474 - val_loss: 35.9057\n",
      "Epoch 1233/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.8658 - val_loss: 35.8596\n",
      "Epoch 1234/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.9398 - val_loss: 35.8243\n",
      "Epoch 1235/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.0258 - val_loss: 35.9052\n",
      "Epoch 1236/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.9706 - val_loss: 35.7720\n",
      "Epoch 1237/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.9191 - val_loss: 35.7246\n",
      "Epoch 1238/2000\n",
      "378/378 [==============================] - 0s 105us/step - loss: 29.8541 - val_loss: 35.8002\n",
      "Epoch 1239/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 30.0944 - val_loss: 35.9894\n",
      "Epoch 1240/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.7600 - val_loss: 35.8250\n",
      "Epoch 1241/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.9034 - val_loss: 35.8471\n",
      "Epoch 1242/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.1219 - val_loss: 35.6785\n",
      "Epoch 1243/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 30.0078 - val_loss: 35.7298\n",
      "Epoch 1244/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 30.1894 - val_loss: 35.6478\n",
      "Epoch 1245/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 30.0462 - val_loss: 35.8368\n",
      "Epoch 1246/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 30.1859 - val_loss: 35.6301\n",
      "Epoch 1247/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.0852 - val_loss: 35.6654\n",
      "Epoch 1248/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.2240 - val_loss: 35.6945\n",
      "Epoch 1249/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.9136 - val_loss: 35.7160\n",
      "Epoch 1250/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.0113 - val_loss: 35.8132\n",
      "Epoch 1251/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.0138 - val_loss: 35.6849\n",
      "Epoch 1252/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.1231 - val_loss: 35.6830\n",
      "Epoch 1253/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.9853 - val_loss: 35.9370\n",
      "Epoch 1254/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.9472 - val_loss: 35.8241\n",
      "Epoch 1255/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.0449 - val_loss: 35.8567\n",
      "Epoch 1256/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.9778 - val_loss: 35.7339\n",
      "Epoch 1257/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 30.1177 - val_loss: 35.6834\n",
      "Epoch 1258/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.1774 - val_loss: 35.9055\n",
      "Epoch 1259/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.0568 - val_loss: 35.7295\n",
      "Epoch 1260/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.9627 - val_loss: 35.8545\n",
      "Epoch 1261/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.1550 - val_loss: 35.8072\n",
      "Epoch 1262/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.0768 - val_loss: 35.9340\n",
      "Epoch 1263/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.0286 - val_loss: 35.8944\n",
      "Epoch 1264/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 30.2005 - val_loss: 35.8335\n",
      "Epoch 1265/2000\n",
      "378/378 [==============================] - 0s 151us/step - loss: 30.0341 - val_loss: 35.7540\n",
      "Epoch 1266/2000\n",
      "378/378 [==============================] - 0s 158us/step - loss: 30.1708 - val_loss: 35.7937\n",
      "Epoch 1267/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.9055 - val_loss: 35.8012\n",
      "Epoch 1268/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.2535 - val_loss: 35.7873\n",
      "Epoch 1269/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.2220 - val_loss: 35.6675\n",
      "Epoch 1270/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.0391 - val_loss: 35.8652\n",
      "Epoch 1271/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.8528 - val_loss: 35.8263\n",
      "Epoch 1272/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.0498 - val_loss: 35.7342\n",
      "Epoch 1273/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.0586 - val_loss: 35.6773\n",
      "Epoch 1274/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 29.8574 - val_loss: 35.8431\n",
      "Epoch 1275/2000\n",
      "378/378 [==============================] - 0s 143us/step - loss: 29.9859 - val_loss: 35.7636\n",
      "Epoch 1276/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 30.1076 - val_loss: 35.8560\n",
      "Epoch 1277/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.9203 - val_loss: 35.8871\n",
      "Epoch 1278/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.0147 - val_loss: 35.6869\n",
      "Epoch 1279/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.0569 - val_loss: 35.8042\n",
      "Epoch 1280/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 30.0783 - val_loss: 35.7272\n",
      "Epoch 1281/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 29.9406 - val_loss: 35.6455\n",
      "Epoch 1282/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 30.1052 - val_loss: 35.5611\n",
      "Epoch 1283/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 30.2645 - val_loss: 35.6242\n",
      "Epoch 1284/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 30.0284 - val_loss: 35.5972\n",
      "Epoch 1285/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 30.1648 - val_loss: 35.5497\n",
      "Epoch 1286/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.9885 - val_loss: 35.5790\n",
      "Epoch 1287/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 126us/step - loss: 30.0344 - val_loss: 35.6716\n",
      "Epoch 1288/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.9437 - val_loss: 35.6490\n",
      "Epoch 1289/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.1799 - val_loss: 35.5772\n",
      "Epoch 1290/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.9038 - val_loss: 35.5347\n",
      "Epoch 1291/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.8520 - val_loss: 35.5413\n",
      "Epoch 1292/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 30.1192 - val_loss: 35.5733\n",
      "Epoch 1293/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.0387 - val_loss: 35.6308\n",
      "Epoch 1294/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.1383 - val_loss: 35.6644\n",
      "Epoch 1295/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 30.0174 - val_loss: 35.6455\n",
      "Epoch 1296/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.0164 - val_loss: 35.7811\n",
      "Epoch 1297/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 30.0835 - val_loss: 35.6319\n",
      "Epoch 1298/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.9426 - val_loss: 35.6687\n",
      "Epoch 1299/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.7532 - val_loss: 35.6963\n",
      "Epoch 1300/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.0151 - val_loss: 35.7484\n",
      "Epoch 1301/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.7865 - val_loss: 35.7045\n",
      "Epoch 1302/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 29.9535 - val_loss: 35.5670\n",
      "Epoch 1303/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 30.1391 - val_loss: 35.5349\n",
      "Epoch 1304/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.1223 - val_loss: 35.7610\n",
      "Epoch 1305/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.0231 - val_loss: 35.7104\n",
      "Epoch 1306/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.2893 - val_loss: 35.6111\n",
      "Epoch 1307/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 29.9273 - val_loss: 35.7957\n",
      "Epoch 1308/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.8331 - val_loss: 35.8266\n",
      "Epoch 1309/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 30.0898 - val_loss: 35.7493\n",
      "Epoch 1310/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 30.0106 - val_loss: 35.6731\n",
      "Epoch 1311/2000\n",
      "378/378 [==============================] - 0s 168us/step - loss: 30.0638 - val_loss: 35.7594\n",
      "Epoch 1312/2000\n",
      "378/378 [==============================] - 0s 169us/step - loss: 30.2638 - val_loss: 35.7668\n",
      "Epoch 1313/2000\n",
      "378/378 [==============================] - 0s 166us/step - loss: 30.0557 - val_loss: 35.6842\n",
      "Epoch 1314/2000\n",
      "378/378 [==============================] - 0s 213us/step - loss: 29.8587 - val_loss: 35.7298\n",
      "Epoch 1315/2000\n",
      "378/378 [==============================] - 0s 168us/step - loss: 30.0277 - val_loss: 35.7276\n",
      "Epoch 1316/2000\n",
      "378/378 [==============================] - 0s 188us/step - loss: 29.9888 - val_loss: 35.7999\n",
      "Epoch 1317/2000\n",
      "378/378 [==============================] - 0s 149us/step - loss: 30.0309 - val_loss: 35.5957\n",
      "Epoch 1318/2000\n",
      "378/378 [==============================] - 0s 147us/step - loss: 30.1628 - val_loss: 35.8051\n",
      "Epoch 1319/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 29.9894 - val_loss: 35.8238\n",
      "Epoch 1320/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.0622 - val_loss: 35.6640\n",
      "Epoch 1321/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 29.9958 - val_loss: 35.6354\n",
      "Epoch 1322/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 30.0983 - val_loss: 35.6894\n",
      "Epoch 1323/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 29.9557 - val_loss: 35.9047\n",
      "Epoch 1324/2000\n",
      "378/378 [==============================] - 0s 147us/step - loss: 29.7690 - val_loss: 35.9224\n",
      "Epoch 1325/2000\n",
      "378/378 [==============================] - 0s 147us/step - loss: 30.0106 - val_loss: 35.6750\n",
      "Epoch 1326/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 30.0701 - val_loss: 35.6642\n",
      "Epoch 1327/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.1583 - val_loss: 35.5373\n",
      "Epoch 1328/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 29.9857 - val_loss: 35.7334\n",
      "Epoch 1329/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.0493 - val_loss: 35.7421\n",
      "Epoch 1330/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 30.1082 - val_loss: 35.5500\n",
      "Epoch 1331/2000\n",
      "378/378 [==============================] - 0s 153us/step - loss: 29.9506 - val_loss: 35.5274\n",
      "Epoch 1332/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 29.8464 - val_loss: 35.6089\n",
      "Epoch 1333/2000\n",
      "378/378 [==============================] - 0s 175us/step - loss: 29.6560 - val_loss: 35.6731\n",
      "Epoch 1334/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 29.7482 - val_loss: 35.8245\n",
      "Epoch 1335/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 30.1533 - val_loss: 35.8390\n",
      "Epoch 1336/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.8880 - val_loss: 35.7270\n",
      "Epoch 1337/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 30.2635 - val_loss: 35.8568\n",
      "Epoch 1338/2000\n",
      "378/378 [==============================] - 0s 151us/step - loss: 29.9326 - val_loss: 35.8293\n",
      "Epoch 1339/2000\n",
      "378/378 [==============================] - 0s 164us/step - loss: 29.9475 - val_loss: 35.9194\n",
      "Epoch 1340/2000\n",
      "378/378 [==============================] - 0s 161us/step - loss: 30.0777 - val_loss: 35.8627\n",
      "Epoch 1341/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.8494 - val_loss: 36.0784\n",
      "Epoch 1342/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.9301 - val_loss: 35.8603\n",
      "Epoch 1343/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.9467 - val_loss: 35.7062\n",
      "Epoch 1344/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.8402 - val_loss: 35.9330\n",
      "Epoch 1345/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 29.8799 - val_loss: 35.9450\n",
      "Epoch 1346/2000\n",
      "378/378 [==============================] - 0s 181us/step - loss: 29.7959 - val_loss: 35.9573\n",
      "Epoch 1347/2000\n",
      "378/378 [==============================] - 0s 182us/step - loss: 29.8962 - val_loss: 35.9980\n",
      "Epoch 1348/2000\n",
      "378/378 [==============================] - 0s 154us/step - loss: 30.1040 - val_loss: 35.9206\n",
      "Epoch 1349/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.0287 - val_loss: 35.7753\n",
      "Epoch 1350/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.8405 - val_loss: 35.7543\n",
      "Epoch 1351/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.1404 - val_loss: 36.0233\n",
      "Epoch 1352/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 30.0087 - val_loss: 35.7885\n",
      "Epoch 1353/2000\n",
      "378/378 [==============================] - 0s 153us/step - loss: 29.8845 - val_loss: 35.7451\n",
      "Epoch 1354/2000\n",
      "378/378 [==============================] - 0s 149us/step - loss: 29.7708 - val_loss: 35.9129\n",
      "Epoch 1355/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 30.1228 - val_loss: 35.8324\n",
      "Epoch 1356/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 29.9143 - val_loss: 35.8479\n",
      "Epoch 1357/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.9592 - val_loss: 36.0793\n",
      "Epoch 1358/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.9999 - val_loss: 35.7380\n",
      "Epoch 1359/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 30.2192 - val_loss: 35.6093\n",
      "Epoch 1360/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 29.8876 - val_loss: 35.8448\n",
      "Epoch 1361/2000\n",
      "378/378 [==============================] - 0s 157us/step - loss: 29.7950 - val_loss: 35.8461\n",
      "Epoch 1362/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 151us/step - loss: 29.7765 - val_loss: 35.8060\n",
      "Epoch 1363/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 30.0834 - val_loss: 35.8086\n",
      "Epoch 1364/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.9826 - val_loss: 35.7379\n",
      "Epoch 1365/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.7550 - val_loss: 35.8475\n",
      "Epoch 1366/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.8263 - val_loss: 35.7889\n",
      "Epoch 1367/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 30.0491 - val_loss: 35.6961\n",
      "Epoch 1368/2000\n",
      "378/378 [==============================] - 0s 155us/step - loss: 29.8189 - val_loss: 35.5734\n",
      "Epoch 1369/2000\n",
      "378/378 [==============================] - 0s 189us/step - loss: 30.0012 - val_loss: 35.9507\n",
      "Epoch 1370/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 29.9458 - val_loss: 35.6821\n",
      "Epoch 1371/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.8591 - val_loss: 35.9570\n",
      "Epoch 1372/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 29.8614 - val_loss: 35.8693\n",
      "Epoch 1373/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 30.0440 - val_loss: 35.6195\n",
      "Epoch 1374/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 29.8169 - val_loss: 35.7831\n",
      "Epoch 1375/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 29.9512 - val_loss: 35.9022\n",
      "Epoch 1376/2000\n",
      "378/378 [==============================] - 0s 156us/step - loss: 29.8137 - val_loss: 35.7445\n",
      "Epoch 1377/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 30.0729 - val_loss: 35.7074\n",
      "Epoch 1378/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 29.9868 - val_loss: 35.5452\n",
      "Epoch 1379/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.8804 - val_loss: 35.7733\n",
      "Epoch 1380/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.9473 - val_loss: 35.8017\n",
      "Epoch 1381/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 30.2340 - val_loss: 35.7385\n",
      "Epoch 1382/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.8478 - val_loss: 35.6308\n",
      "Epoch 1383/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 29.9430 - val_loss: 35.8588\n",
      "Epoch 1384/2000\n",
      "378/378 [==============================] - 0s 158us/step - loss: 29.8034 - val_loss: 35.8561\n",
      "Epoch 1385/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 30.0726 - val_loss: 35.7803\n",
      "Epoch 1386/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.9020 - val_loss: 35.7424\n",
      "Epoch 1387/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.8420 - val_loss: 35.7316\n",
      "Epoch 1388/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.9852 - val_loss: 35.6862\n",
      "Epoch 1389/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.9092 - val_loss: 35.6053\n",
      "Epoch 1390/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.9793 - val_loss: 35.6826\n",
      "Epoch 1391/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 29.8643 - val_loss: 35.7924\n",
      "Epoch 1392/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 29.9749 - val_loss: 35.6129\n",
      "Epoch 1393/2000\n",
      "378/378 [==============================] - 0s 153us/step - loss: 29.6863 - val_loss: 35.6512\n",
      "Epoch 1394/2000\n",
      "378/378 [==============================] - 0s 173us/step - loss: 29.7809 - val_loss: 35.6498\n",
      "Epoch 1395/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.9427 - val_loss: 35.6644\n",
      "Epoch 1396/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 29.9236 - val_loss: 35.9322\n",
      "Epoch 1397/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.8085 - val_loss: 35.6227\n",
      "Epoch 1398/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.0512 - val_loss: 35.5495\n",
      "Epoch 1399/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 29.7148 - val_loss: 35.6569\n",
      "Epoch 1400/2000\n",
      "378/378 [==============================] - 0s 149us/step - loss: 29.9731 - val_loss: 35.7989\n",
      "Epoch 1401/2000\n",
      "378/378 [==============================] - 0s 155us/step - loss: 29.9140 - val_loss: 35.9115\n",
      "Epoch 1402/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.9487 - val_loss: 35.6516\n",
      "Epoch 1403/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.8098 - val_loss: 35.6514\n",
      "Epoch 1404/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.9245 - val_loss: 35.7989\n",
      "Epoch 1405/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 29.9043 - val_loss: 35.7814\n",
      "Epoch 1406/2000\n",
      "378/378 [==============================] - 0s 198us/step - loss: 29.7761 - val_loss: 35.8511\n",
      "Epoch 1407/2000\n",
      "378/378 [==============================] - 0s 179us/step - loss: 29.8101 - val_loss: 35.5887\n",
      "Epoch 1408/2000\n",
      "378/378 [==============================] - 0s 161us/step - loss: 29.7258 - val_loss: 35.7172\n",
      "Epoch 1409/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 29.8178 - val_loss: 35.7275\n",
      "Epoch 1410/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.8009 - val_loss: 35.7595\n",
      "Epoch 1411/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 29.7965 - val_loss: 35.7440\n",
      "Epoch 1412/2000\n",
      "378/378 [==============================] - ETA: 0s - loss: 29.12 - 0s 118us/step - loss: 30.1197 - val_loss: 35.6583\n",
      "Epoch 1413/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.7760 - val_loss: 35.6841\n",
      "Epoch 1414/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 29.9710 - val_loss: 35.7550\n",
      "Epoch 1415/2000\n",
      "378/378 [==============================] - 0s 158us/step - loss: 29.8731 - val_loss: 35.5520\n",
      "Epoch 1416/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 29.7956 - val_loss: 35.6582\n",
      "Epoch 1417/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 30.1438 - val_loss: 35.5908\n",
      "Epoch 1418/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 29.8239 - val_loss: 35.7370\n",
      "Epoch 1419/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 30.0190 - val_loss: 35.7610\n",
      "Epoch 1420/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.7182 - val_loss: 35.6679\n",
      "Epoch 1421/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 29.8039 - val_loss: 35.8375\n",
      "Epoch 1422/2000\n",
      "378/378 [==============================] - 0s 153us/step - loss: 29.9022 - val_loss: 35.7235\n",
      "Epoch 1423/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 29.9197 - val_loss: 35.7200\n",
      "Epoch 1424/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.1081 - val_loss: 35.5541\n",
      "Epoch 1425/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.7843 - val_loss: 35.7784\n",
      "Epoch 1426/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 30.1055 - val_loss: 35.9596\n",
      "Epoch 1427/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 30.0165 - val_loss: 35.7773\n",
      "Epoch 1428/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.1193 - val_loss: 35.6979\n",
      "Epoch 1429/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 29.7823 - val_loss: 35.7885\n",
      "Epoch 1430/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 29.6844 - val_loss: 35.7097\n",
      "Epoch 1431/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 29.9359 - val_loss: 35.4799\n",
      "Epoch 1432/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 29.9741 - val_loss: 35.6651\n",
      "Epoch 1433/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 29.7363 - val_loss: 35.7320\n",
      "Epoch 1434/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.8568 - val_loss: 35.5741\n",
      "Epoch 1435/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 29.8185 - val_loss: 35.8139\n",
      "Epoch 1436/2000\n",
      "378/378 [==============================] - 0s 149us/step - loss: 29.7527 - val_loss: 35.6835\n",
      "Epoch 1437/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 151us/step - loss: 29.7519 - val_loss: 35.8177\n",
      "Epoch 1438/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 29.9279 - val_loss: 35.8309\n",
      "Epoch 1439/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.8324 - val_loss: 35.7528\n",
      "Epoch 1440/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.8352 - val_loss: 35.7331\n",
      "Epoch 1441/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.8808 - val_loss: 35.7379\n",
      "Epoch 1442/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.9125 - val_loss: 35.7247\n",
      "Epoch 1443/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 29.9668 - val_loss: 35.7933\n",
      "Epoch 1444/2000\n",
      "378/378 [==============================] - 0s 161us/step - loss: 30.0467 - val_loss: 35.7081\n",
      "Epoch 1445/2000\n",
      "378/378 [==============================] - 0s 165us/step - loss: 29.9510 - val_loss: 35.7832\n",
      "Epoch 1446/2000\n",
      "378/378 [==============================] - 0s 162us/step - loss: 29.7187 - val_loss: 35.7169\n",
      "Epoch 1447/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 29.7635 - val_loss: 35.9094\n",
      "Epoch 1448/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.8242 - val_loss: 35.4607\n",
      "Epoch 1449/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.7865 - val_loss: 35.8008\n",
      "Epoch 1450/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 29.8070 - val_loss: 35.8963\n",
      "Epoch 1451/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 29.7313 - val_loss: 36.2121\n",
      "Epoch 1452/2000\n",
      "378/378 [==============================] - 0s 182us/step - loss: 29.6965 - val_loss: 35.8710\n",
      "Epoch 1453/2000\n",
      "378/378 [==============================] - 0s 146us/step - loss: 29.6986 - val_loss: 35.9045\n",
      "Epoch 1454/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 29.9900 - val_loss: 35.7428\n",
      "Epoch 1455/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 30.0494 - val_loss: 35.8335\n",
      "Epoch 1456/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.7983 - val_loss: 35.8129\n",
      "Epoch 1457/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.7095 - val_loss: 35.7131\n",
      "Epoch 1458/2000\n",
      "378/378 [==============================] - 0s 183us/step - loss: 29.9896 - val_loss: 35.7403\n",
      "Epoch 1459/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 30.2100 - val_loss: 35.6443\n",
      "Epoch 1460/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 29.5268 - val_loss: 35.8864\n",
      "Epoch 1461/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 30.1744 - val_loss: 35.8454\n",
      "Epoch 1462/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 29.7172 - val_loss: 35.6217\n",
      "Epoch 1463/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 29.7004 - val_loss: 35.7543\n",
      "Epoch 1464/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 29.7443 - val_loss: 35.7756\n",
      "Epoch 1465/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 29.7035 - val_loss: 35.7874\n",
      "Epoch 1466/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 29.9049 - val_loss: 35.6234\n",
      "Epoch 1467/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.7654 - val_loss: 35.7787\n",
      "Epoch 1468/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.8033 - val_loss: 35.7121\n",
      "Epoch 1469/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.7941 - val_loss: 35.5776\n",
      "Epoch 1470/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.8129 - val_loss: 35.7498\n",
      "Epoch 1471/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 29.7646 - val_loss: 35.7695\n",
      "Epoch 1472/2000\n",
      "378/378 [==============================] - 0s 154us/step - loss: 29.8014 - val_loss: 35.5727\n",
      "Epoch 1473/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 29.6794 - val_loss: 35.6995\n",
      "Epoch 1474/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 29.6592 - val_loss: 35.7382\n",
      "Epoch 1475/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 29.9506 - val_loss: 35.6813\n",
      "Epoch 1476/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 30.0227 - val_loss: 35.8030\n",
      "Epoch 1477/2000\n",
      "378/378 [==============================] - 0s 189us/step - loss: 29.7817 - val_loss: 35.6714\n",
      "Epoch 1478/2000\n",
      "378/378 [==============================] - 0s 154us/step - loss: 29.7666 - val_loss: 35.8122\n",
      "Epoch 1479/2000\n",
      "378/378 [==============================] - 0s 147us/step - loss: 29.5780 - val_loss: 35.8776\n",
      "Epoch 1480/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 30.0143 - val_loss: 35.4971\n",
      "Epoch 1481/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.7366 - val_loss: 35.8648\n",
      "Epoch 1482/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.6776 - val_loss: 36.0167\n",
      "Epoch 1483/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.5774 - val_loss: 36.0515\n",
      "Epoch 1484/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.7849 - val_loss: 35.9112\n",
      "Epoch 1485/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.6820 - val_loss: 35.8646\n",
      "Epoch 1486/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.9416 - val_loss: 35.7477\n",
      "Epoch 1487/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.8383 - val_loss: 35.7755\n",
      "Epoch 1488/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.6970 - val_loss: 35.7873\n",
      "Epoch 1489/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.6157 - val_loss: 35.9753\n",
      "Epoch 1490/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 29.9534 - val_loss: 35.7788\n",
      "Epoch 1491/2000\n",
      "378/378 [==============================] - 0s 155us/step - loss: 29.8211 - val_loss: 35.9024\n",
      "Epoch 1492/2000\n",
      "378/378 [==============================] - 0s 160us/step - loss: 29.7511 - val_loss: 35.8252\n",
      "Epoch 1493/2000\n",
      "378/378 [==============================] - 0s 162us/step - loss: 29.8120 - val_loss: 35.9701\n",
      "Epoch 1494/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 29.7985 - val_loss: 35.8191\n",
      "Epoch 1495/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 29.7982 - val_loss: 35.7930\n",
      "Epoch 1496/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 29.8022 - val_loss: 35.9407\n",
      "Epoch 1497/2000\n",
      "378/378 [==============================] - 0s 205us/step - loss: 29.5052 - val_loss: 35.8956\n",
      "Epoch 1498/2000\n",
      "378/378 [==============================] - 0s 187us/step - loss: 29.6508 - val_loss: 35.7744\n",
      "Epoch 1499/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 29.6364 - val_loss: 35.7782\n",
      "Epoch 1500/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 29.5745 - val_loss: 35.9172\n",
      "Epoch 1501/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 29.5862 - val_loss: 35.6611\n",
      "Epoch 1502/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 29.8470 - val_loss: 35.7786\n",
      "Epoch 1503/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 29.9910 - val_loss: 36.2580\n",
      "Epoch 1504/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 29.7165 - val_loss: 36.0419\n",
      "Epoch 1505/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.5354 - val_loss: 35.6728\n",
      "Epoch 1506/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 29.5921 - val_loss: 36.0304\n",
      "Epoch 1507/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.6749 - val_loss: 36.0248\n",
      "Epoch 1508/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.6349 - val_loss: 35.6137\n",
      "Epoch 1509/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 29.9019 - val_loss: 35.2552\n",
      "Epoch 1510/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 29.7910 - val_loss: 35.6201\n",
      "Epoch 1511/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 29.7490 - val_loss: 35.9491\n",
      "Epoch 1512/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 128us/step - loss: 29.7843 - val_loss: 35.6950\n",
      "Epoch 1513/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 29.6821 - val_loss: 35.6450\n",
      "Epoch 1514/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.7243 - val_loss: 35.9781\n",
      "Epoch 1515/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 29.6148 - val_loss: 36.0073\n",
      "Epoch 1516/2000\n",
      "378/378 [==============================] - 0s 156us/step - loss: 29.6755 - val_loss: 35.9967\n",
      "Epoch 1517/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 30.0243 - val_loss: 35.8026\n",
      "Epoch 1518/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 29.6984 - val_loss: 36.2511\n",
      "Epoch 1519/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 29.7555 - val_loss: 35.9630\n",
      "Epoch 1520/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 29.6964 - val_loss: 35.9730\n",
      "Epoch 1521/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.8664 - val_loss: 35.7507\n",
      "Epoch 1522/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 29.6239 - val_loss: 36.0513\n",
      "Epoch 1523/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 29.7241 - val_loss: 36.0098\n",
      "Epoch 1524/2000\n",
      "378/378 [==============================] - 0s 155us/step - loss: 29.8058 - val_loss: 36.0294\n",
      "Epoch 1525/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 29.5975 - val_loss: 36.0691\n",
      "Epoch 1526/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.4381 - val_loss: 36.0416\n",
      "Epoch 1527/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.8673 - val_loss: 36.0464\n",
      "Epoch 1528/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 29.9738 - val_loss: 36.3129\n",
      "Epoch 1529/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 29.7332 - val_loss: 36.3479\n",
      "Epoch 1530/2000\n",
      "378/378 [==============================] - 0s 160us/step - loss: 29.8382 - val_loss: 35.9242\n",
      "Epoch 1531/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 29.7058 - val_loss: 35.8621\n",
      "Epoch 1532/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 29.6783 - val_loss: 35.7939\n",
      "Epoch 1533/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 29.7413 - val_loss: 35.9089\n",
      "Epoch 1534/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.5254 - val_loss: 35.9865\n",
      "Epoch 1535/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.7831 - val_loss: 36.3461\n",
      "Epoch 1536/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.5892 - val_loss: 36.0505\n",
      "Epoch 1537/2000\n",
      "378/378 [==============================] - 0s 155us/step - loss: 29.7879 - val_loss: 36.0340\n",
      "Epoch 1538/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 29.6984 - val_loss: 36.1820\n",
      "Epoch 1539/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.8655 - val_loss: 36.1986\n",
      "Epoch 1540/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.6733 - val_loss: 35.9972\n",
      "Epoch 1541/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.5528 - val_loss: 36.1156\n",
      "Epoch 1542/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.4776 - val_loss: 36.2800\n",
      "Epoch 1543/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 29.5299 - val_loss: 36.0205\n",
      "Epoch 1544/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 29.6653 - val_loss: 36.1083\n",
      "Epoch 1545/2000\n",
      "378/378 [==============================] - 0s 153us/step - loss: 29.5677 - val_loss: 36.3991\n",
      "Epoch 1546/2000\n",
      "378/378 [==============================] - 0s 172us/step - loss: 29.6676 - val_loss: 36.2325\n",
      "Epoch 1547/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.7235 - val_loss: 36.0472\n",
      "Epoch 1548/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.4601 - val_loss: 36.3374\n",
      "Epoch 1549/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 29.6142 - val_loss: 36.2361\n",
      "Epoch 1550/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.7731 - val_loss: 36.1006\n",
      "Epoch 1551/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 29.5049 - val_loss: 36.2205\n",
      "Epoch 1552/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 29.6651 - val_loss: 35.7811\n",
      "Epoch 1553/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 29.4727 - val_loss: 36.5728\n",
      "Epoch 1554/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.6419 - val_loss: 36.3062\n",
      "Epoch 1555/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 29.3698 - val_loss: 36.1602\n",
      "Epoch 1556/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 29.5569 - val_loss: 36.1743\n",
      "Epoch 1557/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.5118 - val_loss: 36.2899\n",
      "Epoch 1558/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 29.7387 - val_loss: 36.2570\n",
      "Epoch 1559/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 29.4903 - val_loss: 36.1575\n",
      "Epoch 1560/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 29.5335 - val_loss: 36.0465\n",
      "Epoch 1561/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 29.4021 - val_loss: 36.1793\n",
      "Epoch 1562/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.6801 - val_loss: 36.2086\n",
      "Epoch 1563/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.4074 - val_loss: 36.1976\n",
      "Epoch 1564/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.6567 - val_loss: 36.2474\n",
      "Epoch 1565/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.7499 - val_loss: 36.6368\n",
      "Epoch 1566/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.6840 - val_loss: 36.2256\n",
      "Epoch 1567/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 29.7363 - val_loss: 36.3135\n",
      "Epoch 1568/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 29.4310 - val_loss: 36.1441\n",
      "Epoch 1569/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 29.4374 - val_loss: 36.2416\n",
      "Epoch 1570/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 29.4869 - val_loss: 36.1554\n",
      "Epoch 1571/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.4451 - val_loss: 36.3155\n",
      "Epoch 1572/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 29.3662 - val_loss: 36.2281\n",
      "Epoch 1573/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.5491 - val_loss: 36.1804\n",
      "Epoch 1574/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 29.6072 - val_loss: 36.2316\n",
      "Epoch 1575/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 29.5073 - val_loss: 36.2027\n",
      "Epoch 1576/2000\n",
      "378/378 [==============================] - 0s 149us/step - loss: 29.4759 - val_loss: 36.4426\n",
      "Epoch 1577/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 29.7522 - val_loss: 36.6206\n",
      "Epoch 1578/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.4213 - val_loss: 36.2593\n",
      "Epoch 1579/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.6312 - val_loss: 36.2308\n",
      "Epoch 1580/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.4954 - val_loss: 36.4293\n",
      "Epoch 1581/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.7483 - val_loss: 36.1151\n",
      "Epoch 1582/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 29.4486 - val_loss: 36.2709\n",
      "Epoch 1583/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 29.3436 - val_loss: 36.3885\n",
      "Epoch 1584/2000\n",
      "378/378 [==============================] - 0s 160us/step - loss: 29.6193 - val_loss: 36.1852\n",
      "Epoch 1585/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 29.4696 - val_loss: 36.0911\n",
      "Epoch 1586/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.7325 - val_loss: 36.1144\n",
      "Epoch 1587/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 125us/step - loss: 29.8383 - val_loss: 36.2655\n",
      "Epoch 1588/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.3581 - val_loss: 36.1761\n",
      "Epoch 1589/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.3880 - val_loss: 36.0702\n",
      "Epoch 1590/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.4458 - val_loss: 36.1523\n",
      "Epoch 1591/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 29.4909 - val_loss: 36.3660\n",
      "Epoch 1592/2000\n",
      "378/378 [==============================] - 0s 169us/step - loss: 29.4450 - val_loss: 36.3732\n",
      "Epoch 1593/2000\n",
      "378/378 [==============================] - 0s 177us/step - loss: 29.4750 - val_loss: 36.5497\n",
      "Epoch 1594/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.6111 - val_loss: 36.1133\n",
      "Epoch 1595/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.8080 - val_loss: 35.9096\n",
      "Epoch 1596/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.6010 - val_loss: 36.2985\n",
      "Epoch 1597/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.3149 - val_loss: 36.4226\n",
      "Epoch 1598/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.2386 - val_loss: 36.7270\n",
      "Epoch 1599/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 29.3134 - val_loss: 36.1153\n",
      "Epoch 1600/2000\n",
      "378/378 [==============================] - 0s 172us/step - loss: 29.5570 - val_loss: 36.2831\n",
      "Epoch 1601/2000\n",
      "378/378 [==============================] - 0s 194us/step - loss: 29.6853 - val_loss: 36.1677\n",
      "Epoch 1602/2000\n",
      "378/378 [==============================] - 0s 149us/step - loss: 29.6067 - val_loss: 36.3440\n",
      "Epoch 1603/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 29.4632 - val_loss: 36.4015\n",
      "Epoch 1604/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.4208 - val_loss: 36.6624\n",
      "Epoch 1605/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.4505 - val_loss: 36.1609\n",
      "Epoch 1606/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.6729 - val_loss: 36.2623\n",
      "Epoch 1607/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 29.4501 - val_loss: 36.3574\n",
      "Epoch 1608/2000\n",
      "378/378 [==============================] - 0s 161us/step - loss: 29.5149 - val_loss: 36.2775\n",
      "Epoch 1609/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 29.3791 - val_loss: 36.1286\n",
      "Epoch 1610/2000\n",
      "378/378 [==============================] - 0s 167us/step - loss: 29.5667 - val_loss: 36.4900\n",
      "Epoch 1611/2000\n",
      "378/378 [==============================] - 0s 154us/step - loss: 29.4708 - val_loss: 35.9669\n",
      "Epoch 1612/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.5485 - val_loss: 35.9927\n",
      "Epoch 1613/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 29.6545 - val_loss: 36.4128\n",
      "Epoch 1614/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.6369 - val_loss: 36.0901\n",
      "Epoch 1615/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 29.4671 - val_loss: 36.2533\n",
      "Epoch 1616/2000\n",
      "378/378 [==============================] - 0s 180us/step - loss: 29.3751 - val_loss: 36.0041\n",
      "Epoch 1617/2000\n",
      "378/378 [==============================] - 0s 165us/step - loss: 29.3991 - val_loss: 36.2290\n",
      "Epoch 1618/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 29.5093 - val_loss: 36.2109\n",
      "Epoch 1619/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.2762 - val_loss: 36.4194\n",
      "Epoch 1620/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 29.4081 - val_loss: 36.5162\n",
      "Epoch 1621/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.6187 - val_loss: 36.3472\n",
      "Epoch 1622/2000\n",
      "378/378 [==============================] - ETA: 0s - loss: 30.69 - 0s 136us/step - loss: 29.4883 - val_loss: 36.3442\n",
      "Epoch 1623/2000\n",
      "378/378 [==============================] - 0s 157us/step - loss: 29.4362 - val_loss: 36.6819\n",
      "Epoch 1624/2000\n",
      "378/378 [==============================] - 0s 144us/step - loss: 29.4181 - val_loss: 36.5553\n",
      "Epoch 1625/2000\n",
      "378/378 [==============================] - 0s 148us/step - loss: 29.4322 - val_loss: 36.6091\n",
      "Epoch 1626/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 29.5204 - val_loss: 36.3632\n",
      "Epoch 1627/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.5520 - val_loss: 36.3535\n",
      "Epoch 1628/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.4462 - val_loss: 36.4984\n",
      "Epoch 1629/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.3892 - val_loss: 36.4681\n",
      "Epoch 1630/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.3588 - val_loss: 36.2111\n",
      "Epoch 1631/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 29.3822 - val_loss: 36.0876\n",
      "Epoch 1632/2000\n",
      "378/378 [==============================] - 0s 149us/step - loss: 29.5048 - val_loss: 36.5050\n",
      "Epoch 1633/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 29.5451 - val_loss: 36.1618\n",
      "Epoch 1634/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 29.8881 - val_loss: 36.4051\n",
      "Epoch 1635/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 29.4003 - val_loss: 36.4929\n",
      "Epoch 1636/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 29.5803 - val_loss: 36.4663\n",
      "Epoch 1637/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 29.6415 - val_loss: 36.2197\n",
      "Epoch 1638/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 29.5890 - val_loss: 36.2332\n",
      "Epoch 1639/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 29.4147 - val_loss: 36.0495\n",
      "Epoch 1640/2000\n",
      "378/378 [==============================] - 0s 161us/step - loss: 29.4072 - val_loss: 36.5526\n",
      "Epoch 1641/2000\n",
      "378/378 [==============================] - 0s 198us/step - loss: 29.6614 - val_loss: 35.9955\n",
      "Epoch 1642/2000\n",
      "378/378 [==============================] - 0s 185us/step - loss: 29.5583 - val_loss: 36.2473\n",
      "Epoch 1643/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.5117 - val_loss: 36.3553\n",
      "Epoch 1644/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 29.6978 - val_loss: 36.5116\n",
      "Epoch 1645/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 29.6328 - val_loss: 36.1607\n",
      "Epoch 1646/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.3846 - val_loss: 36.3927\n",
      "Epoch 1647/2000\n",
      "378/378 [==============================] - 0s 140us/step - loss: 29.5109 - val_loss: 36.1491\n",
      "Epoch 1648/2000\n",
      "378/378 [==============================] - 0s 151us/step - loss: 29.2619 - val_loss: 36.4447\n",
      "Epoch 1649/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 29.3572 - val_loss: 36.1396\n",
      "Epoch 1650/2000\n",
      "378/378 [==============================] - 0s 239us/step - loss: 29.3770 - val_loss: 36.3994\n",
      "Epoch 1651/2000\n",
      "378/378 [==============================] - 0s 147us/step - loss: 29.5867 - val_loss: 36.1088\n",
      "Epoch 1652/2000\n",
      "378/378 [==============================] - 0s 158us/step - loss: 29.4279 - val_loss: 35.9950\n",
      "Epoch 1653/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 29.3464 - val_loss: 36.2269\n",
      "Epoch 1654/2000\n",
      "378/378 [==============================] - 0s 156us/step - loss: 29.5789 - val_loss: 36.1790\n",
      "Epoch 1655/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.6073 - val_loss: 36.2995\n",
      "Epoch 1656/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 29.3832 - val_loss: 36.2673\n",
      "Epoch 1657/2000\n",
      "378/378 [==============================] - 0s 155us/step - loss: 29.6144 - val_loss: 36.1002\n",
      "Epoch 1658/2000\n",
      "378/378 [==============================] - 0s 145us/step - loss: 29.7023 - val_loss: 36.0754\n",
      "Epoch 1659/2000\n",
      "378/378 [==============================] - 0s 161us/step - loss: 29.2943 - val_loss: 36.8797\n",
      "Epoch 1660/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 29.6557 - val_loss: 36.1478\n",
      "Epoch 1661/2000\n",
      "378/378 [==============================] - 0s 143us/step - loss: 29.6337 - val_loss: 36.0876\n",
      "Epoch 1662/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 146us/step - loss: 29.3903 - val_loss: 36.3816\n",
      "Epoch 1663/2000\n",
      "378/378 [==============================] - 0s 138us/step - loss: 29.4332 - val_loss: 36.3287\n",
      "Epoch 1664/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 29.3441 - val_loss: 36.4252\n",
      "Epoch 1665/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 29.3075 - val_loss: 36.4449\n",
      "Epoch 1666/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 29.2974 - val_loss: 36.1782\n",
      "Epoch 1667/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 29.5570 - val_loss: 36.2321\n",
      "Epoch 1668/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 29.4465 - val_loss: 36.5438\n",
      "Epoch 1669/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 29.3717 - val_loss: 36.2949\n",
      "Epoch 1670/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.3143 - val_loss: 36.3628\n",
      "Epoch 1671/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 29.4713 - val_loss: 36.7728\n",
      "Epoch 1672/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 29.5245 - val_loss: 36.6460\n",
      "Epoch 1673/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 29.4388 - val_loss: 36.4172\n",
      "Epoch 1674/2000\n",
      "378/378 [==============================] - 0s 157us/step - loss: 29.5651 - val_loss: 36.4560\n",
      "Epoch 1675/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.3716 - val_loss: 36.6999\n",
      "Epoch 1676/2000\n",
      "378/378 [==============================] - 0s 147us/step - loss: 29.2373 - val_loss: 36.4230\n",
      "Epoch 1677/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 29.5100 - val_loss: 36.0638\n",
      "Epoch 1678/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 29.6590 - val_loss: 36.3609\n",
      "Epoch 1679/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.8526 - val_loss: 37.0586\n",
      "Epoch 1680/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.2413 - val_loss: 36.1887\n",
      "Epoch 1681/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.5562 - val_loss: 36.2853\n",
      "Epoch 1682/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.5200 - val_loss: 36.4793\n",
      "Epoch 1683/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.3490 - val_loss: 36.4777\n",
      "Epoch 1684/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.3784 - val_loss: 36.2216\n",
      "Epoch 1685/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.2338 - val_loss: 36.5059\n",
      "Epoch 1686/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.2302 - val_loss: 36.3288\n",
      "Epoch 1687/2000\n",
      "378/378 [==============================] - 0s 187us/step - loss: 29.2853 - val_loss: 36.2907\n",
      "Epoch 1688/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 29.2955 - val_loss: 36.5216\n",
      "Epoch 1689/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 29.2006 - val_loss: 36.3784\n",
      "Epoch 1690/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.5517 - val_loss: 36.4044\n",
      "Epoch 1691/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.3877 - val_loss: 36.5546\n",
      "Epoch 1692/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 29.4153 - val_loss: 36.3019\n",
      "Epoch 1693/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 29.3588 - val_loss: 36.4061\n",
      "Epoch 1694/2000\n",
      "378/378 [==============================] - 0s 216us/step - loss: 29.3011 - val_loss: 36.5305\n",
      "Epoch 1695/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.3958 - val_loss: 36.5141\n",
      "Epoch 1696/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.3620 - val_loss: 36.2528\n",
      "Epoch 1697/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.6944 - val_loss: 36.5317\n",
      "Epoch 1698/2000\n",
      "378/378 [==============================] - 0s 166us/step - loss: 29.5396 - val_loss: 36.2480\n",
      "Epoch 1699/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.5034 - val_loss: 36.2580\n",
      "Epoch 1700/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.3510 - val_loss: 36.3308\n",
      "Epoch 1701/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 29.4608 - val_loss: 36.7009\n",
      "Epoch 1702/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.3637 - val_loss: 36.1274\n",
      "Epoch 1703/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.4125 - val_loss: 36.0093\n",
      "Epoch 1704/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.2963 - val_loss: 36.4791\n",
      "Epoch 1705/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.4953 - val_loss: 36.3834\n",
      "Epoch 1706/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.5041 - val_loss: 36.6055\n",
      "Epoch 1707/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.6313 - val_loss: 36.5364\n",
      "Epoch 1708/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.3300 - val_loss: 36.1794\n",
      "Epoch 1709/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.2828 - val_loss: 36.4426\n",
      "Epoch 1710/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.3833 - val_loss: 35.9655\n",
      "Epoch 1711/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.4176 - val_loss: 36.2066\n",
      "Epoch 1712/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.3656 - val_loss: 36.2303\n",
      "Epoch 1713/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.7648 - val_loss: 36.0810\n",
      "Epoch 1714/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.5146 - val_loss: 36.3676\n",
      "Epoch 1715/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.2289 - val_loss: 36.6258\n",
      "Epoch 1716/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.3468 - val_loss: 36.2821\n",
      "Epoch 1717/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.3668 - val_loss: 36.7184\n",
      "Epoch 1718/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.9176 - val_loss: 36.3540\n",
      "Epoch 1719/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.3918 - val_loss: 36.4182\n",
      "Epoch 1720/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.3150 - val_loss: 36.7276\n",
      "Epoch 1721/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 29.1371 - val_loss: 36.3074\n",
      "Epoch 1722/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.2484 - val_loss: 36.5445\n",
      "Epoch 1723/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.4964 - val_loss: 36.5734\n",
      "Epoch 1724/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.2708 - val_loss: 36.7071\n",
      "Epoch 1725/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.6196 - val_loss: 36.3042\n",
      "Epoch 1726/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.3314 - val_loss: 36.3544\n",
      "Epoch 1727/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 29.2658 - val_loss: 36.4991\n",
      "Epoch 1728/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.4540 - val_loss: 36.3459\n",
      "Epoch 1729/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.6203 - val_loss: 36.1872\n",
      "Epoch 1730/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.1121 - val_loss: 36.5444\n",
      "Epoch 1731/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.3677 - val_loss: 36.0853\n",
      "Epoch 1732/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.4463 - val_loss: 36.6253\n",
      "Epoch 1733/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.6296 - val_loss: 36.3042\n",
      "Epoch 1734/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.3596 - val_loss: 36.2377\n",
      "Epoch 1735/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.4673 - val_loss: 36.5178\n",
      "Epoch 1736/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.5124 - val_loss: 36.1812\n",
      "Epoch 1737/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 142us/step - loss: 29.1358 - val_loss: 36.5152\n",
      "Epoch 1738/2000\n",
      "378/378 [==============================] - 0s 141us/step - loss: 29.3025 - val_loss: 36.4826\n",
      "Epoch 1739/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 29.4578 - val_loss: 36.3777\n",
      "Epoch 1740/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.6915 - val_loss: 36.2768\n",
      "Epoch 1741/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.5123 - val_loss: 36.1904\n",
      "Epoch 1742/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.2870 - val_loss: 36.7662\n",
      "Epoch 1743/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.5146 - val_loss: 36.8124\n",
      "Epoch 1744/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.5655 - val_loss: 36.1615\n",
      "Epoch 1745/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.6471 - val_loss: 36.1648\n",
      "Epoch 1746/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.4712 - val_loss: 36.5928\n",
      "Epoch 1747/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 29.4480 - val_loss: 36.6170\n",
      "Epoch 1748/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.4298 - val_loss: 36.5309\n",
      "Epoch 1749/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.4710 - val_loss: 36.3661\n",
      "Epoch 1750/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.2641 - val_loss: 36.6674\n",
      "Epoch 1751/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.3054 - val_loss: 36.6089\n",
      "Epoch 1752/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 29.3872 - val_loss: 36.5316\n",
      "Epoch 1753/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.1820 - val_loss: 36.0943\n",
      "Epoch 1754/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.4256 - val_loss: 36.1990\n",
      "Epoch 1755/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.4042 - val_loss: 36.5660\n",
      "Epoch 1756/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.5298 - val_loss: 36.3931\n",
      "Epoch 1757/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.3377 - val_loss: 36.2446\n",
      "Epoch 1758/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.4586 - val_loss: 36.6013\n",
      "Epoch 1759/2000\n",
      "378/378 [==============================] - 0s 137us/step - loss: 29.4856 - val_loss: 36.4243\n",
      "Epoch 1760/2000\n",
      "378/378 [==============================] - 0s 143us/step - loss: 29.0957 - val_loss: 36.5046\n",
      "Epoch 1761/2000\n",
      "378/378 [==============================] - 0s 159us/step - loss: 29.1398 - val_loss: 36.4803\n",
      "Epoch 1762/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.2715 - val_loss: 36.2747\n",
      "Epoch 1763/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 29.3233 - val_loss: 36.5907\n",
      "Epoch 1764/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.1963 - val_loss: 36.5838\n",
      "Epoch 1765/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.4440 - val_loss: 36.5373\n",
      "Epoch 1766/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 29.4012 - val_loss: 36.3204\n",
      "Epoch 1767/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.2108 - val_loss: 36.5906\n",
      "Epoch 1768/2000\n",
      "378/378 [==============================] - 0s 132us/step - loss: 29.1945 - val_loss: 36.0710\n",
      "Epoch 1769/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 29.4225 - val_loss: 35.9654\n",
      "Epoch 1770/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 29.4426 - val_loss: 36.2101\n",
      "Epoch 1771/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 29.7835 - val_loss: 36.2192\n",
      "Epoch 1772/2000\n",
      "378/378 [==============================] - 0s 136us/step - loss: 29.4494 - val_loss: 36.5850\n",
      "Epoch 1773/2000\n",
      "378/378 [==============================] - 0s 131us/step - loss: 29.2153 - val_loss: 36.5363\n",
      "Epoch 1774/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 29.2579 - val_loss: 36.3462\n",
      "Epoch 1775/2000\n",
      "378/378 [==============================] - 0s 130us/step - loss: 29.5042 - val_loss: 36.6014\n",
      "Epoch 1776/2000\n",
      "378/378 [==============================] - 0s 133us/step - loss: 29.0842 - val_loss: 36.2613\n",
      "Epoch 1777/2000\n",
      "378/378 [==============================] - 0s 150us/step - loss: 29.4786 - val_loss: 36.5202\n",
      "Epoch 1778/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 29.3818 - val_loss: 36.2778\n",
      "Epoch 1779/2000\n",
      "378/378 [==============================] - 0s 142us/step - loss: 29.6205 - val_loss: 36.0838\n",
      "Epoch 1780/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.2366 - val_loss: 36.2673\n",
      "Epoch 1781/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.3572 - val_loss: 36.2235\n",
      "Epoch 1782/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.6135 - val_loss: 36.2409\n",
      "Epoch 1783/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.4445 - val_loss: 36.4481\n",
      "Epoch 1784/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.3453 - val_loss: 36.3771\n",
      "Epoch 1785/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.2312 - val_loss: 36.2934\n",
      "Epoch 1786/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.3668 - val_loss: 36.5338\n",
      "Epoch 1787/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.4984 - val_loss: 36.1578\n",
      "Epoch 1788/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.5007 - val_loss: 36.2971\n",
      "Epoch 1789/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.2758 - val_loss: 36.3115\n",
      "Epoch 1790/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.6667 - val_loss: 36.1984\n",
      "Epoch 1791/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.4258 - val_loss: 36.3199\n",
      "Epoch 1792/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.3533 - val_loss: 36.1222\n",
      "Epoch 1793/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.5078 - val_loss: 35.9693\n",
      "Epoch 1794/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.2942 - val_loss: 36.5538\n",
      "Epoch 1795/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.3355 - val_loss: 36.2760\n",
      "Epoch 1796/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.2297 - val_loss: 36.2892\n",
      "Epoch 1797/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.2439 - val_loss: 36.3359\n",
      "Epoch 1798/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.4906 - val_loss: 36.3896\n",
      "Epoch 1799/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.3636 - val_loss: 36.1508\n",
      "Epoch 1800/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 29.2543 - val_loss: 36.4123\n",
      "Epoch 1801/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.3941 - val_loss: 36.3994\n",
      "Epoch 1802/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.4554 - val_loss: 36.2610\n",
      "Epoch 1803/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.6139 - val_loss: 36.8566\n",
      "Epoch 1804/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.4271 - val_loss: 36.1422\n",
      "Epoch 1805/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.0900 - val_loss: 36.3607\n",
      "Epoch 1806/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.2746 - val_loss: 36.6683\n",
      "Epoch 1807/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.1773 - val_loss: 36.2193\n",
      "Epoch 1808/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.3759 - val_loss: 36.0884\n",
      "Epoch 1809/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 29.2374 - val_loss: 36.4271\n",
      "Epoch 1810/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.5190 - val_loss: 36.6966\n",
      "Epoch 1811/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.3898 - val_loss: 36.5373\n",
      "Epoch 1812/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 110us/step - loss: 29.4536 - val_loss: 36.2305\n",
      "Epoch 1813/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 29.0660 - val_loss: 36.2806\n",
      "Epoch 1814/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.2968 - val_loss: 36.3735\n",
      "Epoch 1815/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.4524 - val_loss: 36.3592\n",
      "Epoch 1816/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.1166 - val_loss: 36.2470\n",
      "Epoch 1817/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.1650 - val_loss: 36.3159\n",
      "Epoch 1818/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.4496 - val_loss: 36.3797\n",
      "Epoch 1819/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.4813 - val_loss: 36.2817\n",
      "Epoch 1820/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.7485 - val_loss: 36.1770\n",
      "Epoch 1821/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.1626 - val_loss: 36.3164\n",
      "Epoch 1822/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.2889 - val_loss: 36.4685\n",
      "Epoch 1823/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.1983 - val_loss: 36.6100\n",
      "Epoch 1824/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.3249 - val_loss: 36.3281\n",
      "Epoch 1825/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.2603 - val_loss: 36.4885\n",
      "Epoch 1826/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.2749 - val_loss: 36.4383\n",
      "Epoch 1827/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.2377 - val_loss: 36.2675\n",
      "Epoch 1828/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.3378 - val_loss: 36.3746\n",
      "Epoch 1829/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.0556 - val_loss: 36.1288\n",
      "Epoch 1830/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.4397 - val_loss: 36.2404\n",
      "Epoch 1831/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.2649 - val_loss: 36.4093\n",
      "Epoch 1832/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.3745 - val_loss: 36.1994\n",
      "Epoch 1833/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.1956 - val_loss: 36.4644\n",
      "Epoch 1834/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.5588 - val_loss: 36.3919\n",
      "Epoch 1835/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.4663 - val_loss: 36.1434\n",
      "Epoch 1836/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.1929 - val_loss: 36.3720\n",
      "Epoch 1837/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.2424 - val_loss: 36.3412\n",
      "Epoch 1838/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.7080 - val_loss: 36.2484\n",
      "Epoch 1839/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.2176 - val_loss: 36.2473\n",
      "Epoch 1840/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 29.4036 - val_loss: 36.1948\n",
      "Epoch 1841/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.2539 - val_loss: 36.7203\n",
      "Epoch 1842/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.2724 - val_loss: 36.7166\n",
      "Epoch 1843/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.3068 - val_loss: 36.4029\n",
      "Epoch 1844/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.2007 - val_loss: 36.5640\n",
      "Epoch 1845/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 29.2177 - val_loss: 36.1684\n",
      "Epoch 1846/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.2142 - val_loss: 36.3190\n",
      "Epoch 1847/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.4725 - val_loss: 36.3487\n",
      "Epoch 1848/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.4505 - val_loss: 36.3223\n",
      "Epoch 1849/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.1262 - val_loss: 36.5978\n",
      "Epoch 1850/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.4102 - val_loss: 36.4765\n",
      "Epoch 1851/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.1889 - val_loss: 36.8459\n",
      "Epoch 1852/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.6816 - val_loss: 36.6209\n",
      "Epoch 1853/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.1628 - val_loss: 36.2312\n",
      "Epoch 1854/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.3503 - val_loss: 36.3863\n",
      "Epoch 1855/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.1590 - val_loss: 36.3926\n",
      "Epoch 1856/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.1388 - val_loss: 36.2719\n",
      "Epoch 1857/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.2380 - val_loss: 36.7052\n",
      "Epoch 1858/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.4661 - val_loss: 36.2294\n",
      "Epoch 1859/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.5589 - val_loss: 36.4091\n",
      "Epoch 1860/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.0879 - val_loss: 36.5121\n",
      "Epoch 1861/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.1988 - val_loss: 36.6102\n",
      "Epoch 1862/2000\n",
      "378/378 [==============================] - 0s 135us/step - loss: 29.2626 - val_loss: 36.4439\n",
      "Epoch 1863/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 29.1943 - val_loss: 36.8375\n",
      "Epoch 1864/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.3442 - val_loss: 36.4935\n",
      "Epoch 1865/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.4084 - val_loss: 36.4430\n",
      "Epoch 1866/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.3839 - val_loss: 36.5163\n",
      "Epoch 1867/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.0815 - val_loss: 36.4119\n",
      "Epoch 1868/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.6273 - val_loss: 36.6498\n",
      "Epoch 1869/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.1732 - val_loss: 36.4417\n",
      "Epoch 1870/2000\n",
      "378/378 [==============================] - 0s 108us/step - loss: 29.3993 - val_loss: 36.1892\n",
      "Epoch 1871/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.2673 - val_loss: 36.4463\n",
      "Epoch 1872/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.1040 - val_loss: 36.6804\n",
      "Epoch 1873/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.2442 - val_loss: 36.3599\n",
      "Epoch 1874/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.1827 - val_loss: 36.8257\n",
      "Epoch 1875/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.2191 - val_loss: 36.4404\n",
      "Epoch 1876/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.2097 - val_loss: 36.3472\n",
      "Epoch 1877/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.1499 - val_loss: 36.6175\n",
      "Epoch 1878/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.2058 - val_loss: 36.5373\n",
      "Epoch 1879/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.5918 - val_loss: 36.1027\n",
      "Epoch 1880/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.4739 - val_loss: 36.3599\n",
      "Epoch 1881/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.1229 - val_loss: 36.2757\n",
      "Epoch 1882/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.3684 - val_loss: 36.4786\n",
      "Epoch 1883/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.3987 - val_loss: 36.1589\n",
      "Epoch 1884/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.6708 - val_loss: 36.2204\n",
      "Epoch 1885/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.3671 - val_loss: 36.2900\n",
      "Epoch 1886/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.1852 - val_loss: 36.2443\n",
      "Epoch 1887/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 113us/step - loss: 29.1094 - val_loss: 36.4880\n",
      "Epoch 1888/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.1098 - val_loss: 36.4141\n",
      "Epoch 1889/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.3609 - val_loss: 36.2984\n",
      "Epoch 1890/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.2926 - val_loss: 36.4830\n",
      "Epoch 1891/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.2069 - val_loss: 36.3913\n",
      "Epoch 1892/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.0362 - val_loss: 36.5425\n",
      "Epoch 1893/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.3906 - val_loss: 36.2677\n",
      "Epoch 1894/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.1259 - val_loss: 36.3892\n",
      "Epoch 1895/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.1406 - val_loss: 36.4009\n",
      "Epoch 1896/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.1495 - val_loss: 36.3753\n",
      "Epoch 1897/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.3874 - val_loss: 36.2565\n",
      "Epoch 1898/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 28.9892 - val_loss: 36.4630\n",
      "Epoch 1899/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.5255 - val_loss: 36.6488\n",
      "Epoch 1900/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.0754 - val_loss: 36.3848\n",
      "Epoch 1901/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.2562 - val_loss: 36.4018\n",
      "Epoch 1902/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.2759 - val_loss: 36.3986\n",
      "Epoch 1903/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.2741 - val_loss: 36.1783\n",
      "Epoch 1904/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.1831 - val_loss: 36.2797\n",
      "Epoch 1905/2000\n",
      "378/378 [==============================] - 0s 107us/step - loss: 29.3858 - val_loss: 36.5850\n",
      "Epoch 1906/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.5155 - val_loss: 36.4496\n",
      "Epoch 1907/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.1966 - val_loss: 36.8119\n",
      "Epoch 1908/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.1874 - val_loss: 36.4616\n",
      "Epoch 1909/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.4360 - val_loss: 36.3076\n",
      "Epoch 1910/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.4071 - val_loss: 36.6487\n",
      "Epoch 1911/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.1803 - val_loss: 36.2363\n",
      "Epoch 1912/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.1596 - val_loss: 36.5315\n",
      "Epoch 1913/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.1588 - val_loss: 36.7214\n",
      "Epoch 1914/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.5171 - val_loss: 36.2590\n",
      "Epoch 1915/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.2780 - val_loss: 36.5648\n",
      "Epoch 1916/2000\n",
      "378/378 [==============================] - 0s 115us/step - loss: 29.1111 - val_loss: 36.4804\n",
      "Epoch 1917/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.0096 - val_loss: 36.5125\n",
      "Epoch 1918/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.2689 - val_loss: 36.2552\n",
      "Epoch 1919/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.0844 - val_loss: 36.3778\n",
      "Epoch 1920/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.2548 - val_loss: 36.2989\n",
      "Epoch 1921/2000\n",
      "378/378 [==============================] - 0s 103us/step - loss: 29.0382 - val_loss: 36.5004\n",
      "Epoch 1922/2000\n",
      "378/378 [==============================] - 0s 103us/step - loss: 29.2864 - val_loss: 36.3413\n",
      "Epoch 1923/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.0902 - val_loss: 36.3751\n",
      "Epoch 1924/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.2456 - val_loss: 36.3539\n",
      "Epoch 1925/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.1198 - val_loss: 36.4112\n",
      "Epoch 1926/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.3584 - val_loss: 36.4828\n",
      "Epoch 1927/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.1463 - val_loss: 36.4714\n",
      "Epoch 1928/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.3033 - val_loss: 35.9157\n",
      "Epoch 1929/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.3461 - val_loss: 36.5516\n",
      "Epoch 1930/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.2140 - val_loss: 36.0006\n",
      "Epoch 1931/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.4731 - val_loss: 36.4346\n",
      "Epoch 1932/2000\n",
      "378/378 [==============================] - 0s 116us/step - loss: 29.2346 - val_loss: 36.4341\n",
      "Epoch 1933/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.1679 - val_loss: 36.1940\n",
      "Epoch 1934/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.1774 - val_loss: 36.4348\n",
      "Epoch 1935/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.3904 - val_loss: 36.6980\n",
      "Epoch 1936/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.3131 - val_loss: 36.4070\n",
      "Epoch 1937/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.1534 - val_loss: 36.6378\n",
      "Epoch 1938/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.2365 - val_loss: 36.4778\n",
      "Epoch 1939/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.2907 - val_loss: 36.0800\n",
      "Epoch 1940/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.1924 - val_loss: 36.4080\n",
      "Epoch 1941/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.1393 - val_loss: 36.4964\n",
      "Epoch 1942/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.3658 - val_loss: 36.6641\n",
      "Epoch 1943/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.3757 - val_loss: 36.3981\n",
      "Epoch 1944/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.2830 - val_loss: 36.2919\n",
      "Epoch 1945/2000\n",
      "378/378 [==============================] - 0s 110us/step - loss: 29.3066 - val_loss: 36.4058\n",
      "Epoch 1946/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.2878 - val_loss: 36.4196\n",
      "Epoch 1947/2000\n",
      "378/378 [==============================] - 0s 109us/step - loss: 29.2297 - val_loss: 36.5618\n",
      "Epoch 1948/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.0608 - val_loss: 36.8281\n",
      "Epoch 1949/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.1774 - val_loss: 36.5080\n",
      "Epoch 1950/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.3514 - val_loss: 36.4702\n",
      "Epoch 1951/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.5278 - val_loss: 36.4122\n",
      "Epoch 1952/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.3462 - val_loss: 36.3105\n",
      "Epoch 1953/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 29.1784 - val_loss: 36.4440\n",
      "Epoch 1954/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.0688 - val_loss: 36.4924\n",
      "Epoch 1955/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.0680 - val_loss: 36.4239\n",
      "Epoch 1956/2000\n",
      "378/378 [==============================] - 0s 113us/step - loss: 29.3260 - val_loss: 36.2854\n",
      "Epoch 1957/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.1629 - val_loss: 36.5101\n",
      "Epoch 1958/2000\n",
      "378/378 [==============================] - 0s 114us/step - loss: 29.2335 - val_loss: 36.5117\n",
      "Epoch 1959/2000\n",
      "378/378 [==============================] - 0s 112us/step - loss: 28.9942 - val_loss: 36.6847\n",
      "Epoch 1960/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.1829 - val_loss: 36.3757\n",
      "Epoch 1961/2000\n",
      "378/378 [==============================] - 0s 111us/step - loss: 29.2616 - val_loss: 36.0339\n",
      "Epoch 1962/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 117us/step - loss: 29.1151 - val_loss: 36.6130\n",
      "Epoch 1963/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.3439 - val_loss: 36.3368\n",
      "Epoch 1964/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.0381 - val_loss: 36.6103\n",
      "Epoch 1965/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.3226 - val_loss: 36.2251\n",
      "Epoch 1966/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 28.8804 - val_loss: 36.5848\n",
      "Epoch 1967/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 28.9353 - val_loss: 36.8244\n",
      "Epoch 1968/2000\n",
      "378/378 [==============================] - 0s 128us/step - loss: 28.9447 - val_loss: 36.6095\n",
      "Epoch 1969/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.1820 - val_loss: 36.1654\n",
      "Epoch 1970/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.1849 - val_loss: 36.2112\n",
      "Epoch 1971/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.0895 - val_loss: 36.4628\n",
      "Epoch 1972/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.1409 - val_loss: 36.3573\n",
      "Epoch 1973/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.2710 - val_loss: 36.2135\n",
      "Epoch 1974/2000\n",
      "378/378 [==============================] - 0s 139us/step - loss: 29.3100 - val_loss: 36.2706\n",
      "Epoch 1975/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 29.2934 - val_loss: 36.4800\n",
      "Epoch 1976/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.3370 - val_loss: 36.2945\n",
      "Epoch 1977/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.2016 - val_loss: 36.3781\n",
      "Epoch 1978/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.3750 - val_loss: 36.1390\n",
      "Epoch 1979/2000\n",
      "378/378 [==============================] - 0s 127us/step - loss: 29.1983 - val_loss: 36.3250\n",
      "Epoch 1980/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.0267 - val_loss: 36.4516\n",
      "Epoch 1981/2000\n",
      "378/378 [==============================] - 0s 123us/step - loss: 29.1445 - val_loss: 36.4219\n",
      "Epoch 1982/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.0215 - val_loss: 36.5134\n",
      "Epoch 1983/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.5173 - val_loss: 36.6371\n",
      "Epoch 1984/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.2348 - val_loss: 36.3353\n",
      "Epoch 1985/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.2303 - val_loss: 36.7783\n",
      "Epoch 1986/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.0856 - val_loss: 36.2724\n",
      "Epoch 1987/2000\n",
      "378/378 [==============================] - 0s 117us/step - loss: 29.2506 - val_loss: 36.3824\n",
      "Epoch 1988/2000\n",
      "378/378 [==============================] - 0s 120us/step - loss: 29.0885 - val_loss: 36.2211\n",
      "Epoch 1989/2000\n",
      "378/378 [==============================] - 0s 118us/step - loss: 29.4790 - val_loss: 36.5431\n",
      "Epoch 1990/2000\n",
      "378/378 [==============================] - 0s 124us/step - loss: 29.3105 - val_loss: 36.1248\n",
      "Epoch 1991/2000\n",
      "378/378 [==============================] - 0s 126us/step - loss: 29.4018 - val_loss: 36.2078\n",
      "Epoch 1992/2000\n",
      "378/378 [==============================] - 0s 119us/step - loss: 29.1111 - val_loss: 36.2148\n",
      "Epoch 1993/2000\n",
      "378/378 [==============================] - 0s 152us/step - loss: 29.0400 - val_loss: 36.2317\n",
      "Epoch 1994/2000\n",
      "378/378 [==============================] - 0s 143us/step - loss: 29.2110 - val_loss: 36.2380\n",
      "Epoch 1995/2000\n",
      "378/378 [==============================] - 0s 149us/step - loss: 29.2858 - val_loss: 36.4075\n",
      "Epoch 1996/2000\n",
      "378/378 [==============================] - 0s 134us/step - loss: 29.4446 - val_loss: 36.7368\n",
      "Epoch 1997/2000\n",
      "378/378 [==============================] - 0s 129us/step - loss: 28.9481 - val_loss: 36.2917\n",
      "Epoch 1998/2000\n",
      "378/378 [==============================] - 0s 121us/step - loss: 29.3297 - val_loss: 36.1729\n",
      "Epoch 1999/2000\n",
      "378/378 [==============================] - 0s 125us/step - loss: 29.4310 - val_loss: 36.5095\n",
      "Epoch 2000/2000\n",
      "378/378 [==============================] - 0s 122us/step - loss: 29.1290 - val_loss: 36.4876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a529037f0>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "model.compile('adam','mean_absolute_error')\n",
    "model.fit(train_agg, train_all_appliances, epochs=2000, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6893106745816882"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame((train_hvac/train_agg)).mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_agg)[:, :24].shape\n",
    "test_hvac.shape\n",
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.728842441\n",
      "34.8210505671\n",
      "6.31343235173\n",
      "14.5348578049\n",
      "5.65803159846\n",
      "20.7971985344\n"
     ]
    }
   ],
   "source": [
    "pred_hvac = model.predict(test_agg)[:, :24]\n",
    "pred_fridge = model.predict(test_agg)[:, 24:48]\n",
    "pred_mw = model.predict(test_agg)[:, 48:72]\n",
    "pred_dw = model.predict(test_agg)[:, 72:96]\n",
    "pred_wm = model.predict(test_agg)[:, 96:120]\n",
    "pred_oven = model.predict(test_agg)[:, 120:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(pred_hvac, test_hvac))\n",
    "print(mean_absolute_error(pred_fridge, test_fridge))\n",
    "print(mean_absolute_error(pred_mw, test_mw))\n",
    "print(mean_absolute_error(pred_dw, test_dw))\n",
    "print(mean_absolute_error(pred_wm, test_wm))\n",
    "print(mean_absolute_error(pred_oven, test_oven))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(mean_absolute_error(test_agg, pred_hvac))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190.15069580078125"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.predict(test_agg)[:, :]).mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 24)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a45bcc5c0>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nj7JwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a44806128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pd.Series(train_appliance[0]).plot()\n",
    "pd.Series(train_all_appliances[0]).plot(label='GT')\n",
    "pd.Series(model.predict(test_agg)[0, :]).plot(label='Pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a3afb24a8>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzLnQm5llzoXezCxzLvRmZpn7F6XLr/sxKXumAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a45cb0e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pd.Series(test_agg[1, :]).plot(label='GT Agg')\n",
    "#pd.Series(test_hvac[1, :]).plot(label='GT HVAC')\n",
    "pd.Series(test_fridge[1, :]).plot(label='GT HVAC')\n",
    "\n",
    "\n",
    "#pd.Series(test_fridge[1, :]).plot(label='GT Fridge')\n",
    "\n",
    "\n",
    "#pd.Series(test_mw[1, :]).plot(label='GT MW')\n",
    "#pd.Series(test_oven[1, :]).plot(label='GT Oven')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pd.Series(model.predict(test_agg[1:2])[0, :24]).plot(label='Pred HVAC')\n",
    "pd.Series(model.predict(test_agg[1:2])[0, 24:48]).plot(label='Pred HVAC')\n",
    "\n",
    "\n",
    "#pd.Series(model.predict(test_agg[1:2])[0, 24:]).plot(label='Pred Fridge')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.6465 88.9532061557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1034.436, 937.65674098776663, 1072.7840002015712)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.mean(pred_fridge), np.mean(test_fridge))\n",
    "np.mean(pred_hvac), np.mean(test_hvac), np.mean(test_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.DataFrame(test_fridge)-pd.DataFrame(pred_fridge)).abs().sum(axis=1).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a45f2afd0>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFxGpcQp6EZEa9/8BcAncbKJdI7IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a40238278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(test_fridge[1, :]).plot(label='GT')\n",
    "pd.Series(model.predict(test_agg_new[1:2])[0, 24:]).plot(label='Pred')\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
