{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/nipun/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "import numpy as np\n",
    "\n",
    "from attention_utils import get_activations, get_data\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.models import *\n",
    "from keras.layers import Input, Dense, merge\n",
    "\n",
    "input_dim = 32\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    # ATTENTION PART STARTS HERE\n",
    "    attention_probs = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)\n",
    "    attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul', mode='mul')\n",
    "    # ATTENTION PART FINISHES HERE\n",
    "\n",
    "    attention_mul = Dense(64)(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nipun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/nipun/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/Users/nipun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Dense)           (None, 32)           1056        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Merge)           (None, 32)           0           input_1[0][0]                    \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           2112        attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            65          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,233\n",
      "Trainable params: 3,233\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 0s 88us/step - loss: 0.6821 - acc: 0.6232 - val_loss: 0.6697 - val_acc: 0.6956\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 0.6380 - acc: 0.7472 - val_loss: 0.6008 - val_acc: 0.7832\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 0s 27us/step - loss: 0.5420 - acc: 0.7990 - val_loss: 0.4934 - val_acc: 0.8154\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 0.4286 - acc: 0.8406 - val_loss: 0.3850 - val_acc: 0.8654\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 0s 30us/step - loss: 0.3147 - acc: 0.8904 - val_loss: 0.2712 - val_acc: 0.9026\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 0s 24us/step - loss: 0.1990 - acc: 0.9472 - val_loss: 0.1571 - val_acc: 0.9596\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 0s 24us/step - loss: 0.0990 - acc: 0.9846 - val_loss: 0.0699 - val_acc: 0.9900\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 0s 27us/step - loss: 0.0394 - acc: 0.9980 - val_loss: 0.0282 - val_acc: 0.9982\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 0.0156 - acc: 0.9998 - val_loss: 0.0129 - val_acc: 0.9994\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 0s 25us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 0s 30us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 0s 33us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 9.9794e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 0s 25us/step - loss: 8.3094e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 7.0426e-04 - acc: 1.0000 - val_loss: 8.6542e-04 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 6.0485e-04 - acc: 1.0000 - val_loss: 7.4633e-04 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 0s 28us/step - loss: 5.2477e-04 - acc: 1.0000 - val_loss: 6.4944e-04 - val_acc: 1.0000\n",
      "----- activations -----\n",
      "(1, 32)\n",
      "attention = [ 0.01150401  0.52674651  0.00755742  0.01150589  0.02424546  0.01212222\n",
      "  0.04358535  0.00705368  0.0210755   0.01617499  0.00856732  0.01050172\n",
      "  0.00966774  0.01577939  0.02054703  0.01533236  0.00677264  0.01053563\n",
      "  0.01014007  0.00950928  0.01210809  0.04222351  0.01706294  0.0078827\n",
      "  0.00999842  0.01663759  0.01594953  0.0153873   0.00839415  0.02027451\n",
      "  0.02437302  0.01078409]\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "inputs_1, outputs = get_data(N, input_dim)\n",
    "\n",
    "m = build_model()\n",
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())\n",
    "\n",
    "m.fit([inputs_1], outputs, epochs=20, batch_size=64, validation_split=0.5)\n",
    "\n",
    "testing_inputs_1, testing_outputs = get_data(1, input_dim)\n",
    "\n",
    "# Attention vector corresponds to the second matrix.\n",
    "# The first one is the Inputs output.\n",
    "attention_vector = get_activations(m, testing_inputs_1,\n",
    "                                   print_shape_only=True,\n",
    "                                   layer_name='attention_vec')[0].flatten()\n",
    "print('attention =', attention_vector)\n",
    "\n",
    "# plot part.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x181ea9e2e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD/CAYAAAAKVJb/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4S5JBfo/pz2TofyLUekAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1818bd1940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "pd.Series(attention_vector).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "\n",
    "from attention_utils import get_activations, get_data_recurrent\n",
    "\n",
    "INPUT_DIM = 2\n",
    "TIME_STEPS = 20\n",
    "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False\n",
    "\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "\n",
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_attention_applied_before_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    lstm_units = 32\n",
    "    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nipun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/nipun/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/Users/nipun/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 20, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20, 32)       4480        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 32, 20)       0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 32, 20)       0           permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32, 20)       420         reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 20, 32)       0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Merge)           (None, 20, 32)       0           lstm_1[0][0]                     \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 640)          0           attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            641         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,541\n",
      "Trainable params: 5,541\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/1\n",
      "270000/270000 [==============================] - 88s 327us/step - loss: 0.0464 - acc: 0.9793 - val_loss: 1.1967e-04 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "N = 300000\n",
    "# N = 300 -> too few = no training\n",
    "inputs_1, outputs = get_data_recurrent(N, TIME_STEPS, INPUT_DIM)\n",
    "\n",
    "if APPLY_ATTENTION_BEFORE_LSTM:\n",
    "    m = model_attention_applied_before_lstm()\n",
    "else:\n",
    "    m = model_attention_applied_after_lstm()\n",
    "\n",
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())\n",
    "\n",
    "m.fit([inputs_1], outputs, epochs=1, batch_size=64, validation_split=0.1)\n",
    "\n",
    "attention_vectors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00219333  0.00232133  0.00242945  0.00309207  0.0023527   0.00298569\n",
      "  0.00257227  0.00225874  0.00228695  0.0035669   0.59385484  0.00524928\n",
      "  0.35540175  0.00301347  0.00290094  0.00281188  0.00294731  0.00208525\n",
      "  0.00285069  0.00282516]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00115312  0.00143374  0.00188214  0.00144839  0.00165808  0.00145347\n",
      "  0.00124488  0.0010887   0.00141622  0.00248096  0.58267438  0.00310863\n",
      "  0.38659108  0.00271614  0.00215945  0.00177226  0.00149431  0.00149383\n",
      "  0.00124745  0.00148275]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00232666  0.00229853  0.00297646  0.00275319  0.00221134  0.00232335\n",
      "  0.00197967  0.00253047  0.00195957  0.003157    0.59138513  0.00446098\n",
      "  0.36147904  0.00255452  0.0017961   0.00413846  0.0023488   0.00222059\n",
      "  0.00238167  0.00271844]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00237926  0.00248061  0.00259802  0.0027959   0.00250207  0.00245417\n",
      "  0.00245222  0.00245178  0.00271065  0.0037774   0.61856395  0.00476936\n",
      "  0.32469082  0.00438615  0.00403217  0.00375716  0.00389838  0.00320543\n",
      "  0.00276335  0.00333114]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00099696  0.00103509  0.00129089  0.00134412  0.00117284  0.0011154\n",
      "  0.00119179  0.00086317  0.00110163  0.00224521  0.58969426  0.00229969\n",
      "  0.38728422  0.00202825  0.00115812  0.00128634  0.00075813  0.00095524\n",
      "  0.001141    0.00103764]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00134539  0.00140837  0.00138085  0.00124159  0.00134469  0.00138855\n",
      "  0.00120476  0.00117953  0.00139213  0.00177178  0.59232551  0.0027149\n",
      "  0.37869236  0.00349756  0.00138143  0.00235623  0.00095018  0.00151713\n",
      "  0.00129744  0.00160958]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00121798  0.00138404  0.00154697  0.00136517  0.00132162  0.00135281\n",
      "  0.0013766   0.00119485  0.00139397  0.00219214  0.58579415  0.00276013\n",
      "  0.38495845  0.0027911   0.00165818  0.00193695  0.00145674  0.00132885\n",
      "  0.00146053  0.00150883]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00153345  0.00155772  0.00175584  0.00151689  0.00150209  0.00156776\n",
      "  0.00162258  0.00140275  0.00158066  0.00243691  0.59060383  0.00309315\n",
      "  0.37597561  0.00340023  0.0017677   0.00230718  0.00141827  0.00150531\n",
      "  0.00168102  0.00177103]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0015862   0.00210165  0.00225138  0.0020425   0.00203818  0.00220745\n",
      "  0.00224908  0.00135176  0.00237315  0.00378449  0.57502317  0.00395644\n",
      "  0.38063371  0.00431429  0.00360127  0.00242826  0.00201746  0.00204933\n",
      "  0.00187675  0.00211348]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00111113  0.00233181  0.00194544  0.00171614  0.00169148  0.00163004\n",
      "  0.00180642  0.00152925  0.00216074  0.00434829  0.58386576  0.00324006\n",
      "  0.37709126  0.00355756  0.00240613  0.00234918  0.00110354  0.00228023\n",
      "  0.00205366  0.00178193]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00253996  0.00264729  0.00290346  0.00316029  0.0029422   0.00242751\n",
      "  0.00306655  0.00271541  0.00255214  0.00615769  0.59440488  0.00659924\n",
      "  0.34195191  0.00445871  0.00587551  0.00315619  0.00301102  0.00259514\n",
      "  0.0028693   0.00396559]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00156613  0.00179245  0.00194568  0.00201356  0.00191212  0.00179079\n",
      "  0.00203901  0.0017225   0.00186725  0.00364243  0.59088087  0.00476558\n",
      "  0.36832938  0.00393559  0.00261549  0.00216767  0.00130698  0.0015327\n",
      "  0.00205977  0.00211406]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00084852  0.00102143  0.00106534  0.0009858   0.00100379  0.00089222\n",
      "  0.00098701  0.00083074  0.00101382  0.00209078  0.59186077  0.00221108\n",
      "  0.3871941   0.00213245  0.00097926  0.00132574  0.00059656  0.0008989\n",
      "  0.00100391  0.00105782]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00150273  0.00141612  0.00151848  0.00143175  0.00135772  0.00139701\n",
      "  0.00130392  0.00131822  0.001426    0.00206241  0.59055525  0.0028275\n",
      "  0.37853855  0.00302058  0.00144077  0.00269611  0.00135505  0.00162676\n",
      "  0.0014207   0.00178441]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00094853  0.00147928  0.00143791  0.00138713  0.00132179  0.0012725\n",
      "  0.00128051  0.00130794  0.00138829  0.00236782  0.58909202  0.00297142\n",
      "  0.38363734  0.00187573  0.00178488  0.0014281   0.00098081  0.00141163\n",
      "  0.00124154  0.00138482]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00093559  0.00113299  0.0013348   0.00149662  0.00119469  0.00114007\n",
      "  0.00127394  0.00104182  0.00112905  0.00283553  0.59035218  0.00260958\n",
      "  0.38484836  0.0019434   0.00107098  0.00131497  0.00092302  0.00095275\n",
      "  0.00137473  0.00109497]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00109628  0.00122651  0.00143119  0.0014236   0.00130097  0.00120477\n",
      "  0.0012262   0.00105422  0.00135299  0.00251248  0.58754319  0.00273134\n",
      "  0.38509828  0.00252907  0.00174646  0.00156382  0.00096182  0.0013187\n",
      "  0.00126736  0.00141078]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00129627  0.0013843   0.00156657  0.00129029  0.00129215  0.00133026\n",
      "  0.0012948   0.00121793  0.00136387  0.00183104  0.58058435  0.00273354\n",
      "  0.39091361  0.00248019  0.00163142  0.00192904  0.00151603  0.00132894\n",
      "  0.0014745   0.00154089]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00354069  0.00435924  0.00457663  0.00481773  0.00399237  0.00407511\n",
      "  0.00405975  0.00391843  0.00441412  0.00824991  0.56861758  0.00972428\n",
      "  0.3371461   0.00715012  0.0078855   0.00510317  0.00442266  0.00362173\n",
      "  0.00419571  0.00612916]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00108035  0.00140388  0.00147035  0.00148746  0.00147186  0.00121546\n",
      "  0.0013784   0.00142517  0.00169728  0.0024139   0.58419085  0.00252518\n",
      "  0.38636118  0.00261845  0.00162854  0.00170188  0.00134219  0.00159971\n",
      "  0.00167331  0.00131459]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00224713  0.00203726  0.00275479  0.00279173  0.00203537  0.00215496\n",
      "  0.00411134  0.0019138   0.00233435  0.0079756   0.58279133  0.00429774\n",
      "  0.36023659  0.00357073  0.00419835  0.00258185  0.0041352   0.00192231\n",
      "  0.0032347   0.00267485]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0020689   0.00306116  0.00537708  0.00218041  0.00290853  0.00271936\n",
      "  0.00231323  0.00205389  0.00295203  0.00413182  0.5498153   0.00478713\n",
      "  0.3916387   0.00308465  0.00491977  0.00283755  0.00406278  0.00271774\n",
      "  0.0031578   0.00321215]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0011038   0.0011963   0.00149557  0.00178946  0.00136896  0.0013716\n",
      "  0.00177252  0.00099254  0.001458    0.00494192  0.59073377  0.00303549\n",
      "  0.37827954  0.00234521  0.00201869  0.00125671  0.00098786  0.00118878\n",
      "  0.0014115   0.00125178]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [  6.25532120e-04   6.55567856e-04   7.21387041e-04   6.35799486e-04\n",
      "   6.31706091e-04   6.52932213e-04   6.56273973e-04   5.91095944e-04\n",
      "   7.12552108e-04   9.99785727e-04   6.18864417e-01   1.32582290e-03\n",
      "   3.66400689e-01   1.58176920e-03   8.33390921e-04   1.07493089e-03\n",
      "   8.17756634e-04   7.21105374e-04   6.88126893e-04   8.09404184e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00115008  0.00129178  0.00150781  0.00125663  0.00111212  0.0013793\n",
      "  0.00140228  0.00108573  0.00172014  0.00207016  0.59109175  0.00249679\n",
      "  0.37923443  0.00408209  0.00156651  0.00203936  0.00109779  0.00151472\n",
      "  0.00134424  0.00155633]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00172962  0.00237168  0.00310981  0.00264342  0.00235692  0.0018411\n",
      "  0.00337435  0.00215759  0.00254084  0.01000036  0.58522379  0.00577415\n",
      "  0.35453898  0.00365398  0.00669677  0.00194658  0.00294332  0.00173107\n",
      "  0.00288403  0.00248169]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00180396  0.00195007  0.00230678  0.00217451  0.0018105   0.00215397\n",
      "  0.00227119  0.0017036   0.00201482  0.00325556  0.58752763  0.00360692\n",
      "  0.37084949  0.00282243  0.00211984  0.00257481  0.00262352  0.00177026\n",
      "  0.00249831  0.00216184]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00164197  0.00154973  0.00160735  0.00165797  0.0015311   0.0014926\n",
      "  0.00140654  0.00162969  0.00159865  0.00224334  0.59300733  0.00335686\n",
      "  0.37142467  0.00351228  0.00201979  0.00313627  0.00157985  0.00180583\n",
      "  0.0015077   0.00229044]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00167995  0.0014381   0.00152005  0.00144178  0.00146563  0.00142361\n",
      "  0.00134087  0.00158597  0.0016419   0.00192643  0.59651411  0.00321978\n",
      "  0.36792359  0.00408884  0.00203525  0.00314387  0.00187594  0.00196848\n",
      "  0.00153746  0.00222843]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00384689  0.00344256  0.00385587  0.00417104  0.00398339  0.00339019\n",
      "  0.00348493  0.00384043  0.00352096  0.00597628  0.60123795  0.00850556\n",
      "  0.31531927  0.00610326  0.00672898  0.00552525  0.0042401   0.00391293\n",
      "  0.00375514  0.00515901]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00152381  0.0018738   0.00198538  0.00348314  0.00181315  0.00194276\n",
      "  0.00278021  0.00147075  0.00225529  0.00419955  0.58876377  0.00306982\n",
      "  0.36921978  0.00356845  0.00221432  0.00229556  0.00199003  0.00159405\n",
      "  0.00197464  0.00198164]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [ 0.00156702  0.00181267  0.00257713  0.00180258  0.00222495  0.00187467\n",
      "  0.00165384  0.00149288  0.00177654  0.00265113  0.58578777  0.00389145\n",
      "  0.37631443  0.00307948  0.00250746  0.00206359  0.00142026  0.00169143\n",
      "  0.00206401  0.00174673]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00119897  0.00142774  0.00141895  0.00133402  0.00128046  0.00158428\n",
      "  0.00147604  0.00116291  0.00157103  0.00173342  0.60391992  0.00260315\n",
      "  0.36513823  0.00286238  0.00235015  0.00188309  0.00215279  0.00150703\n",
      "  0.00149465  0.00190076]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0021259   0.00224122  0.00231955  0.00258899  0.0024504   0.0022218\n",
      "  0.00202949  0.00219152  0.00206992  0.00314404  0.59207404  0.0051355\n",
      "  0.35991925  0.00411593  0.00295751  0.00356223  0.00175145  0.00206894\n",
      "  0.00209185  0.00294052]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00149915  0.00138543  0.00231119  0.00188873  0.00141097  0.00156089\n",
      "  0.0020333   0.00099421  0.00152519  0.005486    0.58355004  0.0031681\n",
      "  0.37942553  0.00290694  0.00323015  0.00158847  0.00116294  0.00136316\n",
      "  0.0016995   0.00181013]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00201662  0.00245012  0.00270425  0.00352713  0.00243025  0.00216262\n",
      "  0.00379665  0.00206864  0.00272682  0.00538173  0.58336604  0.0034093\n",
      "  0.36295182  0.00371139  0.00283588  0.00310609  0.00331224  0.00228013\n",
      "  0.00347909  0.00228316]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00263915  0.00311968  0.00314176  0.0036785   0.00299392  0.00274507\n",
      "  0.00314566  0.00341121  0.0032121   0.00694353  0.58294713  0.00735375\n",
      "  0.34626466  0.00535781  0.00566562  0.00378222  0.00300758  0.00301288\n",
      "  0.00307498  0.00450274]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00304863  0.00296432  0.00453401  0.00442713  0.00304484  0.00506236\n",
      "  0.00320257  0.0026484   0.00350923  0.00469968  0.55677176  0.00898393\n",
      "  0.37020478  0.00578973  0.00437885  0.00377288  0.00314994  0.00284529\n",
      "  0.00308996  0.00387168]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00341695  0.00250317  0.00461968  0.00234055  0.00275891  0.00227009\n",
      "  0.0023259   0.00220877  0.00231491  0.00400243  0.57434905  0.00577149\n",
      "  0.3655948   0.00517819  0.00440139  0.00444992  0.00234771  0.00247586\n",
      "  0.0027313   0.00393897]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00141216  0.00198319  0.002806    0.00187817  0.00182883  0.00185062\n",
      "  0.00207391  0.00160048  0.00207352  0.00541558  0.58243078  0.00490028\n",
      "  0.37071896  0.00285114  0.00420259  0.00191943  0.00387894  0.00182892\n",
      "  0.00167959  0.00266692]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00147041  0.00143114  0.00204397  0.00129648  0.00143018  0.00128168\n",
      "  0.00156517  0.00147158  0.00153069  0.00279681  0.58452469  0.00321576\n",
      "  0.38040334  0.00290388  0.00258143  0.00204493  0.00197322  0.00177726\n",
      "  0.00178902  0.00246834]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00176083  0.00152613  0.00166163  0.00158458  0.00154444  0.00161839\n",
      "  0.00145987  0.00170087  0.00160655  0.00209014  0.59263963  0.00332139\n",
      "  0.37269211  0.00309678  0.00148984  0.0031316   0.00154152  0.00179645\n",
      "  0.0017118   0.00202546]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00130806  0.00143619  0.0022154   0.00163372  0.00135963  0.00131063\n",
      "  0.00194451  0.00126564  0.00136505  0.00576293  0.58740592  0.00310664\n",
      "  0.37736747  0.0024401   0.00219116  0.00173892  0.0014024   0.00117527\n",
      "  0.00180774  0.00176264]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00215062  0.00216983  0.00237203  0.00239494  0.00234233  0.00206758\n",
      "  0.00273475  0.00186435  0.00241077  0.00483802  0.58742976  0.00410529\n",
      "  0.36314118  0.00495764  0.00332304  0.00268529  0.00190068  0.00212069\n",
      "  0.00269869  0.00229258]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00126217  0.00138563  0.00167677  0.0013101   0.00126747  0.00138963\n",
      "  0.0014028   0.00125187  0.001675    0.00219153  0.58798897  0.00264192\n",
      "  0.38067043  0.00370481  0.00185228  0.00217493  0.00124925  0.00167431\n",
      "  0.00149505  0.00173504]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00163287  0.00212323  0.00214923  0.00201804  0.00222982  0.00162487\n",
      "  0.00196062  0.00193797  0.00221098  0.00357953  0.57936126  0.00438394\n",
      "  0.37922102  0.00293198  0.00288262  0.00210258  0.00215476  0.00157763\n",
      "  0.00220538  0.00171166]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00262183  0.00267589  0.00288693  0.00327395  0.00290921  0.00237017\n",
      "  0.00335244  0.00281388  0.00249051  0.00734967  0.58623588  0.00596942\n",
      "  0.34961042  0.00414936  0.00510619  0.00324803  0.00314499  0.00270853\n",
      "  0.00330912  0.00377358]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00197132  0.00256353  0.00275515  0.00228949  0.00240653  0.00228119\n",
      "  0.00216177  0.00243518  0.00212791  0.00332776  0.59691763  0.00492828\n",
      "  0.35427433  0.00339452  0.00273751  0.00358303  0.00234266  0.00205414\n",
      "  0.00246625  0.00298185]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00238477  0.00233639  0.00266469  0.00345057  0.00261537  0.00237279\n",
      "  0.00249515  0.00272542  0.00254007  0.00543546  0.58814716  0.00606222\n",
      "  0.3555159   0.00373259  0.0037421   0.00309279  0.00225787  0.00265645\n",
      "  0.00266793  0.00310432]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00235104  0.00239296  0.00255224  0.00293529  0.00262961  0.00238816\n",
      "  0.00225915  0.00235812  0.00219646  0.00333833  0.58910966  0.00540045\n",
      "  0.35939372  0.00410691  0.00306923  0.00376717  0.00211208  0.00221313\n",
      "  0.00235779  0.00306847]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00108231  0.00119315  0.00133966  0.00116997  0.00115538  0.00110512\n",
      "  0.00108703  0.00101325  0.00120114  0.00193973  0.59240657  0.00242267\n",
      "  0.38256752  0.00276124  0.00115752  0.00190369  0.00076435  0.00121835\n",
      "  0.00116725  0.0013441 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00093702  0.00101634  0.00121056  0.00103857  0.00094379  0.00105631\n",
      "  0.00096284  0.00084863  0.00108502  0.00159189  0.59205806  0.00221721\n",
      "  0.38555861  0.00271598  0.00100569  0.0017145   0.00066496  0.00115436\n",
      "  0.00097537  0.00124437]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00179025  0.00173717  0.00220028  0.00182437  0.00190288  0.00162595\n",
      "  0.00191259  0.00173425  0.00179708  0.00327925  0.58544886  0.00426951\n",
      "  0.37408984  0.00332335  0.00271473  0.00235512  0.00183826  0.0017617\n",
      "  0.00204468  0.00234988]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00328949  0.00296904  0.003471    0.00407051  0.00337085  0.00298767\n",
      "  0.0036378   0.00368501  0.00351699  0.00645609  0.61034024  0.00721751\n",
      "  0.3114543   0.00569356  0.00675251  0.00442518  0.00449777  0.00377893\n",
      "  0.00363811  0.00474743]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00119641  0.00119969  0.0012346   0.00126603  0.00117847  0.00122853\n",
      "  0.00114822  0.00110358  0.00119858  0.00177322  0.59153748  0.00264454\n",
      "  0.38193837  0.00276488  0.00144841  0.00187087  0.00103088  0.00130902\n",
      "  0.00125689  0.00167137]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00106047  0.00148922  0.00160001  0.0012555   0.00128227  0.00124499\n",
      "  0.00124438  0.00117815  0.00169501  0.00242882  0.58224165  0.00268936\n",
      "  0.38836089  0.00272899  0.00202793  0.00178625  0.001174    0.00163775\n",
      "  0.00133899  0.00153531]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00112764  0.00099907  0.0013835   0.00095156  0.00100849  0.00087664\n",
      "  0.00128962  0.00100869  0.0010215   0.0017221   0.58122164  0.00218402\n",
      "  0.39345241  0.00232195  0.00291555  0.00166877  0.00124905  0.00095992\n",
      "  0.0012079   0.00142995]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00123639  0.00171147  0.00170771  0.00227799  0.00163068  0.00179566\n",
      "  0.00176974  0.00140769  0.00198046  0.00382777  0.58346784  0.00372591\n",
      "  0.37821263  0.00400404  0.00262496  0.00212284  0.0013248   0.00176813\n",
      "  0.00158776  0.00181552]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00140963  0.00155962  0.00183597  0.00178635  0.00162104  0.00165863\n",
      "  0.00157174  0.00149178  0.00154035  0.00289022  0.58243704  0.00300846\n",
      "  0.38508081  0.00236435  0.00141327  0.00208786  0.00142117  0.00149397\n",
      "  0.001776    0.00155168]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [ 0.00102901  0.00127366  0.00133221  0.00138786  0.00119129  0.00132548\n",
      "  0.00114997  0.00106255  0.00130422  0.00216746  0.58723682  0.00256086\n",
      "  0.38616639  0.00271796  0.00152152  0.00164225  0.00100265  0.00131999\n",
      "  0.00115608  0.00145172]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00238433  0.00217827  0.00287915  0.00285433  0.00237031  0.00264483\n",
      "  0.00242446  0.00247507  0.00221392  0.00354886  0.58683825  0.00612582\n",
      "  0.35946479  0.0039203   0.00311881  0.0032684   0.00266769  0.00233548\n",
      "  0.00252944  0.00375745]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00137892  0.00147749  0.00164033  0.00162648  0.00144607  0.00153132\n",
      "  0.00149228  0.00140727  0.00154343  0.00245005  0.58707929  0.00331253\n",
      "  0.3799071   0.00301339  0.00179496  0.00215519  0.00158166  0.00155979\n",
      "  0.00159531  0.00200719]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00223883  0.00261554  0.00272436  0.00284938  0.00290175  0.00240513\n",
      "  0.00241719  0.00251512  0.00243037  0.00509193  0.5950222   0.00722205\n",
      "  0.34611773  0.00451452  0.0053226   0.00283927  0.00220619  0.00243398\n",
      "  0.00245567  0.00367621]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.001217    0.00164664  0.001892    0.0014867   0.0015297   0.00127099\n",
      "  0.0017006   0.00134941  0.00196269  0.00344334  0.58191693  0.00317695\n",
      "  0.38362902  0.00307041  0.00277131  0.00191458  0.00142955  0.00153978\n",
      "  0.00156052  0.00149192]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00242845  0.00241217  0.00258197  0.0034172   0.00277283  0.00250377\n",
      "  0.00279588  0.00257021  0.00240694  0.00570286  0.59056628  0.00635306\n",
      "  0.35189679  0.00365132  0.00461288  0.00270951  0.00216666  0.00248817\n",
      "  0.00270991  0.00325316]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00174695  0.0015788   0.00219397  0.00214787  0.00163816  0.00201563\n",
      "  0.00188169  0.00145073  0.00205319  0.00262271  0.59161377  0.0036048\n",
      "  0.36954409  0.00313581  0.00215162  0.00246146  0.00255706  0.00176733\n",
      "  0.00182424  0.00201007]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00205574  0.00200181  0.00317816  0.00311091  0.00196087  0.00193992\n",
      "  0.00409114  0.00179695  0.00217995  0.00964138  0.58797944  0.00366787\n",
      "  0.35611546  0.00284205  0.00368583  0.00203104  0.0039641   0.00192069\n",
      "  0.00348047  0.00235619]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00177652  0.00174034  0.00186626  0.00193318  0.00174472  0.00182736\n",
      "  0.00182166  0.00181401  0.00187055  0.00284478  0.58054799  0.00402793\n",
      "  0.37844881  0.00479463  0.00240143  0.00287347  0.00157095  0.00176205\n",
      "  0.00194066  0.0023927 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00174322  0.00153877  0.00154039  0.00161227  0.00152052  0.00172327\n",
      "  0.00150515  0.00150001  0.00160306  0.00197011  0.59403014  0.00343765\n",
      "  0.3699877   0.00387727  0.0019547   0.0027477   0.00167108  0.00188102\n",
      "  0.00181698  0.00233896]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00182145  0.00173885  0.0021136   0.00222597  0.00191909  0.00168935\n",
      "  0.00219428  0.00191417  0.00196524  0.00411442  0.61494035  0.00430457\n",
      "  0.34191635  0.00335388  0.00319799  0.0023892   0.00201682  0.00177998\n",
      "  0.00210528  0.0022992 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00102845  0.00203399  0.00201936  0.00178244  0.00158418  0.00163214\n",
      "  0.00200466  0.00159714  0.00193542  0.00663436  0.58998001  0.00371328\n",
      "  0.36999425  0.00260135  0.00274921  0.00154245  0.00138053  0.0017611\n",
      "  0.00199579  0.00202987]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00096213  0.00104725  0.00117423  0.00106903  0.00101567  0.00109805\n",
      "  0.00103832  0.00089002  0.00110823  0.00164362  0.59176749  0.00237564\n",
      "  0.38456437  0.00310728  0.00121322  0.00160364  0.00084244  0.0010899\n",
      "  0.00105351  0.00133597]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00092013  0.00144015  0.00131188  0.00133571  0.00126465  0.00118836\n",
      "  0.00116648  0.00105854  0.0012522   0.0021468   0.58850861  0.00227826\n",
      "  0.38684535  0.00195295  0.00121655  0.00157757  0.0008842   0.00131094\n",
      "  0.00116507  0.00117557]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00184779  0.00172722  0.00230259  0.00229201  0.0016869   0.00195935\n",
      "  0.00209157  0.00164567  0.00205717  0.00318355  0.59027225  0.00334941\n",
      "  0.36980075  0.00283765  0.00172883  0.00252665  0.002615    0.00186193\n",
      "  0.00236424  0.00184947]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00128044  0.00128258  0.00134239  0.00140109  0.00128097  0.00134662\n",
      "  0.00131468  0.00129797  0.0013192   0.0019305   0.587376    0.00294091\n",
      "  0.38366994  0.00280005  0.00169279  0.00191352  0.00125417  0.00133219\n",
      "  0.0014976   0.00172635]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00219872  0.00288908  0.00244811  0.00313368  0.00267203  0.00282738\n",
      "  0.00320272  0.00179695  0.00252544  0.00446729  0.56585407  0.00365693\n",
      "  0.3797543   0.00441875  0.00319982  0.00399404  0.00354866  0.00272722\n",
      "  0.00215807  0.00252674]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00108225  0.00114221  0.00123768  0.00119013  0.00110712  0.0010957\n",
      "  0.00115436  0.00107472  0.00115108  0.00189683  0.59166312  0.00266517\n",
      "  0.38303441  0.00268438  0.00131956  0.00177361  0.00082259  0.00112103\n",
      "  0.00116322  0.0016209 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00198619  0.0017604   0.00237896  0.00396401  0.00207298  0.00267739\n",
      "  0.00305083  0.00225759  0.00219783  0.00712512  0.58309871  0.00752751\n",
      "  0.35858601  0.00352537  0.00544048  0.0017075   0.00315592  0.00166083\n",
      "  0.00250469  0.00332164]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00125761  0.00135303  0.00195873  0.0014449   0.00132072  0.00135218\n",
      "  0.00144367  0.00110651  0.00127773  0.00304366  0.58858228  0.00286283\n",
      "  0.38191611  0.00252016  0.00160089  0.00177551  0.00105819  0.00124526\n",
      "  0.0013312   0.00154881]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00255193  0.00332709  0.00324945  0.00327146  0.00321674  0.00318664\n",
      "  0.00318142  0.00314427  0.00321579  0.00489363  0.58792651  0.00716164\n",
      "  0.34428796  0.00511323  0.00433284  0.00407557  0.00340352  0.00295652\n",
      "  0.00354228  0.00396151]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00125687  0.00141558  0.00152688  0.00136005  0.00124345  0.00134945\n",
      "  0.00130614  0.00137448  0.00153239  0.00208257  0.59134233  0.00276449\n",
      "  0.37814671  0.00303902  0.00138127  0.00268629  0.00136655  0.00158269\n",
      "  0.001435    0.00180778]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00161824  0.00163727  0.00185973  0.00166032  0.0016623   0.00148795\n",
      "  0.00153319  0.00134514  0.00161609  0.00258384  0.58408475  0.00346301\n",
      "  0.3814359   0.00344352  0.00219987  0.00218788  0.00135762  0.00141091\n",
      "  0.00155448  0.00185793]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00231592  0.00302583  0.00310138  0.00300452  0.00299034  0.00254372\n",
      "  0.00256713  0.00313569  0.00281006  0.00426234  0.58892775  0.00550695\n",
      "  0.35042399  0.00404138  0.00383249  0.00418512  0.00363045  0.00296807\n",
      "  0.0030248   0.00370207]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00121736  0.00150007  0.00214888  0.00147532  0.00132867  0.00152127\n",
      "  0.00184294  0.00133832  0.00149788  0.00290432  0.58061767  0.00294403\n",
      "  0.38605621  0.00245957  0.00271909  0.00187346  0.00142206  0.00149419\n",
      "  0.00161431  0.00202432]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0013114   0.00122693  0.0013807   0.00162651  0.00132126  0.00135684\n",
      "  0.00130899  0.00122292  0.00132727  0.00213906  0.61678326  0.00294056\n",
      "  0.35450253  0.00222777  0.00175572  0.00179309  0.00144765  0.00141443\n",
      "  0.00129107  0.00162208]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00239291  0.00241272  0.00274923  0.00355103  0.00262772  0.00232331\n",
      "  0.00299278  0.00259076  0.00242917  0.0068709   0.59150195  0.00617463\n",
      "  0.34821194  0.00371172  0.00507659  0.00287725  0.00270669  0.00246343\n",
      "  0.00264107  0.00369426]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00160176  0.00147883  0.00182149  0.00174015  0.00148887  0.00153947\n",
      "  0.00179488  0.00131748  0.001943    0.00260225  0.59264213  0.00269204\n",
      "  0.37296036  0.00389623  0.00177128  0.00232787  0.00134897  0.00167012\n",
      "  0.00186905  0.00149376]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [ 0.00270217  0.00294768  0.0027399   0.0034881   0.00307639  0.00270676\n",
      "  0.00336259  0.00273825  0.00247823  0.00700257  0.59218132  0.00616136\n",
      "  0.34352398  0.0044417   0.00497705  0.00339111  0.00289473  0.00234104\n",
      "  0.00301833  0.00382676]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00136166  0.00123401  0.00163838  0.00157784  0.00133785  0.0014397\n",
      "  0.00142983  0.00108263  0.0014408   0.00233746  0.59135091  0.00307088\n",
      "  0.37967494  0.0025901   0.00158036  0.00160718  0.00113859  0.00125011\n",
      "  0.00143658  0.00142017]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00222223  0.00252331  0.00260697  0.00294367  0.00276916  0.00271115\n",
      "  0.00232844  0.00275198  0.00266561  0.0036314   0.57852226  0.00674486\n",
      "  0.36584851  0.00400746  0.00347085  0.00326884  0.00285937  0.00237027\n",
      "  0.00250187  0.00325177]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00163799  0.00144014  0.00205711  0.00182923  0.0015641   0.00167595\n",
      "  0.00145507  0.00169183  0.00162211  0.00237195  0.60116494  0.00377827\n",
      "  0.36340839  0.00271982  0.0018364   0.00255846  0.00157608  0.00171569\n",
      "  0.0017625   0.00213401]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00129116  0.0019398   0.00166406  0.00227304  0.00194801  0.00179487\n",
      "  0.00188041  0.00169656  0.00170027  0.00455941  0.58822536  0.00418931\n",
      "  0.37287736  0.00301761  0.00234054  0.00191601  0.00130716  0.00164228\n",
      "  0.00183463  0.00190212]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00214779  0.00301596  0.00200536  0.00206872  0.00226963  0.00160491\n",
      "  0.00271765  0.00178213  0.00241336  0.00702166  0.58360177  0.00340166\n",
      "  0.36151439  0.00585137  0.00484646  0.00402543  0.00243322  0.00217525\n",
      "  0.00224372  0.00285954]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00110847  0.00112846  0.001254    0.00117211  0.00109338  0.00111418\n",
      "  0.00108006  0.0010729   0.00118288  0.00185664  0.5917201   0.00254544\n",
      "  0.38320476  0.00265049  0.0011812   0.0019808   0.00085699  0.00120298\n",
      "  0.00114286  0.00145128]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00151492  0.00155552  0.00158507  0.00153891  0.00145417  0.00163291\n",
      "  0.0014781   0.00136185  0.00149108  0.00215621  0.59062004  0.003221\n",
      "  0.37651461  0.00315706  0.00167452  0.00241451  0.00139387  0.00160035\n",
      "  0.00163048  0.00200476]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00097716  0.00132616  0.00138109  0.00173716  0.00131979  0.00146917\n",
      "  0.00145577  0.00097195  0.00139768  0.00215399  0.58728969  0.00249137\n",
      "  0.38632959  0.00233629  0.00145746  0.0014619   0.00100053  0.00107138\n",
      "  0.00115151  0.00122036]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00085544  0.00096726  0.00115719  0.0010241   0.0009228   0.00100299\n",
      "  0.00101694  0.00080439  0.00102498  0.00164014  0.58972442  0.00200496\n",
      "  0.38904583  0.00228754  0.00119679  0.00130189  0.00084023  0.00104163\n",
      "  0.00101269  0.00112784]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00088117  0.00105453  0.00128791  0.00123413  0.00101727  0.00119706\n",
      "  0.00105568  0.00088409  0.00129549  0.00219481  0.58943129  0.00249911\n",
      "  0.38652441  0.00235442  0.00126901  0.00142328  0.0008497   0.00114572\n",
      "  0.00109928  0.00130165]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00176145  0.00140423  0.00181856  0.00188641  0.00166936  0.00169047\n",
      "  0.00162729  0.00140743  0.00179123  0.0024379   0.59825391  0.00314013\n",
      "  0.36788642  0.00258237  0.00194219  0.00209052  0.00181778  0.0015563\n",
      "  0.00169833  0.00153772]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00213022  0.00198268  0.00220489  0.00211865  0.00189409  0.00222048\n",
      "  0.00200646  0.00210869  0.00225276  0.00278095  0.59063947  0.0039199\n",
      "  0.36320442  0.00397338  0.0021363   0.00377005  0.00277578  0.00258508\n",
      "  0.00256155  0.00273424]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00175809  0.00166536  0.00181112  0.00188516  0.00168582  0.00176739\n",
      "  0.00170201  0.00174886  0.00173701  0.00265562  0.5885731   0.00423489\n",
      "  0.37172201  0.00378311  0.00241907  0.00269171  0.00180865  0.00192169\n",
      "  0.00176005  0.00266925]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00133785  0.0013203   0.00173982  0.00164057  0.0014832   0.00137284\n",
      "  0.00150978  0.001609    0.00132959  0.00292458  0.58299899  0.00326158\n",
      "  0.38562384  0.00187975  0.00174476  0.00177699  0.00162031  0.00119756\n",
      "  0.0019482   0.00168049]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00164122  0.00200937  0.00225087  0.00243293  0.0016872   0.00174955\n",
      "  0.00248679  0.00228283  0.00211804  0.00512176  0.58494198  0.00277873\n",
      "  0.36879057  0.00276645  0.00265129  0.00316899  0.0027808   0.00291885\n",
      "  0.00249514  0.00292665]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00125439  0.00168124  0.00270428  0.00188208  0.0014969   0.00162632\n",
      "  0.0022188   0.00149858  0.00214571  0.00507624  0.5877651   0.00410594\n",
      "  0.37025517  0.00275448  0.00432082  0.00164098  0.00201208  0.00163179\n",
      "  0.00176678  0.00216232]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00142306  0.00138203  0.00138931  0.00131454  0.00134755  0.00131555\n",
      "  0.0013488   0.00123571  0.00136854  0.00205605  0.59079111  0.0028292\n",
      "  0.37846792  0.00359811  0.00175706  0.00229301  0.00126641  0.0014665\n",
      "  0.00150641  0.00184314]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00139722  0.00157297  0.00181415  0.00190995  0.00174286  0.001734\n",
      "  0.00153463  0.00156804  0.00154485  0.00252346  0.59175587  0.0047608\n",
      "  0.37260824  0.00300661  0.0021592   0.00211268  0.00112147  0.00141578\n",
      "  0.00152126  0.00219593]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00139021  0.0013544   0.00160935  0.00186933  0.00147183  0.00142122\n",
      "  0.00139962  0.00140573  0.00140616  0.00242555  0.59102899  0.00321797\n",
      "  0.3785921   0.00247233  0.00147385  0.00205244  0.00095545  0.00136506\n",
      "  0.00150045  0.00158797]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00144212  0.00171736  0.00196363  0.00168742  0.00166889  0.0015042\n",
      "  0.00177217  0.00170833  0.00184628  0.0034931   0.58667815  0.00377904\n",
      "  0.3751117   0.00352691  0.00290478  0.00215213  0.00128189  0.00183699\n",
      "  0.0018899   0.00203502]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00267741  0.00306564  0.00296554  0.00299428  0.00315222  0.00280937\n",
      "  0.00321074  0.00280855  0.00305855  0.00512607  0.60039175  0.00592022\n",
      "  0.33288607  0.00619926  0.00507068  0.00405812  0.00358276  0.00291637\n",
      "  0.00327207  0.00383436]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00179265  0.00163194  0.00162407  0.00170165  0.00160516  0.00155126\n",
      "  0.00153091  0.0016422   0.00167967  0.00227597  0.5915314   0.00345013\n",
      "  0.3702755   0.00426938  0.0022661   0.00336102  0.00186626  0.0019332\n",
      "  0.00159048  0.00242108]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00123935  0.00120911  0.00139843  0.00162863  0.00134376  0.00127043\n",
      "  0.00126316  0.00111896  0.00135198  0.00236075  0.59041727  0.00317994\n",
      "  0.3813571   0.00264063  0.00162637  0.00163388  0.0010462   0.00131733\n",
      "  0.00115845  0.00143823]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0014087   0.0013672   0.0013512   0.00129547  0.0014109   0.00125593\n",
      "  0.00119267  0.00132139  0.00135821  0.00182174  0.59343088  0.0028246\n",
      "  0.37652865  0.00348356  0.00153604  0.00267251  0.00110961  0.00150635\n",
      "  0.00134797  0.0017764 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00337464  0.00442375  0.00337159  0.00593091  0.00477207  0.00322673\n",
      "  0.00419796  0.00350791  0.00328389  0.00725969  0.56706488  0.00506545\n",
      "  0.35494608  0.00422981  0.00366555  0.0060292   0.00488737  0.0030438\n",
      "  0.00410975  0.00360893]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00094271  0.00097716  0.00125785  0.0010738   0.00101134  0.00099187\n",
      "  0.00102917  0.00091222  0.00101504  0.00185099  0.58894026  0.00224006\n",
      "  0.38872343  0.00213885  0.00118948  0.00141991  0.00093528  0.00103179\n",
      "  0.00106664  0.00125215]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00142385  0.00144635  0.0015527   0.00138437  0.00144343  0.00142229\n",
      "  0.00154627  0.0012746   0.00144841  0.00238342  0.60659528  0.002537\n",
      "  0.36155087  0.00276558  0.00191125  0.00213857  0.00206133  0.00154794\n",
      "  0.00187976  0.00168673]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00301575  0.00326214  0.00358127  0.00463663  0.00362268  0.00334359\n",
      "  0.00376385  0.00371481  0.00352511  0.00643928  0.59173405  0.00785193\n",
      "  0.32884794  0.0043233   0.00587846  0.00378912  0.00597272  0.00377239\n",
      "  0.00418574  0.00473921]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0015661   0.00200441  0.00265853  0.00275329  0.00197987  0.00240623\n",
      "  0.00304749  0.00191676  0.00256733  0.0094981   0.58424795  0.00565128\n",
      "  0.35968447  0.00375318  0.00481826  0.0016845   0.0023673   0.00195261\n",
      "  0.00258068  0.00286163]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00107799  0.00110493  0.00202201  0.0012628   0.00114929  0.00109999\n",
      "  0.0013424   0.00092632  0.00119365  0.00309348  0.58851945  0.00258655\n",
      "  0.38459754  0.00227906  0.00167793  0.0014008   0.00081064  0.0011517\n",
      "  0.00134316  0.00136033]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00143496  0.001293    0.00170978  0.00159837  0.00158431  0.00152394\n",
      "  0.00159442  0.00179025  0.00141753  0.00207733  0.57953322  0.00396094\n",
      "  0.38718569  0.0022651   0.00223777  0.00178439  0.00206184  0.0013617\n",
      "  0.00183058  0.00175487]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [ 0.00074261  0.00077566  0.00093312  0.0009107   0.00074445  0.00081432\n",
      "  0.00092957  0.00070436  0.0009562   0.00137311  0.59109879  0.00164378\n",
      "  0.3909362   0.00184964  0.00099644  0.00115575  0.00076108  0.00081583\n",
      "  0.0009491   0.00090932]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00435309  0.00540638  0.0053431   0.00543641  0.00515877  0.00458143\n",
      "  0.00420224  0.0049177   0.00493829  0.00741285  0.56132346  0.01022342\n",
      "  0.3315078   0.00919963  0.00770862  0.00808623  0.00413797  0.00443122\n",
      "  0.00511865  0.00651274]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00103109  0.00109795  0.00117362  0.00103606  0.0011301   0.00104395\n",
      "  0.0010366   0.00091401  0.00111594  0.00178745  0.58708674  0.00233422\n",
      "  0.38884869  0.00284161  0.00135807  0.00168044  0.0008993   0.00126569\n",
      "  0.00107307  0.00124537]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.000886    0.00092775  0.000988    0.0011267   0.00094911  0.00093631\n",
      "  0.0009468   0.00087138  0.00096477  0.00160875  0.62166637  0.00222456\n",
      "  0.35751468  0.00202174  0.00131064  0.00127546  0.00076751  0.00091465\n",
      "  0.00088701  0.00121181]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00125791  0.00167576  0.00185276  0.00156053  0.0014031   0.00151363\n",
      "  0.0014809   0.00134687  0.00178341  0.00265001  0.58171582  0.00314619\n",
      "  0.38467014  0.00299555  0.00265738  0.00182526  0.00136931  0.00163819\n",
      "  0.00160133  0.00185594]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00176994  0.00178437  0.00205527  0.00209231  0.00182774  0.00183616\n",
      "  0.00193205  0.00201835  0.00176565  0.00286536  0.59390575  0.00452876\n",
      "  0.3646099   0.00326316  0.00277407  0.00264689  0.00188217  0.00180034\n",
      "  0.0018217   0.00282003]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00147309  0.00157401  0.00153675  0.00159428  0.00156777  0.00165734\n",
      "  0.00149339  0.00150678  0.00169393  0.00216799  0.59006202  0.00368841\n",
      "  0.37405986  0.00409745  0.00217132  0.0024492   0.00161261  0.00174187\n",
      "  0.00162098  0.00223102]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00302987  0.00354048  0.0035926   0.00389214  0.00345452  0.00352962\n",
      "  0.00385047  0.00369411  0.00371852  0.00575884  0.60108805  0.00747675\n",
      "  0.31822449  0.00555469  0.00634223  0.004881    0.00572401  0.00360778\n",
      "  0.00394659  0.00509328]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.001176    0.00143638  0.00161351  0.00155297  0.00132442  0.00157938\n",
      "  0.00141443  0.00118467  0.00162742  0.00233852  0.58516198  0.00294436\n",
      "  0.38429463  0.00276813  0.00192941  0.00186696  0.00128678  0.00147768\n",
      "  0.00132788  0.00169449]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00120232  0.00129341  0.00148073  0.00143621  0.00137179  0.00123906\n",
      "  0.00140268  0.00135011  0.00144026  0.00233907  0.6150589   0.00310706\n",
      "  0.35492051  0.00274901  0.00209352  0.00176078  0.0013193   0.00132942\n",
      "  0.00152168  0.00158417]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00192316  0.0024226   0.00284249  0.00274354  0.00212174  0.00208129\n",
      "  0.00369372  0.00209273  0.00258078  0.00975395  0.57928872  0.00437384\n",
      "  0.36131263  0.00397882  0.00451073  0.00283298  0.00357526  0.00206841\n",
      "  0.00303526  0.00276734]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00119088  0.00148774  0.00154962  0.00212579  0.00135882  0.00155457\n",
      "  0.00189143  0.00123727  0.00155087  0.00297573  0.59085584  0.002835\n",
      "  0.37753019  0.00278035  0.00163596  0.00181918  0.0010896   0.00123299\n",
      "  0.00158758  0.00171059]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00178645  0.00208817  0.00241889  0.00185143  0.00209614  0.00169954\n",
      "  0.00189862  0.00182876  0.00193026  0.00385436  0.58375692  0.00411624\n",
      "  0.37450483  0.00329675  0.00274563  0.00243251  0.00156725  0.00184142\n",
      "  0.00214183  0.00214396]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0026776   0.00282374  0.0031276   0.00279694  0.00284587  0.00301557\n",
      "  0.00299733  0.00269392  0.00301095  0.004322    0.60022271  0.00567034\n",
      "  0.33441323  0.00565899  0.00474988  0.00405456  0.00426801  0.00319463\n",
      "  0.00342471  0.00403132]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00179533  0.00172374  0.00204447  0.00203457  0.00192283  0.00184916\n",
      "  0.00183061  0.0017888   0.00161897  0.00282928  0.58704269  0.00475878\n",
      "  0.37268963  0.00306473  0.00271956  0.00230106  0.00191975  0.00164775\n",
      "  0.00182823  0.00259003]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00125163  0.00141429  0.00180556  0.00191418  0.00147663  0.001419\n",
      "  0.00167881  0.00106347  0.00144756  0.00341399  0.5866406   0.00263894\n",
      "  0.38269699  0.00240442  0.0016065   0.00158907  0.00110355  0.00133584\n",
      "  0.00170936  0.00138964]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00145768  0.00142717  0.00173403  0.00175227  0.00144538  0.00154529\n",
      "  0.00172167  0.00155235  0.00184633  0.00285112  0.60700166  0.00318406\n",
      "  0.35777426  0.00314555  0.00225668  0.00211588  0.00170283  0.00174307\n",
      "  0.00190418  0.00183851]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00269635  0.0022932   0.00273283  0.00341405  0.00262157  0.0026873\n",
      "  0.00250798  0.0028497   0.00250976  0.00417751  0.6143744   0.00594214\n",
      "  0.32657897  0.00349221  0.00402218  0.00358543  0.00407953  0.00288765\n",
      "  0.002783    0.00376425]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00091362  0.00101235  0.00110033  0.00104244  0.0009414   0.00101717\n",
      "  0.0009331   0.00079798  0.00106614  0.00158721  0.59151459  0.00205333\n",
      "  0.38747144  0.00219109  0.00105212  0.00141558  0.00068322  0.00114365\n",
      "  0.00091741  0.0011458 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00169725  0.00189138  0.00301935  0.00206918  0.00170933  0.00163377\n",
      "  0.0029306   0.00155409  0.00184798  0.00767283  0.58803654  0.00358538\n",
      "  0.36488861  0.00324773  0.00367275  0.00220104  0.00197269  0.00158626\n",
      "  0.002365    0.00241823]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00132965  0.00202428  0.00189075  0.00235769  0.00176415  0.00170925\n",
      "  0.00187782  0.00209947  0.00209269  0.0046467   0.58197451  0.00397751\n",
      "  0.3765184   0.00344132  0.0023599   0.00214346  0.00138962  0.0020726\n",
      "  0.00232583  0.00200438]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00325028  0.00240869  0.00383876  0.00356123  0.00370313  0.00315505\n",
      "  0.00347657  0.00318176  0.00350947  0.00396753  0.578161    0.00660325\n",
      "  0.35290295  0.00504224  0.00391282  0.00395939  0.00410036  0.00451212\n",
      "  0.00392511  0.00282835]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00126945  0.00133682  0.0014898   0.00165533  0.00138802  0.0014269\n",
      "  0.00152338  0.00136382  0.00138761  0.00224268  0.5883314   0.00308438\n",
      "  0.38129783  0.00276754  0.00153342  0.00207295  0.00112738  0.00140106\n",
      "  0.00156406  0.00173613]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00109907  0.0012118   0.00127808  0.0012251   0.00115811  0.00119799\n",
      "  0.00127309  0.00117981  0.00120354  0.00170931  0.60965651  0.0022078\n",
      "  0.36339429  0.00179568  0.00166309  0.00180036  0.00294032  0.00114638\n",
      "  0.00136615  0.00149356]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0016781   0.00138982  0.00199174  0.00147606  0.00150596  0.00163456\n",
      "  0.00152223  0.00130885  0.0017519   0.00194204  0.59549636  0.00315705\n",
      "  0.37088418  0.00373035  0.00176015  0.00241333  0.00155662  0.00151656\n",
      "  0.00161478  0.00166935]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00171226  0.00186202  0.00190978  0.00218847  0.00202936  0.00195715\n",
      "  0.00190613  0.00182388  0.00182873  0.00298399  0.61104435  0.00460493\n",
      "  0.34724003  0.00307338  0.00311091  0.00230001  0.00228487  0.001762\n",
      "  0.00189559  0.00248214]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00160252  0.00146976  0.0016339   0.00166363  0.00147278  0.00153593\n",
      "  0.00144647  0.00156162  0.0015569   0.00226467  0.58740616  0.00362324\n",
      "  0.37754005  0.00337792  0.0019502   0.00266216  0.00168666  0.00168197\n",
      "  0.00157789  0.00228556]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00178292  0.0022241   0.00201267  0.00192861  0.00203606  0.00208977\n",
      "  0.00191158  0.00170904  0.0024007   0.0026547   0.5849371   0.00394371\n",
      "  0.3696523   0.00560861  0.0028038   0.00336472  0.00195481  0.00254996\n",
      "  0.00193535  0.00249956]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [ 0.00250857  0.00223853  0.00274572  0.00266267  0.0026513   0.00212836\n",
      "  0.0032165   0.00217662  0.00281198  0.00556351  0.58833164  0.00381717\n",
      "  0.35706931  0.00387085  0.00252885  0.0032259   0.00367006  0.00299111\n",
      "  0.00361158  0.00217968]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00384204  0.0040386   0.00401564  0.00422875  0.0042713   0.00347463\n",
      "  0.00412877  0.00444088  0.00393511  0.00649154  0.6078552   0.00737421\n",
      "  0.30072725  0.00603082  0.00759074  0.00638397  0.00634457  0.00470809\n",
      "  0.00460168  0.00551628]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00098137  0.00119367  0.00128412  0.00112885  0.00111452  0.00106141\n",
      "  0.00109259  0.0010084   0.00113223  0.0018221   0.5873723   0.00236491\n",
      "  0.38838035  0.00251045  0.00138066  0.00170623  0.0008036   0.00112959\n",
      "  0.0011877   0.0013449 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00142604  0.00210508  0.00199881  0.00151972  0.00167107  0.00169674\n",
      "  0.00173377  0.00140551  0.00211363  0.00243934  0.58913714  0.00303884\n",
      "  0.37285036  0.00414024  0.00240033  0.00268945  0.00182462  0.002063\n",
      "  0.00180454  0.00194175]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00096117  0.00119109  0.00140039  0.00129924  0.00113289  0.00111868\n",
      "  0.00115621  0.00097686  0.00119473  0.00218095  0.58992589  0.00230582\n",
      "  0.38588983  0.0022231   0.00117863  0.00151885  0.00091654  0.00109108\n",
      "  0.00111158  0.00122647]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00173827  0.00200419  0.0020531   0.00223541  0.00208811  0.00192097\n",
      "  0.00194807  0.00209521  0.00216243  0.00351562  0.58915281  0.00511597\n",
      "  0.36683232  0.00418184  0.00262077  0.00264305  0.00135732  0.00196653\n",
      "  0.00205372  0.00231421]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00252203  0.0033112   0.00319217  0.00439049  0.00242311  0.00401322\n",
      "  0.0031654   0.00221292  0.00317216  0.00447123  0.58039808  0.00409521\n",
      "  0.35942599  0.00484858  0.00273811  0.00426685  0.00299635  0.0022121\n",
      "  0.00323414  0.00291064]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00145685  0.00173387  0.00168664  0.00184901  0.00172351  0.00156125\n",
      "  0.00193001  0.00179885  0.00180647  0.00293659  0.60760152  0.00327206\n",
      "  0.35355178  0.00301338  0.00303021  0.00217288  0.00286586  0.00173208\n",
      "  0.00210987  0.00216731]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00264611  0.00229359  0.00262121  0.00315668  0.00245384  0.00261624\n",
      "  0.00273218  0.00232908  0.00230143  0.0047226   0.60077941  0.00491758\n",
      "  0.34324765  0.00356363  0.00320823  0.00347595  0.00417787  0.0025713\n",
      "  0.00302045  0.00316502]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0012639   0.00126993  0.00136034  0.00128806  0.00130788  0.0011823\n",
      "  0.00122462  0.00128079  0.00125447  0.00208985  0.5891031   0.00298009\n",
      "  0.3824029   0.00296364  0.00162822  0.00195976  0.00116585  0.00129987\n",
      "  0.00128755  0.00168687]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00234388  0.00297216  0.0032179   0.00323419  0.00309106  0.00256127\n",
      "  0.00258916  0.00295693  0.00273806  0.0059975   0.5852654   0.00706191\n",
      "  0.35209802  0.00423535  0.00440326  0.0032241   0.00249252  0.00280046\n",
      "  0.00300268  0.00371419]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00269181  0.00313587  0.00334575  0.00345901  0.00327983  0.00273854\n",
      "  0.00288525  0.00335462  0.00299365  0.00578094  0.60938293  0.00726534\n",
      "  0.32190055  0.00451868  0.00520858  0.00425182  0.00339684  0.00318761\n",
      "  0.00301489  0.00420748]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00357303  0.00634235  0.00533966  0.00350587  0.00612908  0.00377258\n",
      "  0.00417035  0.00995914  0.00921092  0.00822282  0.4880524   0.0160031\n",
      "  0.34852129  0.02004902  0.02288506  0.00813022  0.00860458  0.00964667\n",
      "  0.00780452  0.01007731]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00172418  0.00172558  0.00181767  0.00178595  0.00152917  0.00186076\n",
      "  0.00171434  0.00136402  0.0022421   0.0023273   0.59495342  0.00254016\n",
      "  0.36693838  0.00384472  0.00191526  0.00311325  0.00227433  0.00270116\n",
      "  0.00168941  0.00193884]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00177372  0.00177924  0.00295361  0.00244299  0.00156632  0.00157954\n",
      "  0.0021953   0.00203035  0.00177078  0.00648117  0.58303201  0.00370457\n",
      "  0.37003624  0.00252395  0.00360593  0.00226678  0.00197774  0.00215075\n",
      "  0.00260583  0.00352312]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00159623  0.00141741  0.00165077  0.00160667  0.00146211  0.00159601\n",
      "  0.00142437  0.00154344  0.00159825  0.00220238  0.59180427  0.00392906\n",
      "  0.37303561  0.00363508  0.00180646  0.00260635  0.00140601  0.00184422\n",
      "  0.00156755  0.00226778]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0018132   0.00182467  0.00242076  0.00201472  0.00193146  0.00168219\n",
      "  0.00169456  0.0014381   0.00164028  0.00405592  0.57594657  0.0034944\n",
      "  0.3855482   0.00246709  0.00202802  0.00239086  0.00264746  0.00154462\n",
      "  0.00159797  0.00181891]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00101829  0.00104472  0.00150198  0.00113337  0.00114672  0.00098847\n",
      "  0.00102117  0.00093829  0.00099068  0.00208066  0.58879125  0.0025569\n",
      "  0.38834018  0.00183202  0.00116844  0.00133158  0.00099973  0.00089271\n",
      "  0.00106611  0.00115671]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00093067  0.00107493  0.0012845   0.00118991  0.00106851  0.00103427\n",
      "  0.00103542  0.0009186   0.00114891  0.0020199   0.5883677   0.0022801\n",
      "  0.38836068  0.00234833  0.00131055  0.0014521   0.00085053  0.00110819\n",
      "  0.00103865  0.00117749]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00148383  0.00164422  0.00246263  0.00184345  0.00157736  0.00151676\n",
      "  0.00212809  0.00159145  0.00151157  0.00478237  0.58581328  0.00390758\n",
      "  0.37496382  0.00278141  0.00259277  0.00196101  0.00172018  0.00127258\n",
      "  0.00217867  0.00226701]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00251153  0.00333212  0.00319028  0.00385683  0.0029985   0.00391925\n",
      "  0.00433658  0.00298779  0.00335661  0.00607725  0.54682803  0.0065329\n",
      "  0.38103414  0.00443848  0.00582857  0.00374006  0.00566795  0.00234161\n",
      "  0.00312973  0.00389179]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0013705   0.00137983  0.00153867  0.00167877  0.00154454  0.00148883\n",
      "  0.0013544   0.00137934  0.0014115   0.00225632  0.61319405  0.00375533\n",
      "  0.35514131  0.00258688  0.00212945  0.00188695  0.00136218  0.00137854\n",
      "  0.00134881  0.00181381]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00161107  0.00180295  0.00216236  0.00156458  0.001556    0.0014632\n",
      "  0.00170401  0.00185486  0.00178025  0.00300816  0.60054153  0.00277875\n",
      "  0.36059269  0.00247703  0.00195529  0.00337887  0.00381726  0.00186362\n",
      "  0.00202764  0.00205989]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00097922  0.00107027  0.00121402  0.00116881  0.00107579  0.0010389\n",
      "  0.00107802  0.00097533  0.00105372  0.00188984  0.58954269  0.00232557\n",
      "  0.38726246  0.00221876  0.00123659  0.00153314  0.00088907  0.00101088\n",
      "  0.00112424  0.00131266]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00115799  0.00133493  0.00150934  0.00146517  0.00132711  0.0014362\n",
      "  0.00155667  0.00125233  0.00144091  0.00225687  0.59221476  0.00333617\n",
      "  0.37778038  0.00309218  0.00191917  0.00167347  0.00099854  0.00109465\n",
      "  0.00152415  0.00162896]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00106382  0.00120807  0.00125965  0.00127117  0.00122918  0.00111029\n",
      "  0.00121497  0.00108452  0.00128438  0.00279102  0.58883321  0.00287562\n",
      "  0.38382185  0.0028113   0.00179157  0.00151357  0.00100615  0.00118693\n",
      "  0.00118055  0.00146219]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0016242   0.00176669  0.00217253  0.00175394  0.00163837  0.00173219\n",
      "  0.00186414  0.00135904  0.0013934   0.00344015  0.58281791  0.00330423\n",
      "  0.38087669  0.00248585  0.00227692  0.00222099  0.00178137  0.00139839\n",
      "  0.00165354  0.00243949]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00145759  0.00182802  0.00186497  0.00152066  0.00159751  0.00205646\n",
      "  0.00167813  0.00132985  0.00225102  0.0020457   0.59408689  0.00334684\n",
      "  0.36671478  0.00543995  0.00239047  0.00272781  0.0017469   0.00211369\n",
      "  0.00161812  0.00218462]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00129878  0.00234761  0.0025728   0.00211237  0.00192449  0.00222479\n",
      "  0.00191199  0.00207666  0.00314331  0.00315569  0.57059133  0.00396163\n",
      "  0.38463318  0.00356867  0.00314464  0.00225662  0.00162194  0.00252364\n",
      "  0.00260615  0.00232374]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [ 0.00276302  0.00281673  0.00270047  0.00359412  0.00326879  0.00294152\n",
      "  0.00270004  0.00286051  0.00265691  0.00449538  0.59487921  0.00688561\n",
      "  0.34390092  0.00416057  0.00396477  0.00370243  0.00268393  0.00279718\n",
      "  0.00283517  0.00339271]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00097887  0.00107529  0.00127606  0.001087    0.00103724  0.00118073\n",
      "  0.00102105  0.000975    0.00125027  0.00155091  0.58899415  0.00225097\n",
      "  0.3870734   0.00261323  0.00111305  0.00169917  0.00094843  0.00144315\n",
      "  0.00108775  0.00134431]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00104513  0.00130434  0.00121284  0.00117397  0.00123735  0.00103815\n",
      "  0.00115748  0.00098714  0.00123375  0.0028101   0.58979779  0.00272787\n",
      "  0.38319257  0.0029006   0.00186599  0.00155516  0.00103557  0.00120256\n",
      "  0.00111287  0.00140874]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00181156  0.00318275  0.00285217  0.00413354  0.00297381  0.00295867\n",
      "  0.00364203  0.00192927  0.00333048  0.00409975  0.57421368  0.00388202\n",
      "  0.37011963  0.00412039  0.0033587   0.00288833  0.00324148  0.00198804\n",
      "  0.00307822  0.00219549]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00090949  0.00090837  0.00111646  0.00107498  0.00092004  0.00095958\n",
      "  0.00097197  0.00087168  0.00106373  0.00164919  0.59076381  0.00209068\n",
      "  0.38832629  0.00213627  0.00098332  0.00138794  0.00081915  0.00096268\n",
      "  0.00101814  0.00106624]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0011293   0.00120573  0.00145801  0.00133326  0.00118986  0.00138545\n",
      "  0.00128262  0.00110087  0.00125995  0.00193116  0.59029013  0.00254903\n",
      "  0.38320735  0.00246766  0.00126161  0.00184471  0.00115772  0.00120743\n",
      "  0.00133109  0.00140703]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00099933  0.00113832  0.00130736  0.00149568  0.00107436  0.0011303\n",
      "  0.00141065  0.00094953  0.00117146  0.00223428  0.59020799  0.00223553\n",
      "  0.38469079  0.00230766  0.00135527  0.00140654  0.00129083  0.00097285\n",
      "  0.00133865  0.00128258]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00141306  0.00144594  0.00175523  0.00162701  0.00136603  0.00140715\n",
      "  0.00140148  0.0016709   0.0015423   0.00232215  0.60345793  0.00266044\n",
      "  0.3634854   0.00209974  0.00183317  0.00246017  0.00302401  0.00161304\n",
      "  0.00162662  0.00178829]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00106024  0.00114125  0.00129226  0.00117848  0.00111143  0.00115509\n",
      "  0.00115366  0.00102966  0.00123552  0.00180018  0.58859825  0.0024118\n",
      "  0.3859145   0.00305413  0.00139613  0.00165881  0.00111524  0.00115317\n",
      "  0.00124654  0.00129365]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00153023  0.0013766   0.00179737  0.00161001  0.00135059  0.00150616\n",
      "  0.00150881  0.00119531  0.00199267  0.00235791  0.59144789  0.00270383\n",
      "  0.37516791  0.00367913  0.00171826  0.00242797  0.00152924  0.00202617\n",
      "  0.00144325  0.00163065]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00077495  0.00091619  0.00117396  0.00094805  0.00086981  0.00086266\n",
      "  0.00108186  0.0007838   0.00091387  0.00243796  0.5920316   0.00214705\n",
      "  0.38744241  0.00170019  0.00119964  0.00114704  0.00068612  0.00082531\n",
      "  0.00102017  0.00103731]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00559608  0.00677172  0.01376263  0.00489579  0.00663802  0.00413195\n",
      "  0.00498418  0.00488073  0.00536992  0.01286928  0.53025174  0.01136481\n",
      "  0.33488864  0.00731377  0.01351569  0.00743476  0.00652619  0.00469045\n",
      "  0.00715865  0.00695504]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00121995  0.00120386  0.0014926   0.00183243  0.00133359  0.00135628\n",
      "  0.00135548  0.0010024   0.00126549  0.00261832  0.58513486  0.00268129\n",
      "  0.38760006  0.00224662  0.00152239  0.00149784  0.00109028  0.00104824\n",
      "  0.00130202  0.00119602]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00102754  0.00100016  0.00128599  0.00111694  0.00104266  0.00106631\n",
      "  0.00104224  0.00103466  0.00104869  0.00177568  0.58928454  0.00282687\n",
      "  0.38699979  0.0021621   0.00126071  0.00141385  0.00097097  0.00109179\n",
      "  0.00111262  0.00143588]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00158292  0.00186536  0.00211125  0.00183994  0.00168779  0.00200497\n",
      "  0.00192511  0.00144023  0.00210816  0.00269361  0.59155762  0.00334457\n",
      "  0.36921051  0.00346035  0.00251837  0.00231068  0.00218349  0.00192333\n",
      "  0.00211254  0.00211925]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00091973  0.00094136  0.00108162  0.00099989  0.00092827  0.0010164\n",
      "  0.00087941  0.00082771  0.00101295  0.00137381  0.59172565  0.00207011\n",
      "  0.38743624  0.00225335  0.00105722  0.00150385  0.00073554  0.00113667\n",
      "  0.00089765  0.00120257]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00301667  0.00402379  0.00390368  0.00392601  0.0038212   0.00323709\n",
      "  0.00371532  0.0039646   0.00369926  0.00815185  0.58197206  0.00799029\n",
      "  0.33276856  0.00616101  0.00798832  0.00456022  0.00399223  0.00374791\n",
      "  0.0040468   0.00531313]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00249958  0.00262625  0.00320047  0.00325422  0.00291437  0.00266038\n",
      "  0.0025016   0.00280222  0.00261953  0.0056064   0.59258842  0.00834908\n",
      "  0.34391111  0.00439271  0.00530597  0.00312076  0.00228565  0.00273768\n",
      "  0.00256632  0.00405727]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00111361  0.00166386  0.00155314  0.00197591  0.00139049  0.00140826\n",
      "  0.00209341  0.0011779   0.00165153  0.00343949  0.58917058  0.00240735\n",
      "  0.37940329  0.00236486  0.0017274   0.00166349  0.00140956  0.00119563\n",
      "  0.00172126  0.00146893]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00095496  0.00112891  0.00135034  0.00139311  0.00114023  0.00111434\n",
      "  0.0012519   0.0009262   0.00137692  0.00325562  0.58982956  0.00267989\n",
      "  0.38351232  0.00268913  0.00157566  0.00142485  0.00080433  0.00116547\n",
      "  0.00114163  0.00128457]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00219431  0.0030119   0.00226067  0.00389566  0.00317302  0.00230198\n",
      "  0.00344504  0.0036732   0.00368181  0.00530808  0.57154232  0.00524503\n",
      "  0.36284965  0.0049207   0.00477104  0.00341011  0.0046166   0.00261497\n",
      "  0.00423111  0.00285281]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00164762  0.00156952  0.00163425  0.00159512  0.00165047  0.00164011\n",
      "  0.00153521  0.00157797  0.00157388  0.00229725  0.58854556  0.00378535\n",
      "  0.3756128   0.00378635  0.00194971  0.0026011   0.00157724  0.00164652\n",
      "  0.00163238  0.00214162]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00149243  0.00141415  0.0015794   0.00141114  0.00144681  0.0014641\n",
      "  0.0013589   0.00134729  0.00148398  0.00200635  0.59024513  0.00321724\n",
      "  0.37714243  0.00367054  0.0016831   0.0024895   0.00138055  0.00171316\n",
      "  0.00154783  0.00190595]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0010159   0.00115331  0.00144276  0.00115069  0.00113555  0.00109305\n",
      "  0.00107002  0.00099191  0.00120707  0.00179133  0.59187359  0.00252743\n",
      "  0.38419756  0.0024276   0.00117432  0.00158681  0.00073569  0.00106301\n",
      "  0.00111306  0.00124932]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0010022   0.00096591  0.00116628  0.00130521  0.00109481  0.00106437\n",
      "  0.00094397  0.00106358  0.00101983  0.00158195  0.6185056   0.00275773\n",
      "  0.35853285  0.00178893  0.00126755  0.00150092  0.00107     0.00104897\n",
      "  0.00098891  0.00133045]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0012862   0.00144391  0.00144627  0.00120718  0.0012677   0.00145037\n",
      "  0.00130483  0.00112794  0.00171036  0.00177935  0.60172826  0.00237831\n",
      "  0.36723146  0.00404856  0.00181274  0.00240377  0.00132896  0.00198657\n",
      "  0.0013109   0.00174636]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00155013  0.00161971  0.00194887  0.00177307  0.00175291  0.0015065\n",
      "  0.00162606  0.00163707  0.00157131  0.00312449  0.58268219  0.00370077\n",
      "  0.38134694  0.00299673  0.00219425  0.00220143  0.00140243  0.00150315\n",
      "  0.0019632   0.00189874]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00144328  0.00154649  0.00163127  0.00155494  0.00146204  0.00148201\n",
      "  0.00150071  0.00153648  0.00154029  0.00230776  0.58883595  0.00332636\n",
      "  0.37724569  0.00336685  0.00189852  0.00251479  0.00141767  0.00159312\n",
      "  0.0016554   0.00214038]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [ 0.0020288   0.00179181  0.00233271  0.0022825   0.0020105   0.00203822\n",
      "  0.00196898  0.00214084  0.00181487  0.00297706  0.58733052  0.0053373\n",
      "  0.36731237  0.00352726  0.0030479   0.00282086  0.00217398  0.0019754\n",
      "  0.00194366  0.00314444]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00267012  0.0025512   0.00277964  0.00371064  0.00282707  0.00270431\n",
      "  0.00262237  0.0028513   0.00279414  0.00476489  0.6209349   0.00600675\n",
      "  0.31776124  0.00412531  0.00438625  0.00369855  0.00330945  0.00312849\n",
      "  0.00270388  0.0036695 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00083045  0.00112994  0.00128141  0.00116643  0.00102879  0.00107123\n",
      "  0.00104816  0.00086106  0.00102277  0.00184006  0.59146345  0.0021705\n",
      "  0.38684893  0.00206213  0.0011102   0.00133159  0.00068306  0.00088109\n",
      "  0.00098878  0.00117999]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00084719  0.00124721  0.00157331  0.00110391  0.00112654  0.00107211\n",
      "  0.00145157  0.00093324  0.00124453  0.00388085  0.59060121  0.00250873\n",
      "  0.38282883  0.00215122  0.00186694  0.00113055  0.0008863   0.00103918\n",
      "  0.00131955  0.00118704]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0021813   0.00212365  0.00243917  0.00251426  0.00221581  0.00199012\n",
      "  0.00243303  0.00227465  0.00246285  0.00450836  0.59606791  0.00451239\n",
      "  0.35270542  0.00490721  0.00320407  0.00356761  0.00240086  0.00232953\n",
      "  0.00258232  0.00257946]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00247061  0.00267232  0.00283753  0.00315855  0.00285473  0.00255269\n",
      "  0.00282966  0.00280475  0.00281687  0.00530427  0.58748388  0.00647678\n",
      "  0.35075685  0.00504136  0.00412862  0.00369102  0.00264051  0.00285802\n",
      "  0.00307692  0.00354408]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00250937  0.00361673  0.00607788  0.00303627  0.00387216  0.00277116\n",
      "  0.00315946  0.00445199  0.00401339  0.00555965  0.55866224  0.00891322\n",
      "  0.36197633  0.0042137   0.00728433  0.00376752  0.00426427  0.00369858\n",
      "  0.0044512   0.00370056]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00148854  0.00134428  0.00159279  0.00129122  0.00150093  0.00126638\n",
      "  0.00158574  0.00106502  0.00149792  0.00235464  0.59149915  0.00246839\n",
      "  0.37779754  0.0038781   0.00177863  0.00200545  0.00136858  0.00129655\n",
      "  0.00158346  0.00133662]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00154506  0.00177661  0.0016224   0.00170082  0.00149948  0.00125601\n",
      "  0.00194793  0.00116712  0.00166481  0.00536911  0.58926785  0.00283816\n",
      "  0.37302631  0.00341381  0.00314514  0.00200227  0.00159018  0.00150953\n",
      "  0.00165767  0.00199977]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [  8.39381129e-04   9.23706917e-04   1.14611594e-03   9.35446355e-04\n",
      "   8.82694381e-04   9.06772155e-04   9.55037074e-04   7.46791251e-04\n",
      "   9.79534001e-04   1.77757523e-03   5.92698216e-01   1.96484569e-03\n",
      "   3.87468636e-01   2.19497713e-03   8.85874382e-04   1.34848594e-03\n",
      "   5.69182390e-04   8.52512545e-04   9.61456215e-04   9.62769380e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00097831  0.00107495  0.00117676  0.00112136  0.00110511  0.0010387\n",
      "  0.00113492  0.00097722  0.00107817  0.00230044  0.59090853  0.00267853\n",
      "  0.38525981  0.00216623  0.00142623  0.00130026  0.00087188  0.00100401\n",
      "  0.00110822  0.0012903 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00259712  0.0024132   0.00264299  0.00332672  0.00271325  0.00271333\n",
      "  0.00238831  0.0024832   0.00231989  0.003688    0.59860206  0.00547761\n",
      "  0.34764308  0.00337773  0.00315471  0.00358125  0.00267913  0.002436\n",
      "  0.00263761  0.00312478]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00202012  0.00219333  0.00241207  0.0022928   0.00240901  0.00224076\n",
      "  0.00237586  0.00168766  0.0021673   0.0028677   0.57690811  0.00471058\n",
      "  0.37654659  0.00375762  0.00356802  0.00262583  0.00297193  0.00173503\n",
      "  0.00247003  0.00203964]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00105641  0.0012005   0.00135288  0.00129865  0.00120068  0.00112436\n",
      "  0.00140851  0.00105992  0.0013915   0.00256738  0.58985901  0.00262067\n",
      "  0.38243586  0.0031893   0.00198979  0.00146724  0.00095954  0.00112984\n",
      "  0.00132235  0.00136557]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00197561  0.00223356  0.00231517  0.00268448  0.00248069  0.00250594\n",
      "  0.00201314  0.0023339   0.00235643  0.0033175   0.59218967  0.00669069\n",
      "  0.35761738  0.00422037  0.0029483   0.00309631  0.00187021  0.00220222\n",
      "  0.00207018  0.0028782 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00126103  0.00156221  0.00158805  0.00169794  0.00155097  0.00145193\n",
      "  0.00163665  0.00140956  0.00166957  0.00333495  0.58670026  0.00350569\n",
      "  0.37972531  0.00286754  0.00213335  0.00176369  0.00139483  0.00143002\n",
      "  0.00160415  0.00171229]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00192433  0.00196789  0.00224381  0.00255297  0.00202998  0.00219496\n",
      "  0.00196025  0.00195589  0.00212409  0.00363859  0.59904653  0.00477846\n",
      "  0.35455224  0.00329131  0.00333159  0.00259872  0.00243508  0.00239378\n",
      "  0.00211805  0.00286148]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00091475  0.00101443  0.00138251  0.00110665  0.00096536  0.00105712\n",
      "  0.00106912  0.00082434  0.00104509  0.00231091  0.58998239  0.00221583\n",
      "  0.38738054  0.00217337  0.00113047  0.00134497  0.00074883  0.00105727\n",
      "  0.0011301   0.001146  ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00179537  0.00168262  0.00212666  0.00190203  0.00150336  0.00205801\n",
      "  0.00173541  0.0012944   0.0021206   0.00254779  0.59120822  0.00289578\n",
      "  0.37095556  0.00344707  0.00171238  0.00280878  0.00193705  0.00245744\n",
      "  0.00185061  0.00196089]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00180132  0.00154563  0.00178672  0.00177762  0.00142236  0.00166764\n",
      "  0.00196624  0.00234647  0.00185881  0.00359233  0.57495463  0.00387321\n",
      "  0.38034976  0.00338043  0.00341041  0.00249747  0.00402959  0.00213196\n",
      "  0.00231383  0.00329361]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00394471  0.00428262  0.0043942   0.00479303  0.0047518   0.00434366\n",
      "  0.00444154  0.00454475  0.00457008  0.00647085  0.5603205   0.01038717\n",
      "  0.34152186  0.00995623  0.00643564  0.00590318  0.00459591  0.0045184\n",
      "  0.004467    0.00535684]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00111761  0.00108763  0.00123882  0.00118116  0.00113112  0.00111355\n",
      "  0.00113662  0.00096268  0.001167    0.00177208  0.59078002  0.00238963\n",
      "  0.38437885  0.00306522  0.00136602  0.00167461  0.00088471  0.00109755\n",
      "  0.001189    0.00126613]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00126544  0.0013766   0.00159136  0.00173537  0.00137169  0.00152133\n",
      "  0.0013727   0.00138636  0.00135165  0.00220472  0.59030735  0.00347586\n",
      "  0.37942147  0.00258151  0.00143128  0.00195617  0.00113729  0.001355\n",
      "  0.00137774  0.00177907]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00229656  0.00201798  0.00269764  0.00236497  0.00215941  0.0021453\n",
      "  0.00246465  0.00233392  0.00242418  0.00449356  0.59442079  0.00460354\n",
      "  0.3536419   0.00452164  0.0029288   0.00363674  0.00312148  0.00224775\n",
      "  0.00267916  0.00280005]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00099472  0.00101452  0.00118573  0.00136104  0.00105929  0.0010979\n",
      "  0.00108449  0.00095569  0.00106517  0.00180806  0.58949208  0.002689\n",
      "  0.3870042   0.00226889  0.00125745  0.00149234  0.00080822  0.0010056\n",
      "  0.00112216  0.00123349]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00186652  0.00253755  0.00241484  0.00249099  0.0022853   0.00213449\n",
      "  0.00262419  0.00222663  0.00232419  0.00491228  0.5860405   0.00515683\n",
      "  0.36138901  0.00399062  0.00480535  0.0026302   0.002566    0.00195446\n",
      "  0.002423    0.00322707]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00147658  0.0017345   0.00183851  0.00175114  0.00172383  0.00162958\n",
      "  0.00154954  0.0016246   0.00166572  0.00271235  0.58864677  0.0039112\n",
      "  0.37512249  0.00360812  0.00203425  0.00233503  0.00130994  0.00163197\n",
      "  0.00165719  0.00203667]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00083248  0.00116587  0.00126977  0.00111713  0.00111476  0.00096582\n",
      "  0.00125446  0.00083038  0.00117195  0.00318119  0.59175402  0.00233978\n",
      "  0.38456374  0.00214764  0.00131148  0.00118547  0.00064312  0.00093073\n",
      "  0.00119295  0.00102725]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [ 0.00092086  0.00094449  0.00114975  0.00099638  0.00097565  0.00095862\n",
      "  0.00100131  0.00101322  0.00093316  0.00159418  0.59011328  0.00242623\n",
      "  0.38858166  0.00202086  0.00111109  0.00129964  0.00076835  0.00093667\n",
      "  0.00104709  0.00120751]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0027125   0.00248846  0.00294208  0.00313798  0.00287065  0.0024663\n",
      "  0.00274334  0.0028106   0.00291738  0.00520197  0.61534154  0.00580648\n",
      "  0.3221381   0.00511927  0.00436138  0.00395529  0.00350809  0.00318612\n",
      "  0.00284406  0.0034484 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00164437  0.00227834  0.00203698  0.00227226  0.00192981  0.00194063\n",
      "  0.00183852  0.00178382  0.00204032  0.00336605  0.58453172  0.00404156\n",
      "  0.37439442  0.00342513  0.00228769  0.00261663  0.00157325  0.00194553\n",
      "  0.00178569  0.00226729]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0016381   0.00173251  0.00217135  0.00205244  0.00185526  0.00167945\n",
      "  0.00160781  0.00171577  0.00182782  0.00339869  0.58358824  0.00445466\n",
      "  0.37638468  0.00367951  0.00262411  0.00242979  0.00135484  0.00183483\n",
      "  0.00168887  0.0022813 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00174439  0.00170065  0.00161235  0.00160234  0.00170178  0.00165747\n",
      "  0.00149347  0.00173211  0.00173441  0.00216125  0.59274936  0.00384173\n",
      "  0.36792159  0.00489683  0.00242362  0.00315655  0.001751    0.00194712\n",
      "  0.00166172  0.00251021]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00194754  0.00176304  0.00193844  0.00214359  0.00178981  0.00191954\n",
      "  0.0018648   0.00191958  0.0020161   0.002866    0.5902549   0.00407269\n",
      "  0.36628017  0.00380434  0.00242872  0.00315943  0.00255674  0.00246592\n",
      "  0.00207141  0.00273726]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00122139  0.00117244  0.00124422  0.00136158  0.00118894  0.00119791\n",
      "  0.00119881  0.00130332  0.00121616  0.0019245   0.5881868   0.00314707\n",
      "  0.38312685  0.00289431  0.00177727  0.00190314  0.00150755  0.00128503\n",
      "  0.00120471  0.00193796]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00316492  0.00288778  0.0034564   0.00354321  0.00321708  0.00281435\n",
      "  0.00327642  0.00326605  0.00298208  0.00632215  0.60187823  0.00737382\n",
      "  0.32556278  0.00517349  0.00597492  0.00438453  0.00381926  0.00318382\n",
      "  0.00303818  0.00468054]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00121503  0.00103877  0.00134713  0.001194    0.00101835  0.00119926\n",
      "  0.0011454   0.00103054  0.00100093  0.00181622  0.5827769   0.00254214\n",
      "  0.39196953  0.00223497  0.00149019  0.00171149  0.00133925  0.00100769\n",
      "  0.00112572  0.00179648]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00284579  0.00290394  0.00328504  0.00340167  0.00294164  0.00258758\n",
      "  0.00557246  0.00268481  0.00317183  0.00869314  0.57659626  0.0049281\n",
      "  0.3495447   0.00495535  0.005475    0.00348451  0.00613445  0.00280762\n",
      "  0.00445055  0.00353553]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00129396  0.00141342  0.00139091  0.00129923  0.00141992  0.00135285\n",
      "  0.00126452  0.00117888  0.00148595  0.00193375  0.61243087  0.00284908\n",
      "  0.35713077  0.00344957  0.00203191  0.00208313  0.00126607  0.00164923\n",
      "  0.00130365  0.00177234]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00178652  0.00174772  0.00207043  0.00181477  0.00183767  0.00179625\n",
      "  0.0019601   0.00151243  0.00207894  0.00282861  0.60230863  0.00357651\n",
      "  0.35772789  0.00421636  0.00279026  0.00240478  0.00189021  0.00177293\n",
      "  0.00192584  0.00195311]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00112905  0.00115643  0.00119734  0.00111287  0.00116981  0.0010481\n",
      "  0.00107857  0.00106957  0.00111425  0.00176254  0.59156173  0.00263551\n",
      "  0.38334835  0.0027867   0.00146475  0.00177231  0.00079568  0.00114666\n",
      "  0.00111093  0.00153889]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0020262   0.00234545  0.00264294  0.00231264  0.00217331  0.00195993\n",
      "  0.00249056  0.00195894  0.00232778  0.00445272  0.59021163  0.0035501\n",
      "  0.36321849  0.00333991  0.00233583  0.00302787  0.00254392  0.00210907\n",
      "  0.00279038  0.00218225]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00167237  0.00198873  0.00170018  0.00184327  0.00172759  0.00187434\n",
      "  0.00205723  0.00179562  0.00192846  0.00264263  0.58946776  0.00317273\n",
      "  0.36835963  0.00379724  0.00335743  0.00290367  0.00291978  0.00205953\n",
      "  0.00203893  0.00269291]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00161301  0.001868    0.00193075  0.00188412  0.00172568  0.00171342\n",
      "  0.00210395  0.0016053   0.00188174  0.00420847  0.58737755  0.0039532\n",
      "  0.37193429  0.0037402   0.00278372  0.00219612  0.00162354  0.0015851\n",
      "  0.00204303  0.00222879]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0037687   0.00372135  0.00438192  0.00418838  0.00384095  0.00381552\n",
      "  0.00384738  0.00394954  0.00378321  0.00714407  0.56651187  0.01048283\n",
      "  0.34407783  0.00700454  0.00706101  0.00525774  0.00389496  0.00362133\n",
      "  0.00364297  0.00600394]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0009621   0.00107167  0.00120835  0.0012281   0.0010416   0.00105346\n",
      "  0.00103634  0.00097061  0.00105355  0.0020551   0.58962309  0.00228871\n",
      "  0.38742995  0.00208616  0.00113721  0.0014795   0.00085348  0.00109524\n",
      "  0.00114435  0.00118142]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00127844  0.00128292  0.00151572  0.00126622  0.00125051  0.00127807\n",
      "  0.00121824  0.0012527   0.00132003  0.00202287  0.59173244  0.00304463\n",
      "  0.37902856  0.00326357  0.0013962   0.00233175  0.00112315  0.00135159\n",
      "  0.00129731  0.0017451 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00136625  0.00161252  0.00182414  0.00144356  0.00145437  0.00149405\n",
      "  0.00157924  0.00137596  0.00154204  0.0022821   0.58581388  0.0029542\n",
      "  0.38043004  0.00321279  0.00225101  0.0026437   0.00174086  0.00154405\n",
      "  0.00157079  0.00186446]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00151926  0.0016446   0.00222776  0.00186223  0.00167858  0.00165273\n",
      "  0.00174781  0.00142208  0.001577    0.00404207  0.59047431  0.00387105\n",
      "  0.37309617  0.00284108  0.0020257   0.0020071   0.0010905   0.00155523\n",
      "  0.00172837  0.00193633]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00162367  0.00146803  0.00226358  0.00241556  0.00184534  0.00209327\n",
      "  0.00214571  0.00156665  0.00190383  0.00255078  0.58849895  0.00392808\n",
      "  0.37295246  0.00301137  0.00191626  0.00207337  0.0015655   0.00196584\n",
      "  0.00236678  0.00184502]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00206435  0.00180915  0.00235228  0.00233871  0.00226482  0.00180158\n",
      "  0.00262243  0.0017904   0.00230022  0.00466596  0.58723897  0.00363453\n",
      "  0.36704999  0.00381965  0.00222282  0.00285099  0.00216471  0.00261178\n",
      "  0.00253428  0.00186236]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00108524  0.00113957  0.00132472  0.00124927  0.0011272   0.00117219\n",
      "  0.00118881  0.00102537  0.00119522  0.00193348  0.59093988  0.00250267\n",
      "  0.3838641   0.00263209  0.00129712  0.00169846  0.00093065  0.00110995\n",
      "  0.00121988  0.00136406]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00087039  0.00086224  0.00132126  0.00096148  0.00083488  0.00083792\n",
      "  0.00093613  0.00102288  0.00084548  0.00156798  0.58763719  0.00209431\n",
      "  0.39183426  0.00155691  0.00126473  0.00125854  0.00095731  0.00082837\n",
      "  0.00110601  0.00140172]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00106753  0.00116061  0.00136798  0.00118747  0.00112549  0.00119726\n",
      "  0.00115155  0.00094805  0.00116803  0.00180726  0.58994108  0.00219307\n",
      "  0.38578418  0.00251404  0.00110023  0.00166153  0.00099098  0.00115163\n",
      "  0.0012625   0.00121953]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00180445  0.00206053  0.00283758  0.00284402  0.00195799  0.00212479\n",
      "  0.0037569   0.00159878  0.00255188  0.00820735  0.58229381  0.00426326\n",
      "  0.36169702  0.00431403  0.00563222  0.00221745  0.00242008  0.00187073\n",
      "  0.00275779  0.00278938]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00225091  0.00206672  0.00219565  0.00217858  0.00213074  0.00195392\n",
      "  0.00226246  0.00231113  0.00212546  0.0036318   0.58278376  0.00442357\n",
      "  0.36823159  0.00441784  0.00326008  0.00357765  0.00245554  0.00235298\n",
      "  0.00237294  0.00301665]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00145561  0.001713    0.00203241  0.00165112  0.00154791  0.00176939\n",
      "  0.00185494  0.00142745  0.00181421  0.00258521  0.58925104  0.00292569\n",
      "  0.37596434  0.00291058  0.00166883  0.00218912  0.00186674  0.00148945\n",
      "  0.00226127  0.00162169]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [ 0.00306125  0.0029072   0.00294602  0.00285358  0.00291434  0.00315231\n",
      "  0.00273849  0.0031319   0.00295991  0.00374272  0.55996156  0.00630105\n",
      "  0.37399283  0.00655523  0.00393993  0.00537071  0.00349421  0.00294522\n",
      "  0.00318618  0.00384541]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00263035  0.00321802  0.00329559  0.00346463  0.00295923  0.00263159\n",
      "  0.00291833  0.00320472  0.00292833  0.00601404  0.59089172  0.00655991\n",
      "  0.34167758  0.00479162  0.00536078  0.00415276  0.00295331  0.0029284\n",
      "  0.00284271  0.00457635]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00264458  0.00318946  0.00198837  0.00259117  0.0025861   0.00196514\n",
      "  0.0035526   0.00277552  0.00294919  0.00932789  0.58334333  0.00411124\n",
      "  0.34507209  0.0068889   0.0067317   0.00441687  0.00402999  0.0044233\n",
      "  0.00352792  0.00388464]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00184628  0.00218208  0.00241274  0.00198964  0.00206414  0.0020206\n",
      "  0.00228094  0.00184319  0.00239609  0.00294234  0.59457648  0.00371137\n",
      "  0.36041352  0.00385666  0.00277135  0.0029959   0.00309611  0.00200816\n",
      "  0.00238182  0.0022106 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00179881  0.00169504  0.00227086  0.00194304  0.00169728  0.00231854\n",
      "  0.00204843  0.00147566  0.00220489  0.00239437  0.59314275  0.00383932\n",
      "  0.36539739  0.00399345  0.00246447  0.00256932  0.00270701  0.00178049\n",
      "  0.00207019  0.00218871]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0009865   0.00100072  0.00142196  0.001188    0.00109478  0.00105057\n",
      "  0.00109631  0.00088035  0.00104769  0.00203913  0.58793759  0.00225517\n",
      "  0.38954425  0.00181571  0.00119166  0.00127617  0.00101986  0.00095894\n",
      "  0.00115441  0.00104023]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00188004  0.00195143  0.00209145  0.00173894  0.00212087  0.00158329\n",
      "  0.00158556  0.00180415  0.00202356  0.0033289   0.57449538  0.00442035\n",
      "  0.38276601  0.00424622  0.00355538  0.0026267   0.00149322  0.00171531\n",
      "  0.00233284  0.00224038]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00104973  0.0014538   0.00169686  0.00163457  0.00149421  0.00114791\n",
      "  0.00146624  0.00149103  0.00142007  0.00408748  0.58985198  0.00336695\n",
      "  0.37939179  0.00220145  0.00163013  0.00158216  0.00104102  0.00112027\n",
      "  0.00152474  0.00134761]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00180273  0.00172607  0.00161319  0.00155375  0.00174455  0.00153331\n",
      "  0.00165161  0.00174358  0.00172718  0.00232693  0.58894581  0.00379957\n",
      "  0.36981672  0.00486418  0.00365907  0.00304361  0.00195671  0.00202165\n",
      "  0.0017772   0.00269257]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00124791  0.00116311  0.00131349  0.00127899  0.00121355  0.00120139\n",
      "  0.0012975   0.00107435  0.0011695   0.00198583  0.58676308  0.00246822\n",
      "  0.38632897  0.0024199   0.00168639  0.0016499   0.00177386  0.0011623\n",
      "  0.00137521  0.00142649]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00228397  0.00178825  0.00247222  0.00233029  0.00191146  0.00182327\n",
      "  0.00229573  0.0021384   0.00294922  0.00394332  0.58371782  0.00360735\n",
      "  0.36517048  0.00448388  0.0031743   0.00355788  0.00373955  0.00363358\n",
      "  0.00254706  0.00243194]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00088082  0.00097712  0.00109643  0.00098765  0.00095867  0.00095643\n",
      "  0.00093442  0.00081371  0.00099567  0.00152919  0.59139097  0.00209123\n",
      "  0.38735056  0.002664    0.00107884  0.00142623  0.00071191  0.00103836\n",
      "  0.00094985  0.00116789]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00120856  0.0013474   0.00172232  0.0016474   0.00135138  0.00145243\n",
      "  0.00150774  0.00117987  0.00136282  0.00276503  0.59071356  0.00301032\n",
      "  0.37932536  0.00272524  0.00153366  0.00186757  0.00101466  0.00124208\n",
      "  0.00144501  0.00157761]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00317813  0.00208887  0.00322197  0.00332983  0.00303584  0.00244157\n",
      "  0.00303251  0.00250322  0.00214831  0.00489968  0.58166027  0.0061754\n",
      "  0.35917261  0.00356509  0.00315635  0.00315933  0.0051593   0.00217602\n",
      "  0.00322779  0.00266782]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00178973  0.0024091   0.00197821  0.00232207  0.0020614   0.00199134\n",
      "  0.00237144  0.001969    0.00225707  0.00430194  0.58363008  0.00427143\n",
      "  0.36796239  0.0038438   0.0046534   0.00248957  0.00250114  0.00190444\n",
      "  0.00224304  0.00304942]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00195606  0.00181715  0.00206137  0.00213355  0.00186907  0.00187906\n",
      "  0.00211487  0.00188023  0.0021581   0.00314173  0.59300834  0.00349103\n",
      "  0.3643603   0.0038698   0.00241857  0.00298599  0.00213644  0.00221759\n",
      "  0.00233883  0.00216183]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00387698  0.00485063  0.00478862  0.00420579  0.00457244  0.00407212\n",
      "  0.00460306  0.0047964   0.00448663  0.00716495  0.59029067  0.0093418\n",
      "  0.30943763  0.00730203  0.00765109  0.00679943  0.00617137  0.00451454\n",
      "  0.00496906  0.00610473]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00207552  0.00197667  0.00231049  0.00239179  0.00207432  0.00204665\n",
      "  0.0021361   0.00222944  0.00203261  0.00352216  0.58424604  0.00469911\n",
      "  0.36916348  0.00375629  0.00264369  0.00328784  0.0020665   0.00220128\n",
      "  0.00226615  0.00287387]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00174559  0.00201965  0.00216331  0.00221631  0.00192512  0.00174746\n",
      "  0.00200748  0.0020032   0.00217788  0.00365163  0.57657218  0.00424413\n",
      "  0.37909555  0.00410359  0.00333261  0.00274621  0.00163729  0.00189824\n",
      "  0.00218022  0.00253231]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00093691  0.00092471  0.00103736  0.00103201  0.00095661  0.00101135\n",
      "  0.00094601  0.00082654  0.00099944  0.00153558  0.59079546  0.001947\n",
      "  0.38896462  0.00193993  0.00096506  0.00137354  0.0007547   0.00104208\n",
      "  0.00100841  0.0010027 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00470757  0.0055117   0.00591067  0.00512765  0.00481357  0.00592036\n",
      "  0.00551786  0.00495978  0.00527945  0.0067441   0.57564855  0.00930965\n",
      "  0.3103641   0.00857466  0.0071748   0.007646    0.00801725  0.00513827\n",
      "  0.00648401  0.00715004]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00130414  0.00137611  0.00166907  0.00152062  0.00146701  0.00133082\n",
      "  0.00140999  0.00133188  0.00144615  0.0023881   0.58724993  0.00336059\n",
      "  0.38190997  0.00274504  0.00192909  0.00180772  0.00127542  0.00138438\n",
      "  0.00139058  0.00170339]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00089328  0.00099611  0.00113517  0.0009652   0.00093558  0.00099086\n",
      "  0.0009767   0.00088304  0.00101797  0.00152749  0.59145319  0.00204698\n",
      "  0.38715661  0.00259995  0.00105065  0.00150173  0.00075114  0.00094579\n",
      "  0.00104159  0.00113102]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00171267  0.0016557   0.00217205  0.00181822  0.00171934  0.0019457\n",
      "  0.00201333  0.00150066  0.00225733  0.0026828   0.59388262  0.00359703\n",
      "  0.3660869   0.00445335  0.00231612  0.00250505  0.00213605  0.00166615\n",
      "  0.00201173  0.00186723]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00158634  0.00176939  0.00196029  0.00165326  0.00157565  0.00178567\n",
      "  0.00157947  0.00148444  0.00223683  0.0025152   0.59426719  0.00324414\n",
      "  0.36639494  0.00467989  0.00221867  0.00299486  0.00162849  0.00259035\n",
      "  0.00156577  0.00226912]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00107504  0.00116801  0.00135857  0.00122922  0.00112962  0.00115204\n",
      "  0.00123949  0.00101241  0.00121621  0.00223284  0.58920741  0.00284435\n",
      "  0.38450867  0.00255658  0.00156099  0.00161747  0.00093932  0.00126955\n",
      "  0.00116158  0.00152061]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00147021  0.0015958   0.00217318  0.00190979  0.00169645  0.00161435\n",
      "  0.00231082  0.00177177  0.0017078   0.00440616  0.58318913  0.00399754\n",
      "  0.37781918  0.0023146   0.00286953  0.00163121  0.00224833  0.00144419\n",
      "  0.00209766  0.00173227]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00116404  0.00140709  0.00150011  0.00167555  0.00132595  0.00159221\n",
      "  0.00151961  0.00117939  0.00145966  0.00231996  0.58565903  0.0029396\n",
      "  0.38468519  0.00260777  0.00182306  0.00172152  0.00119608  0.0012744\n",
      "  0.00136288  0.00158691]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00230247  0.00256422  0.00259572  0.00359261  0.00275152  0.00272326\n",
      "  0.00282118  0.00287367  0.00273117  0.00476812  0.57573956  0.00588401\n",
      "  0.36444142  0.00423424  0.00388435  0.00334866  0.00359161  0.00270691\n",
      "  0.00292545  0.00351979]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00138065  0.00124841  0.00139041  0.00144048  0.00128433  0.00131711\n",
      "  0.00129454  0.00115593  0.00135872  0.00218528  0.58339846  0.00302938\n",
      "  0.38680473  0.00288953  0.00208537  0.00202592  0.00160567  0.00120734\n",
      "  0.00130518  0.00159251]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00172505  0.00173866  0.00194131  0.00218998  0.00205781  0.00171715\n",
      "  0.00174851  0.00183746  0.00169716  0.00325043  0.61386621  0.00477062\n",
      "  0.34565336  0.00278727  0.00326145  0.0021137   0.00163975  0.00178874\n",
      "  0.00176457  0.0024508 ]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [ 0.00110648  0.00137988  0.00169128  0.00147088  0.00119741  0.00137704\n",
      "  0.00137516  0.00126451  0.00153938  0.00232153  0.58456445  0.00251901\n",
      "  0.38711542  0.00229221  0.00152751  0.00172388  0.00117083  0.00142368\n",
      "  0.0014988   0.00144063]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.0046448   0.00476701  0.00485261  0.00548447  0.00505842  0.00423991\n",
      "  0.00572851  0.00520082  0.00500895  0.01001274  0.59100628  0.00834141\n",
      "  0.29603148  0.00797414  0.00848521  0.00702163  0.00796319  0.00564685\n",
      "  0.00629024  0.00624128]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00167441  0.00209399  0.00221691  0.00316809  0.00199304  0.00205218\n",
      "  0.00284209  0.00181139  0.0020248   0.00491549  0.58702189  0.00398668\n",
      "  0.36769864  0.00334809  0.00248717  0.00234705  0.00188716  0.00174188\n",
      "  0.00238369  0.0023054 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.001092    0.00113721  0.00128595  0.0011641   0.00111725  0.00108447\n",
      "  0.00112716  0.00103675  0.00119353  0.00188382  0.59178281  0.00240942\n",
      "  0.38347805  0.00283221  0.00125282  0.00167859  0.00087213  0.0010838\n",
      "  0.00123476  0.00125321]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00207034  0.0019258   0.00266876  0.0022866   0.00211577  0.00201755\n",
      "  0.00215048  0.0023643   0.00242196  0.00397617  0.59281975  0.00539937\n",
      "  0.35747197  0.00450122  0.00318345  0.00306615  0.00197675  0.00238397\n",
      "  0.00243386  0.00276582]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00161265  0.00164647  0.00182873  0.00265754  0.00180764  0.00179806\n",
      "  0.00151791  0.00171941  0.00148499  0.00293395  0.59203309  0.00440942\n",
      "  0.37182134  0.00236634  0.00181972  0.00221594  0.00110411  0.001513\n",
      "  0.00151128  0.0021984 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00253309  0.00199046  0.00270118  0.00337036  0.00255227  0.00260358\n",
      "  0.00300072  0.00219607  0.00273688  0.00389907  0.57888567  0.00452491\n",
      "  0.3668173   0.00425127  0.00263626  0.00323298  0.00300919  0.00320584\n",
      "  0.00338064  0.00247225]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [ 0.00196003  0.00203121  0.00223667  0.00199851  0.00183286  0.00176589\n",
      "  0.00221822  0.00183853  0.00215018  0.0046279   0.58772391  0.00435867\n",
      "  0.36551186  0.00478997  0.0034388   0.00287116  0.00190244  0.00188892\n",
      "  0.00202822  0.002826  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18297fc048>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAELCAYAAAAiIMZEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKu4iIhn0/wE/ayjqefsPpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1829b1f518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    testing_inputs_1, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)\n",
    "    attention_vector = np.mean(get_activations(m,\n",
    "                                               testing_inputs_1,\n",
    "                                               print_shape_only=True,\n",
    "                                               layer_name='attention_vec')[0], axis=2).squeeze()\n",
    "    print('attention =', attention_vector)\n",
    "    assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
    "    attention_vectors.append(attention_vector)\n",
    "\n",
    "attention_vector_final = np.mean(np.array(attention_vectors), axis=0)\n",
    "# plot part.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(attention_vector_final, columns=['attention (%)']).plot(kind='bar',\n",
    "                                                                     title='Attention Mechanism as '\n",
    "                                                                               'a function of input'\n",
    "                                                                               ' dimensions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 20, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (hidden): Linear (1 -> 10)\n",
      "  (predict): Linear (10 -> 1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAD8CAYAAABO3GKQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFGhERCRS/x9ezxKTPKhBnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1829a2bb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "View more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "My Youtube Channel: https://www.youtube.com/user/MorvanZhou\n",
    "\n",
    "Dependencies:\n",
    "torch: 0.1.11\n",
    "matplotlib\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)\n",
    "y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n",
    "\n",
    "# torch can only train on Variable, so convert them to Variable\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "# plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n",
    "net = Net(n_feature=1, n_hidden=10, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(100):\n",
    "    prediction = net(x)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "\n",
    "    if t % 5 == 0:\n",
    "        # plot and show learning process\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data[0], fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
