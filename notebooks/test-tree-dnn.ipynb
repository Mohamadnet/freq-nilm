{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from dataloader import APPLIANCE_ORDER, get_train_test\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "cuda_av = False\n",
    "if torch.cuda.is_available():\n",
    "    cuda_av = True\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "weight_appliance = {'mw':1, 'dw':1, 'dr':1,'fridge':1, 'hvac':1}\n",
    "\n",
    "# num_hidden, num_iterations, num_layers, p, num_directions = sys.argv[1:6]\n",
    "\n",
    "\n",
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, cell_type, hidden_size, num_layers, bidirectional):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "      \n",
    "        self.lin_1 = nn.Linear(24, 50)\n",
    "        self.lin_2 = nn.Linear(50, 24)\n",
    "        #self.lin_3 = nn.Linear(48, 24)\n",
    "        \n",
    "        \n",
    "        self.act_1 = nn.ReLU()\n",
    "        #self.act_2 = nn.ReLU()\n",
    "        #self.act_3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        pred = self.lin_1(x)\n",
    "        pred = self.lin_2(pred)\n",
    "        pred = self.act_1(pred)\n",
    "        \n",
    "        #pred = torch.clamp(pred, min=0.)\n",
    "        #pred = self.act(pred)\n",
    "        #pred = torch.min(pred, x)\n",
    "        return pred\n",
    "\n",
    "\n",
    "class AppliancesRNN(nn.Module):\n",
    "    def __init__(self, cell_type, hidden_size, num_layers, bidirectional, num_appliance):\n",
    "        super(AppliancesRNN, self).__init__()\n",
    "        self.num_appliance = num_appliance\n",
    "        self.preds = {}\n",
    "        self.order = ORDER\n",
    "        for appliance in range(self.num_appliance):\n",
    "            if cuda_av:\n",
    "                setattr(self, \"Appliance_\" + str(appliance), CustomRNN(cell_type,\n",
    "                                                                       hidden_size,\n",
    "                                                                       num_layers,\n",
    "                                                                       bidirectional).cuda())\n",
    "            else:\n",
    "                setattr(self, \"Appliance_\" + str(appliance), CustomRNN(cell_type,\n",
    "                                                                       hidden_size,\n",
    "                                                                       num_layers,\n",
    "                                                                       bidirectional))\n",
    "\n",
    "    def forward(self, *args):\n",
    "        agg_current = args[0]\n",
    "        flag = False\n",
    "        if np.random.random() > args[1]:\n",
    "            flag = True\n",
    "        # print(\"Subtracting prediction\")\n",
    "        else:\n",
    "            pass\n",
    "        # print(\"Subtracting true\")\n",
    "        for appliance in range(self.num_appliance):\n",
    "            # print(agg_current.mean().data[0])\n",
    "            # print appliance\n",
    "            # print self.order[appliance]\n",
    "            # print args[2+appliance]\n",
    "            #print(getattr(self, \"Appliance_\" + str(appliance)))\n",
    "            self.preds[appliance] = getattr(self, \"Appliance_\" + str(appliance))(agg_current)\n",
    "            if flag:\n",
    "                agg_current = agg_current - self.preds[appliance]\n",
    "            else:\n",
    "                agg_current = agg_current - args[2 + appliance]\n",
    "\n",
    "        return torch.cat([self.preds[a] for a in range(self.num_appliance)])\n",
    "\n",
    "#ORDER = APPLIANCE_ORDER[1:][::-1]\n",
    "ORDER = APPLIANCE_ORDER[1:][:1][::-1]\n",
    "\n",
    "\n",
    "num_hidden = 120\n",
    "num_iterations = 300\n",
    "num_layers = 1\n",
    "num_directions = 1\n",
    "\n",
    "input_dim = 1\n",
    "hidden_size = num_hidden\n",
    "num_layers = num_layers\n",
    "if num_directions == 1:\n",
    "    bidirectional = False\n",
    "else:\n",
    "    bidirectional = True\n",
    "lr = 0.1\n",
    "p = 0.5\n",
    "num_folds = 5\n",
    "fold_num = 0\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train, test = get_train_test(num_folds=num_folds, fold_num=fold_num)\n",
    "train_aggregate = train[:, 0, :, :].reshape(-1, 24)\n",
    "test_aggregate = test[:, 0, :, :].reshape(-1, 24)\n",
    "\n",
    "out_train = [None for temp in range(len(ORDER))]\n",
    "for a_num, appliance in enumerate(ORDER):\n",
    "    out_train[a_num] = Variable(\n",
    "        torch.Tensor(train[:, APPLIANCE_ORDER.index(appliance), :, :].reshape((train_aggregate.shape[0], -1))))\n",
    "    if cuda_av:\n",
    "        out_train[a_num] = out_train[a_num].cuda()\n",
    "\n",
    "out_test = [None for temp in range(len(ORDER))]\n",
    "for a_num, appliance in enumerate(ORDER):\n",
    "    out_test[a_num] = Variable(\n",
    "        torch.Tensor(test[:, APPLIANCE_ORDER.index(appliance), :, :].reshape((test_aggregate.shape[0], -1))))\n",
    "    if cuda_av:\n",
    "        out_test[a_num] = out_test[a_num].cuda()\n",
    "\n",
    "loss_func = nn.L1Loss()\n",
    "a = AppliancesRNN(cell_type, hidden_size, num_layers, bidirectional, len(ORDER))\n",
    "#for param in a.parameters():\n",
    "#    param.data = param.data.abs()\n",
    "#print(a)\n",
    "if cuda_av:\n",
    "    a = a.cuda()\n",
    "    loss_func = loss_func.cuda()\n",
    "optimizer = torch.optim.Adam(a.parameters(), lr=lr)\n",
    "\n",
    "inp = Variable(torch.Tensor(train_aggregate.reshape((train_aggregate.shape[0], -1, 1))).type(torch.FloatTensor),\n",
    "               requires_grad=True)\n",
    "for t in range(num_iterations):\n",
    "    inp = Variable(torch.Tensor(train_aggregate), requires_grad=True)\n",
    "    out = torch.cat([out_train[appliance_num] for appliance_num, appliance in enumerate(ORDER)])\n",
    "    ot =  torch.cat([out_test[appliance_num] for appliance_num, appliance in enumerate(ORDER)])\n",
    "    if cuda_av:\n",
    "        inp = inp.cuda()\n",
    "        out = out.cuda()\n",
    "        ot = ot.cuda()\n",
    "\n",
    "    params = [inp, p]\n",
    "    for a_num, appliance in enumerate(ORDER):\n",
    "        params.append(out_train[a_num])\n",
    "    # print(params)\n",
    "    pred = a(*params)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    pred_split = torch.split(pred, pred.size(0)//len(ORDER))\n",
    "\n",
    "    losses= [loss_func(pred_split[appliance_num], out_train[appliance_num])*weight_appliance[appliance] for appliance_num, appliance in enumerate(ORDER)]\n",
    "    \n",
    "    loss = sum(losses)\n",
    "    if t % 1 == 0:\n",
    "        print(t, loss.data[0])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "test_inp = Variable(torch.Tensor(test_aggregate), requires_grad=False)\n",
    "if cuda_av:\n",
    "    test_inp = test_inp.cuda()\n",
    "\n",
    "params = [test_inp, -2]\n",
    "for i in range(len(ORDER)):\n",
    "    params.append(None)\n",
    "pr = a(*params)\n",
    "pr = torch.clamp(pr, min=0.)\n",
    "test_pred = torch.split(pr, test_aggregate.shape[0])\n",
    "prediction_fold = [None for x in range(len(ORDER))]\n",
    "\n",
    "if cuda_av:\n",
    "    for appliance_num, appliance in enumerate(ORDER):\n",
    "        prediction_fold[appliance_num] = test_pred[appliance_num].cpu().data.numpy().reshape(-1, 24)\n",
    "else:\n",
    "    for appliance_num, appliance in enumerate(ORDER):\n",
    "        prediction_fold[appliance_num] = test_pred[appliance_num].data.numpy().reshape(-1, 24)\n",
    "gt_fold = [None for x in range(len(ORDER))]\n",
    "for appliance_num, appliance in enumerate(ORDER):\n",
    "    gt_fold[appliance_num] = test[:, APPLIANCE_ORDER.index(appliance), :, :].reshape(test_aggregate.shape[0], -1,\n",
    "                                                                                         1).reshape(-1, 24)\n",
    "\n",
    "\n",
    "print([x.mean() for x in pred_split])\n",
    "error = pd.Series({appliance:mean_absolute_error(gt_fold[appliance_num], prediction_fold[appliance_num]) for appliance_num, appliance in enumerate(ORDER)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
