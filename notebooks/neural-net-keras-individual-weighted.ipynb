{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from common import APPLIANCES_ORDER\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = np.load('../1H-input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subset_dataset(tensor):\n",
    "    t_subset = tensor[:, :, 180:194, :]\n",
    "    all_indices = np.array(list(range(320)))\n",
    "    for i in range(1, 7):\n",
    "        valid_homes = pd.DataFrame(t_subset[:, i, :].reshape(320, 14*24)).dropna().index\n",
    "        all_indices = np.intersect1d(all_indices, valid_homes)\n",
    "    t_subset = t_subset[all_indices, :, :, :].reshape(52, 7, 14*24)\n",
    "    \n",
    "    # Create artificial aggregate\n",
    "    t_subset[:, 0, :] = 0.0\n",
    "    for i in range(1, 7):\n",
    "        t_subset[:, 0, :] = t_subset[:, 0, :] + t_subset[:, i, :]\n",
    "    # t_subset is of shape (#home, appliance, days*hours)\n",
    "    return t_subset, all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all, valid_homes = create_subset_dataset(tensor)\n",
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 7, 336)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_objective(y_pred, y_true):\n",
    "    with tf.name_scope(None):\n",
    "        return tf.losses.absolute_difference(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/nipun/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "import keras\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "n_movies = 3\n",
    "n_users=3\n",
    "n_latent_factors=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aggregate', 'hvac', 'fridge', 'mw', 'dw', 'wm', 'oven']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLIANCES_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg = t_all[:30, 0, :].reshape(30*14, 24)\n",
    "train_appliance = {}\n",
    "test_appliance = {}\n",
    "for appliance_num, appliance in enumerate(APPLIANCES_ORDER[1:]):\n",
    "    train_appliance[appliance] = t_all[:30, appliance_num+1, :].reshape(30*14, 24)\n",
    "    test_appliance[appliance] = t_all[30:, appliance_num+1, :].reshape(22*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_hvac = t_all[30:, 1, :].reshape(22*14, 24)\n",
    "test_fridge = t_all[30:, 2, :].reshape(22*14, 24)\n",
    "\n",
    "test_mw = t_all[30:, 3, :].reshape(22*14, 24)\n",
    "\n",
    "\n",
    "\n",
    "test_agg = t_all[30:, 0, :].reshape(22*14, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       50.549999\n",
       "1      124.449997\n",
       "2       39.433334\n",
       "3       44.033333\n",
       "4       44.133335\n",
       "5       73.349998\n",
       "6       39.283333\n",
       "7       45.566666\n",
       "8       75.349998\n",
       "9       46.416668\n",
       "10     257.750000\n",
       "11      39.066666\n",
       "12      45.400002\n",
       "13      39.116665\n",
       "14     113.183334\n",
       "15     161.733337\n",
       "16     168.366669\n",
       "17     136.566666\n",
       "18     173.383331\n",
       "19     153.716660\n",
       "20     167.566666\n",
       "21     147.283340\n",
       "22     173.050003\n",
       "23     209.383331\n",
       "24     225.699997\n",
       "25     154.416672\n",
       "26      57.016666\n",
       "27      96.733330\n",
       "28       6.800000\n",
       "29      94.349998\n",
       "          ...    \n",
       "390      3.016667\n",
       "391      3.000000\n",
       "392     49.833332\n",
       "393     35.466667\n",
       "394     35.533333\n",
       "395     49.900002\n",
       "396     41.916668\n",
       "397     18.783333\n",
       "398     35.766666\n",
       "399     25.283333\n",
       "400     61.750000\n",
       "401     18.583334\n",
       "402      4.000000\n",
       "403     65.583336\n",
       "404     19.000000\n",
       "405     38.916668\n",
       "406    115.150002\n",
       "407     95.216667\n",
       "408     83.566666\n",
       "409     81.300003\n",
       "410     84.183334\n",
       "411     83.400002\n",
       "412     52.766666\n",
       "413    130.383331\n",
       "414    138.983337\n",
       "415    114.766670\n",
       "416     95.316666\n",
       "417      4.000000\n",
       "418     61.500000\n",
       "419    135.533340\n",
       "Length: 420, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_appliance[appliance].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_appliance[appliance].max(axis=1)\n",
    "weights = np.ones(train_appliance[appliance].shape[0])\n",
    "\n",
    "weights[x>50]=1.\n",
    "weights[x<50]=0.1\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw\n",
      "********************\n",
      "Train on 378 samples, validate on 42 samples\n",
      "Epoch 1/250\n",
      "378/378 [==============================] - 1s 2ms/step - loss: 174.9087 - val_loss: 217.3571\n",
      "Epoch 2/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 158.9539 - val_loss: 191.2717\n",
      "Epoch 3/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 145.3181 - val_loss: 174.2093\n",
      "Epoch 4/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 136.5517 - val_loss: 167.9083\n",
      "Epoch 5/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 128.7447 - val_loss: 162.2084\n",
      "Epoch 6/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 126.5620 - val_loss: 155.8831\n",
      "Epoch 7/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 123.0938 - val_loss: 150.4940\n",
      "Epoch 8/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 125.0515 - val_loss: 147.2604\n",
      "Epoch 9/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 119.0525 - val_loss: 146.7474\n",
      "Epoch 10/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 116.5268 - val_loss: 146.8045\n",
      "Epoch 11/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 116.9393 - val_loss: 146.1342\n",
      "Epoch 12/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 116.1446 - val_loss: 147.2259\n",
      "Epoch 13/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 113.6925 - val_loss: 147.7680\n",
      "Epoch 14/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 111.2851 - val_loss: 146.5680\n",
      "Epoch 15/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 110.1532 - val_loss: 145.2555\n",
      "Epoch 16/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 113.1394 - val_loss: 144.8716\n",
      "Epoch 17/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 108.2165 - val_loss: 144.7047\n",
      "Epoch 18/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 107.3722 - val_loss: 143.6744\n",
      "Epoch 19/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 108.5614 - val_loss: 142.3654\n",
      "Epoch 20/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 108.9907 - val_loss: 141.8505\n",
      "Epoch 21/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 107.4401 - val_loss: 141.3656\n",
      "Epoch 22/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 108.7729 - val_loss: 141.7677\n",
      "Epoch 23/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 104.9054 - val_loss: 141.7238\n",
      "Epoch 24/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 105.2461 - val_loss: 141.3395\n",
      "Epoch 25/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 106.3247 - val_loss: 141.2944\n",
      "Epoch 26/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 107.0310 - val_loss: 142.5148\n",
      "Epoch 27/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 103.3470 - val_loss: 142.4867\n",
      "Epoch 28/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 106.5474 - val_loss: 141.5218\n",
      "Epoch 29/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 105.5039 - val_loss: 141.7133\n",
      "Epoch 30/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 104.2042 - val_loss: 141.3083\n",
      "Epoch 31/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 102.2268 - val_loss: 141.1352\n",
      "Epoch 32/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 103.0835 - val_loss: 141.9211\n",
      "Epoch 33/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 101.0796 - val_loss: 140.5775\n",
      "Epoch 34/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 101.2778 - val_loss: 140.5191\n",
      "Epoch 35/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 100.7725 - val_loss: 139.4616\n",
      "Epoch 36/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 102.0690 - val_loss: 138.7977\n",
      "Epoch 37/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 99.2366 - val_loss: 139.7992\n",
      "Epoch 38/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 98.8733 - val_loss: 139.8458\n",
      "Epoch 39/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 101.6500 - val_loss: 140.7209\n",
      "Epoch 40/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 99.8889 - val_loss: 139.0007\n",
      "Epoch 41/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 99.0339 - val_loss: 137.9118\n",
      "Epoch 42/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 96.5361 - val_loss: 138.2948\n",
      "Epoch 43/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 98.5796 - val_loss: 137.2073\n",
      "Epoch 44/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 98.5744 - val_loss: 139.0816\n",
      "Epoch 45/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 96.2942 - val_loss: 138.6577\n",
      "Epoch 46/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 94.3834 - val_loss: 137.6841\n",
      "Epoch 47/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 96.5408 - val_loss: 138.8505\n",
      "Epoch 48/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 97.7929 - val_loss: 137.6173\n",
      "Epoch 49/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 97.6293 - val_loss: 138.4768\n",
      "Epoch 50/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 94.2656 - val_loss: 137.6622\n",
      "Epoch 51/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 94.6149 - val_loss: 137.2928\n",
      "Epoch 52/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 96.1419 - val_loss: 137.6158\n",
      "Epoch 53/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 94.2224 - val_loss: 137.1584\n",
      "Epoch 54/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 93.8324 - val_loss: 136.4306\n",
      "Epoch 55/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 97.2082 - val_loss: 138.7139\n",
      "Epoch 56/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 92.8925 - val_loss: 137.4461\n",
      "Epoch 57/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 94.4412 - val_loss: 136.1130\n",
      "Epoch 58/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 93.9796 - val_loss: 136.8013\n",
      "Epoch 59/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 90.2018 - val_loss: 137.5976\n",
      "Epoch 60/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 93.8245 - val_loss: 136.9915\n",
      "Epoch 61/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 91.8385 - val_loss: 136.0359\n",
      "Epoch 62/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 89.3127 - val_loss: 137.0059\n",
      "Epoch 63/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 95.0481 - val_loss: 139.0335\n",
      "Epoch 64/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 89.2559 - val_loss: 137.0167\n",
      "Epoch 65/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 93.3777 - val_loss: 135.7926\n",
      "Epoch 66/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 93.1498 - val_loss: 135.2659\n",
      "Epoch 67/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 90.7638 - val_loss: 138.0568\n",
      "Epoch 68/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 90.1824 - val_loss: 137.5485\n",
      "Epoch 69/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 92.2799 - val_loss: 137.5554\n",
      "Epoch 70/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 89.4251 - val_loss: 137.7262\n",
      "Epoch 71/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 94.0247 - val_loss: 136.7137\n",
      "Epoch 72/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 64.30 - 0s 72us/step - loss: 89.0163 - val_loss: 137.4949\n",
      "Epoch 73/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 88.0188 - val_loss: 137.3294\n",
      "Epoch 74/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 90.8924 - val_loss: 138.1215\n",
      "Epoch 75/250\n",
      "378/378 [==============================] - 0s 80us/step - loss: 89.0904 - val_loss: 137.9577\n",
      "Epoch 76/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 85.1346 - val_loss: 138.4649\n",
      "Epoch 77/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 63us/step - loss: 89.3435 - val_loss: 137.4956\n",
      "Epoch 78/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 86.1383 - val_loss: 136.7612\n",
      "Epoch 79/250\n",
      "378/378 [==============================] - ETA: 0s - loss: 116.117 - 0s 63us/step - loss: 88.6243 - val_loss: 138.2083\n",
      "Epoch 80/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 86.6984 - val_loss: 139.3518\n",
      "Epoch 81/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 89.4775 - val_loss: 137.3178\n",
      "Epoch 82/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 85.1031 - val_loss: 137.9883\n",
      "Epoch 83/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 86.0724 - val_loss: 138.9626\n",
      "Epoch 84/250\n",
      "378/378 [==============================] - 0s 48us/step - loss: 89.7320 - val_loss: 138.0934\n",
      "Epoch 85/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 89.9681 - val_loss: 136.7540\n",
      "Epoch 86/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 87.4612 - val_loss: 139.5037\n",
      "Epoch 87/250\n",
      "378/378 [==============================] - 0s 51us/step - loss: 91.0250 - val_loss: 138.9104\n",
      "Epoch 88/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 86.3870 - val_loss: 139.0341\n",
      "Epoch 89/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 86.9641 - val_loss: 138.4955\n",
      "Epoch 90/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 86.6591 - val_loss: 138.2644\n",
      "Epoch 91/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 84.9446 - val_loss: 139.2370\n",
      "Epoch 92/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 85.1249 - val_loss: 139.9336\n",
      "Epoch 93/250\n",
      "378/378 [==============================] - 0s 48us/step - loss: 87.8504 - val_loss: 136.7266\n",
      "Epoch 94/250\n",
      "378/378 [==============================] - 0s 49us/step - loss: 85.7943 - val_loss: 137.3107\n",
      "Epoch 95/250\n",
      "378/378 [==============================] - 0s 48us/step - loss: 85.9597 - val_loss: 138.1988\n",
      "Epoch 96/250\n",
      "378/378 [==============================] - 0s 50us/step - loss: 84.9505 - val_loss: 137.5645\n",
      "Epoch 97/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 86.6101 - val_loss: 138.0741\n",
      "Epoch 98/250\n",
      "378/378 [==============================] - 0s 49us/step - loss: 89.2532 - val_loss: 139.6075\n",
      "Epoch 99/250\n",
      "378/378 [==============================] - 0s 50us/step - loss: 86.0083 - val_loss: 136.6416\n",
      "Epoch 100/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 88.5271 - val_loss: 137.0342\n",
      "Epoch 101/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 85.6844 - val_loss: 137.2315\n",
      "Epoch 102/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 84.9400 - val_loss: 138.7596\n",
      "Epoch 103/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 81.9992 - val_loss: 138.7602\n",
      "Epoch 104/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 84.8113 - val_loss: 139.1577\n",
      "Epoch 105/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 87.1422 - val_loss: 136.8504\n",
      "Epoch 106/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 85.1650 - val_loss: 136.5255\n",
      "Epoch 107/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 86.2779 - val_loss: 136.8482\n",
      "Epoch 108/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 84.8215 - val_loss: 136.4091\n",
      "Epoch 109/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 85.5311 - val_loss: 137.3224\n",
      "Epoch 110/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 83.0083 - val_loss: 136.1663\n",
      "Epoch 111/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 82.4321 - val_loss: 136.9350\n",
      "Epoch 112/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 83.7377 - val_loss: 137.5505\n",
      "Epoch 113/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 84.6146 - val_loss: 137.0155\n",
      "Epoch 114/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 83.1750 - val_loss: 137.3530\n",
      "Epoch 115/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 83.7970 - val_loss: 138.2950\n",
      "Epoch 116/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 82.6620 - val_loss: 137.5379\n",
      "Epoch 117/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 82.8875 - val_loss: 137.0264\n",
      "Epoch 118/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 81.0897 - val_loss: 137.9487\n",
      "Epoch 119/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 80.9352 - val_loss: 138.8323\n",
      "Epoch 120/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 82.7829 - val_loss: 137.4463\n",
      "Epoch 121/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 83.8500 - val_loss: 137.8794\n",
      "Epoch 122/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 82.3356 - val_loss: 138.8261\n",
      "Epoch 123/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 82.0738 - val_loss: 137.6581\n",
      "Epoch 124/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 83.0675 - val_loss: 137.5234\n",
      "Epoch 125/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 81.8302 - val_loss: 137.4959\n",
      "Epoch 126/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 79.3272 - val_loss: 138.5537\n",
      "Epoch 127/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 82.0103 - val_loss: 137.5901\n",
      "Epoch 128/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 83.8982 - val_loss: 136.8406\n",
      "Epoch 129/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 83.4414 - val_loss: 136.4357\n",
      "Epoch 130/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 85.3903 - val_loss: 137.1153\n",
      "Epoch 131/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 81.7704 - val_loss: 137.4722\n",
      "Epoch 132/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 79.6406 - val_loss: 138.4801\n",
      "Epoch 133/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 81.6968 - val_loss: 136.4693\n",
      "Epoch 134/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 82.6524 - val_loss: 138.5112\n",
      "Epoch 135/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 83.8323 - val_loss: 139.0058\n",
      "Epoch 136/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 80.9717 - val_loss: 137.7427\n",
      "Epoch 137/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 79.7732 - val_loss: 137.3335\n",
      "Epoch 138/250\n",
      "378/378 [==============================] - 0s 75us/step - loss: 78.1039 - val_loss: 137.5994\n",
      "Epoch 139/250\n",
      "378/378 [==============================] - 0s 70us/step - loss: 80.6226 - val_loss: 136.8718\n",
      "Epoch 140/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 80.4606 - val_loss: 137.0342\n",
      "Epoch 141/250\n",
      "378/378 [==============================] - 0s 73us/step - loss: 82.2660 - val_loss: 138.2339\n",
      "Epoch 142/250\n",
      "378/378 [==============================] - 0s 72us/step - loss: 87.8197 - val_loss: 136.9339\n",
      "Epoch 143/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 79.4953 - val_loss: 138.0179\n",
      "Epoch 144/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 80.0175 - val_loss: 137.5647\n",
      "Epoch 145/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 82.2975 - val_loss: 136.8397\n",
      "Epoch 146/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 80.7093 - val_loss: 137.7323\n",
      "Epoch 147/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 81.1615 - val_loss: 137.2922\n",
      "Epoch 148/250\n",
      "378/378 [==============================] - 0s 66us/step - loss: 80.2787 - val_loss: 137.6438\n",
      "Epoch 149/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 79.3339 - val_loss: 138.1470\n",
      "Epoch 150/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 83.8536 - val_loss: 137.2127\n",
      "Epoch 151/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 80.7380 - val_loss: 137.1204\n",
      "Epoch 152/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 80.8548 - val_loss: 138.2948\n",
      "Epoch 153/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 66us/step - loss: 83.0710 - val_loss: 137.3402\n",
      "Epoch 154/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 78.9107 - val_loss: 136.9286\n",
      "Epoch 155/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 78.1320 - val_loss: 137.2320\n",
      "Epoch 156/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 80.5391 - val_loss: 136.7792\n",
      "Epoch 157/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 77.7740 - val_loss: 136.7405\n",
      "Epoch 158/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 78.8954 - val_loss: 136.1043\n",
      "Epoch 159/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 80.9471 - val_loss: 137.0389\n",
      "Epoch 160/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 76.3978 - val_loss: 136.5105\n",
      "Epoch 161/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 80.2107 - val_loss: 136.1129\n",
      "Epoch 162/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 81.2027 - val_loss: 137.8772\n",
      "Epoch 163/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 80.2100 - val_loss: 137.1017\n",
      "Epoch 164/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 82.5938 - val_loss: 136.2505\n",
      "Epoch 165/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 79.2931 - val_loss: 136.9499\n",
      "Epoch 166/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 76.8823 - val_loss: 137.1793\n",
      "Epoch 167/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 77.4346 - val_loss: 137.5961\n",
      "Epoch 168/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 78.7526 - val_loss: 137.6341\n",
      "Epoch 169/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 79.7370 - val_loss: 137.5097\n",
      "Epoch 170/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 81.6978 - val_loss: 136.6178\n",
      "Epoch 171/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 72.6139 - val_loss: 136.9351\n",
      "Epoch 172/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 76.5115 - val_loss: 136.6243\n",
      "Epoch 173/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 74.6233 - val_loss: 136.5570\n",
      "Epoch 174/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 77.4568 - val_loss: 137.5235\n",
      "Epoch 175/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 78.6389 - val_loss: 137.6904\n",
      "Epoch 176/250\n",
      "378/378 [==============================] - 0s 50us/step - loss: 77.3976 - val_loss: 136.9430\n",
      "Epoch 177/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 75.6686 - val_loss: 136.9776\n",
      "Epoch 178/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 76.1835 - val_loss: 137.1702\n",
      "Epoch 179/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 75.4202 - val_loss: 137.7898\n",
      "Epoch 180/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 75.6821 - val_loss: 137.0767\n",
      "Epoch 181/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 73.2888 - val_loss: 137.5444\n",
      "Epoch 182/250\n",
      "378/378 [==============================] - 0s 71us/step - loss: 79.4115 - val_loss: 137.5873\n",
      "Epoch 183/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 78.1005 - val_loss: 136.7782\n",
      "Epoch 184/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 77.8892 - val_loss: 137.0878\n",
      "Epoch 185/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 76.5971 - val_loss: 136.8942\n",
      "Epoch 186/250\n",
      "378/378 [==============================] - 0s 68us/step - loss: 73.6052 - val_loss: 135.6216\n",
      "Epoch 187/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 74.4758 - val_loss: 136.8850\n",
      "Epoch 188/250\n",
      "378/378 [==============================] - 0s 76us/step - loss: 75.2896 - val_loss: 137.4979\n",
      "Epoch 189/250\n",
      "378/378 [==============================] - 0s 67us/step - loss: 77.2581 - val_loss: 137.2451\n",
      "Epoch 190/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 74.5023 - val_loss: 136.8841\n",
      "Epoch 191/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 76.6854 - val_loss: 136.4061\n",
      "Epoch 192/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 76.7395 - val_loss: 137.0647\n",
      "Epoch 193/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 72.1032 - val_loss: 137.5567\n",
      "Epoch 194/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 74.3083 - val_loss: 137.3909\n",
      "Epoch 195/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 74.0288 - val_loss: 136.8968\n",
      "Epoch 196/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 74.2974 - val_loss: 137.1329\n",
      "Epoch 197/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 75.4718 - val_loss: 137.4685\n",
      "Epoch 198/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 72.7553 - val_loss: 136.6411\n",
      "Epoch 199/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 76.7911 - val_loss: 137.5657\n",
      "Epoch 200/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 73.5359 - val_loss: 137.0241\n",
      "Epoch 201/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 75.9019 - val_loss: 136.2398\n",
      "Epoch 202/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 72.3524 - val_loss: 137.2716\n",
      "Epoch 203/250\n",
      "378/378 [==============================] - 0s 55us/step - loss: 71.1953 - val_loss: 135.4341\n",
      "Epoch 204/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 74.8779 - val_loss: 134.7703\n",
      "Epoch 205/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 76.0594 - val_loss: 136.9296\n",
      "Epoch 206/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 71.5222 - val_loss: 135.3899\n",
      "Epoch 207/250\n",
      "378/378 [==============================] - 0s 64us/step - loss: 75.3513 - val_loss: 135.8859\n",
      "Epoch 208/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 72.6023 - val_loss: 136.4217\n",
      "Epoch 209/250\n",
      "378/378 [==============================] - 0s 74us/step - loss: 75.9685 - val_loss: 136.2832\n",
      "Epoch 210/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 74.3747 - val_loss: 134.1105\n",
      "Epoch 211/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 73.2181 - val_loss: 134.7958\n",
      "Epoch 212/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 73.0005 - val_loss: 134.7567\n",
      "Epoch 213/250\n",
      "378/378 [==============================] - 0s 62us/step - loss: 75.2252 - val_loss: 135.4028\n",
      "Epoch 214/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 73.6191 - val_loss: 133.6045\n",
      "Epoch 215/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 76.1979 - val_loss: 135.5153\n",
      "Epoch 216/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 71.4563 - val_loss: 135.9557\n",
      "Epoch 217/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 74.7850 - val_loss: 135.5490\n",
      "Epoch 218/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 73.0579 - val_loss: 134.6273\n",
      "Epoch 219/250\n",
      "378/378 [==============================] - 0s 50us/step - loss: 71.7658 - val_loss: 134.8169\n",
      "Epoch 220/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 73.4335 - val_loss: 137.1408\n",
      "Epoch 221/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 70.5134 - val_loss: 136.9203\n",
      "Epoch 222/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 75.2873 - val_loss: 134.3823\n",
      "Epoch 223/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 70.1788 - val_loss: 134.6952\n",
      "Epoch 224/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 75.8755 - val_loss: 136.9112\n",
      "Epoch 225/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 73.4460 - val_loss: 134.7417\n",
      "Epoch 226/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 75.0643 - val_loss: 134.6346\n",
      "Epoch 227/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 72.4261 - val_loss: 135.2901\n",
      "Epoch 228/250\n",
      "378/378 [==============================] - 0s 69us/step - loss: 71.0559 - val_loss: 134.0924\n",
      "Epoch 229/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 56us/step - loss: 71.3293 - val_loss: 135.0508\n",
      "Epoch 230/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 73.7765 - val_loss: 135.5634\n",
      "Epoch 231/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 71.0954 - val_loss: 134.0683\n",
      "Epoch 232/250\n",
      "378/378 [==============================] - 0s 52us/step - loss: 75.3892 - val_loss: 134.8660\n",
      "Epoch 233/250\n",
      "378/378 [==============================] - 0s 51us/step - loss: 73.3955 - val_loss: 135.4790\n",
      "Epoch 234/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 71.0189 - val_loss: 136.5997\n",
      "Epoch 235/250\n",
      "378/378 [==============================] - 0s 56us/step - loss: 73.3433 - val_loss: 134.8119\n",
      "Epoch 236/250\n",
      "378/378 [==============================] - 0s 65us/step - loss: 70.8742 - val_loss: 134.6960\n",
      "Epoch 237/250\n",
      "378/378 [==============================] - 0s 60us/step - loss: 73.1608 - val_loss: 134.4555\n",
      "Epoch 238/250\n",
      "378/378 [==============================] - 0s 63us/step - loss: 74.1671 - val_loss: 133.2489\n",
      "Epoch 239/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 72.8716 - val_loss: 135.5590\n",
      "Epoch 240/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 74.0855 - val_loss: 136.2114\n",
      "Epoch 241/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 70.6847 - val_loss: 132.4898\n",
      "Epoch 242/250\n",
      "378/378 [==============================] - 0s 58us/step - loss: 72.5730 - val_loss: 133.7879\n",
      "Epoch 243/250\n",
      "378/378 [==============================] - 0s 57us/step - loss: 71.0531 - val_loss: 135.0729\n",
      "Epoch 244/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 71.1732 - val_loss: 133.6659\n",
      "Epoch 245/250\n",
      "378/378 [==============================] - 0s 61us/step - loss: 68.9034 - val_loss: 133.5875\n",
      "Epoch 246/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 68.4854 - val_loss: 134.8763\n",
      "Epoch 247/250\n",
      "378/378 [==============================] - 0s 53us/step - loss: 74.6454 - val_loss: 134.8585\n",
      "Epoch 248/250\n",
      "378/378 [==============================] - 0s 54us/step - loss: 74.9252 - val_loss: 134.0244\n",
      "Epoch 249/250\n",
      "378/378 [==============================] - 0s 50us/step - loss: 72.5224 - val_loss: 135.2810\n",
      "Epoch 250/250\n",
      "378/378 [==============================] - 0s 59us/step - loss: 71.3560 - val_loss: 134.2857\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Reshape\n",
    "\n",
    "pred_appliance = {}\n",
    "num_iterations_dictionary = {'hvac':1200,'fridge':500,'mw':250,'dw':250,'oven':250, 'wm':300}\n",
    "for appliance in APPLIANCES_ORDER[4:5]:\n",
    "    print(appliance)\n",
    "    print(\"*\"*20)\n",
    "    np.random.seed(0)\n",
    "    from keras.layers.merge import Subtract, Minimum\n",
    "    from keras import regularizers\n",
    "    agg_input = keras.layers.Input(shape=[24],name='Aggregate')\n",
    "    appliance_dense_1 = keras.layers.Dense(units=20,name='Appliance-layer-1',activation='relu')(agg_input)\n",
    "    #appliance_bn = keras.layers.BatchNormalization()(appliance_dense_1)\n",
    "    dropout = keras.layers.Dropout(rate=0.1,name='Droput-Appliance')(appliance_dense_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    out = keras.layers.Dense(units=24,name='Appliance-output',activation='relu')(dropout)\n",
    "    out = Minimum(name='Clip-to-agg')([out, agg_input])\n",
    "    out = Reshape(target_shape=(24,1))(out)\n",
    "\n",
    "\n",
    "    model = keras.Model(agg_input, out)\n",
    "    model.compile('adam','mean_absolute_error',sample_weight_mode=\"temporal\")\n",
    "    x = train_appliance[appliance]\n",
    "    weights = np.ones_like(x)\n",
    "\n",
    "    weights[x>100]=10.\n",
    "    weights[x<100]=0.3\n",
    "    model.fit(train_agg, train_appliance[appliance].reshape(-1,24,1), epochs=num_iterations_dictionary[appliance],\n",
    "              validation_split=0.1, sample_weight=weights)\n",
    "    pred_appliance[appliance] = model.predict(test_agg).reshape(-1,24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('per-appliance.pdf','wb') as f:\n",
    "    f.write(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='pdf'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Aggregate (InputLayer)          (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Appliance-layer-1 (Dense)       (None, 20)           500         Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Droput-Appliance (Dropout)      (None, 20)           0           Appliance-layer-1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Appliance-output (Dense)        (None, 24)           504         Droput-Appliance[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Clip-to-agg (Minimum)           (None, 24)           0           Appliance-output[0][0]           \n",
      "                                                                 Aggregate[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 24, 1)        0           Clip-to-agg[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,004\n",
      "Trainable params: 1,004\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hvac'\n",
      "'fridge'\n",
      "'mw'\n",
      "'wm'\n",
      "'oven'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = {}\n",
    "for appliance in APPLIANCES_ORDER[1:]:\n",
    "    try:\n",
    "        mae[appliance] = mean_absolute_error(test_appliance[appliance], pred_appliance[appliance])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dw    48.21602\n",
       "dtype: float64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dw    14.499389\n",
       "dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 24, 1)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-0c3c11c20fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_hvac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_fridge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_hvac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_agg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m    169\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 170\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    171\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n\u001b[1;32m    172\u001b[0m                                weights=sample_weight, axis=0)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \"\"\"\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 451\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "pred_hvac = model.predict(test_agg)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(pred_hvac, test_fridge))\n",
    "print(mean_absolute_error(pred_hvac, test_agg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-325ec0730a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_hvac\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_agg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 306\u001b[0;31m                                          copy=copy)\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m   5588\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5589\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5590\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Must pass 2-d input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5592\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input"
     ]
    }
   ],
   "source": [
    "(pd.DataFrame(pred_hvac) - pd.DataFrame(test_agg)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a32870be0>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a32870a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(test_mw[5, :]).plot(label='GT')\n",
    "#pd.Series(test_agg[1, :]).plot(label='GT')\n",
    "\n",
    "\n",
    "pd.Series(pred_appliance['dw'][5]).plot(label='Pred')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    308.000000\n",
       "mean      33.767479\n",
       "std       50.790043\n",
       "min        0.000000\n",
       "25%        2.650855\n",
       "50%       13.080427\n",
       "75%       39.675549\n",
       "max      386.673706\n",
       "dtype: float64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred_appliance['dw'].max(axis=1)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
